# Criando prompts avan√ßados

[![Creating Advanced Prompts](../../images/05-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://learn.microsoft.com/_themes/docs.theme/master/en-us/_themes/global/video-embed.html?id=f3615e46-dbca-477c-832d-21eef95cd9f3?WT.mc_id=academic-105485-koreyst)

Vamos recapitular algumas aprendizagens do cap√≠tulo anterior:

> A _engenharia_ de prompts √© o processo pelo qual **guiamos o modelo em dire√ß√£o a respostas mais relevantes** fornecendo instru√ß√µes ou contexto mais √∫teis.

Tamb√©m existem duas etapas para escrever prompts: a constru√ß√£o do prompt, fornecendo contexto relevante, e a segunda parte √© a _otimiza√ß√£o_, como melhorar gradualmente o prompt.

Neste ponto, temos uma compreens√£o b√°sica de como escrever prompts, mas precisamos aprofundar. Neste cap√≠tulo, voc√™ passar√° de experimentar v√°rios prompts para entender por que um prompt √© melhor que outro.

## Introdu√ß√£o

Neste cap√≠tulo, abordaremos os seguintes t√≥picos:

- Amplie seu conhecimento de engenharia de prompts aplicando diferentes t√©cnicas aos seus prompts.
- Configure seus prompts para variar a sa√≠da.

## Metas de aprendizado

Ap√≥s concluir esta li√ß√£o, voc√™ ser√° capaz de:

- Aplicar t√©cnicas de engenharia de prompts que melhorem o resultado dos seus prompts.
- Realizar prompts que sejam variados ou determin√≠sticos.

## Engenharia de prompts

A engenharia de prompts √© o processo de criar prompts que produzir√£o o resultado desejado. H√° mais na engenharia de prompts do que simplesmente escrever um prompt de texto. A engenharia de prompts n√£o √© uma disciplina de engenharia; √© mais um conjunto de t√©cnicas que voc√™ pode aplicar para obter o resultado desejado.

### Um exemplo de prompt

Vamos pegar um prompt b√°sico como este:

> Generate 10 questions on geography.

Neste prompt, voc√™ est√° aplicando na verdade um conjunto de diferentes t√©cnicas de prompt.

Vamos analisar isso.

- **Contexto**, voc√™ especifica que deve ser sobre "geografia".
- **Limita√ß√£o da sa√≠da**, voc√™ n√£o quer mais do que 10 perguntas.

### Limita√ß√µes de prompts simples

Voc√™ pode ou n√£o obter o resultado desejado. Voc√™ ter√° suas perguntas geradas, mas a geografia √© um t√≥pico amplo e voc√™ pode n√£o obter o que deseja devido √†s seguintes raz√µes:

- **T√≥pico amplo**, voc√™ n√£o sabe se ser√° sobre pa√≠ses, capitais, rios e assim por diante.
- **Formato**, e se voc√™ quisesse que as perguntas fossem formatadas de uma maneira espec√≠fica?

Como voc√™ pode ver, h√° muito a considerar ao criar prompts.

At√© agora, vimos um exemplo simples de prompt, mas a IA generativa √© capaz de muito mais para ajudar pessoas em v√°rias fun√ß√µes e ind√∫strias. Vamos explorar algumas t√©cnicas b√°sicas a seguir.

### T√©cnicas de prompting

Primeiro, precisamos entender que o prompting √© uma propriedade _emergente_ de um LLM, o que significa que n√£o √© uma caracter√≠stica incorporada ao modelo, mas sim algo que descobrimos conforme usamos o modelo.

Existem algumas t√©cnicas b√°sicas que podemos usar para fazer um prompt em um LLM. Vamos explor√°-las.

- **Prompt de poucas tentativas**, esta √© a forma mais b√°sica de prompt. √â um prompt √∫nico com alguns exemplos.
- **Corrente de pensamento**, este tipo de prompt diz ao LLM como dividir um problema em etapas.
- **Conhecimento gerado**, para melhorar a resposta de um prompt, voc√™ pode fornecer fatos ou conhecimentos gerados adicionalmente ao seu prompt.
- **Do menos para o mais**, assim como a corrente de pensamento, esta t√©cnica consiste em dividir um problema em uma s√©rie de etapas e depois pedir que essas etapas sejam realizadas em ordem.
- **Auto-aperfei√ßoamento**, esta t√©cnica consiste em criticar a sa√≠da do LLM e depois pedir que ele melhore.
- **Prompt maieutico**. O que voc√™ deseja aqui √© garantir que a resposta do LLM esteja correta e pedir que ele explique v√°rias partes da resposta. Isso √© uma forma de auto-aperfei√ßoamento.

### Prompt de Poucas Tentativas

Este estilo de prompt √© muito simples, pode consistir em um √∫nico prompt e possivelmente alguns exemplos. Essa t√©cnica √© provavelmente o que voc√™ est√° usando ao come√ßar a aprender sobre LLMs. Aqui est√° um exemplo:

- Prompt: "O que √© √Ålgebra?"
- Resposta: "√Ålgebra √© uma ramifica√ß√£o da matem√°tica que estuda s√≠mbolos matem√°ticos e as regras para manipular esses s√≠mbolos."

### Prompt - Corrente de Pensamento (chain-of-thought prompting)

O prompt corrente de pensamento √© uma t√©cnica muito interessante, pois envolve levar o LLM por uma s√©rie de etapas. A ideia √© instruir o LLM de tal forma que ele entenda como fazer algo. Considere o seguinte exemplo, com e sem corrente de pensamento:

    - Prompt: "Alice tem 5 ma√ß√£s, joga 3 ma√ß√£s, d√° 2 para Bob e Bob devolve uma, quantas ma√ß√£s Alice tem?"
    - Resposta: 5

O LLM responde com 5, o que est√° incorreto. A resposta correta √© 1 ma√ß√£, dada a seguinte opera√ß√£o (5 -3 -2 + 1 = 1).

Ent√£o, como podemos ensinar o LLM a fazer isso corretamente?

Vamos tentar a corrente de pensamento. Aplicar a corrente de pensamento significa:

1. Dar ao LLM um exemplo semelhante.
1. Mostrar o c√°lculo e como calcul√°-lo corretamente.
1. Fornecer o prompt original.

Veja como:

- Prompt: "Lisa tem 7 ma√ß√£s, joga 1 ma√ß√£, d√° 4 ma√ß√£s para Bart e Bart devolve uma:
  7 -1 = 6
  6 -4 = 2
  2 +1 = 3  
  Alice tem 5 ma√ß√£s, joga 3 ma√ß√£s, d√° 2 para Bob e Bob devolve uma, quantas ma√ß√£s Alice tem?"
  Resposta: 1

Note como escrevemos prompts substancialmente mais longos com outro exemplo, um c√°lculo e, em seguida, o prompt original, e chegamos √† resposta correta, que √© 1.

Como voc√™ pode ver, a corrente de pensamento √© uma t√©cnica muito poderosa.

### Conhecimento Gerado

Muitas vezes, ao criar uma solicita√ß√£o, voc√™ deseja faz√™-lo usando os dados de sua pr√≥pria empresa. Voc√™ quer que parte da solicita√ß√£o seja da empresa e a outra parte seja a solicita√ß√£o real que voc√™ est√° interessado.

Como exemplo, √© assim que sua solicita√ß√£o pode parecer se voc√™ estiver no ramo de seguros:

    ```text
    {{company}}: {{company_name}}
    {{products}}:
    {{products_list}}
    Please suggest an insurance given the following budget and requirements:
    Budget: {{budget}}
    Requirements: {{requirements}}
    ```

Acima, voc√™ v√™ como a solicita√ß√£o √© constru√≠da usando um modelo. No modelo, h√° v√°rias vari√°veis, indicadas por `{{vari√°vel}}`, que ser√£o substitu√≠das por valores reais de uma API da empresa.

Aqui est√° um exemplo de como a solicita√ß√£o pode parecer depois que as vari√°veis foram substitu√≠das pelo conte√∫do de sua empresa:

    ```text
    Insurance company: ACME Insurance
    Insurance products (cost per month):
    - Car, cheap, 500 USD
    - Car, expensive, 1100 USD
    - Home, cheap, 600 USD
    - Home, expensive, 1200 USD
    - Life, cheap, 100 USD

    Please suggest an insurance given the following budget and requirements:
    Budget: $1000
    Requirements: Car, Home
    ```

Executando esta solicita√ß√£o em um Modelo de Linguagem com Aprendizado Profundo (LLM), o resultado ser√° algo como:

    ```output
    , and Life insurance

    Given the budget and requirements, we suggest the following insurance package from ACME Insurance:
    - Car, cheap, 500 USD
    - Home, cheap, 600 USD
    - Life, cheap, 100 USD
    Total cost: $1,200 USD
    ```

Como voc√™ pode ver, tamb√©m sugere o seguro de vida, o que n√£o deveria acontecer. Este resultado √© uma indica√ß√£o de que precisamos otimizar a solicita√ß√£o tornando-a mais clara sobre o que √© permitido. Ap√≥s alguma _tentativa e erro_, chegamos √† seguinte solicita√ß√£o:

    ```text
    Insurance company: ACME Insurance
    Insurance products (cost per month):
    - type: Car, cheap, cost: 500 USD
    - type: Car, expensive, cost: 1100 USD
    - type: Home, cheap, cost: 600 USD
    - type: Home, expensive, cost: 1200 USD
    - type: Life, cheap, cost: 100 USD

    Please suggest an insurance given the following budget and requirements:
    Budget: $1000 restrict choice to types: Car, Home
    ```

Observe como adicionar _tipo_ e _custo_ e tamb√©m usar a palavra-chave _restringir_ ajuda o LLM a entender o que queremos.

Agora obtemos a seguinte resposta:

    ```output
    Given the budget and requirements, we suggest the Car, Cheap insurance product which costs 500 USD per month.
    ```

O objetivo deste exemplo foi mostrar que, mesmo que estejamos usando uma t√©cnica b√°sica como _conhecimento gerado_, ainda precisamos otimizar o prompt na maioria dos casos para obter o resultado desejado.

### Least-to-most

The idea with Least-to-most prompting is to break down a bigger problem into subproblems. That way, you help guide the LLM on how to "conquer" the bigger problem. A good example could be for data science where you can ask the LLM to divide up a problem like so:

> Prompt: How to perform data science in 5 steps?

With your AI assistant answering with:

1. Collect data
1. Clean data
1. Analyze data
1. Plot data
1. Present data

### Self-refine, critique the results

With generative AIs and LLMs, you can't trust the output. You need to verify it. After all, the LLM is just presenting you what's the next most likely thing to say, not what's correct. Therefore, a good idea is to ask the LLM to critique itself, which leads us to the self-refine technique.

How it works is that you follow the following steps:

1. Initial prompt asking the LLM to solve a problem
1. LLM answers
1. You critique the answer and ask the AI to improve
1. LLM answers again, this time considering the critique and suggest solutions it came up with

You can repeat this process as many times as you want.

Here's an example using this technique:

### Menos para mais

A ideia com a t√©cnica do prompt "Menos para mais" √© decompor um problema maior em subproblemas. Dessa forma, voc√™ ajuda a orientar o LLM sobre como "conquistar" o problema maior. Um bom exemplo pode ser em ci√™ncia de dados, onde voc√™ pode pedir ao LLM para dividir um problema da seguinte maneira:

> Prompt: How to perform data science in 5 steps?

Com seu assistente de IA respondendo:

1. Coletar dados
1. Limpar dados
1. Analisar dados
1. Plotar dados
1. Apresentar dados

### Autoaperfei√ßoamento, cr√≠tica dos resultados

Com AIs generativas e LLMs, voc√™ n√£o pode confiar na sa√≠da. √â preciso verific√°-la. Afinal, o LLM est√° apenas apresentando o que √© mais prov√°vel dizer a seguir, n√£o o que est√° correto. Portanto, uma boa ideia √© pedir ao LLM que fa√ßa uma autocr√≠tica, o que nos leva √† t√©cnica de autoaperfei√ßoamento.

Como funciona √© como voc√™ segue os seguintes passos:

1. Prompt inicial pedindo ao LLM para resolver um problema
1. LLM responde
1. Voc√™ critica a resposta e pede √† IA para melhorar
1. LLM responde novamente, desta vez considerando a cr√≠tica e sugerindo solu√ß√µes que elaborou

Voc√™ pode repetir esse processo quantas vezes quiser.

Aqui est√° um exemplo usando essa t√©cnica:

> Prompt: "Create a Python Web API with routes products and customers"

Resposta da IA:

    ```python
    import flask
    app = Flask(__name__)

    @app.route('/products')
    def products():
        return 'Products'

    @app.route('/customers')
    def customers():
        return 'Customers'
    ```

> Prompt: suggest 3 improvements of the above code

Resposta da IA:

    ```text
    Sure, here are three suggestions to improve the code:

    Import the correct module: The first line should import Flask instead of flask. The correct import statement should be from flask import Flask.

    Add a main function: Add a if __name__ == '__main__': block to the end of the file to run the application when the file is executed.

    Use f-strings: Use f-strings to format the return statements in the route functions. For example, return 'Products' can be changed to return f'This is the {request.path} page'.
    ```

    ```python
    from flask import Flask, request

    app = Flask(__name__)

    @app.route('/products')
    def products():
        return f'This is the {request.path} page'

    @app.route('/customers')
    def customers():
        return f'This is the {request.path} page'

    if __name__ == '__main__':
        app.run()
    ```

Como voc√™ pode ver, a resposta da IA acima est√° aprimorando o primeiro c√≥digo sugerido, gra√ßas √† cr√≠tica da primeira resposta.

### Maieutic prompting

Maieutic prompting is a technique that is similar to self-refine but it's more about asking the LLM to explain itself. The goal is to reduce inconsistencies in the LLM's output so to ensure it arrives at the correct answer. The workflow to follow is:

1. Ask the LLM to answer a question
1. For each part of the answer, ask the LLM to explain it more in depth.
1. If there are inconsistencies, discard the parts that are inconsistent.

Repeat 2 and 3 until you've gone through all the parts and you're satisfied with the answer.

Here's an example prompt:

### Prompt de Mai√™utico

O prompt de mai√™utico √© uma t√©cnica semelhante ao autoaperfei√ßoamento, mas trata-se mais de pedir ao LLM para se explicar. O objetivo √© reduzir inconsist√™ncias na sa√≠da do LLM para garantir que ele chegue √† resposta correta. O fluxo de trabalho a ser seguido √©:

1. Pe√ßa ao LLM para responder a uma pergunta.
1. Para cada parte da resposta, pe√ßa ao LLM para explic√°-la mais detalhadamente.
1. Se houver inconsist√™ncias, descarte as partes inconsistentes.

Repita os passos 2 e 3 at√© passar por todas as partes e ficar satisfeito com a resposta.

Aqui est√° um exemplo de um prompt:

> prompt: How can I create a crisis plan to mitigate a pandemic in 5 steps?
> LLM answer:

    ```text
    1. Identify the risks
    1. Identify the stakeholders
    1. Identify the resources
    1. Identify the actions
    1. Identify the communication channels
    ```

S√£o identificadas 5 etapas, mas podemos determinar se isso est√° correto? Vamos pedir ao LLM para explicar cada etapa:

> prompt: Explain the first step in more detail, what are the risks in detail with a pandemic?

    ```text
    The risks are the following:

    - Loss of life
    - Loss of business
    - Loss of reputation
    - Loss of customers
    - Loss of employees
    ```

Neste ponto, queremos ter certeza de que o LLM est√° correto, por isso pedimos que explique o primeiro risco com mais detalhes e esperamos que seja consistente com a resposta acima:

> prompt: In a Pandemic, which is the biggest risk and why?

    ```text
    The biggest risk is loss of life because it's the most important thing to protect.
    ```

> What are the two biggest risks in a pandemic?

    ```text
    The two biggest risks are loss of life and loss of business.
    ```

Neste ponto, o LLM √© consistente e menciona "vida" e "neg√≥cios" como os dois principais riscos. Podemos agora prosseguir para o pr√≥ximo passo e sentir confian√ßa razo√°vel. No entanto, n√£o devemos confiar cegamente no LLM; sempre devemos verificar a sa√≠da.

## Varie sua sa√≠da

Os LLMs s√£o, por natureza, n√£o-determin√≠sticos, o que significa que voc√™ obter√° resultados diferentes cada vez que executar o mesmo prompt. Tente o seguinte prompt, por exemplo:

> "Generate code for a Python Web API"

    ```text
    # Import necessary modules
    import flask
    from flask import request, jsonify

    # Create a Flask app
    app = flask.Flask(__name__)

    # Create a route for the API
    @app.route('/api', methods=['GET'])
    def api():
        # Get the data from the request
        data = request.args

        # Process the data
        result = process_data(data)

        # Return the result as JSON
        return jsonify(result)

    # Function to process the data
    def process_data(data):
        # Do something with the data
        result = data + 1

        # Return the result
        return result

    # Run the app
    if __name__ == '__main__':
        app.run()
    ```

Execute o mesmo prompt novamente e voc√™ observar√° que vai gerar uma resposta ligeiramente diferente:

    ```text
    #import necessary packages
    import flask
    from flask import request, jsonify

    #create the Flask application
    app = flask.Flask(__name__)
    app.config["DEBUG"] = True

    #create a list of books
    books = [
        {'id': 0,
         'title': 'A Fire Upon the Deep',
         'author': 'Vernor Vinge',
         'first_sentence': 'The coldsleep itself was dreamless.',
         'year_published': '1992'},
        {'id': 1,
         'title': 'The Ones Who Walk Away From Omelas',
         'author': 'Ursula K. Le Guin',
         'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.',
         'published': '1973'},
        {'id': 2,
         'title': 'Dhalgren',
         'author': 'Samuel R. Delany',
         'first_sentence': 'to wound the autumnal city.',
         'published': '1975'}
    ]

    #create an endpoint for the API
    @app.route('/', methods=['GET'])
    def home():
        return '''<h1>Books API</h1>
    <p>A prototype API for retrieving books.</p>'''

    #create an endpoint to return all books
    @app.route('/api/v1/resources/books/all', methods=['GET'])
    def api_all():
        return jsonify(books)

    #create an endpoint to return a single book
    @app.route('/api/v1/resources/books', methods=['GET'])
    def api_id():
        #check if an ID was provided as part of the URL
        #if ID is provided, assign it to a variable
        #if no ID is provided, display an error
        if 'id' in request.args:
            id = int(request.args['id'])
        else:
            return "Error: No id field provided. Please specify an id."

        #create an empty list for our results
        results = []

        #loop through the data and match results that fit the requested ID
        #IDs are unique, but other fields might return many results
        for book in books:
            if book['id'] == id:
                results.append(book)

        #use the jsonify function from Flask to convert our list of
        #Python dictionaries to the JSON format
        return jsonify(results)

    app.run()
    ```

> Ent√£o, a sa√≠da diferente √© um problema?

Depende do que voc√™ est√° tentando fazer. Se voc√™ deseja uma resposta espec√≠fica, √© um problema. Se voc√™ est√° bem com uma sa√≠da variada como "Gerar 3 perguntas sobre geografia", ent√£o n√£o √© um problema.

### Usando a temperatura para variar sua sa√≠da

Ok! Ent√£o, decidimos que queremos limitar a sa√≠da para ser mais previs√≠vel, ou seja, mais determin√≠stica. Como fazemos isso?

Temperatura √© um valor entre 0 e 1, onde 0 √© o mais determin√≠stico e 1 √© o mais variado. O valor padr√£o √© 0.7. Vamos ver o que acontece com duas execu√ß√µes do mesmo prompt com temperatura definida como 0.1:

> "Generate code for a Python Web API"

    ```python
    #import necessary libraries
    import flask
    from flask import request, jsonify

    #create an instance of the Flask class
    app = flask.Flask(__name__)

    #create an endpoint for the API
    @app.route('/api/v1/endpoint', methods=['GET'])
    def api_endpoint():
        #get the request data
        data = request.get_json()

        #process the data
        result = process_data(data)

        #return the result
        return jsonify(result)

    #function to process the data
    def process_data(data):
        #process the data
        result = {'result': 'success'}

        #return the result
        return result

    #run the Flask app
    if __name__ == '__main__':
        app.run()

    ```

Execute o prompt novamente e voc√™ ver√° este resultado:

    ```python
    #import necessary libraries
    import flask
    from flask import request, jsonify

    #create a Flask app
    app = flask.Flask(__name__)

    #create a route for the API
    @app.route('/api', methods=['GET'])
    def api_call():
        #get the data from the request
        data = request.get_json()

        #process the data
        result = process_data(data)

        #return the result
        return jsonify(result)

    #define the process_data function
    def process_data(data):
        #do something with the data
        result = data + 1

        #return the result
        return result

    #run the app
    if __name__ == '__main__':
        app.run()

    ```

H√° apenas uma pequena diferen√ßa entre essas duas sa√≠das. Vamos fazer o contr√°rio desta vez, vamos definir a temperatura para 0.9:

    ```python
    # Import necessary libraries
    import flask
    from flask import request, jsonify

    # Create a Flask app
    app = flask.Flask(__name__)

    # Create a route for the API
    @app.route('/api', methods=['GET'])
    def api_call():
        # Get the data from the request
        data = request.args

        # Process the data
        result = process_data(data)

        # Return the result
        return jsonify(result)

    # Function to process the data
    def process_data(data):
        # Do something with the data
        result = data + 1

        # Return the result
        return result

    # Run the app
    if __name__ == '__main__':
        app.run()

    ```

e a segunda tentativa em 0,9 como valor de temperatura:

    ```python
    import flask
    from flask import request, jsonify

    # create the Flask app
    app = flask.Flask(__name__)
    app.config['DEBUG'] = True

    # create some test data
    books = [
        {'id': 0, 'title': 'A Fire Upon The Deep', 'author': 'Vernor Vinge', 'first_sentence': 'The coldsleep itself was dreamless.', 'year_published': '1992'},
        {'id': 1, 'title': 'The Ones Who Walk Away From Omelas', 'author': 'Ursula K. Le Guin', 'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.', 'published': '1973'},
        {'id': 2, 'title': 'Dhalgren', 'author': 'Samuel R. Delany', 'first_sentence': 'to wound the autumnal city.', 'published': '1975'}
    ]

    # create an endpoint
    @app.route('/', methods=['GET'])
    def home():
        return '''<h1>Welcome to our book API!</h1>'''

    @app.route('/api/v1/resources/books

    ```

Como voc√™ pode ver, os resultados n√£o poderiam ser mais variados.

> Observe que, existem mais par√¢metros que voc√™ pode alterar para variar a sa√≠da, como top-k, top-p, penalidade de repeti√ß√£o, penalidade de comprimento e penalidade de diversidade, mas estes est√£o fora do escopo deste curr√≠culo.

## Boas pr√°ticas

H√° muitas pr√°ticas que voc√™ pode aplicar para tentar obter o que deseja. Voc√™ encontrar√° seu pr√≥prio estilo √† medida que usar mais e mais prompts.

Al√©m das t√©cnicas que cobrimos, existem algumas boas pr√°ticas a serem consideradas ao solicitar um LLM.

Aqui est√£o algumas boas pr√°ticas a serem consideradas:

- **Especifique o contexto**: O contexto √© importante. Quanto mais voc√™ puder especificar, como dom√≠nio, t√≥pico, etc., ser√° muito melhor.
- **Limite a sa√≠da**: Se voc√™ deseja um n√∫mero espec√≠fico de itens ou um comprimento espec√≠fico, especifique.
- **Especifique tanto o que, quanto e como**: Lembre-se de mencionar tanto o que voc√™ deseja quanto como deseja. Por exemplo: _Crie uma API da Web em Python com rotas para produtos e clientes, dividindo-a em 3 arquivos_.
- **Use modelos**: Muitas vezes, voc√™ desejar√° enriquecer suas solicita√ß√µes com dados da sua empresa. Use modelos para fazer isso. Os modelos podem ter vari√°veis que voc√™ substitui por dados reais.
- **Escreva corretamente**: Os LLMs podem fornecer uma resposta correta. Mas, se voc√™ escrever corretamente, obter√° uma resposta melhor.

## Tarefa

Aqui est√° o c√≥digo em Python que mostra como criar uma API simples usando o Flask:

    ```python
    from flask import Flask, request

    app = Flask(__name__)

    @app.route('/')
    def hello():
        name = request.args.get('name', 'World')
        return f'Hello, {name}!'

    if __name__ == '__main__':
        app.run()
    ```

Use um assistente de IA como o GitHub Copilot ou o ChatGPT e aplique a t√©cnica de "auto-aperfei√ßoamento" para melhorar o c√≥digo.

## Solu√ß√£o

Por favor, tente resolver a tarefa adicionando prompts adequados ao c√≥digo.

> [!DICA]
> Formule um prompt para pedir que ele melhor. √â uma boa ideia limitar quantas melhorias devem ter. Voc√™ tamb√©m pode pedir para melhorar de uma certa maneira, por exemplo, arquitetura, desempenho, seguran√ßa, etc.

[Solution](../../python/aoai-solution.py?WT.mc_id=academic-105485-koreyst)

## Verifica√ß√£o de conhecimento

Por que eu usaria a t√©cnica de prompt de corrente de pensamento? Mostre-me 1 resposta correta e 2 respostas incorretas.

Para ensinar o LLM a resolver um problema.
B, Para ensinar o LLM a encontrar erros no c√≥digo.
C, Para instruir o LLM a propor diferentes solu√ß√µes.

Resposta correta: A, 1, porque o prompt de corrente de pensamento trata de mostrar ao LLM como resolver um problema, fornecendo uma s√©rie de passos e problemas semelhantes e como foram resolvidos.

## üöÄ Desafio

Voc√™ acabou de usar a t√©cnica de auto-aperfei√ßoamento na atribui√ß√£o. Pegue qualquer programa que voc√™ criou e considere quais melhorias voc√™ gostaria de aplicar a ele. Agora, use a t√©cnica de auto-aperfei√ßoamento para aplicar as altera√ß√µes propostas. O que voc√™ achou do resultado, melhor ou pior?

## √ìtimo trabalho! Continue seu aprendizado

Ap√≥s concluir esta li√ß√£o, confira nossa [cole√ß√£o de aprendizado de IA generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para continuar a aprimorar seus conhecimentos sobre IA generativa!

Agora, vamos para a Li√ß√£o 6, onde aplicaremos nosso conhecimento de Engenharia de Prompt, [construindo aplicativos de gera√ß√£o de texto](../../../06-text-generation-apps/translations/pt-br/README.md?WT.mc_id=academic-105485-koreyst)
