# Usando a IA Generativa de Forma Respons√°vel

[![Usando a IA Generativa de Forma Respons√°vel](../../images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)]()

> **V√≠deo em Breve**

√â f√°cil se encantar com a IA e a IA generativa em particular. Mas √© preciso considerar como voc√™ a usar√° de forma respons√°vel. Voc√™ precisa considerar coisas como como garantir que a sa√≠da seja justa, n√£o prejudicial e muito mais. Este cap√≠tulo tem como objetivo fornecer o contexto mencionado, o que considerar e como tomar medidas ativas para melhorar o uso de sua IA.

## Introdu√ß√£o

Esta li√ß√£o abordar√°:

- Por que voc√™ deve priorizar a IA Respons√°vel ao criar aplica√ß√µes de IA Generativa.
- Princ√≠pios fundamentais da IA Respons√°vel e como eles se relacionam com a IA Generativa.
- Como colocar esses princ√≠pios de IA Respons√°vel em pr√°tica por meio de estrat√©gias e ferramentas.

## Metas de Aprendizado

Ap√≥s completar esta li√ß√£o, voc√™ saber√°:

- A import√¢ncia da IA Respons√°vel ao criar aplica√ß√µes de IA Generativa.
- Quando pensar e aplicar os princ√≠pios fundamentais da IA Respons√°vel ao criar aplica√ß√µes de IA Generativa.
- Quais ferramentas e estrat√©gias est√£o dispon√≠veis para voc√™ colocar o conceito de IA Respons√°vel em pr√°tica.

## Princ√≠pios de IA Respons√°vel

A empolga√ß√£o em torno da IA Generativa nunca foi t√£o alta. Essa empolga√ß√£o trouxe muitos novas pessoas desenvolvedoras, aten√ß√£o e financiamento para esse espa√ßo. Embora isso seja muito positivo para quem deseja construir produtos e empresas usando a IA Generativa, tamb√©m √© importante que procedamos de forma respons√°vel.

Ao longo deste curso, estamos nos concentrando em criar nossa startup e nosso produto de educa√ß√£o em IA. Usaremos os princ√≠pios da IA Respons√°vel: Justi√ßa, Inclus√£o, Confiabilidade/Seguran√ßa, Seguran√ßa e Privacidade, Transpar√™ncia e Responsabilidade. Com esses princ√≠pios, exploraremos como eles se relacionam com o nosso uso da IA Generativa em nossos produtos.

## Por Que Voc√™ Deve Priorizar a IA Respons√°vel

Ao criar um produto, adotar uma abordagem centrada no ser humano, mantendo o melhor interesse de seus usu√°rios em mente, sempre leva aos melhores resultados.

A singularidade da IA Generativa est√° em seu poder de criar respostas √∫teis, informa√ß√µes, orienta√ß√µes e conte√∫do para os usu√°rios. Isso pode ser feito sem muitas etapas manuais, o que pode levar a resultados muito impressionantes. Sem um planejamento adequado e estrat√©gias, tamb√©m pode infelizmente levar a alguns resultados prejudiciais para seus usu√°rios, seu produto e a sociedade como um todo.

Vamos dar uma olhada em alguns (mas n√£o todos) desses resultados potencialmente prejudiciais:

### Alucina√ß√µes

Alucina√ß√µes √© um termo usado para descrever quando um LLM produz conte√∫do que √© completamente sem sentido ou algo que sabemos ser factualmente incorreto com base em outras fontes de informa√ß√£o.

Vamos dar, por exemplo, a cria√ß√£o de um recurso para nossa startup que permite aos estudantes fazer perguntas hist√≥ricas a um modelo. Um estudante faz a pergunta `Quem foi o √∫nico sobrevivente do Titanic?`

O modelo produz uma resposta como a que est√° abaixo:

![Prompt dizendo "Quem foi o √∫nico sobrevivente do Titanic"](<../../../03-using-generative-ai-responsibly/images/2135-ChatGPT(1)_11zon.webp?WT.mc_id=academic-105485-koreyst>)

> _(Fonte: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

Esta √© uma resposta muito confiante e completa. Infelizmente, est√° incorreta. Mesmo com uma quantidade m√≠nima de pesquisa, algu√©m descobriria que houve mais de um sobrevivente do Titanic. Para um estudante que est√° come√ßando a pesquisar esse t√≥pico, essa resposta pode ser persuasiva o suficiente para n√£o ser questionada e tratada como fato. As consequ√™ncias disso podem levar ao sistema de IA sendo pouco confi√°vel e impactar negativamente a reputa√ß√£o de nossa startup.

Com cada itera√ß√£o de qualquer LLM dado, vimos melhorias de desempenho na minimiza√ß√£o de alucina√ß√µes. Mesmo com essa melhoria, n√≥s, como construtores e usu√°rios de aplicativos, ainda precisamos estar cientes dessas limita√ß√µes.

### Conte√∫do Prejudicial

N√≥s cobrimos na se√ß√£o anterior quando um LLM produz respostas incorretas ou sem sentido. Outro risco que precisamos estar cientes √© quando um modelo responde com conte√∫do prejudicial.

Conte√∫do prejudicial pode ser definido como:

- Fornecer instru√ß√µes ou incentivar autoles√£o ou dano a certos grupos.
- Conte√∫do odioso ou humilhante.
- Orientar o planejamento de qualquer tipo de ataque ou atos violentos.
- Fornecer instru√ß√µes sobre como encontrar conte√∫do ilegal ou cometer atos ilegais.
- Exibir conte√∫do sexualmente expl√≠cito.

Para nossa startup, queremos garantir que tenhamos as ferramentas e estrat√©gias certas em vigor para impedir que esse tipo de conte√∫do seja visto pelos estudantes.

### Falta de Equidade

A equidade √© definida como "garantir que um sistema de IA esteja livre de vi√©s e discrimina√ß√£o e que trate todos de forma justa e igual". No mundo da IA Generativa, queremos garantir que vis√µes de mundo excludentes de grupos marginalizados n√£o sejam refor√ßadas pela sa√≠da do modelo.

Esses tipos de sa√≠das n√£o s√£o apenas destrutivos para a cria√ß√£o de experi√™ncias de produtos positivas para nossos usu√°rios. Mas tamb√©m causam mais danos √† sociedade. Como criadores de aplica√ß√µes, devemos sempre ter em mente uma base de usu√°rios ampla e diversificada ao criar solu√ß√µes com IA Generativa.

## Como Usar a IA Generativa de Forma Respons√°vel

Agora que identificamos a import√¢ncia da IA Generativa Respons√°vel, vamos ver 4 etapas que podemos seguir para construir nossas solu√ß√µes de IA de forma respons√°vel:

![Ciclo de Mitiga√ß√£o](../../images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### Medir Danos Potenciais

Na √°rea de testes de software, testamos as a√ß√µes esperadas de um usu√°rio em um aplicativo. Da mesma forma, testar um conjunto diversificado de prompts que os usu√°rios provavelmente usar√£o √© uma boa maneira de medir o dano potencial.

Como nossa startup est√° criando um produto de educa√ß√£o, seria bom preparar uma lista de prompts relacionados √† educa√ß√£o. Isso poderia ser para cobrir um certo assunto, fatos hist√≥ricos e prompts sobre a vida dos estudantes.

### Mitigar Danos Potenciais

Agora √© hora de encontrar maneiras de prevenir ou limitar o dano potencial causado pelo modelo e suas respostas. Podemos analisar isso em 4 camadas diferentes:

![Camadas de Mitiga√ß√£o](../../images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **Modelo**: escolher o modelo certo para o caso de uso certo. Modelos maiores e mais complexos, como o GPT-4, podem causar mais risco de conte√∫do prejudicial quando aplicados a casos de uso menores e mais espec√≠ficos. Usar seus dados de treinamento para ajuste fino tamb√©m reduz o risco de conte√∫do prejudicial.

- **Sistema de Seguran√ßa**: um sistema de seguran√ßa √© um conjunto de ferramentas e configura√ß√µes na plataforma que serve o modelo e ajuda a mitigar o dano. Um exemplo disso √© o sistema de filtragem de conte√∫do no servi√ßo Azure OpenAI. Os sistemas tamb√©m devem detectar ataques de jailbreak e atividades indesejadas, como solicita√ß√µes de bots.

- **Metaprompt**: metaprompts e fundamenta√ß√£o s√£o maneiras de direcionar ou limitar o modelo com base em determinados comportamentos e informa√ß√µes. Isso poderia ser o uso de entradas do sistema para definir certos limites do modelo. Al√©m disso, fornecer sa√≠das mais relevantes para o escopo ou dom√≠nio do sistema.

Tamb√©m pode ser o uso de t√©cnicas como a Recupera√ß√£o de Gera√ß√£o Aumentada (RAG) para fazer com que o modelo obtenha informa√ß√µes apenas de uma sele√ß√£o de fontes confi√°veis. H√° uma li√ß√£o posterior neste curso para [criar aplica√ß√µes de busca](../../../08-building-search-applications/translations/pt-br/README.md?WT.mc_id=academic-105485-koreyst)

- **Experi√™ncia do Usu√°rio**: a camada final √© onde o usu√°rio interage diretamente com o modelo por meio da interface de nosso aplicativo de alguma forma. Dessa forma, podemos projetar a UI/UX para limitar o usu√°rio quanto aos tipos de entradas que podem enviar ao modelo, bem como ao texto ou imagens exibidos ao usu√°rio. Ao implantar o aplicativo de IA, tamb√©m devemos ser transparentes sobre o que nossa aplica√ß√£o de IA Generativa pode e n√£o pode fazer.

Temos uma li√ß√£o inteira dedicada a [Projetar UX para Aplica√ß√µes de IA](../../../12-designing-ux-for-ai-applications/translations/pt-br/README.md?WT.mc_id=academic-105485-koreyst)

- **Avaliar o modelo**: trabalhar com LLMs pode ser desafiador porque nem sempre temos controle sobre os dados em que o modelo foi treinado. Independentemente disso, sempre devemos avaliar o desempenho e as sa√≠das do modelo. Ainda √© importante medir a precis√£o, similaridade, fundamenta√ß√£o e relev√¢ncia do modelo de sa√≠da. Isso ajuda a fornecer transpar√™ncia e confian√ßa aos interessados e usu√°rios.

### Operar uma Solu√ß√£o de IA Generativa Respons√°vel

Criar uma pr√°tica operacional em torno de suas aplica√ß√µes de IA √© a etapa final. Isso inclui a parceria com outras partes de nossa startup, como Jur√≠dico e Seguran√ßa, para garantir que estejamos em conformidade com todas as pol√≠ticas regulat√≥rias. Antes do lan√ßamento, tamb√©m queremos criar planos em torno da entrega, tratamento de incidentes e retorno para evitar qualquer dano crescente aos nossos usu√°rios.

## Ferramentas

Embora o trabalho de desenvolver solu√ß√µes de IA Respons√°vel possa parecer muito, por√©m √© um trabalho que vale a pena. √Ä medida que a √°rea de IA Generativa cresce, mais ferramentas para ajudar os desenvolvedores a integrar eficientemente a responsabilidade em seus fluxos de trabalho amadurecer√£o. Por exemplo, o [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) pode ajudar a detectar conte√∫do e imagens prejudiciais por meio de uma solicita√ß√£o de API.

## Verifica√ß√£o de Conhecimento

Quais s√£o algumas coisas das quais voc√™ precisa se preocupar para garantir o uso respons√°vel da IA?

1. Que a resposta esteja correta.
2. Uso prejudicial, para que a IA n√£o seja usada para fins criminosos.
3. Garantir que a IA esteja livre de vi√©s e discrimina√ß√£o.

R: 2 e 3 est√£o corretas. A IA Respons√°vel ajuda a considerar como mitigar efeitos prejudiciais e vieses, entre outros.

## üöÄ Desafio

Leia sobre o [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) e veja o que voc√™ pode adotar para o seu uso.

## √ìtimo Trabalho, Continue Sua Aprendizagem

Quer aprender mais sobre como construir com IA Generativa de forma respons√°vel? Acesse a [p√°gina de aprendizado cont√≠nuo](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para encontrar outros √≥timos recursos sobre esse t√≥pico.

Vamos agora para a Li√ß√£o 4, onde exploraremos os [Fundamentos da Engenharia de Prompt](../../../04-prompt-engineering-fundamentals/translations/pt-br/README.md?WT.mc_id=academic-105485-koreyst)!
