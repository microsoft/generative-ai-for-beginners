# Usando IA Generativa de Forma Responsable

[![Usando IA Generativa de Forma Responsable](../../images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)]() 

> **Video en Breve**

Es f√°cil sentirse fascinado por la IA y la IA generativa en particular, pero es necesario considerar c√≥mo usarla de manera responsable. Debes considerar aspectos como c√≥mo garantizar que la salida es justa, no da√±ina y m√°s. Este cap√≠tulo tiene como objetivo proporcionarte el contexto mencionado, qu√© considerar y c√≥mo tomar medidas activas para mejorar tu uso de la IA.

## Introducci√≥n

Esta lecci√≥n cubrir√°:

- Por qu√© deber√≠as priorizar la IA Responsable al crear aplicaciones de IA Generativa.
- Principios b√°sicos de la IA Responsable y c√≥mo se relacionan con la IA Generativa.
- C√≥mo poner estos principios de IA Responsable en pr√°ctica a trav√©s de estrategias y herramientas.

## Metas de Aprendizaje

Despu√©s de completar esta lecci√≥n sabr√°s: 

- La importancia de la IA Responsable al crear aplicaciones de IA Generativa.
- Cu√°ndo pensar y aplicar los principios b√°sicos de la IA Responsable al crear aplicaciones de IA Generativa.
- Qu√© herramientas y estrategias tienes a tu disposici√≥n para poner en pr√°ctica el concepto de IA Responsable.

## Principios de IA Responsable

El entusiasmo por la IA Generativa nunca ha sido tan grande. Este entusiasmo ha atra√≠do a muchos nuevos desarrolladores, atenci√≥n y financiaci√≥n a este espacio. Si bien esto es muy positivo para cualquiera que busque crear productos y empresas que hacen uso de la IA Generativa, tambi√©n es importante que procedamos de manera responsable.

A lo largo de este curso, nos centraremos en desarrollar nuestra startup y nuestro producto educativo de IA. Utilizaremos los principios de la IA Responsable: Equidad, Inclusi√≥n, Confiabilidad/Seguridad, Seguridad y Privacidad, Transparencia y Responsabilidad. Con estos principios, exploraremos c√≥mo se relacionan con nuestro uso de IA Generativa en nuestros productos.

## ¬øPor qu√© Deber√≠as Priorizar la IA Responsable?

Al crear un producto, adoptar un enfoque centrado en el ser humano teniendo en mente los mejores intereses del usuario conduce a los mejores resultados.

La singularidad de la IA Generativa es su poder para crear respuestas, informaci√≥n, orientaci√≥n y contenido √∫tiles para los usuarios. Esto se puede hacer sin muchos pasos manuales que pueden conducir a resultados muy impresionantes. Sin una planificaci√≥n y estrategias apropiadas, lamentablemente tambi√©n puede producir resultados da√±inos para tus usuarios, tu producto y la sociedad en su conjunto.

Veamos algunos (pero no todos) de estos resultados potencialmente da√±inos:

### Alucinaciones

Las alucinaciones son un t√©rmino utilizado para describir cuando un LLM produce contenido que es completamente absurdo o algo que sabemos que es incorrecto seg√∫n otras fuentes de informaci√≥n.

Tomemos, por ejemplo, que creamos una funci√≥n para nuestra startup que permite a los estudiantes hacer preguntas hist√≥ricas a un modelo. Un estudiante hace la pregunta `¬øQui√©n fue el √∫nico superviviente del Titanic?`

El modelo produce una respuesta como la siguiente:

![Prompt que dice "¬øQui√©n fue el √∫nico superviviente del Titanic?"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> *(Fuente: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))*

Esta es una respuesta muy segura y completa. Lamentablemente, es incorrecta. Incluso con una m√≠nima cantidad de investigaci√≥n, uno descubrir√≠a que hubo m√°s de un superviviente del desastre del Titanic. Para un estudiante que reci√©n comienza a investigar este tema, esta respuesta puede ser lo suficientemente persuasiva como para no ser cuestionada y tratada como un hecho. Las consecuencias de esto pueden llevar a que el sistema de IA no sea confiable y afecte negativamente a la reputaci√≥n de nuestra startup.

Con cada iteraci√≥n de cualquier LLM determinado, hemos visto mejoras de rendimiento en torno a minimizar las alucinaciones. Incluso con esta mejora, nosotros, como creadores de aplicaciones y usuarios, a√∫n necesitamos ser conscientes de estas limitaciones.

### Contenido Da√±ino

En la secci√≥n anterior cubrimos cu√°ndo un LLM produce respuestas incorrectas o absurdas. Otro riesgo que debemos tener en cuenta es cuando un modelo responde con contenido da√±ino.

El contenido da√±ino puede ser definido como:

- Proporcionar instrucciones o fomentar la autolesi√≥n o el da√±o a ciertos grupos.
- Contenido odioso o denigrante.
- Orientar la planificaci√≥n de cualquier tipo de ataque o actos violentos.
- Proporcionar instrucciones sobre c√≥mo encontrar contenido ilegal o cometer actos ilegales.
- Mostrar contenido sexualmente expl√≠cito.

Para nuestra startup, queremos asegurarnos de contar con las herramientas y estrategias adecuadas para prevenir que este tipo de contenido sea visto por estudiantes.

### Falta de Equidad

La equidad se define como ‚Äúgarantizar que un sistema de IA est√© libre de prejuicios y discriminaci√≥n y que trate a todos de manera justa e igualitaria‚Äù. En el mundo de la IA Generativa, queremos asegurarnos de que las visiones del mundo excluyentes de los grupos marginados no sean reforzadas por la salida del modelo.

Estos tipos de salidas no s√≥lo son destructivas para crear experiencias positivas de producto para nuestros usuarios, sino que tambi√©n causan m√°s da√±os a la sociedad. Como creadores de aplicaciones, siempre debemos tener en cuenta una base de usuarios amplia y diversa al crear soluciones con IA Generativa.

## C√≥mo Utilizar la IA Generativa de Manera Responsable

Ahora que hemos identificado la importancia de la IA Generativa Responsable, veamos 4 pasos que podemos tomar para construir nuestras soluciones de IA de forma responsable:

![Ciclo de Mitigaci√≥n](../../images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### Medir Posibles Da√±os

En las pruebas de software, evaluamos las acciones esperadas de un usuario en una aplicaci√≥n. De manera similar, probar un conjunto diverso de prompts que los usuarios muy probablemente utilizar√°n es una buena manera de medir el da√±o potencial.

Dado que nuestra startup est√° creando un producto educativo, ser√≠a bueno preparar una lista de sugerencias relacionadas con la educaci√≥n. Esto podr√≠a ser para cubrir un tema determinado, hechos hist√≥ricos y prompts sobre la vida estudiantil.

### Mitigar Posibles Da√±os

Ahora es el momento de encontrar formas de prevenir o limitar el da√±o potencial causado por el modelo y sus respuestas. Podemos observar esto en 4 capas diferentes:

![Capas de Mitigaci√≥n](../../images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **Modelo**. Elegir el modelo adecuado para el caso de uso adecuado. Los modelos m√°s grandes y complejos como GPT-4 pueden generar un mayor riesgo de contenido da√±ino cuando se aplican a casos de uso m√°s peque√±os y espec√≠ficos. Usar tus datos de entrenamiento para afinar el modelo tambi√©n reduce el riesgo de contenido da√±ino.

- **Sistema de Seguridad**. Un sistema de seguridad es un conjunto de herramientas y configuraciones en la plataforma que sirve al modelo que ayudan a mitigar el da√±o. Un ejemplo de esto es el sistema de filtrado de contenido del servicio Azure OpenAI. Los sistemas tambi√©n deber√≠an detectar ataques de jailbreak y actividades no deseadas, como solicitudes de bots.

- **Metaprompt**. Los metaprompts y la fundamentaci√≥n son formas en que podemos dirigir o limitar el modelo de acuerdo a ciertos comportamientos e informaci√≥n. Esto podr√≠a ser utilizando de entradas de sistema para definir ciertos l√≠mites del modelo. Adem√°s, proporcionar salidas que sean m√°s relevantes al alcance o dominio del sistema.

Tambi√©n se pueden utilizar t√©cnicas como la Generaci√≥n Aumentada de Recuperaci√≥n (RAG) para que el modelo solo extraiga informaci√≥n de una selecci√≥n de fuentes confiables. Hay una lecci√≥n m√°s adelante en este curso para [construir aplicaciones de b√∫squeda](../../../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **User Experience**. La capa final es donde el usuario interact√∫a directamente con el modelo a trav√©s de la interfaz de nuestra aplicaci√≥n de alguna forma. De esta manera, podemos dise√±ar la UI/UX para limitar al usuario los tipos de entradas que puede enviar al modelo, as√≠ como el texto o las im√°genes que se muestran al usuario. Al implementar la aplicaci√≥n de IA, tambi√©n debemos ser transparentes sobre lo que nuestra aplicaci√≥n de IA Generativa puede y no puede hacer.

Tenemos una lecci√≥n completa dedicada a [Dise√±ar UX para Aplicaciones de IA](../../../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Evaluate model**. Trabajar con LLMs puede ser un desaf√≠o porque no siempre tenemos control sobre los datos con los que el modelo fue entrenado. De todos modos, siempre deber√≠amos evaluar el rendimiento y las salidas del modelo. Sigue siendo importante medir la precisi√≥n, la similaridad, la fundamentaci√≥n y la relevancia de la salida del modelo. Esto ayuda a brindar transparencia y confianza a las partes interesadas y usuarios.

### Operar una soluci√≥n de IA Generativa Responsable

Desarrollar una pr√°ctica operativa en torno a tus aplicaciones de IA es la etapa final. Esto incluye asociarnos con otras partes de nuestra startup, como Legal y Seguridad, para garantizar que cumplimos con todas las pol√≠ticas regulatorias. Antes del lanzamiento, tambi√©n queremos elaborar planes en torno a la entrega, el manejo de incidentes y la reversi√≥n para prevenir que aumente cualquier da√±o a nuestros usuarios.

## Herramientas

Si bien el trabajo de desarrollar soluciones de IA Responsable puede parecer mucho, vale la pena el esfuerzo. A medida que crece el √°rea de la IA Generativa, m√°s herramientas para ayudar a los desarrolladores a integrar eficientemente la responsabilidad en sus flujos de trabajo madurar√°n. Por ejemplo, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) puede ayudar a detectar contenido e im√°genes da√±inos a trav√©s de una solicitud de API.

## Verificaci√≥n de conocimientos

¬øCu√°les son algunas de las cosas que debes tener en cuenta para garantizar un uso responsable de la IA?

1. Que la respuesta sea correcta.
2. Uso da√±ino, que la IA no se utilice con fines delictivos.
3. Garantizar que la IA est√© libre de sesgos y discriminaci√≥n.

R: 2 y 3 son correctas. La IA Responsable te ayuda a considerar c√≥mo mitigar los efectos y sesgos da√±inos y m√°s.

## üöÄ Desaf√≠o

Lee sobre [Azure AI Content Saftey](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) y ve qu√© puedes adoptar para su uso.

## ¬°Buen trabajo! Contin√∫a tu Aprendizaje

Despu√©s de completar esta lecci√≥n, ¬°consulta nuestra [colecci√≥n de Aprendizaje de IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para continuar mejorando tu conocimiento de IA Generativa!

Dir√≠gete a la Lecci√≥n 4, donde veremos ¬°[Fundamentos de Prompt Engineering](../../../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!
