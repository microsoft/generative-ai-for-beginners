# Uso Responsable de la IA Generativa

[![Uso Responsable de la IA Generativa](../../images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst)

> _Haga click en la imagen de arriba para ver el video de esta lecci贸n_

Es f谩cil fascinarse con la IA, y en particular con la IA generativa, pero es necesario considerar c贸mo usarla responsablemente. Debe considerar aspectos como c贸mo garantizar que el resultado sea justo y no da帽ino, entre otros. Este cap铆tulo busca brindarle el contexto mencionado, qu茅 considerar y c贸mo tomar medidas activas para mejorar su uso de la IA.

## Introducci贸n

Esta lecci贸n abordar谩:

- Por qu茅 deber铆a priorizar la IA Responsable al crear aplicaciones de IA Generativa.
- Principios fundamentales de la IA Responsable y su relaci贸n con la IA Generativa.
- C贸mo poner en pr谩ctica estos principios de la IA Responsable mediante estrategias y herramientas.

## Objetivos de aprendizaje

Tras completar esta lecci贸n, sabr谩s:

- La importancia de la IA Responsable al crear aplicaciones de IA Generativa.
- Cu谩ndo considerar y aplicar los principios fundamentales de la IA Responsable al crear aplicaciones de IA Generativa.
- Qu茅 herramientas y estrategias tienes a tu disposici贸n para poner en pr谩ctica el concepto de IA Responsable.

## Principios de la IA Responsable

El entusiasmo por la IA Generativa nunca ha sido tan grande. Este entusiasmo ha atra铆do a muchos nuevos desarrolladores, atenci贸n y financiaci贸n a este espacio. Si bien esto es muy positivo para quienes buscan crear productos y empresas utilizando IA Generativa, tambi茅n es importante que procedamos con responsabilidad.

A lo largo de este curso, nos centraremos en el desarrollo de nuestra startup y nuestro producto educativo de IA. Utilizaremos los principios de la IA Responsable: Equidad, Inclusi贸n, Fiabilidad/Seguridad, Seguridad y Privacidad, Transparencia y Responsabilidad. Con estos principios, exploraremos c贸mo se relacionan con el uso de la IA Generativa en nuestros productos.

## Por qu茅 deber铆a priorizar la IA responsable?

Al desarrollar un producto, adoptar un enfoque centrado en el usuario, priorizando el bienestar del usuario, genera los mejores resultados.

La singularidad de la IA generativa reside en su capacidad para crear respuestas, informaci贸n, orientaci贸n y contenido 煤tiles para los usuarios. Esto se puede lograr sin muchos pasos manuales, lo que puede generar resultados impresionantes. Sin una planificaci贸n y estrategias adecuadas, tambi茅n puede, lamentablemente, tener consecuencias perjudiciales para los usuarios, el producto y la sociedad en su conjunto.

Analicemos algunos (pero no todos) de estos resultados potencialmente da帽inos:

### Alucinaciones

El t茅rmino alucinaciones se utiliza para describir cuando un M谩ster en Derecho (LLM) produce contenido completamente absurdo o algo que sabemos que es factualmente incorrecto seg煤n otras fuentes de informaci贸n.

Por ejemplo, creamos una funci贸n para nuestra startup que permite a los estudiantes formular preguntas hist贸ricas a un modelo. Un estudiante pregunta: "Qui茅n fue el 煤nico superviviente del Titanic?".

El modelo produce una respuesta como la siguiente:

![Indicaci贸n: "Qui茅n fue el 煤nico superviviente del Titanic?"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> _(Fuente: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

Esta es una respuesta muy fiable y exhaustiva. Desafortunadamente, es incorrecta. Incluso con una investigaci贸n m铆nima, se descubrir铆a que hubo m谩s de un superviviente del desastre del Titanic. Para un estudiante que apenas comienza a investigar este tema, esta respuesta puede ser lo suficientemente convincente como para no ser cuestionada y considerada un hecho. Las consecuencias de esto pueden hacer que el sistema de IA sea poco fiable y afectar negativamente la reputaci贸n de nuestra startup.

Con cada iteraci贸n de cualquier LLM, hemos observado mejoras de rendimiento en la minimizaci贸n de las alucinaciones. Aun con esta mejora, como desarrolladores de aplicaciones y usuarios, debemos ser conscientes de estas limitaciones.

### Contenido da帽ino

En la secci贸n anterior, abordamos cu谩ndo un LLM produce respuestas incorrectas o sin sentido. Otro riesgo que debemos tener en cuenta es cuando un modelo responde con contenido da帽ino.

El contenido da帽ino se puede definir como:

- Proporcionar instrucciones o incitar a la autolesi贸n o a da帽ar a ciertos grupos.
- Contenido de odio o degradante.
- Orientar la planificaci贸n de cualquier tipo de ataque o acto violento.
- Proporcionar instrucciones sobre c贸mo encontrar contenido ilegal o cometer actos ilegales.
- Mostrar contenido sexualmente expl铆cito.

En nuestra startup, queremos asegurarnos de contar con las herramientas y estrategias adecuadas para evitar que los estudiantes vean este tipo de contenido.

### Falta de imparcialidad

La imparcialidad se define como 芦garantizar que un sistema de IA est茅 libre de sesgos y discriminaci贸n, y que trate a todos de forma justa e igualitaria禄. En el 谩mbito de la IA generativa, queremos asegurarnos de que las visiones excluyentes de los grupos marginados no se vean reforzadas por los resultados del modelo.

Este tipo de resultados no solo perjudica la creaci贸n de experiencias positivas de producto para nuestros usuarios, sino que tambi茅n causa un mayor da帽o social. Como desarrolladores de aplicaciones, siempre debemos tener en cuenta una base de usuarios amplia y diversa al crear soluciones con IA Generativa.

## C贸mo usar la IA Generativa de forma responsable

Ahora que hemos identificado la importancia de la IA Generativa Responsable, veamos cuatro pasos que podemos seguir para desarrollar nuestras soluciones de IA de forma responsable:

![Mitigar el ciclo](../../images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### Medir posibles da帽os

En las pruebas de software, evaluamos las acciones esperadas de un usuario en una aplicaci贸n. De igual forma, evaluar un conjunto diverso de indicaciones que los usuarios probablemente utilizar谩n es una buena manera de medir posibles da帽os.

Dado que nuestra startup est谩 desarrollando un producto educativo, ser铆a recomendable preparar una lista de indicaciones relacionadas con la educaci贸n. Esta podr铆a abarcar un tema espec铆fico, hechos hist贸ricos e indicaciones sobre la vida estudiantil.

### Mitigar posibles da帽os

Ahora es el momento de encontrar maneras de prevenir o limitar los posibles da帽os causados por el modelo y sus respuestas. Podemos analizar esto en cuatro niveles diferentes:

![Capas de mitigaci贸n](../../images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **Modelo**. Elegir el modelo adecuado para el caso de uso adecuado. Los modelos m谩s grandes y complejos, como GPT-4, pueden generar un mayor riesgo de contenido da帽ino al aplicarse a casos de uso m谩s peque帽os y espec铆ficos. Usar los datos de entrenamiento para ajustar el modelo tambi茅n reduce el riesgo de contenido da帽ino.

- **Sistema de Seguridad**. Un sistema de seguridad es un conjunto de herramientas y configuraciones en la plataforma que sirve al modelo y que ayudan a mitigar los da帽os. Un ejemplo de esto es el sistema de filtrado de contenido del servicio Azure OpenAI. Los sistemas tambi茅n deben detectar ataques de jailbreak y actividad no deseada, como solicitudes de bots.

- **Metaprompt**. Los metaprompts y la puesta a tierra son formas de dirigir o limitar el modelo en funci贸n de ciertos comportamientos e informaci贸n. Esto podr铆a implicar el uso de entradas del sistema para definir ciertos l铆mites del modelo. Adem谩s, se pueden proporcionar salidas m谩s relevantes para el alcance o dominio del sistema.

Tambi茅n se pueden utilizar t茅cnicas como la Generaci贸n Aumentada por Recuperaci贸n (RAG) para que el modelo solo extraiga informaci贸n de una selecci贸n de fuentes confiables. M谩s adelante en este curso, se incluye una lecci贸n sobre [desarrollo de aplicaciones de b煤squeda](../../../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Experiencia de usuario**. La capa final es donde el usuario interact煤a directamente con el modelo a trav茅s de la interfaz de nuestra aplicaci贸n. De esta manera, podemos dise帽ar la UI/UX para limitar los tipos de entradas que el usuario puede enviar al modelo, as铆 como el texto o las im谩genes que se le muestran. Al implementar la aplicaci贸n de IA, tambi茅n debemos ser transparentes sobre lo que nuestra aplicaci贸n de IA generativa puede y no puede hacer.

Tenemos una lecci贸n completa dedicada a [Dise帽o de UX para Aplicaciones de IA](../../../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Evaluar el modelo**. Trabajar con LLM puede ser un desaf铆o porque no siempre tenemos control sobre los datos con los que se entren贸 el modelo. En cualquier caso, siempre debemos evaluar el rendimiento y los resultados del modelo. Sigue siendo importante medir la precisi贸n, la similitud, la solidez y la relevancia del resultado del modelo. Esto ayuda a brindar transparencia y confianza a las partes interesadas y a los usuarios.

### Operar una soluci贸n de IA Generativa Responsable

Desarrollar una pr谩ctica operativa en torno a sus aplicaciones de IA es la etapa final. Esto incluye la colaboraci贸n con otras 谩reas de nuestra startup, como los departamentos Legal y de Seguridad, para garantizar el cumplimiento de todas las pol铆ticas regulatorias. Antes del lanzamiento, tambi茅n queremos desarrollar planes para la entrega, la gesti贸n de incidentes y la reversi贸n para evitar que los da帽os a nuestros usuarios se agraven.

## Herramientas

Aunque desarrollar soluciones de IA responsable pueda parecer una tarea ardua, vale la pena. A medida que crece el campo de la IA generativa, se desarrollar谩n m谩s herramientas que ayuden a los desarrolladores a integrar la responsabilidad de forma eficiente en sus flujos de trabajo. Por ejemplo, la [Seguridad de contenido de IA de Azure](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) puede ayudar a detectar contenido e im谩genes da帽inos mediante una solicitud de API.

## Comprobaci贸n de conocimientos

Qu茅 aspectos debes tener en cuenta para garantizar un uso responsable de la IA?

1. Que la respuesta sea correcta.
1. Uso perjudicial: que la IA no se utilice con fines delictivos.
1. Garantizar que la IA est茅 libre de sesgos y discriminaci贸n.

R: Las respuestas 2 y 3 son correctas. La IA responsable te ayuda a considerar c贸mo mitigar los efectos perjudiciales, los sesgos y mucho m谩s.

##  Desaf铆o

Inf贸rmate sobre [Seguridad del contenido de la IA de Azure](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) y descubre qu茅 puedes adoptar para tu uso.

## 隆Buen trabajo! 隆Contin煤a aprendiendo!

Despu茅s de completar esta lecci贸n, consulta nuestra [Colecci贸n de aprendizaje de IA generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para seguir mejorando tus conocimientos sobre IA generativa.

Dir铆gete a la Lecci贸n 4, donde veremos los [Fundamentos de ingenier铆a de indicaciones](../../../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst).