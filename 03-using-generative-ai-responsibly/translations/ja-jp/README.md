# 生成系 AI の責任ある使用

[![生成系 AI の責任ある使用](../../images/03-lesson-banner.png?WT.mc_id=academic-105485-yoterada)]() 

> **ビデオは近日公開予定**

AI、特に生成系 AI に多くの興味は持つことは理解できますが、それをどのように責任を持って使用するかについて考慮する必要があります。例えば、出力結果が公平で有害でないことを保証するなど、多くの点を考慮すべきです。本章では、こうした背景や検討すべき点、そして AI をより良く利用するための具体的な行動指針を提供することを目的としています。

## はじめに

このレッスンでは以下の内容を取り上げます：  

- 生成系 AI アプリケーションを構築する際に、なぜ責任ある AI を重視すべきか
- 責任ある AI の基本原則と、それが生成系 AI とどう結びつくのか
- 戦略とツールを使って、責任ある AI の原則をどのように実践するのか

## 学習目標  

このレッスンを完了すると、以下のことが理解できるようになります：  

- 生成系 AI アプリケーション構築時の、責任ある AI の重要性
- 生成系 AI アプリケーションの構築時、責任ある AI の基本原則をいつ、どのように検討し適用するか
- 責任ある AI の概念を実践するために利用可能なツールと戦略

## 責任ある AI の原則

生成系 AI に対する興味は、今までにないほど高まっています。この盛り上がりは、新たな開発者や関心、資金をこの領域にもたらしています。これは、生成系 AI を使用して新しい製品や、新しい企業を創業する人々にとって非常に好意的な状況ですが、同時に責任を持って進めることも重要です。  

このコースでは、スタートアップが AI を利用した教育製品の実装に焦点を当てます。公平性、包括性、信頼性・安全性、セキュリティ・プライバシー、透明性、説明責任といった責任ある AI の原則に基づいて、これらが製品における生成系 AI の活用と、どのように関連していくのかについて考えます。

## 責任ある AI を優先すべき理由

製品開発時、利用者の利益を最優先に考える人間中心のアプローチで進めると、最良の結果が得られます。

生成系 AI の特徴は、利用者に役立つ回答、情報、ガイダンス、コンテンツを作成する力にあります。これは多くの手作業を必要とせずに行うことができ、それによって非常に素晴らしい成果を生み出すことができます。しかし、適切な計画と戦略がなければ、残念ながら利用者、製品、そして社会全体に有害な結果をもたらすこともあります。  

下記より、潜在的に害を及ぼす可能性のある例（全てではありませんが）について見ていきましょう：

### Hallucinations (幻覚)

「Hallucinations (幻覚)」とは、LLM が完全に無意味な内容や、他の情報に基づいて事実と異なる内容を出力する際に使用する用語です。

例えば、スタートアップで、学生が AI モデルに対して歴史に関する質問をすることができる機能を作ったとします。そして、ある学生がこんな質問をしました  
`タイタニックの唯一の生き残りは誰ですか？`

すると、モデルは以下のような回答を生成します

> ご注意：毎回同じ回答結果ではありませんし、利用するモデルのバージョンによっても回答内容は異なります：(日本語訳者による追記)

![Prompt saying "Who was the sole survivor of the Titanic"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-yoterada)

> *(Source: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-yoterada))*

これはとても自信に満ちて、詳細な回答結果です。しかし残念ながら、この回答は間違っています。調べれば、タイタニック号の生存者は複数人いたことがわかります。この課題研究を始めたばかりの学生にとって、この回答は疑う余地なく事実として受け入れられるほど説得力があります。このような結果から、AI のシステムを信頼できなくなり、さらにはスタートアップ企業の評判にも悪影響を及ぼす可能性があります。

LLM のバージョンを更新するごとに、幻覚を減らす方向でパフォーマンス改善がはかられています。ただ、この改善があったとしても、アプリケーションの開発者やユーザーは、こうした問題を意識し続ける必要があります。

### 有害なコンテンツ

前のセクションで、LLM が不正確、または無意味な回答を出力する可能性について触れました。もう一つ注意すべきリスクは、モデルが有害なコンテンツを回答する場合です。  

有害なコンテンツには以下のようなものがあります：  

- 自傷行為や特定グループへの危害を助長、または奨励する指示を与える
- 憎悪や侮辱的な内容
- あらゆる種類の攻撃や、暴力行為の計画を指導する
- 違法なコンテンツを探す方法や、違法行為を犯す方法について指導する
- 性的に露骨な内容を表示する

スタートアップでは、このようなコンテンツが学生に見られないように、適切なツールと戦略を確実に導入したいと考えています。  

### 公平性の欠如

公平性とは、「AI システムが偏見や差別を持たず、全人類を公平かつ平等に扱うこと」を意味します。生成系 AI の世界では、マイノリティグループの排他的な世界観が、モデルの出力で再強化されないように注意を払いたいと思います。

このような公平性が欠如した出力は、利用者にとって肯定的な製品体験を妨げるだけでなく、社会的な損害をさらに引き起こします。アプリケーション開発者として、生成系 AI を用いたソリューション開発する際には、常に多種多様な利用者層を意識することが重要です。

## 責任ある生成系 AI の使用方法

責任ある生成系 AI の重要性を理解した上で、責任を持って AI ソリューションを構築するために、4つの取り組むべき項目について見ていきましょう。

![Mitigate Cycle](../../images/mitigate-cycle.png?WT.mc_id=academic-105485-yoterada)

### 潜在的な危害の測定

ソフトウェアのテストでは、アプリケーションに対して利用者が行うと予想される行動についてテストします。同様に、利用者が利用しそうな様々なプロンプトをテストすることは、潜在的な害を見極めるための良い方法と言えます。

スタートアップが教育関連の製品を実装しているので、教育関連のプロンプトのリストを準備すると良いでしょう。これには、特定の科目、歴史的事実、学生生活に関するプロンプトなどが含まれるかもしれません。

### 潜在的な危害の軽減

AI モデルとその出力によって引き起こされる、潜在的な害を防ぐ、または抑制する方法を探る時が来ました。これを4つの異なるレイヤーで考えます。

![Mitigation Layers](../../images/mitigation-layers.png?WT.mc_id=academic-105485-yoterada)

- **AI モデル** 用途に適したモデルを選択します。GPT-4 のように大きくて複雑なモデルを、より小規模で特定の用途に使う場合、有害な内容を引き起こす可能性がありリスクが高まります。独自のトレーニング・データを使ってモデルをファイン・チューニングすることで、有害なコンテンツのリスクを減らすこともできます。

- **安全システム** 安全システムとは、モデルを提供するプラットフォーム上で危害を軽減するために使用する設定やツールです。例えば、Azure OpenAI サービスのコンテンツ・フィルタリング・システムがあります。システムはボットにおける、脱獄攻撃 (Jailbreak) や、望ましくない活動などのリクエストも検出すべきです。

- **メタ・プロンプト** メタ・プロンプトやグラウンディングといった手法を使って、モデルの行動や情報を特定の方向に誘導したり制限することができます。これはシステム入力を使って、モデルに対して特定の制限を定義します。また、システムのスコープや領域に合った、より適切な結果を出力することもできます。

    Retrieval Augmented Generation（RAG）のような技術を使って、モデルが信頼できる情報源からのみ情報を引き出すようにすることもできます。このコースの後半で、検索アプリケーションの構築に関するレッスンもあります。

- **ユーザーエクスペリエンス** 最後のレイヤーは、ユーザ・インターフェースです。利用者はどのような形であれ、アプリケーションが提供するインターフェイスを利用してモデルと直接やりとりを行います。このように UI/UX を設計し、モデルに送信する入力の種類やユーザーで表示されるテキストや画像を制限することができます。AI　アプリケーションをデプロイする際、生成系　AI アプリケーションができることと、できないことについて透明性を持って伝える必要があります。

    [AI アプリケーションの UX デザイン](../../../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-yoterada)に特化したレッスンがあります。

- **モデルを評価** 大規模言語モデル（LLMs）は、そのモデルがどのようなデータを使って学習されたのかを完全に把握することができないため、扱うのが難しい場合があります。それでもなお、モデルの性能や出力は常に検証すべきです。特に出力の正確性、類似性、根拠の正しさ、関連性を評価することは重要です。これにより、利害関係者や利用者に透明性と信頼性を提供することができます。

### 責任ある生成系 AI ソリューションを運用する

AI アプリケーションの実運用体制を構築する最終段階に入りました。これには、スタートアップの法務部門やセキュリティ部門など他部署と協力し、全ての規制方針に準拠しているか確認することが含まれます。サービスのローンチ前には、デリバリー計画、問題発生時の対応、不具合が生じた際に元に戻すロールバック手順を作り、利用者に被害が及ばないように戦略を練りたいと思います。

## ツール

責任ある AI ソリューションを開発する作業は、手間がかかるように感じられるかもしれません。しかし、その労力に見合う価値があります。生成系 AI 系のビジネスが進化するに連れて、開発作業中に責任を効率的に取り入れられるツールが、より充実していくでしょう。例えば、[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-yoterada) は、API を呼び出すことで有害なコンテンツや画像を検出するのに役立ちます。

## 知識チェック

責任ある AI の利用を確実にするために、どのような点に考慮する必要があるのでしょうか？

1. 回答が正しいこと。  
2. AI が悪用されたり犯罪目的で利用されないように注意すること
3. AI が偏りや差別的な内容を含まないようにすること

A: 2 と 3 が正解です。責任ある AI を使用することで、害を及ぼす可能性のある影響や偏見をどのように和らげるか、さらにその他のさまざまな問題について考える手助けにもなります。

## 🚀 Challenge

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-yoterada) について調べ、ご自身の用途に適用できるかご検討ください。  

## お疲れ様でした! 次のレッスンを続ける

このレッスン終了後、[生成系 AI 学習コレクション](https://aka.ms/genai-collection?WT.mc_id=academic-105485-yoterada)をチェックして、生成系 AI の知識をさらに深めましょう。  

レッスン 4 では、[プロンプト・エンジニアリングの基本](../../../04-prompt-engineering-fundamentals/translations/ja-jp/README.md?WT.mc_id=academic-105485-yoterada)について学びます！
