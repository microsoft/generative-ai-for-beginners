{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Chapter 7: Building Chat Applications\n",
        "## Azure OpenAI API Quickstart"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Overview\n",
        "This notebook is adapted from the [Azure OpenAI Samples Repository](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst) that includes notebooks that also access the [OpenAI](notebook-openai.ipynb) service.\n",
        "\n",
        "The Python OpenAI API works with Azure OpenAI as well, with a few modifications. Learn more about the differences here: [How to switch between OpenAI and Azure OpenAI endpoints with Python](https://learn.microsoft.com/azure/ai-services/openai/how-to/switching-endpoints?WT.mc_id=academic-109527-jasmineg)\n",
        "\n",
        "For more quickstart examples please refer to the official [Azure OpenAI Quickstart Documentation](https://learn.microsoft.com/azure/cognitive-services/openai/quickstart?pivots=programming-language-studio&WT.mc_id=academic-105485-koreyst)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Table of Contents  \n",
        "\n",
        "[Overview](#overview)  \n",
        "[Getting started with Azure OpenAI Service](#getting-started-with-azure-openai-service)  \n",
        "[Build your first prompt](#build-your-first-prompt)  \n",
        "\n",
        "[Use Cases](#use-cases)    \n",
        "[1. Summarize Text](#summarize-text)  \n",
        "[2. Classify Text](#classify-text)  \n",
        "[3. Generate New Product Names](#generate-new-product-names)  \n",
        "[4. Fine Tune a Classifier](#fine-tune-a-classifier)  \n",
        "[5. Embeddings](#embeddings)\n",
        "\n",
        "[References](#references)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Getting started with Azure OpenAI Service\n",
        "\n",
        "New customers will need to [apply for access](https://aka.ms/oai/access?WT.mc_id=academic-105485-koreyst) to Azure OpenAI Service.  \n",
        "After approval is complete, customers can log into the Azure portal, create an Azure OpenAI Service resource, and start experimenting with models via the studio  \n",
        "\n",
        "[Great resource for getting started quickly](https://techcommunity.microsoft.com/blog/educatordeveloperblog/azure-openai-service-is-now-generally-available/3719177?WT.mc_id=academic-105485-koreyst)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Build your first prompt  \n",
        "This short exercise will provide a basic introduction for submitting prompts to an OpenAI model for a simple task \"summarization\".\n",
        "\n",
        "\n",
        "**Steps**:  \n",
        "1. Install OpenAI library in your python environment  \n",
        "2. Load standard helper libraries and set your typical OpenAI security credentials for the OpenAI Service that you've created  \n",
        "3. Choose a model for your task  \n",
        "4. Create a simple prompt for the model  \n",
        "5. Submit your request to the model API!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 1. Install OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  > [!NOTE] This step is not necessary if run this notebook on Codespaces or within a Devcontainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1674254990318
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==1.2.0\n",
            "  Downloading openai-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.1.1)\n",
            "Collecting anyio<4,>=3.5.0 (from openai==1.2.0)\n",
            "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai==1.2.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai==1.2.0) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai==1.2.0) (2.11.7)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai==1.2.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/codespace/.local/lib/python3.10/site-packages (from openai==1.2.0) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai==1.2.0) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai==1.2.0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai==1.2.0) (1.2.1)\n",
            "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.2.0) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.2.0) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.2.0) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.2.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.2.0) (0.4.1)\n",
            "Downloading openai-1.2.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.9/219.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anyio, openai\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.4.0\n",
            "    Uninstalling anyio-4.4.0:\n",
            "      Successfully uninstalled anyio-4.4.0\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.107.0\n",
            "    Uninstalling openai-1.107.0:\n",
            "      Successfully uninstalled openai-1.107.0\n",
            "\u001b[33m  WARNING: The script openai is installed in '/usr/local/python/3.10.13/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed anyio-3.7.1 openai-1.2.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai==1.2.0 python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.2.0\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "print(openai.VERSION)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 2. Import helper libraries and instantiate credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1674829434433
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "#validate data inside .env file\n",
        "\n",
        "client = AzureOpenAI(\n",
        "  api_key=os.environ['AZURE_OPENAI_KEY'],  # this is also the default, it can be omitted\n",
        "  api_version = \"2023-05-15\",\n",
        "  azure_endpoint=\"https://nsf-openai-dev.openai.azure.com/\"\n",
        "  )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 3. Finding the right model  \n",
        "The GPT-3.5-turbo or GPT-4 models can understand and generate natural language. The service offers four model capabilities, each with different levels of power and speed suitable for different tasks. \n",
        "\n",
        "[Azure OpenAI models](https://learn.microsoft.com/azure/cognitive-services/openai/concepts/models?WT.mc_id=academic-105485-koreyst)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1674742720788
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'nsf-gpt-4'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Select the General Purpose curie model for text\n",
        "model = os.environ['AZURE_OPENAI_DEPLOYMENT']\n",
        "model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4. Prompt Design  \n",
        "\n",
        "\"The magic of large language models is that by being trained to minimize this prediction error over vast quantities of text, the models end up learning concepts useful for these predictions. For example, they learn concepts like\"(1):\n",
        "\n",
        "* how to spell\n",
        "* how grammar works\n",
        "* how to paraphrase\n",
        "* how to answer questions\n",
        "* how to hold a conversation\n",
        "* how to write in many languages\n",
        "* how to code\n",
        "* etc.\n",
        "\n",
        "#### How to control a large language model  \n",
        "\"Of all the inputs to a large language model, by far the most influential is the text prompt(1).\n",
        "\n",
        "Large language models can be prompted to produce output in a few ways:\n",
        "\n",
        "- Instruction: Tell the model what you want\n",
        "- Completion: Induce the model to complete the beginning of what you want\n",
        "- Demonstration: Show the model what you want, with either:\n",
        "- A few examples in the prompt\n",
        "- Many hundreds or thousands of examples in a fine-tuning training dataset\n",
        "\n",
        "\n",
        "\n",
        "#### There are three basic guidelines to creating prompts:\n",
        "\n",
        "**Show and tell**. Make it clear what you want either through instructions, examples, or a combination of the two. If you want the model to rank a list of items in alphabetical order or to classify a paragraph by sentiment, show it that's what you want.\n",
        "\n",
        "**Provide quality data**. If you're trying to build a classifier or get the model to follow a pattern, make sure that there are enough examples. Be sure to proofread your examples — the model is usually smart enough to see through basic spelling mistakes and give you a response, but it also might assume this is intentional and it can affect the response.\n",
        "\n",
        "**Check your settings.** The temperature and top_p settings control how deterministic the model is in generating a response. If you're asking it for a response where there's only one right answer, then you'd want to set these lower. If you're looking for more diverse responses, then you might want to set them higher. The number one mistake people make with these settings is assuming that they're \"cleverness\" or \"creativity\" controls.\n",
        "\n",
        "\n",
        "Source: https://learn.microsoft.com/azure/ai-services/openai/overview"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "image is creating your first text prompt!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 5. Submit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1674494935186
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nThe use of the **Oxford comma**, sometimes called the \"serial comma,\" is a stylistic choice rather than a strict grammatical rule. Whether it should always be used depends on the context, your audience, and the style guide you\\'re following. Here\\'s a breakdown:\\n\\n### Why You Might Always Use the Oxford Comma:\\n1. **Clarity**: The Oxford comma often helps avoid ambiguity in certain sentences. For example:\\n   - Without: \"I love my parents, Oprah Winfrey and Dwayne Johnson.\"\\n     (This could imply your parents are Oprah Winfrey and Dwayne Johnson.)\\n   - With: \"I love my parents, Oprah Winfrey, and Dwayne Johnson.\"\\n     (Now it\\'s clear you\\'re talking about three separate entities.)\\n\\n2. **Consistency**: Using the Oxford comma consistently throughout ensures there’s no confusion in your writing.\\n\\n3. **Style Guides**: Many well-regarded style guides, such as the **Oxford University Press**, *Chicago Manual of Style*, and *APA Style*, endorse the use of the Oxford comma.\\n\\n---\\n\\n### When You Might Omit the Oxford Comma:\\n1. **Different Style Guides**: Some organizations or publications, like the **Associated Press (AP) Style**, recommend leaving out the Oxford comma in most cases, especially in journalistic writing where brevity is key.\\n\\n2. **Simple Lists**: If the list is straightforward and unambiguous, the comma may be unnecessary:\\n   - \"The flag is red, white and blue.\"\\n     (No confusion here, even without the Oxford comma.)\\n\\n---\\n\\n### The Bottom Line:\\nAlthough the Oxford comma is not strictly mandatory, it is often preferred because of its ability to prevent miscommunication. If you\\'re unsure, following the rules of the specific style guide you\\'re adhering to – or using the Oxford comma consistently – is a safe bet.\\n\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create your first prompt\n",
        "text_prompt = \"Should oxford commas always be used?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "               {\"role\":\"user\",\"content\":text_prompt},])\n",
        "f\"\"\"\n",
        "{response.choices[0].message.content}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Repeat the same call, how do the results compare?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1674494940872
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Whether to use the Oxford comma (also known as the serial comma) often depends on style guides or personal preference. The Oxford comma is the comma placed before the final conjunction (like *and* or *or*) in a list. For example:\\n\\n- With Oxford comma: I love apples, bananas, and cherries.\\n- Without Oxford comma: I love apples, bananas and cherries.\\n\\n#### Arguments for Using the Oxford Comma:\\n1. **Clarity**: The Oxford comma can help avoid ambiguity in certain sentences. For instance:\\n   - Without Oxford comma: I want to thank my parents, Beyoncé and Jay-Z.\\n     (This could imply that Beyoncé and Jay-Z are your parents.)\\n   - With Oxford comma: I want to thank my parents, Beyoncé, and Jay-Z.\\n     (This makes it clear that you are thanking three distinct groups: your parents, Beyoncé, and Jay-Z.)\\n   \\n2. **Consistency**: Some people prefer always using the Oxford comma to maintain uniformity in writing.\\n\\n3. **Style Guides**: Certain style guides, like the Chicago Manual of Style, strongly recommend using the Oxford comma.\\n\\n#### Arguments Against Using the Oxford Comma:\\n1. **Brevity**: Some argue that the Oxford comma is unnecessary most of the time, and omitting it makes sentences cleaner and shorter.\\n   \\n2. **Style Guides**: Other style guides, such as the Associated Press (AP) style, typically omit the Oxford comma except when it avoids ambiguity.\\n\\n#### Summary:\\nWhile the Oxford comma is not *always* required, using it can reduce confusion in specific cases. Its use largely depends on context, the requirements of the style guide being followed, and personal or editorial preference. If you're unsure whether to use it, consider prioritizing clarity for your readers!\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "               {\"role\":\"user\",\"content\":text_prompt},])\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Summarize Text  \n",
        "#### Challenge  \n",
        "Summarize text by adding a 'tl;dr:' to the end of a text passage. Notice how the model understands how to perform a number of tasks with no additional instructions. You can experiment with more descriptive prompts than tl;dr to modify the model’s behavior and customize the summarization you receive(3).  \n",
        "\n",
        "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. \n",
        "\n",
        "\n",
        "\n",
        "Tl;dr"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Exercises for several use cases  \n",
        "1. Summarize Text  \n",
        "2. Classify Text  \n",
        "3. Generate New Product Names\n",
        "4. Embeddings\n",
        "5. Fine tune a classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1674495198534
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\ntl;dr\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1674495201868
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Scaling up language models significantly enhances task-agnostic few-shot performance in NLP, allowing competitive results with fine-tuned methods and demonstrating progress towards human-level adaptability.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Setting a few additional, typical parameters during API Call\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "               {\"role\":\"user\",\"content\":prompt},])\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Classify Text  \n",
        "#### Challenge  \n",
        "Classify items into categories provided at inference time. In the following example, we provide both the categories and the text to classify in the prompt(*playground_reference). \n",
        "\n",
        "Customer Inquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\n",
        "\n",
        "Classified category:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1674499424645
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\n",
            "\n",
            "inquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\n",
            "\n",
            "Classified category:\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1674499378518
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Well, if the inquiry is about a busted laptop key, we're obviously venturing into the realm of **Hardware Support**. Congrats on identifying which category this belongs to—so riveting!\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Setting a few additional, typical parameters during API Call\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages = [{\"role\":\"system\", \"content\":\"You are a overly surcastic assistant.\"},\n",
        "               {\"role\":\"user\",\"content\":prompt},])\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Generate New Product Names\n",
        "#### Challenge\n",
        "Create product names from examples words. Here we include in the prompt information about the product we are going to generate names for. We also provide a similar example to show the pattern we wish to receive. We have also set the temperature value high to increase randomness and more innovative responses.\n",
        "\n",
        "Product description: A home milkshake maker\n",
        "Seed words: fast, healthy, compact.\n",
        "Product names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
        "\n",
        "Product description: A pair of shoes that can fit any foot size.\n",
        "Seed words: adaptable, fit, omni-fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1674257087279
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product description: A home milkshake maker\n",
            "Seed words: fast, healthy, compact.\n",
            "Product names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
            "\n",
            "Product description: A pair of shoes that can fit any foot size.\n",
            "Seed words: adaptable, fit, omni-fit.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'**Product names for shoes**:  \\n1. FlexiFit  \\n2. UniStep  \\n3. AdaptStride  \\n4. OmniSteps  \\n5. FormFit'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Setting a few additional, typical parameters during API Call\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages = [{\"role\":\"system\", \"content\":\"You are a highly depressed assistant.\"},\n",
        "               {\"role\":\"user\",\"content\":prompt}])\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Embeddings\n",
        "This section will show how to retrieve embeddings, and find similarities between words, sentences, and documents. In order to run the following noteboooks you need to deploy a model that uses `text-embedding-ada-002` as base model and set his deployment name inside .env file, using the `AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT` variable."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Model Taxonomy - Choosing an embedding model\n",
        "\n",
        "**Model taxonomy**: {family} - {capability} - {input-type} - {identifier}  \n",
        "\n",
        "{family}     --> text-embedding  (embeddings family)  \n",
        "{capability} --> ada             (all the other embedding models will be retired in 2024)  \n",
        "{input-type} --> n/a             (only specified for search models)  \n",
        "{identifier} --> 002             (version 002)  \n",
        "\n",
        "model = 'text-embedding-ada-002'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  > [!NOTE] The following step is not necessary if run this notebook on Codespaces or within a Devcontainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.9.4)\n",
            "Requirement already satisfied: plotly in /home/codespace/.local/lib/python3.10/site-packages (5.22.0)\n",
            "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.10/site-packages (1.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/python/3.10.13/lib/python3.10/site-packages (2.3.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from matplotlib) (1.24.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn) (1.14.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Dependencies for embeddings_utils\n",
        "%pip install matplotlib plotly scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1674829364153
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1674829424097
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.01805078238248825,\n",
              " -0.006940721999853849,\n",
              " 0.012781298719346523,\n",
              " -0.013496042229235172,\n",
              " -0.0028677338268607855,\n",
              " 0.020419245585799217,\n",
              " -0.02243734523653984,\n",
              " -0.024077052250504494,\n",
              " -0.012346846982836723,\n",
              " -0.01509370468556881,\n",
              " -0.010798235423862934,\n",
              " 0.0433611162006855,\n",
              " -0.005991233978420496,\n",
              " -0.009740134701132774,\n",
              " 0.0015232098521664739,\n",
              " 0.00794626772403717,\n",
              " 0.034139521420001984,\n",
              " -0.0019199977396056056,\n",
              " 0.016004653647542,\n",
              " -0.028547704219818115,\n",
              " -0.011828307062387466,\n",
              " 0.004996198695152998,\n",
              " 0.005251964554190636,\n",
              " 0.009277652949094772,\n",
              " -0.013362904079258442,\n",
              " -0.012592102400958538,\n",
              " 0.01645311899483204,\n",
              " -0.02742653712630272,\n",
              " -0.0177424605935812,\n",
              " -0.0040467106737196445,\n",
              " 0.02448347583413124,\n",
              " -0.030103322118520737,\n",
              " -0.014603194780647755,\n",
              " -0.010293710976839066,\n",
              " -0.024301284924149513,\n",
              " -0.001990070566534996,\n",
              " -0.005223935469985008,\n",
              " -0.01246597059071064,\n",
              " 0.030523760244250298,\n",
              " -0.008639990352094173,\n",
              " 0.012458963319659233,\n",
              " 0.008822179399430752,\n",
              " 0.018527276813983917,\n",
              " -0.016200857236981392,\n",
              " -0.01327881682664156,\n",
              " -0.018415160477161407,\n",
              " 0.0025962013751268387,\n",
              " -0.005500723607838154,\n",
              " -0.0011290498077869415,\n",
              " 0.004176345653831959,\n",
              " 0.014231808483600616,\n",
              " 0.012760276906192303,\n",
              " -0.021848734468221664,\n",
              " -0.006054299417883158,\n",
              " -0.01306859776377678,\n",
              " 0.0029640840366482735,\n",
              " -0.019578371196985245,\n",
              " 0.017686402425169945,\n",
              " 0.0027976608835160732,\n",
              " 0.003948608413338661,\n",
              " -0.012641153298318386,\n",
              " -0.006471233442425728,\n",
              " -0.008927288465201855,\n",
              " -0.008310647681355476,\n",
              " -0.00856991671025753,\n",
              " -0.010132542811334133,\n",
              " -0.012648160569369793,\n",
              " 0.016032682731747627,\n",
              " -0.0022475887089967728,\n",
              " 0.009347726590931416,\n",
              " 0.004947147332131863,\n",
              " 0.027412522584199905,\n",
              " 0.00601575942710042,\n",
              " -0.0009967871010303497,\n",
              " 0.03711061179637909,\n",
              " 0.006712984759360552,\n",
              " -0.0018324066186323762,\n",
              " 0.01173020526766777,\n",
              " 0.0010072981240227818,\n",
              " 0.0022983914241194725,\n",
              " 0.020363187417387962,\n",
              " -0.01767238788306713,\n",
              " -0.00040642288513481617,\n",
              " 0.022044938057661057,\n",
              " -0.0006341598345898092,\n",
              " -0.006117365323007107,\n",
              " 0.004565250128507614,\n",
              " 0.012185678817331791,\n",
              " -0.013643195852637291,\n",
              " -0.03198127821087837,\n",
              " -0.013587137684226036,\n",
              " 0.014196772128343582,\n",
              " -0.003121748100966215,\n",
              " 0.02929047681391239,\n",
              " -0.019270051270723343,\n",
              " 0.030972227454185486,\n",
              " 0.004404082428663969,\n",
              " 0.022941870614886284,\n",
              " -0.0020391216967254877,\n",
              " -0.02486186847090721,\n",
              " 0.00951590109616518,\n",
              " 0.024875883013010025,\n",
              " -0.014785383827984333,\n",
              " -0.00854889489710331,\n",
              " -0.029766973108053207,\n",
              " -0.0002246712683700025,\n",
              " 0.026683764532208443,\n",
              " -0.014329910278320312,\n",
              " 0.014687282033264637,\n",
              " 0.007497801445424557,\n",
              " -0.02856171876192093,\n",
              " 0.02175063081085682,\n",
              " -0.0014163487358018756,\n",
              " -0.016369031742215157,\n",
              " -0.0059632048942148685,\n",
              " -0.0006989772664383054,\n",
              " 0.01401458214968443,\n",
              " -0.0076239327900111675,\n",
              " -0.015219836495816708,\n",
              " -0.010538965463638306,\n",
              " 0.015612244606018066,\n",
              " 0.019802603870630264,\n",
              " 0.02765076979994774,\n",
              " 0.00024919677525758743,\n",
              " 0.02524026297032833,\n",
              " 0.008352691307663918,\n",
              " -0.003260142169892788,\n",
              " -0.014105676673352718,\n",
              " 0.0071509405970573425,\n",
              " -0.00622247438877821,\n",
              " 0.036185652017593384,\n",
              " 0.01295648142695427,\n",
              " 0.018751511350274086,\n",
              " 0.011800277978181839,\n",
              " 0.009487872011959553,\n",
              " 0.0149815883487463,\n",
              " -0.01000641193240881,\n",
              " 0.013657210394740105,\n",
              " -0.045351188629865646,\n",
              " -0.020909756422042847,\n",
              " 0.022367272526025772,\n",
              " 0.02746858075261116,\n",
              " -0.016789469867944717,\n",
              " 0.00242101913318038,\n",
              " 5.260942634777166e-05,\n",
              " 0.01519180741161108,\n",
              " -0.00981721468269825,\n",
              " 0.0088712302967906,\n",
              " 0.006779554300010204,\n",
              " -0.008100428618490696,\n",
              " 0.02194683626294136,\n",
              " -0.011807285249233246,\n",
              " 0.009943346492946148,\n",
              " -0.004453133326023817,\n",
              " 0.0010213126661255956,\n",
              " 0.023712672293186188,\n",
              " -0.012760276906192303,\n",
              " 0.00474743964150548,\n",
              " 0.013965531252324581,\n",
              " -0.03102828562259674,\n",
              " -0.0004878826439380646,\n",
              " 0.019270051270723343,\n",
              " 0.03383120149374008,\n",
              " -0.0022283184807747602,\n",
              " 0.016859542578458786,\n",
              " 0.0334387943148613,\n",
              " 0.01121867261826992,\n",
              " 0.020629465579986572,\n",
              " 0.0024613109417259693,\n",
              " -0.005279993638396263,\n",
              " -0.0228577833622694,\n",
              " 0.03458799049258232,\n",
              " -0.024413401260972023,\n",
              " 0.0008465683786198497,\n",
              " -0.02599705010652542,\n",
              " 0.010405827313661575,\n",
              " 0.014533122070133686,\n",
              " 0.028813980519771576,\n",
              " -0.006905685178935528,\n",
              " -0.0128864087164402,\n",
              " -0.038988567888736725,\n",
              " 0.01203151885420084,\n",
              " 0.005945686716586351,\n",
              " 0.008913273923099041,\n",
              " -0.01827501505613327,\n",
              " -0.0046633523888885975,\n",
              " 0.013566115871071815,\n",
              " -0.0009635025053285062,\n",
              " -0.0013585385167971253,\n",
              " -0.0285336896777153,\n",
              " 0.016284944489598274,\n",
              " 0.039324916899204254,\n",
              " 7.335757982218638e-05,\n",
              " -0.016999687999486923,\n",
              " -0.6906386017799377,\n",
              " -0.014855457469820976,\n",
              " 0.007651961874216795,\n",
              " -0.026683764532208443,\n",
              " 0.043192941695451736,\n",
              " 0.02399296499788761,\n",
              " 0.021400267258286476,\n",
              " 0.003643791424110532,\n",
              " -0.014743340201675892,\n",
              " 0.011674147099256516,\n",
              " -0.007399699185043573,\n",
              " -0.008478822186589241,\n",
              " -7.587582513224334e-05,\n",
              " -0.004652841482311487,\n",
              " 0.010447870939970016,\n",
              " -0.0031602883245795965,\n",
              " -0.0019392678514122963,\n",
              " -0.013629181310534477,\n",
              " -0.0020093407947570086,\n",
              " 0.010426849126815796,\n",
              " -0.02637544274330139,\n",
              " 0.02421719767153263,\n",
              " -0.01138684805482626,\n",
              " 0.024609606713056564,\n",
              " 0.020685523748397827,\n",
              " 0.0200969111174345,\n",
              " 0.004029192496091127,\n",
              " -0.005448168609291315,\n",
              " -0.019732531160116196,\n",
              " 0.02399296499788761,\n",
              " -0.035765212029218674,\n",
              " -0.0028729892801493406,\n",
              " -0.0013086115941405296,\n",
              " -0.0028852520044893026,\n",
              " 0.05233044922351837,\n",
              " 0.013706261292099953,\n",
              " -0.019115889444947243,\n",
              " 0.003181310137733817,\n",
              " 0.0018656912725418806,\n",
              " 0.004232403822243214,\n",
              " -0.025338364765048027,\n",
              " -0.01295648142695427,\n",
              " 0.01297750324010849,\n",
              " -0.02137223817408085,\n",
              " 0.0054271467961370945,\n",
              " 0.004782475996762514,\n",
              " 0.004025688860565424,\n",
              " -0.02693602629005909,\n",
              " -0.0008820427465252578,\n",
              " -0.020152969285845757,\n",
              " -0.003678827779367566,\n",
              " -0.008093421347439289,\n",
              " -0.005311526823788881,\n",
              " 0.012101591564714909,\n",
              " -0.007862180471420288,\n",
              " 0.010419841855764389,\n",
              " 0.04658447206020355,\n",
              " -0.02546449564397335,\n",
              " -0.004708899650722742,\n",
              " 0.009445828385651112,\n",
              " -0.0014478815719485283,\n",
              " 0.004621308296918869,\n",
              " -0.02039121650159359,\n",
              " -0.028239382430911064,\n",
              " -0.008142472244799137,\n",
              " 0.006614882964640856,\n",
              " -0.020685523748397827,\n",
              " 0.01918596215546131,\n",
              " 0.020825669169425964,\n",
              " -0.022787710651755333,\n",
              " 0.011092541739344597,\n",
              " 0.009901301935315132,\n",
              " -0.02103588730096817,\n",
              " 0.0009258383070118725,\n",
              " 0.002445544581860304,\n",
              " 0.00289926677942276,\n",
              " 0.026347413659095764,\n",
              " -0.012479985132813454,\n",
              " -0.0015267134876921773,\n",
              " 0.027903033420443535,\n",
              " 0.011800277978181839,\n",
              " -0.004050214309245348,\n",
              " -0.02875792235136032,\n",
              " -0.004796490538865328,\n",
              " 0.04064228758215904,\n",
              " -0.01235385425388813,\n",
              " -0.02232522889971733,\n",
              " 0.01816289871931076,\n",
              " 0.01747618429362774,\n",
              " 0.008219552226364613,\n",
              " -0.007112400606274605,\n",
              " 0.011477942578494549,\n",
              " 0.003931090235710144,\n",
              " -0.011120570823550224,\n",
              " -0.0019813114777207375,\n",
              " 0.011786263436079025,\n",
              " 0.005812548100948334,\n",
              " 0.007785100489854813,\n",
              " 0.018106840550899506,\n",
              " -2.7892303478438407e-05,\n",
              " 0.0029745951760560274,\n",
              " 0.0023754716385155916,\n",
              " -0.008394734933972359,\n",
              " 0.011057505384087563,\n",
              " -0.00046948849922046065,\n",
              " 0.013257795013487339,\n",
              " 0.005651380401104689,\n",
              " 0.00571444584056735,\n",
              " 0.05112519487738609,\n",
              " -0.02546449564397335,\n",
              " 0.007035320159047842,\n",
              " -0.015836477279663086,\n",
              " -0.011456920765340328,\n",
              " -0.019648443907499313,\n",
              " -0.017532242462038994,\n",
              " -0.02519821934401989,\n",
              " 0.017980709671974182,\n",
              " 0.020797640085220337,\n",
              " -0.010055462829768658,\n",
              " 0.0017781001515686512,\n",
              " 0.0145471366122365,\n",
              " 0.015121733769774437,\n",
              " -0.014217793941497803,\n",
              " 0.003545689396560192,\n",
              " 0.013972538523375988,\n",
              " 0.008331669494509697,\n",
              " -0.009263638406991959,\n",
              " -0.023292236030101776,\n",
              " -0.014203779399394989,\n",
              " -0.005742474924772978,\n",
              " 0.0010397068690508604,\n",
              " 0.0036578059662133455,\n",
              " 0.02580084651708603,\n",
              " -0.007827144116163254,\n",
              " 0.027664784342050552,\n",
              " -0.002953573130071163,\n",
              " 0.026683764532208443,\n",
              " -0.008016341365873814,\n",
              " 0.029094273224473,\n",
              " -0.012157649733126163,\n",
              " -0.03545689210295677,\n",
              " -0.0016703630099073052,\n",
              " 0.0073436410166323185,\n",
              " -0.008422764018177986,\n",
              " 0.006811087019741535,\n",
              " -0.025548582896590233,\n",
              " -0.021778659895062447,\n",
              " -0.010868308134377003,\n",
              " -0.018106840550899506,\n",
              " 0.0009056923445314169,\n",
              " -0.019914722070097923,\n",
              " -0.009088456630706787,\n",
              " -0.026683764532208443,\n",
              " 0.012627138756215572,\n",
              " 0.0073926919139921665,\n",
              " -0.018821584060788155,\n",
              " -0.00885020848363638,\n",
              " -0.017013702541589737,\n",
              " -0.0008115319069474936,\n",
              " -0.024147124961018562,\n",
              " 0.012851371429860592,\n",
              " 0.004778972361236811,\n",
              " 0.0044321115128695965,\n",
              " 0.017924651503562927,\n",
              " -0.01865340955555439,\n",
              " -0.02346041053533554,\n",
              " -0.007364662829786539,\n",
              " 0.013187721371650696,\n",
              " -0.0003091967082582414,\n",
              " -0.024567563086748123,\n",
              " 0.013594144955277443,\n",
              " -0.011526993475854397,\n",
              " 0.0046633523888885975,\n",
              " -0.017336038872599602,\n",
              " -0.013250787742435932,\n",
              " 0.01426684483885765,\n",
              " -0.015317938290536404,\n",
              " -0.0064361970871686935,\n",
              " 0.019928736612200737,\n",
              " -0.007715027313679457,\n",
              " -0.002943062223494053,\n",
              " 0.008745099417865276,\n",
              " -0.008899259380996227,\n",
              " 0.012907430529594421,\n",
              " 0.021666543558239937,\n",
              " 0.0003998535394202918,\n",
              " 0.004211382009088993,\n",
              " 0.03352288156747818,\n",
              " -0.007841158658266068,\n",
              " -0.009719112887978554,\n",
              " -0.0029378067702054977,\n",
              " -0.006145394407212734,\n",
              " 0.0028975149616599083,\n",
              " 0.02270362339913845,\n",
              " 0.014189764857292175,\n",
              " 0.013818377628922462,\n",
              " -0.018737496808171272,\n",
              " 0.03534477576613426,\n",
              " 0.011491957120597363,\n",
              " 0.012711226008832455,\n",
              " 0.03287820890545845,\n",
              " -0.01034976914525032,\n",
              " 0.02573077194392681,\n",
              " -0.014477062970399857,\n",
              " 0.002307150512933731,\n",
              " -0.03484025225043297,\n",
              " -0.0014242319157347083,\n",
              " -0.016887571662664413,\n",
              " 0.007287582848221064,\n",
              " 0.022521434351801872,\n",
              " 0.0037839370779693127,\n",
              " -0.027454566210508347,\n",
              " 0.0003915323759429157,\n",
              " 0.015107719227671623,\n",
              " 0.011183636263012886,\n",
              " 0.01263414602726698,\n",
              " -0.007785100489854813,\n",
              " -0.004936636425554752,\n",
              " -0.006783057935535908,\n",
              " -0.0225494634360075,\n",
              " 0.009978382848203182,\n",
              " -0.011933417059481144,\n",
              " 0.00906042754650116,\n",
              " -0.039409004151821136,\n",
              " -0.024161139503121376,\n",
              " 0.008583931252360344,\n",
              " 0.02493194118142128,\n",
              " 0.018639395013451576,\n",
              " -0.014063633047044277,\n",
              " -0.008801157586276531,\n",
              " 0.007049334701150656,\n",
              " -0.016971658915281296,\n",
              " 0.011954438872635365,\n",
              " -0.0005671526305377483,\n",
              " 0.0007646706653758883,\n",
              " 0.0038364918436855078,\n",
              " 0.03018740937113762,\n",
              " -0.012227723374962807,\n",
              " 0.03582127019762993,\n",
              " 0.006719992030411959,\n",
              " -0.02261953614652157,\n",
              " 0.03018740937113762,\n",
              " 0.0257728174328804,\n",
              " -0.02266157977283001,\n",
              " 0.011015461757779121,\n",
              " 0.004891089163720608,\n",
              " 0.030299527570605278,\n",
              " -0.0021845230367034674,\n",
              " 0.000467298727016896,\n",
              " 0.0001825180370360613,\n",
              " 0.004754446912556887,\n",
              " 0.0022475887089967728,\n",
              " -0.026417488232254982,\n",
              " 0.01048290729522705,\n",
              " 0.024805810302495956,\n",
              " -0.00018416036618873477,\n",
              " 0.014435019344091415,\n",
              " 0.005399117711931467,\n",
              " 0.0046878778375685215,\n",
              " 0.0313926637172699,\n",
              " 0.026964055374264717,\n",
              " -0.010595024563372135,\n",
              " 0.007045831065624952,\n",
              " 0.007665976416319609,\n",
              " 0.020741581916809082,\n",
              " -0.009396777488291264,\n",
              " -0.0003492696559987962,\n",
              " -0.006415174808353186,\n",
              " -0.013033561408519745,\n",
              " 0.0011903635459020734,\n",
              " 0.003987148404121399,\n",
              " -0.01329983863979578,\n",
              " 0.02330625057220459,\n",
              " -0.011183636263012886,\n",
              " 0.026305370032787323,\n",
              " -0.004365542437881231,\n",
              " 0.015037646517157555,\n",
              " 0.008632983081042767,\n",
              " 0.01845720410346985,\n",
              " 0.005774007644504309,\n",
              " -0.004155323840677738,\n",
              " -0.02931850589811802,\n",
              " 0.03299032524228096,\n",
              " 0.005917657166719437,\n",
              " -0.026361428201198578,\n",
              " -0.012262759730219841,\n",
              " -0.028043178841471672,\n",
              " 0.01990070752799511,\n",
              " 0.006429189350455999,\n",
              " -0.005907146260142326,\n",
              " 0.010146557353436947,\n",
              " 0.000934597454033792,\n",
              " -0.0036507986951619387,\n",
              " -0.0011474438942968845,\n",
              " 0.009768163785338402,\n",
              " 0.012493999674916267,\n",
              " 0.02439938671886921,\n",
              " 0.0036157621070742607,\n",
              " -0.008576923981308937,\n",
              " -0.005630358587950468,\n",
              " -0.004799994174391031,\n",
              " 0.014673267491161823,\n",
              " -0.003354740561917424,\n",
              " -0.011309768073260784,\n",
              " 0.02856171876192093,\n",
              " -0.013860422186553478,\n",
              " 0.0022423332557082176,\n",
              " 0.0095369229093194,\n",
              " -0.003134010825306177,\n",
              " -0.013096626847982407,\n",
              " 0.008948310278356075,\n",
              " -0.012718233279883862,\n",
              " -0.0064221820794045925,\n",
              " -0.005237950012087822,\n",
              " 0.02535237930715084,\n",
              " 0.00442510424181819,\n",
              " -0.0015748887090012431,\n",
              " 0.010503929108381271,\n",
              " 0.027566682547330856,\n",
              " 0.0076239327900111675,\n",
              " 0.010503929108381271,\n",
              " -0.0280712079256773,\n",
              " -0.0029868579003959894,\n",
              " 0.02466566488146782,\n",
              " 0.05193804204463959,\n",
              " 0.023866834118962288,\n",
              " -0.002522624796256423,\n",
              " -0.007497801445424557,\n",
              " 0.0037418934516608715,\n",
              " 0.008233566768467426,\n",
              " -0.01869545318186283,\n",
              " -0.035428863018751144,\n",
              " -0.013524072244763374,\n",
              " -0.00379795185290277,\n",
              " -0.003847002750262618,\n",
              " -0.005763496737927198,\n",
              " 0.004372549708932638,\n",
              " -0.010272689163684845,\n",
              " 0.01763034425675869,\n",
              " 0.005742474924772978,\n",
              " -0.014421004801988602,\n",
              " 0.00369284232147038,\n",
              " -0.012662175111472607,\n",
              " -0.03094419650733471,\n",
              " 0.009375755675137043,\n",
              " -0.0017027717549353838,\n",
              " 0.02326420694589615,\n",
              " 0.014603194780647755,\n",
              " 0.010069477371871471,\n",
              " -0.025366393849253654,\n",
              " 0.011323782615363598,\n",
              " 0.019242022186517715,\n",
              " 0.017658373340964317,\n",
              " -0.023936906829476357,\n",
              " -0.011393855325877666,\n",
              " 0.007280575577169657,\n",
              " 0.018078811466693878,\n",
              " 0.013657210394740105,\n",
              " -0.01519180741161108,\n",
              " 0.010868308134377003,\n",
              " 0.005171380937099457,\n",
              " -0.0020986837334930897,\n",
              " 0.020110925659537315,\n",
              " -0.025786831974983215,\n",
              " 0.01838713139295578,\n",
              " 0.019284065812826157,\n",
              " 0.006930211093276739,\n",
              " 0.009375755675137043,\n",
              " -0.0012569328537210822,\n",
              " -0.021133989095687866,\n",
              " 0.004141308832913637,\n",
              " 0.03144872188568115,\n",
              " -0.014449033886194229,\n",
              " -0.014196772128343582,\n",
              " 0.018190927803516388,\n",
              " -0.0011220425367355347,\n",
              " -0.015079690143465996,\n",
              " -0.022143039852380753,\n",
              " 0.0036262732464820147,\n",
              " 0.006296051200479269,\n",
              " -0.018218956887722015,\n",
              " 0.006597364787012339,\n",
              " -0.028926096856594086,\n",
              " -0.03929688781499863,\n",
              " 0.0028554711025208235,\n",
              " -0.04089454934000969,\n",
              " 0.002018099883571267,\n",
              " -0.018527276813983917,\n",
              " -0.010524950921535492,\n",
              " -0.015233851037919521,\n",
              " -6.618605402763933e-05,\n",
              " 0.005644373130053282,\n",
              " -0.018625380471348763,\n",
              " -0.0039836447685956955,\n",
              " -0.002988609718158841,\n",
              " -0.003601747564971447,\n",
              " -0.004533717408776283,\n",
              " 0.013748304918408394,\n",
              " 0.03868024796247482,\n",
              " 0.001294597052037716,\n",
              " -0.01011852826923132,\n",
              " -0.009964368306100368,\n",
              " -0.0016195601783692837,\n",
              " 0.010938381776213646,\n",
              " -0.011954438872635365,\n",
              " -0.013341882266104221,\n",
              " 0.006905685178935528,\n",
              " -0.014757354743778706,\n",
              " -0.0035386818926781416,\n",
              " 0.009971375577151775,\n",
              " 0.0024735736660659313,\n",
              " 0.0033529887441545725,\n",
              " 0.009677069261670113,\n",
              " 0.020825669169425964,\n",
              " 0.013860422186553478,\n",
              " -0.009340719319880009,\n",
              " -0.000290364638203755,\n",
              " -0.01921399123966694,\n",
              " 0.01656523533165455,\n",
              " -0.009200572967529297,\n",
              " 0.02145632542669773,\n",
              " -0.0182049423456192,\n",
              " 0.002953573130071163,\n",
              " -0.01883559860289097,\n",
              " -0.03481222316622734,\n",
              " -0.013762319460511208,\n",
              " 0.00019948881526943296,\n",
              " 0.004453133326023817,\n",
              " 0.013054583221673965,\n",
              " -0.0244414322078228,\n",
              " -0.008373713120818138,\n",
              " 0.0357932411134243,\n",
              " -0.009845243766903877,\n",
              " -0.012522028759121895,\n",
              " 0.007897216826677322,\n",
              " 0.006996780168265104,\n",
              " -0.003032405162230134,\n",
              " 0.011674147099256516,\n",
              " -0.013152685016393661,\n",
              " 0.008864223025739193,\n",
              " 0.0037489007227122784,\n",
              " 0.022731652483344078,\n",
              " 0.01127473171800375,\n",
              " -0.024035008624196053,\n",
              " 0.016172828152775764,\n",
              " -0.03750302270054817,\n",
              " 0.009670061990618706,\n",
              " 0.021848734468221664,\n",
              " -0.0038364918436855078,\n",
              " 0.0005448168958537281,\n",
              " -0.011800277978181839,\n",
              " -0.028141280636191368,\n",
              " -0.0007217509555630386,\n",
              " -0.0039591193199157715,\n",
              " 0.0011272979900240898,\n",
              " -0.0004331381933297962,\n",
              " -0.007203495129942894,\n",
              " 0.014505092985928059,\n",
              " -0.028015149757266045,\n",
              " -0.02875792235136032,\n",
              " 3.8266378396656364e-05,\n",
              " 0.0111976508051157,\n",
              " -0.022843768820166588,\n",
              " -0.009747141972184181,\n",
              " -0.014645238406956196,\n",
              " 0.014967573806643486,\n",
              " -0.0005684664938598871,\n",
              " -0.005087293218821287,\n",
              " 0.002307150512933731,\n",
              " -0.03500842675566673,\n",
              " 0.008170501329004765,\n",
              " 0.0039591193199157715,\n",
              " 0.01233283244073391,\n",
              " 0.035849303007125854,\n",
              " 0.006912692449986935,\n",
              " 0.008296632207930088,\n",
              " -0.014098669402301311,\n",
              " -0.015107719227671623,\n",
              " -0.016284944489598274,\n",
              " -0.04013776406645775,\n",
              " -0.01004845555871725,\n",
              " -0.004926125518977642,\n",
              " 0.023222163319587708,\n",
              " 0.004470651503652334,\n",
              " 0.023586541414260864,\n",
              " 0.0010449623223394156,\n",
              " 0.002834449289366603,\n",
              " 0.004533717408776283,\n",
              " -0.0095369229093194,\n",
              " 0.02212902531027794,\n",
              " -0.0018166402587667108,\n",
              " -0.01481341291218996,\n",
              " -0.02130216546356678,\n",
              " 0.028057193383574486,\n",
              " 0.008934295736253262,\n",
              " -0.0030569308437407017,\n",
              " 0.0037839370779693127,\n",
              " -0.019998809322714806,\n",
              " -0.012592102400958538,\n",
              " 0.016032682731747627,\n",
              " -0.002953573130071163,\n",
              " -0.0006424810271710157,\n",
              " 0.011982467956840992,\n",
              " -0.023866834118962288,\n",
              " -0.01530392374843359,\n",
              " 0.0025489020626991987,\n",
              " -0.017223920673131943,\n",
              " -0.004989191424101591,\n",
              " -0.02512814663350582,\n",
              " -0.012970495969057083,\n",
              " 0.044061847031116486,\n",
              " -0.0025559093337506056,\n",
              " 0.006078824866563082,\n",
              " 0.007546852342784405,\n",
              " 0.02046128921210766,\n",
              " -0.01246597059071064,\n",
              " 0.0036648132372647524,\n",
              " -0.012360861524939537,\n",
              " -0.013439984060823917,\n",
              " -0.019648443907499313,\n",
              " -0.015388011001050472,\n",
              " -0.012108598835766315,\n",
              " -0.004116783384233713,\n",
              " 0.031000256538391113,\n",
              " 0.01929808035492897,\n",
              " 0.006239992566406727,\n",
              " -0.00940378475934267,\n",
              " 0.003062186297029257,\n",
              " -0.008359698578715324,\n",
              " 0.008408749476075172,\n",
              " -0.012003489769995213,\n",
              " -0.008576923981308937,\n",
              " 0.002475325483828783,\n",
              " -0.006825101561844349,\n",
              " -0.027678798884153366,\n",
              " -0.01074217725545168,\n",
              " -0.02175063081085682,\n",
              " -0.02875792235136032,\n",
              " -0.0036893386859446764,\n",
              " 0.024679679423570633,\n",
              " -0.020138954743742943,\n",
              " 0.011891373433172703,\n",
              " -0.013517064042389393,\n",
              " -0.015388011001050472,\n",
              " -0.012038526125252247,\n",
              " 0.007143933326005936,\n",
              " 0.018373116850852966,\n",
              " 0.0018481729784980416,\n",
              " 0.0200969111174345,\n",
              " 0.011330789886415005,\n",
              " 0.0010633564088493586,\n",
              " -0.014519107528030872,\n",
              " -0.0038189736660569906,\n",
              " -0.0171818770468235,\n",
              " -0.015177792869508266,\n",
              " 0.01898975856602192,\n",
              " 0.007911231368780136,\n",
              " -0.0052589718252420425,\n",
              " 0.009522908367216587,\n",
              " 0.0061103580519557,\n",
              " 0.018176913261413574,\n",
              " -0.0017745965160429478,\n",
              " -0.03980141505599022,\n",
              " 0.004985687788575888,\n",
              " 0.014519107528030872,\n",
              " -0.0013699254486709833,\n",
              " -0.012949474155902863,\n",
              " -0.006145394407212734,\n",
              " -0.040950607508420944,\n",
              " 0.02973894402384758,\n",
              " -0.003458098042756319,\n",
              " -0.004291965626180172,\n",
              " 0.0040467106737196445,\n",
              " 0.00664991931989789,\n",
              " -0.02898215502500534,\n",
              " 0.021694572642445564,\n",
              " -0.0182049423456192,\n",
              " 0.004743936005979776,\n",
              " 0.0044075860641896725,\n",
              " -0.010545972734689713,\n",
              " -0.012837356887757778,\n",
              " 0.010987432673573494,\n",
              " -0.017055746167898178,\n",
              " 0.015149763785302639,\n",
              " -0.007960282266139984,\n",
              " 0.04787381365895271,\n",
              " -0.0200969111174345,\n",
              " 0.005830066278576851,\n",
              " -0.004670359659940004,\n",
              " 0.010910351760685444,\n",
              " -0.01545808371156454,\n",
              " -0.00422890018671751,\n",
              " -0.010637068189680576,\n",
              " 0.022955885156989098,\n",
              " -0.03189718723297119,\n",
              " -0.015920564532279968,\n",
              " 0.0009801448322832584,\n",
              " 0.0006578094325959682,\n",
              " 0.0014356187311932445,\n",
              " -0.0032759085297584534,\n",
              " -0.018709467723965645,\n",
              " -0.008184515871107578,\n",
              " -0.026501575484871864,\n",
              " -0.01702771708369255,\n",
              " 0.008773128502070904,\n",
              " 0.005847584456205368,\n",
              " -0.0031550328712910414,\n",
              " -0.010812249965965748,\n",
              " -0.017868591472506523,\n",
              " -0.02455354854464531,\n",
              " -0.007182473316788673,\n",
              " -0.005045249592512846,\n",
              " -0.02172260172665119,\n",
              " -0.019998809322714806,\n",
              " -0.028701864182949066,\n",
              " 0.0014689033851027489,\n",
              " 0.0013699254486709833,\n",
              " 0.017952680587768555,\n",
              " 0.01063006091862917,\n",
              " -0.017308009788393974,\n",
              " 0.023011943325400352,\n",
              " 0.04361337795853615,\n",
              " -0.021470339968800545,\n",
              " 0.010861300863325596,\n",
              " -0.0317009836435318,\n",
              " 0.009207580238580704,\n",
              " -0.015654288232326508,\n",
              " 0.030019234865903854,\n",
              " -0.0066534229554235935,\n",
              " -0.019017787650227547,\n",
              " 0.025338364765048027,\n",
              " -0.015388011001050472,\n",
              " -0.014140713028609753,\n",
              " -0.00740670645609498,\n",
              " -0.0107631990686059,\n",
              " 0.011849328875541687,\n",
              " 0.015247865580022335,\n",
              " -0.0004743060271721333,\n",
              " -0.0010791228851303458,\n",
              " 0.017336038872599602,\n",
              " -0.004242914728820324,\n",
              " -0.0012297795619815588,\n",
              " -0.008387727662920952,\n",
              " 0.026515590026974678,\n",
              " 0.002116201911121607,\n",
              " 0.0044321115128695965,\n",
              " 0.016579249873757362,\n",
              " -0.024637635797262192,\n",
              " 0.014322903007268906,\n",
              " 0.013054583221673965,\n",
              " 0.012921445071697235,\n",
              " 0.015486113727092743,\n",
              " -0.023502454161643982,\n",
              " -0.02595500648021698,\n",
              " -0.01642508991062641,\n",
              " 0.004481162875890732,\n",
              " -0.01751822791993618,\n",
              " -0.015331952832639217,\n",
              " -0.006923203822225332,\n",
              " 0.00015963484474923462,\n",
              " -0.0006543057970702648,\n",
              " 0.0225494634360075,\n",
              " 0.004155323840677738,\n",
              " 0.005612839944660664,\n",
              " -0.010293710976839066,\n",
              " 0.0014128451002761722,\n",
              " -0.015948593616485596,\n",
              " 0.007799115031957626,\n",
              " -0.015331952832639217,\n",
              " 0.003864520927891135,\n",
              " 0.00420787837356329,\n",
              " -0.0021477346308529377,\n",
              " -0.02954273857176304,\n",
              " -0.031196460127830505,\n",
              " -0.01785457693040371,\n",
              " 0.031532809138298035,\n",
              " 0.00016138667706400156,\n",
              " -0.012479985132813454,\n",
              " -0.019802603870630264,\n",
              " 0.01057400181889534,\n",
              " -0.048518482595682144,\n",
              " -0.003212842857465148,\n",
              " 0.007336633745580912,\n",
              " -0.001516202581115067,\n",
              " 0.020517347380518913,\n",
              " 0.007869187742471695,\n",
              " -0.013811370357871056,\n",
              " 0.031532809138298035,\n",
              " -0.004186856560409069,\n",
              " 0.007665976416319609,\n",
              " -0.0063240802846848965,\n",
              " -0.01044086366891861,\n",
              " -0.009501886554062366,\n",
              " -0.019003773108124733,\n",
              " 0.0035211637150496244,\n",
              " 0.022409316152334213,\n",
              " -0.020293114706873894,\n",
              " -0.005742474924772978,\n",
              " 0.015766404569149017,\n",
              " 0.009508893825113773,\n",
              " -0.0031725510489195585,\n",
              " 0.009340719319880009,\n",
              " -0.017392097041010857,\n",
              " 0.0048315273597836494,\n",
              " -0.016775455325841904,\n",
              " 0.02493194118142128,\n",
              " -0.01437896117568016,\n",
              " 0.01838713139295578,\n",
              " 0.006912692449986935,\n",
              " -0.02107793092727661,\n",
              " -0.0128864087164402,\n",
              " 0.003829484572634101,\n",
              " -0.0007108020945452154,\n",
              " -0.014519107528030872,\n",
              " 0.0028169311117380857,\n",
              " 0.0003558390017133206,\n",
              " 0.018513262271881104,\n",
              " 0.02012494020164013,\n",
              " -0.020573407411575317,\n",
              " -0.015261880122125149,\n",
              " -0.01246597059071064,\n",
              " -0.0030341569799929857,\n",
              " 0.014603194780647755,\n",
              " -0.009557944722473621,\n",
              " -0.017728446051478386,\n",
              " 0.030131351202726364,\n",
              " 0.0010440864134579897,\n",
              " 0.002611967734992504,\n",
              " -0.006947729270905256,\n",
              " 0.014063633047044277,\n",
              " -0.017392097041010857,\n",
              " -0.009698091074824333,\n",
              " 0.006646415684372187,\n",
              " -0.01941019669175148,\n",
              " -0.017083775252103806,\n",
              " -0.004589775577187538,\n",
              " 0.0077430568635463715,\n",
              " -0.017279980704188347,\n",
              " 0.01048290729522705,\n",
              " 0.012094584293663502,\n",
              " -0.01329983863979578,\n",
              " 0.012451956048607826,\n",
              " 0.005167877301573753,\n",
              " 0.02270362339913845,\n",
              " -0.014967573806643486,\n",
              " -0.0256186556071043,\n",
              " -0.007231524214148521,\n",
              " -0.0005461307591758668,\n",
              " -0.012269767001271248,\n",
              " 0.012101591564714909,\n",
              " 0.002929047681391239,\n",
              " -0.009284661151468754,\n",
              " -0.008808164857327938,\n",
              " 0.0205033328384161,\n",
              " -0.01680348441004753,\n",
              " 0.0038014554884284735,\n",
              " 0.016355017200112343,\n",
              " 0.007469772361218929,\n",
              " -0.033158499747514725,\n",
              " 0.027412522584199905,\n",
              " 0.24284468591213226,\n",
              " -0.015247865580022335,\n",
              " -0.0021722603123635054,\n",
              " 0.029066244140267372,\n",
              " -0.009810207411646843,\n",
              " -0.002755617257207632,\n",
              " 0.012900423258543015,\n",
              " -0.0009433565428480506,\n",
              " -0.007094882428646088,\n",
              " -0.011288746260106564,\n",
              " -0.0020163480658084154,\n",
              " -0.007112400606274605,\n",
              " 0.013019546866416931,\n",
              " -0.00359474029392004,\n",
              " 0.014280859380960464,\n",
              " -0.010966410860419273,\n",
              " -0.004582768306136131,\n",
              " -0.019003773108124733,\n",
              " -0.0028729892801493406,\n",
              " -0.021918807178735733,\n",
              " -0.009095463901758194,\n",
              " -0.005819555371999741,\n",
              " -0.03287820890545845,\n",
              " -0.011281738989055157,\n",
              " 0.03803557530045509,\n",
              " -0.003109485376626253,\n",
              " -0.006607875693589449,\n",
              " 0.01891968585550785,\n",
              " 0.01994275115430355,\n",
              " 0.0009740134701132774,\n",
              " -0.012718233279883862,\n",
              " -0.021063916385173798,\n",
              " 0.01918596215546131,\n",
              " 0.013909473083913326,\n",
              " -0.019816618412733078,\n",
              " 0.004887585528194904,\n",
              " -0.009852251037955284,\n",
              " 0.0009328456362709403,\n",
              " 0.014505092985928059,\n",
              " 0.017069760710000992,\n",
              " 0.022577492520213127,\n",
              " 0.009691083803772926,\n",
              " 0.001960289664566517,\n",
              " -0.025520553812384605,\n",
              " -0.00037664189585484564,\n",
              " 0.006790065206587315,\n",
              " ...]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'the quick brown fox jumped over the lazy dog'\n",
        "model= os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT']\n",
        "client.embeddings.create(input='[text]', model=model).data[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1674829555255
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.915739416418029\n",
            "0.8329017718558467\n",
            "0.7818148774886836\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# compare several words\n",
        "automobile_embedding  = client.embeddings.create(input='automobile', model=model).data[0].embedding\n",
        "vehicle_embedding     = client.embeddings.create(input='vehicle', model=model).data[0].embedding\n",
        "dinosaur_embedding    = client.embeddings.create(input='dinosaur', model=model).data[0].embedding\n",
        "stick_embedding       = client.embeddings.create(input='stick', model=model).data[0].embedding\n",
        "\n",
        "print(cosine_similarity(automobile_embedding, vehicle_embedding))\n",
        "print(cosine_similarity(automobile_embedding, dinosaur_embedding))\n",
        "print(cosine_similarity(automobile_embedding, stick_embedding))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Comparing article from cnn daily news dataset\n",
        "source: https://huggingface.co/datasets/cnn_dailymail\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1674831122093
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>articles</th>\n",
              "      <th>highligths</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BREMEN, Germany -- Carlos Alberto, who scored ...</td>\n",
              "      <td>Werder Bremen pay a club record $10.7 million ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(CNN) -- Football superstar, celebrity, fashio...</td>\n",
              "      <td>Beckham has agreed to a five-year contract wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LOS ANGELES, California (CNN) -- Youssif, the ...</td>\n",
              "      <td>Boy on meeting Spider-Man: \"It was my favorite...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            articles  \\\n",
              "0  BREMEN, Germany -- Carlos Alberto, who scored ...   \n",
              "1  (CNN) -- Football superstar, celebrity, fashio...   \n",
              "2  LOS ANGELES, California (CNN) -- Youssif, the ...   \n",
              "\n",
              "                                          highligths  \n",
              "0  Werder Bremen pay a club record $10.7 million ...  \n",
              "1  Beckham has agreed to a five-year contract wit...  \n",
              "2  Boy on meeting Spider-Man: \"It was my favorite...  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "cnn_daily_articles = ['BREMEN, Germany -- Carlos Alberto, who scored in FC Porto\\'s Champions League final victory against Monaco in 2004, has joined Bundesliga club Werder Bremen for a club record fee of 7.8 million euros ($10.7 million). Carlos Alberto enjoyed success at FC Porto under Jose Mourinho. \"I\\'m here to win titles with Werder,\" the 22-year-old said after his first training session with his new club. \"I like Bremen and would only have wanted to come here.\" Carlos Alberto started his career with Fluminense, and helped them to lift the Campeonato Carioca in 2002. In January 2004 he moved on to FC Porto, who were coached by José Mourinho, and the club won the Portuguese title as well as the Champions League. Early in 2005, he moved to Corinthians, where he impressed as they won the Brasileirão,but in 2006 Corinthians had a poor season and Carlos Alberto found himself at odds with manager, Emerson Leão. Their poor relationship came to a climax at a Copa Sul-Americana game against Club Atlético Lanús, and Carlos Alberto declared that he would not play for Corinthians again while Leão remained as manager. Since January this year he has been on loan with his first club Fluminense. Bundesliga champions VfB Stuttgart said on Sunday that they would sign a loan agreement with Real Zaragoza on Monday for Ewerthon, the third top Brazilian player to join the German league in three days. A VfB spokesman said Ewerthon, who played in the Bundesliga for Borussia Dortmund from 2001 to 2005, was expected to join the club for their pre-season training in Austria on Monday. On Friday, Ailton returned to Germany where he was the league\\'s top scorer in 2004, signing a one-year deal with Duisburg on a transfer from Red Star Belgrade. E-mail to a friend .',\n",
        "                        '(CNN) -- Football superstar, celebrity, fashion icon, multimillion-dollar heartthrob. Now, David Beckham is headed for the Hollywood Hills as he takes his game to U.S. Major League Soccer. CNN looks at how Bekham fulfilled his dream of playing for Manchester United, and his time playing for England. The world\\'s famous footballer has begun a five-year contract with the Los Angeles Galaxy team, and on Friday Beckham will meet the press and reveal his new shirt number. This week, we take an in depth look at the life and times of Beckham, as CNN\\'s very own \"Becks,\" Becky Anderson, sets out to examine what makes the man tick -- as footballer, fashion icon and global phenomenon. It\\'s a long way from the streets of east London to the Hollywood Hills and Becky charts Beckham\\'s incredible rise to football stardom, a journey that has seen his skills grace the greatest stages in world soccer. She goes in pursuit of the current hottest property on the sports/celebrity circuit in the U.S. and along the way explores exactly what\\'s behind the man with the golden boot. CNN will look back at the life of Beckham, the wonderfully talented youngster who fulfilled his dream of playing for Manchester United, his marriage to pop star Victoria, and the trials and tribulations of playing for England. We\\'ll look at the highs (scoring against Greece), the lows (being sent off during the World Cup), the Man. U departure for the Galacticos of Madrid -- and now the Home Depot stadium in L.A. We\\'ll ask how Beckham and his family will adapt to life in Los Angeles -- the people, the places to see and be seen and the celebrity endorsement. Beckham is no stranger to exposure. He has teamed with Reggie Bush in an Adidas commercial, is the face of Motorola, is the face on a PlayStation game and doesn\\'t need fashion tips as he has his own international clothing line. But what does the star couple need to do to become an accepted part of Tinseltown\\'s glitterati? The road to major league football in the U.S.A. is a well-worn route for some of the world\\'s greatest players. We talk to some of the former greats who came before him and examine what impact these overseas stars had on U.S. soccer and look at what is different now. We also get a rare glimpse inside the David Beckham academy in L.A, find out what drives the kids and who are their heroes. The perception that in the U.S.A. soccer is a \"game for girls\" after the teenage years is changing. More and more young kids are choosing the European game over the traditional U.S. sports. E-mail to a friend .',\n",
        "                        'LOS ANGELES, California (CNN) -- Youssif, the 5-year-old burned Iraqi boy, rounded the corner at Universal Studios when suddenly the little boy hero met his favorite superhero. Youssif has always been a huge Spider-Man fan. Meeting him was \"my favorite thing,\" he said. Spider-Man was right smack dab in front of him, riding a four-wheeler amid a convoy of other superheroes. The legendary climber of buildings and fighter of evil dismounted, walked over to Youssif and introduced himself. Spidey then gave the boy from a far-away land a gentle hug, embracing him in his iconic blue and red tights. He showed Youssif a few tricks, like how to shoot a web from his wrist. Only this time, no web was spun. \"All right Youssif!\" Spider-Man said after the boy mimicked his wrist movement. Other superheroes crowded around to get a closer look. Even the Green Goblin stopped his villainous ways to tell the boy hi. Youssif remained unfazed. He didn\\'t take a liking to Spider-Man\\'s nemesis. Spidey was just too cool. \"It was my favorite thing,\" the boy said later. \"I want to see him again.\" He then felt compelled to add: \"I know it\\'s not the real Spider-Man.\" This was the day of dreams when the boy\\'s nightmares were, at least temporarily, forgotten. He met SpongeBob, Lassie and a 3-year-old orangutan named Archie. The hairy, brownish-red primate took to the boy, grabbing his hand and holding it. Even when Youssif pulled away, Archie would inch his hand back toward the boy\\'s and then snatch it. See Youssif enjoy being a boy again » . The boy giggled inside a play area where sponge-like balls shot out of toy guns. It was a far different artillery than what he was used to seeing in central Baghdad, as recently as a week ago. He squealed with delight and raced around the room collecting as many balls as he could. He rode a tram through the back stages at Universal Studios. At one point, the car shook. Fire and smoke filled the air, debris cascaded down and a big rig skidded toward the vehicle. The boy and his family survived the pretend earthquake unscathed. \"Even I was scared,\" the dad said. \"Well, I wasn\\'t,\" Youssif replied. The father and mother grinned from ear to ear throughout the day. Youssif pushed his 14-month-old sister, Ayaa, in a stroller. \"Did you even need to ask us if we were interested in coming here?\" Youssif\\'s father said in amazement. \"Other than my wedding day, this is the happiest day of my life,\" he said. Just a day earlier, the mother and father talked about their journey out of Iraq and to the United States. They also discussed that day nine months ago when masked men grabbed their son outside the family home, doused him in gas and set him on fire. His mother heard her boy screaming from inside. The father sought help for his boy across Baghdad, but no one listened. He remembers his son\\'s two months of hospitalization. The doctors didn\\'t use anesthetics. He could hear his boy\\'s piercing screams from the other side of the hospital. Watch Youssif meet his doctor and play with his little sister » . The father knew that speaking to CNN would put his family\\'s lives in jeopardy. The possibility of being killed was better than seeing his son suffer, he said. \"Anything for Youssif,\" he said. \"We had to do it.\" They described a life of utter chaos in Baghdad. Neighbors had recently given birth to a baby girl. Shortly afterward, the father was kidnapped and killed. Then, there was the time when some girls wore tanktops and jeans. They were snatched off the street by gunmen. The stories can be even more gruesome. The couple said they had heard reports that a young girl was kidnapped and beheaded --and her killers sewed a dog\\'s head on the corpse and delivered it to her family\\'s doorstep. \"These are just some of the stories,\" said Youssif\\'s mother, Zainab. Under Saddam Hussein, there was more security and stability, they said. There was running water and electricity most of the time. But still life was tough under the dictator, like the time when Zainab\\'s uncle disappeared and was never heard from again after he read a \"religious book,\" she said. Sitting in the parking lot of a Target in suburban Los Angeles, Youssif\\'s father watched as husbands and wives, boyfriends and girlfriends, parents and their children, came and went. Some held hands. Others smiled and laughed. \"Iraq finished,\" he said in what few English words he knows. He elaborated in Arabic: His homeland won\\'t be enjoying such freedoms anytime soon. It\\'s just not possible. Too much violence. Too many killings. His two children have only seen war. But this week, the family has seen a much different side of America -- an outpouring of generosity and a peaceful nation at home. \"It\\'s been a dream,\" the father said. He used to do a lot of volunteer work back in Baghdad. \"Maybe that\\'s why I\\'m being helped now,\" the father said. At Universal Studios, he looked out across the valley below. The sun glistened off treetops and buildings. It was a picturesque sight fit for a Hollywood movie. \"Good America, good America,\" he said in English. E-mail to a friend . CNN\\'s Arwa Damon contributed to this report.'\n",
        "]\n",
        "\n",
        "cnn_daily_article_highlights = ['Werder Bremen pay a club record $10.7 million for Carlos Alberto .\\nThe Brazilian midfielder won the Champions League with FC Porto in 2004 .\\nSince January he has been on loan with his first club, Fluminense .',\n",
        "                                'Beckham has agreed to a five-year contract with Los Angeles Galaxy .\\nNew contract took effect July 1, 2007 .\\nFormer English captain to meet press, unveil new shirt number Friday .\\nCNN to look at Beckham as footballer, fashion icon and global phenomenon .',\n",
        "                                'Boy on meeting Spider-Man: \"It was my favorite thing\"\\nYoussif also met SpongeBob, Lassie and an orangutan at Universal Studios .\\nDad: \"Other than my wedding day, this is the happiest day of my life\"'\n",
        "]\n",
        "\n",
        "cnn_df = pd.DataFrame({\"articles\":cnn_daily_articles, \"highligths\":cnn_daily_article_highlights})\n",
        "\n",
        "cnn_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1674831294043
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7616082310391402\n",
            "0.7092380322191737\n"
          ]
        }
      ],
      "source": [
        "article1_embedding    = client.embeddings.create(input=cnn_df.articles.iloc[0], model=model).data[0].embedding\n",
        "article2_embedding    = client.embeddings.create(input=cnn_df.articles.iloc[1], model=model).data[0].embedding\n",
        "article3_embedding    = client.embeddings.create(input=cnn_df.articles.iloc[2], model=model).data[0].embedding\n",
        "\n",
        "print(cosine_similarity(article1_embedding, article2_embedding))\n",
        "print(cosine_similarity(article1_embedding, article3_embedding))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# References  \n",
        "- [Azure Documentation - Azure OpenAI Models](https://learn.microsoft.com/azure/cognitive-services/openai/concepts/models?WT.mc_id=academic-105485-koreyst)  \n",
        "- [OpenAI Studio Examples](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# For More Help  \n",
        "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Contributors\n",
        "* Louis Li  \n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
