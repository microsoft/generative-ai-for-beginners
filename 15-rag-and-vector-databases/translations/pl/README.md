# Retrieval Augmented Generation (RAG) i Bazy Danych Wektorowe

[![Retrieval Augmented Generation (RAG) i Bazy Danych Wektorowe](../../images/15-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

W lekcji dotyczÄ…cej aplikacji wyszukiwania krÃ³tko nauczyliÅ›my siÄ™, jak integrowaÄ‡ wÅ‚asne dane z DuÅ¼ymi Modelami JÄ™zykowymi (LLM). W tej lekcji zagÅ‚Ä™bimy siÄ™ w koncepcje osadzania danych w aplikacji LLM, mechanizmy tego procesu oraz metody przechowywania danych, w tym zarÃ³wno embeddingÃ³w, jak i tekstu.

> **Wideo wkrÃ³tce**

## Wprowadzenie

W tej lekcji omÃ³wimy nastÄ™pujÄ…ce zagadnienia:

- Wprowadzenie do RAG, czym jest i dlaczego jest uÅ¼ywane w AI (sztucznej inteligencji).

- Zrozumienie, czym sÄ… bazy danych wektorowe i stworzenie jednej dla naszej aplikacji.

- Praktyczny przykÅ‚ad integracji RAG w aplikacji.

## Cele Nauki

Po ukoÅ„czeniu tej lekcji bÄ™dziesz potrafiÅ‚:

- WyjaÅ›niÄ‡ znaczenie RAG w odzyskiwaniu i przetwarzaniu danych.

- SkonfigurowaÄ‡ aplikacjÄ™ RAG i osadziÄ‡ dane w LLM.

- Efektywnie integrowaÄ‡ RAG i Bazy Danych Wektorowe w Aplikacjach LLM.

## Nasz Scenariusz: wzbogacanie naszych LLM o wÅ‚asne dane

W tej lekcji chcemy dodaÄ‡ wÅ‚asne notatki do startupu edukacyjnego, co pozwoli chatbotowi uzyskaÄ‡ wiÄ™cej informacji na rÃ³Å¼ne tematy. KorzystajÄ…c z naszych notatek, uczniowie bÄ™dÄ… mogli lepiej siÄ™ uczyÄ‡ i rozumieÄ‡ rÃ³Å¼ne tematy, co uÅ‚atwi powtÃ³rki do egzaminÃ³w. Aby stworzyÄ‡ nasz scenariusz, uÅ¼yjemy:

- `Azure OpenAI:` LLM, ktÃ³rego uÅ¼yjemy do stworzenia naszego chatbota

- `Lekcja AI dla poczÄ…tkujÄ…cych o Sieciach Neuronowych:` to bÄ™dÄ… dane, na ktÃ³rych osadzimy nasz LLM

- `Azure AI Search` i `Azure Cosmos DB:` baza danych wektorowa do przechowywania naszych danych i tworzenia indeksu wyszukiwania

UÅ¼ytkownicy bÄ™dÄ… mogli tworzyÄ‡ quizy z wÅ‚asnych notatek, fiszki do powtÃ³rek i podsumowywaÄ‡ je do zwiÄ™zÅ‚ych przeglÄ…dÃ³w. Aby zaczÄ…Ä‡, przyjrzyjmy siÄ™, czym jest RAG i jak dziaÅ‚a:

## Retrieval Augmented Generation (RAG)

Chatbot zasilany przez LLM przetwarza prompty uÅ¼ytkownika, aby generowaÄ‡ odpowiedzi. Jest zaprojektowany tak, aby byÅ‚ interaktywny i angaÅ¼owaÅ‚ uÅ¼ytkownikÃ³w w szeroki zakres tematÃ³w. Jednak jego odpowiedzi sÄ… ograniczone do dostarczonego kontekstu i danych treningowych. Na przykÅ‚ad, wiedza GPT-4 koÅ„czy siÄ™ we wrzeÅ›niu 2021 roku, co oznacza, Å¼e brakuje mu wiedzy o wydarzeniach, ktÃ³re miaÅ‚y miejsce po tym okresie. Ponadto, dane uÅ¼yte do treningu LLM wykluczajÄ… informacje poufne, takie jak osobiste notatki czy podrÄ™cznik produktu firmy.

### Jak dziaÅ‚ajÄ… RAG (Retrieval Augmented Generation)

![rysunek pokazujÄ…cy jak dziaÅ‚ajÄ… RAG](../../images/how-rag-works.png?WT.mc_id=academic-105485-koreyst)

ZaÅ‚Ã³Å¼my, Å¼e chcesz wdroÅ¼yÄ‡ chatbota, ktÃ³ry tworzy quizy z Twoich notatek, bÄ™dziesz potrzebowaÄ‡ poÅ‚Ä…czenia z bazÄ… wiedzy. Tutaj z pomocÄ… przychodzi RAG. RAG dziaÅ‚ajÄ… w nastÄ™pujÄ…cy sposÃ³b:

- **Baza wiedzy:** Przed odzyskiwaniem te dokumenty muszÄ… zostaÄ‡ przetworzone i przetworzone wstÄ™pnie, zazwyczaj dzielÄ…c duÅ¼e dokumenty na mniejsze czÄ™Å›ci, przeksztaÅ‚cajÄ…c je w embeddingi tekstowe i przechowujÄ…c je w bazie danych.

- **Zapytanie uÅ¼ytkownika:** uÅ¼ytkownik zadaje pytanie

- **Odzyskiwanie:** Gdy uÅ¼ytkownik zadaje pytanie, model embeddingu pobiera istotne informacje z naszej bazy wiedzy, aby zapewniÄ‡ wiÄ™cej kontekstu, ktÃ³ry zostanie wÅ‚Ä…czony do promptu.

- **Generowanie wzbogacone:** LLM wzbogaca swojÄ… odpowiedÅº na podstawie pobranych danych. Pozwala to na generowanie odpowiedzi nie tylko na podstawie wstÄ™pnie wytrenowanych danych, ale takÅ¼e istotnych informacji z dodanego kontekstu. Pobrane dane sÄ… uÅ¼ywane do wzbogacania odpowiedzi LLM. NastÄ™pnie LLM zwraca odpowiedÅº na pytanie uÅ¼ytkownika.

![rysunek pokazujÄ…cy architekturÄ™ RAG](../../images/encoder-decode.png?WT.mc_id=academic-105485-koreyst)

Architektura RAG jest implementowana przy uÅ¼yciu transformatorÃ³w skÅ‚adajÄ…cych siÄ™ z dwÃ³ch czÄ™Å›ci: kodera i dekodera. Na przykÅ‚ad, gdy uÅ¼ytkownik zadaje pytanie, tekst wejÅ›ciowy jest â€kodowanyâ€ w wektory przechwytujÄ…ce znaczenie sÅ‚Ã³w, a wektory sÄ… â€dekodowaneâ€ w naszym indeksie dokumentÃ³w i generujÄ… nowy tekst na podstawie zapytania uÅ¼ytkownika. LLM uÅ¼ywa zarÃ³wno modelu koder-dekoder do generowania wyniku.

Dwa podejÅ›cia przy wdraÅ¼aniu RAG zgodnie z proponowanym artykuÅ‚em: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) to:

- **_RAG-Sequence_** uÅ¼ywanie pobranych dokumentÃ³w do przewidywania najlepszej moÅ¼liwej odpowiedzi na zapytanie uÅ¼ytkownika

- **RAG-Token** uÅ¼ywanie dokumentÃ³w do generowania nastÄ™pnego tokena, a nastÄ™pnie pobieranie ich w celu odpowiedzi na zapytanie uÅ¼ytkownika

### Dlaczego warto uÅ¼ywaÄ‡ RAG?

- **Bogactwo informacji:** zapewnia, Å¼e odpowiedzi tekstowe sÄ… aktualne. W zwiÄ…zku z tym zwiÄ™ksza wydajnoÅ›Ä‡ w zadaniach specyficznych dla domeny poprzez dostÄ™p do wewnÄ™trznej bazy wiedzy.

- Zmniejsza fabrykacjÄ™ poprzez wykorzystanie **weryfikowalnych danych** w bazie wiedzy do zapewnienia kontekstu dla zapytaÅ„ uÅ¼ytkownikÃ³w.

- Jest **opÅ‚acalny**, poniewaÅ¼ jest bardziej ekonomiczny w porÃ³wnaniu do dostrajania LLM

## Tworzenie bazy wiedzy

Nasza aplikacja opiera siÄ™ na naszych danych osobowych, tj. lekcji Sieci Neuronowych z programu AI Dla PoczÄ…tkujÄ…cych.

### Bazy Danych Wektorowe

Baza danych wektorowa, w przeciwieÅ„stwie do tradycyjnych baz danych, jest specjalistycznÄ… bazÄ… danych zaprojektowanÄ… do przechowywania, zarzÄ…dzania i przeszukiwania osadzonych wektorÃ³w. Przechowuje numeryczne reprezentacje dokumentÃ³w. PodziaÅ‚ danych na numeryczne embeddingi uÅ‚atwia naszemu systemowi AI zrozumienie i przetwarzanie danych.

Przechowujemy nasze embeddingi w bazach danych wektorowych, poniewaÅ¼ LLM majÄ… limit liczby tokenÃ³w, ktÃ³re akceptujÄ… jako dane wejÅ›ciowe. PoniewaÅ¼ nie moÅ¼na przekazaÄ‡ caÅ‚ych embeddingÃ³w do LLM, bÄ™dziemy musieli podzieliÄ‡ je na fragmenty, a gdy uÅ¼ytkownik zada pytanie, embeddingi najbardziej podobne do pytania zostanÄ… zwrÃ³cone wraz z promptem. PodziaÅ‚ na fragmenty zmniejsza rÃ³wnieÅ¼ koszty zwiÄ…zane z liczbÄ… tokenÃ³w przekazywanych przez LLM.

NiektÃ³re popularne bazy danych wektorowe to Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant i DeepLake. MoÅ¼esz utworzyÄ‡ model Azure Cosmos DB za pomocÄ… Azure CLI za pomocÄ… nastÄ™pujÄ…cego polecenia:

```bash
az login
az group create -n <nazwa-grupy-zasobÃ³w> -l <lokalizacja>
az cosmosdb create -n <nazwa-cosmos-db> -r <nazwa-grupy-zasobÃ³w>
az cosmosdb list-keys -n <nazwa-cosmos-db> -g <nazwa-grupy-zasobÃ³w>
```

### Od tekstu do embeddingÃ³w

Zanim zapiszemy nasze dane, bÄ™dziemy musieli przekonwertowaÄ‡ je na embeddingi wektorowe, zanim zostanÄ… zapisane w bazie danych. JeÅ›li pracujesz z duÅ¼ymi dokumentami lub dÅ‚ugimi tekstami, moÅ¼esz je podzieliÄ‡ na fragmenty w oparciu o oczekiwane zapytania. PodziaÅ‚ na fragmenty moÅ¼na wykonaÄ‡ na poziomie zdania lub akapitu. PoniewaÅ¼ podziaÅ‚ na fragmenty czerpie znaczenie ze sÅ‚Ã³w wokÃ³Å‚ nich, moÅ¼esz dodaÄ‡ inny kontekst do fragmentu, na przykÅ‚ad dodajÄ…c tytuÅ‚ dokumentu lub doÅ‚Ä…czajÄ…c tekst przed lub po fragmencie. MoÅ¼esz podzieliÄ‡ dane w nastÄ™pujÄ…cy sposÃ³b:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # JeÅ›li ostatni fragment nie osiÄ…gnÄ…Å‚ minimalnej dÅ‚ugoÅ›ci, dodaj go mimo wszystko
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Po podziale na fragmenty moÅ¼emy nastÄ™pnie osadziÄ‡ nasz tekst za pomocÄ… rÃ³Å¼nych modeli embeddingu. NiektÃ³re modele, ktÃ³rych moÅ¼esz uÅ¼yÄ‡, to: word2vec, ada-002 firmy OpenAI, Azure Computer Vision i wiele innych. WybÃ³r modelu do uÅ¼ycia bÄ™dzie zaleÅ¼aÅ‚ od jÄ™zykÃ³w, ktÃ³rych uÅ¼ywasz, rodzaju kodowanej treÅ›ci (tekst/obrazy/audio), rozmiaru danych wejÅ›ciowych, ktÃ³re moÅ¼e zakodowaÄ‡ i dÅ‚ugoÅ›ci wyniku embeddingu.

PrzykÅ‚ad osadzonego tekstu przy uÅ¼yciu modelu `text-embedding-ada-002` firmy OpenAI:
![embedding sÅ‚owa kot](../../images/cat.png?WT.mc_id=academic-105485-koreyst)

## Odzyskiwanie i Wyszukiwanie Wektorowe

Gdy uÅ¼ytkownik zadaje pytanie, retriever przeksztaÅ‚ca je w wektor za pomocÄ… kodera zapytaÅ„, a nastÄ™pnie przeszukuje nasz indeks wyszukiwania dokumentÃ³w w poszukiwaniu odpowiednich wektorÃ³w w dokumencie, ktÃ³re sÄ… powiÄ…zane z danymi wejÅ›ciowymi. Po zakoÅ„czeniu konwertuje zarÃ³wno wektor wejÅ›ciowy, jak i wektory dokumentÃ³w na tekst i przekazuje je do LLM.

### Odzyskiwanie

Odzyskiwanie ma miejsce, gdy system prÃ³buje szybko znaleÅºÄ‡ dokumenty z indeksu, ktÃ³re speÅ‚niajÄ… kryteria wyszukiwania. Celem retrievera jest uzyskanie dokumentÃ³w, ktÃ³re zostanÄ… wykorzystane do zapewnienia kontekstu i osadzenia LLM na Twoich danych.

Istnieje kilka sposobÃ³w przeprowadzania wyszukiwania w naszej bazie danych, takich jak:

- **Wyszukiwanie sÅ‚Ã³w kluczowych** - uÅ¼ywane do wyszukiwania tekstÃ³w

- **Wyszukiwanie semantyczne** - wykorzystuje semantyczne znaczenie sÅ‚Ã³w

- **Wyszukiwanie wektorowe** - konwertuje dokumenty z tekstu na reprezentacje wektorowe za pomocÄ… modeli embeddingu. Odzyskiwanie odbywa siÄ™ poprzez zapytanie o dokumenty, ktÃ³rych reprezentacje wektorowe sÄ… najbliÅ¼sze pytaniu uÅ¼ytkownika.

- **Hybrydowe** - poÅ‚Ä…czenie wyszukiwania sÅ‚Ã³w kluczowych i wektorowego.

Wyzwanie zwiÄ…zane z odzyskiwaniem pojawia siÄ™, gdy w bazie danych nie ma podobnej odpowiedzi na zapytanie, system zwrÃ³ci wtedy najlepsze dostÄ™pne informacje, jednak moÅ¼na zastosowaÄ‡ taktyki, takie jak ustawienie maksymalnej odlegÅ‚oÅ›ci dla istotnoÅ›ci lub uÅ¼ycie wyszukiwania hybrydowego, ktÃ³re Å‚Ä…czy wyszukiwanie sÅ‚Ã³w kluczowych i wektorowe. W tej lekcji uÅ¼yjemy wyszukiwania hybrydowego, poÅ‚Ä…czenia wyszukiwania wektorowego i sÅ‚Ã³w kluczowych. BÄ™dziemy przechowywaÄ‡ nasze dane w ramce danych z kolumnami zawierajÄ…cymi fragmenty oraz embeddingi.

### PodobieÅ„stwo Wektorowe

Retriever przeszuka bazÄ™ wiedzy w poszukiwaniu embeddingÃ³w, ktÃ³re sÄ… blisko siebie, najbliÅ¼szego sÄ…siada, poniewaÅ¼ sÄ… to teksty podobne. W scenariuszu, gdy uÅ¼ytkownik zadaje zapytanie, jest ono najpierw osadzane, a nastÄ™pnie dopasowywane do podobnych embeddingÃ³w. PowszechnÄ… miarÄ… uÅ¼ywanÄ… do okreÅ›lenia podobieÅ„stwa rÃ³Å¼nych wektorÃ³w jest podobieÅ„stwo kosinusowe, ktÃ³re opiera siÄ™ na kÄ…cie miÄ™dzy dwoma wektorami.

MoÅ¼emy mierzyÄ‡ podobieÅ„stwo za pomocÄ… innych alternatyw, takich jak odlegÅ‚oÅ›Ä‡ euklidesowa, ktÃ³ra jest prostÄ… liniÄ… miÄ™dzy punktami koÅ„cowymi wektorÃ³w, oraz iloczyn skalarny, ktÃ³ry mierzy sumÄ™ iloczynÃ³w odpowiadajÄ…cych sobie elementÃ³w dwÃ³ch wektorÃ³w.

### Indeks wyszukiwania

Podczas odzyskiwania bÄ™dziemy musieli zbudowaÄ‡ indeks wyszukiwania dla naszej bazy wiedzy przed wykonaniem wyszukiwania. Indeks bÄ™dzie przechowywaÄ‡ nasze embeddingi i moÅ¼e szybko odzyskaÄ‡ najbardziej podobne fragmenty nawet w duÅ¼ej bazie danych. MoÅ¼emy stworzyÄ‡ nasz indeks lokalnie za pomocÄ…:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# UtwÃ³rz indeks wyszukiwania
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Aby zapytaÄ‡ indeks, moÅ¼esz uÅ¼yÄ‡ metody kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```

### Ponowne rangowanie

Po zapytaniu bazy danych moÅ¼e byÄ‡ konieczne posortowanie wynikÃ³w od najbardziej istotnych. LLM do ponownego rangowania wykorzystuje uczenie maszynowe do poprawy trafnoÅ›ci wynikÃ³w wyszukiwania poprzez uporzÄ…dkowanie ich od najbardziej istotnych. KorzystajÄ…c z Azure AI Search, ponowne rangowanie jest wykonywane automatycznie za pomocÄ… semantycznego rerankera. PrzykÅ‚ad dziaÅ‚ania ponownego rangowania przy uÅ¼yciu najbliÅ¼szych sÄ…siadÃ³w:

```python
# ZnajdÅº najbardziej podobne dokumenty
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Wydrukuj najbardziej podobne dokumenty
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Indeks {index} nie znaleziony w ramce danych")
```

## ÅÄ…czenie wszystkiego w caÅ‚oÅ›Ä‡

Ostatnim krokiem jest dodanie naszego LLM do miksu, aby mÃ³c uzyskaÄ‡ odpowiedzi oparte na naszych danych. MoÅ¼emy to zaimplementowaÄ‡ w nastÄ™pujÄ…cy sposÃ³b:

```python
user_input = "czym jest perceptron?"

def chatbot(user_input):
    # Przekonwertuj pytanie na wektor zapytania
    query_vector = create_embeddings(user_input)

    # ZnajdÅº najbardziej podobne dokumenty
    distances, indices = nbrs.kneighbors([query_vector])

    # dodaj dokumenty do zapytania, aby zapewniÄ‡ kontekst
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # poÅ‚Ä…cz historiÄ™ i dane wejÅ›ciowe uÅ¼ytkownika
    history.append(user_input)

    # utwÃ³rz obiekt wiadomoÅ›ci
    messages=[
        {"role": "system", "content": "JesteÅ› asystentem AI, ktÃ³ry pomaga w pytaniach dotyczÄ…cych AI."},
        {"role": "user", "content": history[-1]}
    ]

    # uÅ¼yj uzupeÅ‚niania czatu do wygenerowania odpowiedzi
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Ocena naszej aplikacji

### Metryki Oceny

- JakoÅ›Ä‡ dostarczonych odpowiedzi zapewniajÄ…ca, Å¼e brzmiÄ… naturalnie, pÅ‚ynnie i jak ludzkie

- Ugruntowanie danych: ocena, czy odpowiedÅº pochodziÅ‚a z dostarczonych dokumentÃ³w

- TrafnoÅ›Ä‡: ocena, czy odpowiedÅº pasuje i jest zwiÄ…zana z zadanym pytaniem

- PÅ‚ynnoÅ›Ä‡ - czy odpowiedÅº ma sens gramatyczny

## Przypadki uÅ¼ycia RAG (Retrieval Augmented Generation) i baz danych wektorowych

Istnieje wiele rÃ³Å¼nych przypadkÃ³w uÅ¼ycia, w ktÃ³rych wywoÅ‚ania funkcji mogÄ… ulepszyÄ‡ TwojÄ… aplikacjÄ™, na przykÅ‚ad:

- Odpowiadanie na pytania: osadzenie danych firmowych w czacie, ktÃ³ry moÅ¼e byÄ‡ uÅ¼ywany przez pracownikÃ³w do zadawania pytaÅ„.

- Systemy rekomendacji: gdzie moÅ¼na stworzyÄ‡ system, ktÃ³ry dopasowuje najbardziej podobne wartoÅ›ci, np. filmy, restauracje i wiele innych.

- UsÅ‚ugi chatbotÃ³w: moÅ¼esz przechowywaÄ‡ historiÄ™ czatÃ³w i personalizowaÄ‡ rozmowÄ™ w oparciu o dane uÅ¼ytkownika.

- Wyszukiwanie obrazÃ³w oparte na embeddingach wektorowych, przydatne przy rozpoznawaniu obrazÃ³w i wykrywaniu anomalii.

## Podsumowanie

OmÃ³wiliÅ›my podstawowe obszary RAG, od dodawania naszych danych do aplikacji, zapytania uÅ¼ytkownika i wyniku. Aby uproÅ›ciÄ‡ tworzenie RAG, moÅ¼esz uÅ¼yÄ‡ frameworkÃ³w takich jak Semantic Kernel, Langchain lub Autogen.

## Zadanie

Aby kontynuowaÄ‡ naukÄ™ o Retrieval Augmented Generation (RAG), moÅ¼esz zbudowaÄ‡:

- Zbuduj front-end dla aplikacji za pomocÄ… wybranego frameworka

- Wykorzystaj framework, LangChain lub Semantic Kernel, i odtwÃ³rz swojÄ… aplikacjÄ™.

Gratulacje za ukoÅ„czenie lekcji ğŸ‘.

## Nauka siÄ™ tu nie koÅ„czy, kontynuuj podrÃ³Å¼

Po ukoÅ„czeniu tej lekcji sprawdÅº naszÄ… [KolekcjÄ™ Nauki o Generatywnej AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby dalej podnosiÄ‡ swojÄ… wiedzÄ™ o Generatywnej AI!
