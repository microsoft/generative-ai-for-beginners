# Explorando y comparando diferentes Modelos Grandes de Lenguaje (LLMs)

[![Explorando y comparando diferentes LLMs](../../images/02-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://youtu.be/J1mWzw0P74c?WT.mc_id=academic-105485-koreyst)

> *Haz clic en la imagen de arriba para ver el video de esta lecci√≥n*

Con la lecci√≥n anterior, hemos visto c√≥mo la IA Generativa est√° cambiando el panorama tecnol√≥gico, c√≥mo funcionan los Modelos Grandes de Lenguaje (LLMs) y c√≥mo un negocio - como nuestra startup - puede aplicarlos a sus casos de uso y expandirse. En este cap√≠tulo, buscamos comparar y contrastar diferentes tipos de Modelos Grandes de Lenguaje, LLMs, para entender sus pros y contras.

El siguiente paso en el viaje de nuestra startup es explorar el panorama actual de los Modelos Grandes de Lenguaje (LLMs) y entender cu√°les son los apropiados para nuestro caso de uso.

## Introducci√≥n

En esta lecci√≥n se cubrir√°:

- Diferentes tipos de LLMs en el panorama actual.
- Pruebas, iteraciones y comparaciones de diferentes modelos para tu caso de uso en Azure.
- C√≥mo implementar un LLM.

## Objetivos de Aprendizaje

Despu√©s de completar esta lecci√≥n, ser√°s capaz de:

- Seleccionar el modelo correcto para tu caso de uso.
- Entender c√≥mo probar, iterar y mejorar el rendimiento de tu modelo.
- Saber c√≥mo las empresas implementan modelos.

## Entender los diferentes tipos de LLMs

Los Modelos Grandes de Lenguaje (LLMs) pueden clasificarse de m√∫ltiples formas seg√∫n su arquitectura, datos de entrenamiento y caso de uso. Comprender estas diferencias ayudar√° a nuestra startup a elegir el modelo m√°s adecuado para el escenario en el que se utilizar√°, as√≠ como a entender c√≥mo probar, iterar y mejorar su rendimiento.

Existen muchos tipos diferentes de modelos LLM. Tu elecci√≥n de modelo depender√° de para qu√© planeas usarlos, los datos que tienes, cu√°nto est√°s dispuesto a pagar y m√°s.

Seg√∫n el prop√≥sito de uso de los modelos, ya sea para procesar texto, audio, v√≠deo, generar im√°genes y otros, es posible que decidas elegir un tipo diferente de modelo.

- **Reconocimiento de audio y voz**. Para este prop√≥sito, los modelos tipo Whisper son una excelente elecci√≥n, ya que son de prop√≥sito general y est√°n orientados al reconocimiento de voz. Est√°n entrenados con una amplia variedad de audios y pueden realizar reconocimiento de voz en m√∫ltiples idiomas. Aprende m√°s sobre [modelos tipo Whisper aqu√≠](https://platform.openai.com/docs/models/whisper?WT.mc_id=academic-105485-koreyst).

- **Generaci√≥n de im√°genes**. Para la generaci√≥n de im√°genes, DALL-E y Midjourney son dos opciones muy conocidas. DALL-E est√° disponible a trav√©s de Azure OpenAI. [Lee m√°s sobre DALL-E aqu√≠](https://platform.openai.com/docs/models/dall-e?WT.mc_id=academic-105485-koreyst) y tambi√©n en el Cap√≠tulo 9 de este curr√≠culo.

- **Generaci√≥n de texto**. La mayor√≠a de los modelos est√°n entrenados en generaci√≥n de texto y tienes una gran variedad de opciones, desde GPT-3.5 hasta GPT-4. Vienen con diferentes costos, siendo GPT-4 el m√°s caro. Es recomendable revisar el [Azure Open AI playground](https://oai.azure.com/portal/playground?WT.mc_id=academic-105485-koreyst) para determinar qu√© modelos se ajustan mejor a tus necesidades en cuanto a capacidad y costo.

Seleccionar un modelo significa que obtienes algunas capacidades b√°sicas, que sin embargo podr√≠an no ser suficientes. Frecuentemente tienes datos espec√≠ficos de la empresa que de alguna manera necesitas comunicar al LLM. Hay varias opciones sobre c√≥mo abordar eso, m√°s sobre eso en las secciones siguientes.

### Modelos de Base versus Modelos Grandes de Lenguaje

El t√©rmino Modelo Fundacional fue [creado por investigadores de Stanford](https://arxiv.org/abs/2108.07258?WT.mc_id=academic-105485-koreyst) y definido como un modelo de IA que sigue algunos criterios, tales como:

- **Se entrenan utilizando aprendizaje no supervisado o aprendizaje auto-supervisado**, lo que significa que se entrenan en datos multimodales no etiquetados, y no requieren anotaci√≥n humana o etiquetado de datos para su proceso de entrenamiento.
- **Son modelos muy grandes**, basados en redes neuronales muy profundas entrenadas con miles de millones de par√°metros.
- **Normalmente est√°n destinados a servir como una 'fundaci√≥n' para otros modelos**, lo que significa que pueden usarse como un punto de partida para que otros modelos se construyan sobre ellos, lo cual se puede hacer mediante ajustes finos.

![Modelos Fundacionales versus LLMs](../../images/FoundationModel.png?WT.mc_id=academic-105485-koreyst)

Fuente de la imagen: [Gu√≠a Esencial de Modelos Fundacionales y Modelos Grandes de Lenguaje | por Babar M Bhatti | Medium
](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404)

Para aclarar a√∫n m√°s esta distinci√≥n, tomemos ChatGPT como ejemplo. Para construir la primera versi√≥n de ChatGPT, un modelo llamado GPT-3.5 sirvi√≥ como el modelo fundacional. Esto significa que OpenAI utiliz√≥ algunos datos espec√≠ficos de chat para crear una versi√≥n ajustada de GPT-3.5 que estaba especializada en desempe√±arse bien en escenarios conversacionales, como los chatbots.

![Modelo Fundacional](../../images/Multimodal.png?WT.mc_id=academic-105485-koreyst)

Fuente de la imagen: [2108.07258.pdf (arxiv.org)](https://arxiv.org/pdf/2108.07258.pdf?WT.mc_id=academic-105485-koreyst)

### Modelos de C√≥digo Abierto versus Modelos Propietarios

Otra manera de categorizar los Modelos Grandes de Lenguaje es si son de c√≥digo abierto (open source) o propietarios.

Los modelos de c√≥digo abierto son modelos que se ponen a disposici√≥n del p√∫blico y pueden ser utilizados por cualquiera. A menudo son publicados por la empresa que los cre√≥ o por la comunidad investigadora. Estos modelos pueden ser inspeccionados, modificados y personalizados para diversos casos de uso en Modelos Grandes de Lenguaje. Sin embargo, no siempre est√°n optimizados para uso en producci√≥n y pueden no ser tan eficientes como los modelos propietarios. Adem√°s, la financiaci√≥n para modelos de c√≥digo abierto puede ser limitada, y es posible que no se mantengan a largo plazo o que no se actualicen con las √∫ltimas investigaciones. Algunos ejemplos de modelos de c√≥digo abierto populares incluyen:  [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html?WT.mc_id=academic-105485-koreyst), [Bloom](https://sapling.ai/llm/bloom?WT.mc_id=academic-105485-koreyst) and [LLaMA](https://sapling.ai/llm/llama?WT.mc_id=academic-105485-koreyst).

Los modelos propietarios son modelos que son propiedad de una empresa y no se ponen a disposici√≥n del p√∫blico. Estos modelos suelen estar optimizados para su uso en producci√≥n. Sin embargo, no se permite su inspecci√≥n, modificaci√≥n o personalizaci√≥n para diferentes casos de uso. Adem√°s, no siempre est√°n disponibles de forma gratuita y pueden requerir una suscripci√≥n o pago para su uso. Asimismo, los usuarios no tienen control sobre los datos utilizados para entrenar el modelo, lo que significa que deben confiar en el propietario del modelo para garantizar el compromiso con la privacidad de los datos y el uso responsable de la inteligencia artificial. Algunos ejemplos de modelos propietarios populares: [OpenAI models](https://platform.openai.com/docs/models/overview?WT.mc_id=academic-105485-koreyst), [Google Bard](https://sapling.ai/llm/bard?WT.mc_id=academic-105485-koreyst) or [Claude 2](https://www.anthropic.com/index/claude-2?WT.mc_id=academic-105485-koreyst).

### Incrustaciones (embeddings) versus Generaci√≥n de Im√°genes versus Generaci√≥n de Texto y C√≥digo

Los LLMs tambi√©n pueden ser categorizados por la salida que generan.

Los embeddings son un conjunto de modelos que pueden convertir texto en una forma num√©rica, llamada incrustaci√≥n, que es una representaci√≥n num√©rica del texto de entrada. Las incrustaciones facilitan que las m√°quinas comprendan las relaciones entre palabras u oraciones y pueden ser utilizadas como entradas por otros modelos, como modelos de clasificaci√≥n o modelos de agrupaci√≥n que tienen un mejor rendimiento con los datos num√©ricos. Los modelos de incrustaci√≥n usualmente se utilizan para el aprendizaje por transferencia, donde se construye un modelo para una tarea sustituta para la cual hay gran cantidad de datos, y luego los pesos del modelo (incrustaciones) se reutilizan para otras tareas posteriores. Un ejemplo de esta categor√≠a es [OpenAI embeddings](https://platform.openai.com/docs/models/embeddings?WT.mc_id=academic-105485-koreyst).

![Incrustaci√≥n](../../images/Embedding.png?WT.mc_id=academic-105485-koreyst)

Los modelos de generaci√≥n de im√°genes son modelos que crean im√°genes. Estos modelos suelen utilizarse para la edici√≥n de im√°genes, la s√≠ntesis de im√°genes y la traducci√≥n de im√°genes. Los modelos de generaci√≥n de im√°genes con regularidad son entrenados en grandes conjuntos de datos de im√°genes, como [LAION-5B](https://laion.ai/blog/laion-5b/?WT.mc_id=academic-105485-koreyst), y pueden ser utilizados para generar nuevas im√°genes o para editar im√°genes existentes con t√©cnicas de inpainting, super-resoluci√≥n y colorizaci√≥n. Algunos ejemplos: [DALL-E-3](https://openai.com/dall-e-3?WT.mc_id=academic-105485-koreyst) y [Stable Diffusion models](https://github.com/Stability-AI/StableDiffusion?WT.mc_id=academic-105485-koreyst).

![Generaci√≥n de im√°genes](../../images/Image.png?WT.mc_id=academic-105485-koreyst)

Los modelos de generaci√≥n de texto y c√≥digo son modelos que generan texto o c√≥digo. Estos modelos suelen utilizarse para resumir texto, traducir y responder preguntas. Los modelos de generaci√≥n de texto suelen ser entrenados en grandes conjuntos de datos de texto, como [BookCorpus](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html?WT.mc_id=academic-105485-koreyst), y pueden ser utilizados para generar texto o responder preguntas. Los modelos de generaci√≥n de c√≥digo, como [CodeParrot](https://huggingface.co/codeparrot?WT.mc_id=academic-105485-koreyst), en muchas ocasiones se entrenan en grandes conjuntos de datos de c√≥digo, como GitHub, y pueden utilizarse para crear c√≥digo o para corregir errores en c√≥digo existente.

 ![Generaci√≥n de texto y c√≥digo](../../images/Text.png?WT.mc_id=academic-105485-koreyst)

### Codificador-Decodificador versus Solo-Decodificador

Para hablar sobre los diferentes tipos de arquitecturas de LLMs, utilicemos una analog√≠a.

Imagina que tu jefe te asign√≥ la tarea de escribir un cuestionario para los estudiantes. Tienes dos colegas; uno se encarga de crear el contenido y el otro se encarga de revisarlo.

El creador de contenido es como un modelo Decodificador o Decoder, puede mirar el tema y ver lo que escribiste, y luego puede escribir un curso basado en eso. Son muy buenos escribiendo contenido atractivo e informativo, pero no son muy buenos entendiendo el tema y los objetivos de aprendizaje. Algunos ejemplos de modelos decodificadores son los modelos de la familia GPT, como GPT-3.

El que revisa es como un modelo Codificador o Encoder, observa el curso escrito y las respuestas, notando la relaci√≥n entre ellos y comprendiendo el contexto, pero no es bueno generando contenido. Un ejemplo de modelo Codificador ser√≠a BERT.

Imagina que tambi√©n podemos tener a alguien que pueda crear y revisar el cuestionario, esto ser√≠a un modelo Codificador-Decodificador. Algunos ejemplos como BART y T5.

### Servicio versus Modelo

Ahora, hablemos de la diferencia entre un servicio y un modelo. Un servicio es un producto ofrecido por un proveedor de servicios en la nube y suele ser una combinaci√≥n de modelos, datos y otros componentes. Un modelo es el componente central de un servicio y a veces es un modelo base, como un LLM.

Los servicios suelen estar optimizados para su uso en producci√≥n y son m√°s f√°ciles de utilizar que los modelos, a trav√©s de una interfaz gr√°fica de usuario. Sin embargo, los servicios no siempre est√°n disponibles de forma gratuita y pueden requerir una suscripci√≥n o pago para su uso, a cambio de aprovechar la infraestructura y los recursos del propietario del servicio, optimizando gastos y escalando f√°cilmente. Un ejemplo de servicio es [Azure OpenAI service](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=academic-105485-koreyst), este servicio ofrece un plan de tarifas de pago por uso (pay-as-you-go) , lo que implica que los usuarios pagan en proporci√≥n a su utilizaci√≥n del servicio. Adem√°s, el servicio Azure OpenAI brinda seguridad de nivel empresarial y un marco de inteligencia artificial responsable, complementando las capacidades de los modelos.

Los modelos son simplemente la red neuronal, con sus par√°metros, pesos y otros elementos. Permitir que las empresas los ejecuten localmente requerir√≠a la compra de hardware, la construcci√≥n de una infraestructura para escalar y la adquisici√≥n de una licencia o el uso de un modelo de c√≥digo abierto. Un modelo como LLaMA est√° disponible para su uso, pero requiere una potencia de computo significativa para ejecutarlo.

## C√≥mo probar e iterar con diferentes modelos para entender el rendimiento en Azure

Una vez que nuestro equipo haya explorado el panorama actual de los LLMs y se haya identificado algunos candidatos prometedores para sus escenarios, el siguiente paso implica poner a prueba estos modelos con sus propios datos y cargas de trabajo. Este proceso es iterativo y se basa en experimentos y mediciones.
La mayor√≠a de los modelos que mencionamos en p√°rrafos anteriores (modelos de OpenAI, modelos de c√≥digo abierto como Llama2 y transformers de Hugging Face) est√°n disponibles en el siguiente enlace. [Modelos Base](https://learn.microsoft.com/azure/machine-learning/concept-foundation-models?WT.mc_id=academic-105485-koreyst) cat√°logo en [Azure Machine Learning studio](https://ml.azure.com/?WT.mc_id=academic-105485-koreyst).

[Azure Machine Learning](https://azure.microsoft.com/products/machine-learning/?WT.mc_id=academic-105485-koreyst) es un Servicio en la Nube dise√±ado para cient√≠ficos de datos e ingenieros de aprendizaje autom√°tico (ML) para gestionar todo el ciclo de vida del aprendizaje autom√°tico (entrenar, probar, implementar y gestionar MLOps) en una sola plataforma. El estudio de aprendizaje autom√°tico ofrece una interfaz gr√°fica de usuario para este servicio y permite al usuario:

- Encontrar el Modelo Base de inter√©s en el cat√°logo, filtrando por tarea, licencia o nombre. Tambi√©n es posible importar nuevos modelos que a√∫n no est√©n incluidos en el cat√°logo
- Revisar la tarjeta del modelo, que incluye una descripci√≥n detallada y ejemplos de c√≥digo, y probarlo con el widget de Inferencia de Muestra, proporcionando un ejemplo de prompt para evaluar el resultado.

![Tarjeta del modelo](../../images/Llama1.png?WT.mc_id=academic-105485-koreyst)

- Evaluar el rendimiento del modelo utilizando m√©tricas objetivas de evaluaci√≥n en una carga de trabajo espec√≠fica y un conjunto de datos espec√≠fico proporcionado como entrada o input.

![Evaluaci√≥n del modelo](../../images/Llama2.png?WT.mc_id=academic-105485-koreyst)

- Ajustar finamente (fine-tune) el modelo con datos de entrenamiento personalizados para mejorar el rendimiento del modelo en una carga de trabajo espec√≠fica, aprovechando las capacidades de experimentaci√≥n y seguimiento de Azure Machine Learning.

![Ajuste fino del modelo](../../images/Llama3.png?WT.mc_id=academic-105485-koreyst)

- Desplegar el modelo original preentrenado o la versi√≥n afinada (fine-tuned) en un endpoint remoto de inferencia en tiempo real o por lotes, para que las aplicaciones puedan utilizarlo.

![Despliegue del modelo.](../../images/Llama4.png?WT.mc_id=academic-105485-koreyst)

## Mejorar los resultados de los LLMs

Nuestro equipo de startup ha explorado diferentes tipos de LLMs y la Plataforma en la Nube (Azure Machine Learning) para comparar diferentes modelos, evaluar su desempe√±o con datos de prueba, y mejorarlos antes de desplegarlos en endpoints de inferencia.

Pero, ¬øcu√°ndo deber√≠an considerar ajustar finamente (fine-tuning) un modelo en lugar de usar uno preentrenado? ¬øExisten otras estrategias para mejorar el rendimiento del modelo en cargas de trabajo espec√≠ficas?

Existen varios enfoques que una empresa puede utilizar para obtener los resultados que necesita de un LLM. Pueden seleccionar diferentes tipos de modelos con diferentes niveles de entrenamiento.

Desplegar un LLM en producci√≥n, con diferentes niveles de complejidad, costos y calidad. Aqu√≠ hay algunas estrategias diferentes:

- **Prompt engineering con contexto**. La idea es proporcionar un prompt con suficiente contexto para asegurarse de obtener las respuestas que uno necesita.

- **Generaci√≥n aumentada con recuperaci√≥n (Retrieval Augmented Generation, RAG)**. Por ejemplo, tus datos pueden existir en una base de datos o un endpoint web, y para garantizar que estos datos, o una parte de ellos, se incluyan al realizar el prompting y puedas obtener respuestas precisas, puedes recuperar los datos relevantes y agregarlos como parte del prompt del usuario.

- **Ajuste Fino de Modelo (Fine-tuned)**. En este caso, has entrenado el modelo con mayor detalle utilizando tus propios datos, lo que lo hace m√°s preciso y capaz de responder a tus necesidades espec√≠ficas, aunque puede tener costos asociados.

![Implementaci√≥n de LLMs](../../images/Deploy.png?WT.mc_id=academic-105485-koreyst)

Fuente de la imagen: [Four Ways that Enterprises Deploy LLMs | Fiddler AI Blog](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)

### Prompt Engineering con contexto

Los LLMs preentrenados funcionan muy bien en tareas de lenguaje natural generalizadas, incluso al llamarlos con un breve prompt, como una oraci√≥n por completar o una pregunta, se lo conoce como "aprendizaje de cero disparos" (zero-shot learning).

No obstante, cuanto m√°s detallado sea el planteamiento de la consulta por parte del usuario, incluyendo solicitudes precisas y ejemplos concretos, es decir, proporcionando contexto, m√°s precisa y acorde a las expectativas del usuario ser√° la respuesta. En este, hablamos de aprendizaje de ‚Äúone-shot‚Äù si el prompt incluye solo un ejemplo y de ‚Äúfew shot learning‚Äù si se proporcionan varios ejemplos. Prompt engineering con contexto es el enfoque m√°s eficaz desde el punto de vista econ√≥mico para comenzar.

### Retrieval Augmented Generation (RAG)

Los LLMs tienen la limitaci√≥n de que solo pueden utilizar los datos que se utilizaron durante su entrenamiento para generar una respuesta. Esto significa que no saben nada acerca de los hechos que ocurrieron despu√©s de su proceso de entrenamiento y no pueden acceder a informaci√≥n no p√∫blica (como datos de la empresa).
Esto se puede superar mediante RAG, una t√©cnica que ampl√≠a el prompt con datos externos en forma de fragmentos de documentos, teniendo en cuenta los l√≠mites de longitud del prompt. Esto es respaldado por las herramientas de base de datos vectoriales (como [Azure Vector Search](https://learn.microsoft.com/azure/search/vector-search-overview?WT.mc_id=academic-105485-koreyst)) estas herramientas de base de datos vectoriales recuperan los fragmentos (chunks) √∫tiles de diversas fuentes de datos predefinidas y los a√±aden al contexto del prompt.

Esta t√©cnica es muy √∫til cuando una empresa no dispone de suficientes datos, tiempo o recursos para ajustar finamente (fine-tune) un LLM, pero a√∫n desea mejorar el rendimiento en una carga de trabajo espec√≠fica y reducir los riesgos de generar informaci√≥n falsa, es decir, la distorsi√≥n de la realidad o contenido perjudicial.

### Modelo afinado (Fine-tuned)

El ajuste fino es un proceso que utiliza el aprendizaje por transferencia para "adaptar" el modelo a una tarea o para resolver un problema espec√≠fico. A diferencia del aprendizaje few-shot y RAG, esto resulta en la generaci√≥n de un nuevo modelo con pesos y sesgos actualizados. Requiere un conjunto de ejemplos de entrenamiento que consisten en una √∫nica entrada (prompt) y su salida asociada (respuesta).

Este ser√≠a el enfoque preferido si:

- **Utilizando modelos ajustados finamente**. Una empresa preferir√≠a utilizar modelos ajustados finamente menos capaces (como modelos de embedding) en lugar de modelos de alto rendimiento, lo que resultar√≠a en una soluci√≥n m√°s rentable y r√°pida.

- **Teniendo en cuenta la latencia**. La latencia es importante para un caso de uso espec√≠fico, por lo que no es posible utilizar prompts extensos o un n√∫mero de ejemplos que no se ajuste al l√≠mite de longitud del prompt del modelo.

- **Mantenerse actualizado**. Una empresa cuenta con una gran cantidad de datos de alta calidad y etiquetas reales, as√≠ como los recursos necesarios para mantener actualizados estos datos con el tiempo.

### Modelo entrenado

Entrenar un LLM desde cero es, sin lugar a dudas, es el enfoque m√°s dif√≠cil y complejo de adoptar, ya que requiere grandes cantidades de datos, recursos capacitados y la potencia computacional adecuada. Esta opci√≥n solo debe considerarse en un escenario en el que una empresa tenga un caso de uso espec√≠fico para un dominio y una gran cantidad de datos centrados en ese dominio.

## Evaluaci√≥n de conocimientos

¬øCu√°l podr√≠a ser un buen enfoque para mejorar los resultados finalizaci√≥n de LLM?

1. Prompt engineering con contexto
1. RAG
1. Modelo Fine-tuned 

A:3, si tienes el tiempo y los recursos, y cuentas con datos de alta calidad, el ajuste fino (fine-tuning) es la mejor opci√≥n para mantenerte actualizado. Sin embargo, si est√°s buscando mejoras y tienes limitaciones de tiempo, vale la pena considerar RAG en primer lugar.

## üöÄ Desaf√≠o

Investiga m√°s sobre c√≥mo puedes [utilizar RAG](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview?WT.mc_id=academic-105485-koreyst) para tu negocio.

## Gran trabajo, contin√∫a aprendiendo

Despu√©s de completar esta lecci√≥n, ¬°consulta nuestra [colecci√≥n de Aprendizaje de IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para continuar mejorando tu conocimiento de IA Generativa!

Dir√≠gete a la Lecci√≥n 3, donde veremos ¬°c√≥mo  [Usar IA Generativa de manera Responsable](../../03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)!
