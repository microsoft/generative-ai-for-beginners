# Zabezpieczanie aplikacji generatywnej AI

[![Zabezpieczanie aplikacji generatywnej AI](../../images/13-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Wprowadzenie

Ta lekcja obejmie:

- Bezpieczestwo w kontekcie system贸w AI.
- Najczstsze zagro偶enia i ryzyka dla system贸w AI.
- Metody i aspekty zabezpieczania system贸w AI.

## Cele nauczania

Po ukoczeniu tej lekcji bdziesz rozumia:

- Zagro偶enia i ryzyka dla system贸w AI.
- Powszechne metody i praktyki zabezpieczania system贸w AI.
- Jak wdra偶anie test贸w bezpieczestwa mo偶e zapobiec nieoczekiwanym rezultatom i erozji zaufania u偶ytkownik贸w.

## Co oznacza bezpieczestwo w kontekcie generatywnej AI?

Poniewa偶 technologie Sztucznej Inteligencji (AI) i Uczenia Maszynowego (ML) coraz bardziej ksztatuj nasze 偶ycie, kluczowe jest zabezpieczanie nie tylko danych klient贸w, ale tak偶e samych system贸w AI. AI/ML jest coraz czciej wykorzystywane do wspierania proces贸w decyzyjnych o wysokiej wartoci w bran偶ach, gdzie ze decyzje mog prowadzi do powa偶nych konsekwencji.

Oto kluczowe punkty do rozwa偶enia:

- **Wpyw AI/ML**: AI/ML ma znaczcy wpyw na codzienne 偶ycie, dlatego zabezpieczanie tych technologii stao si niezbdne.
- **Wyzwania bezpieczestwa**: Ten wpyw AI/ML wymaga odpowiedniej uwagi, aby zaspokoi potrzeb ochrony produkt贸w opartych na AI przed zaawansowanymi atakami, czy to ze strony trolli, czy zorganizowanych grup.
- **Problemy strategiczne**: Bran偶a technologiczna musi proaktywnie rozwizywa wyzwania strategiczne, aby zapewni dugoterminowe bezpieczestwo klient贸w i ochron danych.

Dodatkowo, modele uczenia maszynowego nie s w stanie rozr贸偶ni midzy zoliwymi danymi wejciowymi a agodnymi anomaliami. Znaczce 藕r贸do danych treningowych pochodzi z niekuratorowanych, niemoderowanych, publicznych zbior贸w danych, kt贸re s otwarte na wkad os贸b trzecich. Atakujcy nie musz narusza zbior贸w danych, gdy mog swobodnie przyczynia si do ich tworzenia. Z czasem zoliwe dane o niskiej pewnoci staj si zaufanymi danymi o wysokiej pewnoci, jeli struktura/format danych pozostaje poprawny.

Dlatego kluczowe jest zapewnienie integralnoci i ochrony magazyn贸w danych, kt贸rych Twoje modele u偶ywaj do podejmowania decyzji.

## Zrozumienie zagro偶e i ryzyka AI

W kontekcie AI i powizanych system贸w, zatruwanie danych (data poisoning) stanowi obecnie najistotniejsze zagro偶enie bezpieczestwa. Zatruwanie danych polega na celowej zmianie informacji u偶ywanych do trenowania AI, powodujc bdy. Jest to wynikiem braku ustandaryzowanych metod wykrywania i agodzenia, w poczeniu z naszym poleganiem na niezaufanych lub niekuratorowanych publicznych zbiorach danych do treningu. Aby zachowa integralno danych i zapobiec wadliwemu procesowi treningu, kluczowe jest ledzenie pochodzenia i linii danych. W przeciwnym razie, stare powiedzenie "mieci na wejciu, mieci na wyjciu" pozostaje prawdziwe, prowadzc do pogorszonej wydajnoci modelu.

Oto przykady, jak zatruwanie danych mo偶e wpywa na Twoje modele:

1. **Odwracanie etykiet**: W zadaniu klasyfikacji binarnej przeciwnik celowo zmienia etykiety maego podzbioru danych treningowych. Na przykad agodne pr贸bki s oznaczone jako zoliwe, co prowadzi model do nauki bdnych powiza.\
   **Przykad**: Filtr spamu bdnie klasyfikujcy prawdziwe e-maile jako spam ze wzgldu na zmanipulowane etykiety.
2. **Zatruwanie cech**: Atakujcy subtelnie modyfikuje cechy w danych treningowych, aby wprowadzi stronniczo lub wprowadzi model w bd.\
   **Przykad**: Dodawanie nieistotnych s贸w kluczowych do opis贸w produkt贸w w celu manipulowania systemami rekomendacji.
3. **Wstrzykiwanie danych**: Wstrzykiwanie zoliwych danych do zbioru treningowego, aby wpyn na zachowanie modelu.\
   **Przykad**: Wprowadzanie faszywych recenzji u偶ytkownik贸w, aby znieksztaci wyniki analizy sentymentu.
4. **Ataki typu backdoor**: Przeciwnik wstawia ukryty wzorzec (backdoor) do danych treningowych. Model uczy si rozpoznawa ten wzorzec i zachowuje si zoliwie, gdy zostanie wyzwolony.\
   **Przykad**: System rozpoznawania twarzy trenowany z obrazami z backdoorem, kt贸ry bdnie identyfikuje okrelon osob.

Korporacja MITRE stworzya [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), baz wiedzy o taktykach i technikach stosowanych przez przeciwnik贸w w rzeczywistych atakach na systemy AI.

> Istnieje rosnca liczba podatnoci w systemach wykorzystujcych AI, poniewa偶 wczenie AI zwiksza powierzchni ataku istniejcych system贸w poza te zwizane z tradycyjnymi atakami cybernetycznymi. Opracowalimy ATLAS, aby zwikszy wiadomo tych unikalnych i ewoluujcych podatnoci, w miar jak globalna spoeczno coraz czciej wcza AI do r贸偶nych system贸w. ATLAS jest wzorowany na ramach MITRE ATT&CK庐 i jego taktyki, techniki i procedury (TTP) s uzupenieniem tych w ATT&CK.

Podobnie jak ramy MITRE ATT&CK庐, kt贸re s szeroko stosowane w tradycyjnym cyberbezpieczestwie do planowania zaawansowanych scenariuszy emulacji zagro偶e, ATLAS zapewnia atwy do przeszukiwania zbi贸r TTP, kt贸re mog pom贸c lepiej zrozumie i przygotowa si do obrony przed pojawiajcymi si atakami.

Dodatkowo, Open Web Application Security Project (OWASP) stworzy "[List Top 10](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" najbardziej krytycznych podatnoci znalezionych w aplikacjach wykorzystujcych LLM. Lista podkrela ryzyka zagro偶e, takich jak wspomniane wczeniej zatruwanie danych, wraz z innymi, takimi jak:

- **Wstrzykiwanie prompt贸w**: technika, w kt贸rej atakujcy manipuluj du偶ym modelem jzykowym (LLM) poprzez starannie spreparowane dane wejciowe, powodujc, 偶e zachowuje si poza swoim zamierzonym zachowaniem.
- **Podatnoci acucha dostaw**: Komponenty i oprogramowanie, kt贸re skadaj si na aplikacje u偶ywane przez LLM, takie jak moduy Pythona lub zewntrzne zbiory danych, mog same zosta naruszone, prowadzc do nieoczekiwanych rezultat贸w, wprowadzenia stronniczoci, a nawet podatnoci w podstawowej infrastrukturze.
- **Nadmierne poleganie**: LLM s omylne i s podatne na halucynacje, dostarczajc niedokadne lub niebezpieczne wyniki. W kilku udokumentowanych przypadkach ludzie przyjmowali wyniki bez weryfikacji, co prowadzio do niezamierzonych, negatywnych konsekwencji w rzeczywistym wiecie.

Microsoft Cloud Advocate Rod Trent napisa darmowy ebook, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), kt贸ry gboko analizuje te i inne pojawiajce si zagro偶enia AI oraz dostarcza obszernych wskaz贸wek, jak najlepiej radzi sobie z tymi scenariuszami.

## Testowanie bezpieczestwa dla system贸w AI i LLM

Sztuczna inteligencja (AI) transformuje r贸偶ne domeny i bran偶e, oferujc nowe mo偶liwoci i korzyci dla spoeczestwa. Jednak AI stwarza r贸wnie偶 znaczce wyzwania i ryzyka, takie jak prywatno danych, stronniczo, brak wyjanialnoci i potencjalne nadu偶ycia. Dlatego kluczowe jest zapewnienie, aby systemy AI byy bezpieczne i odpowiedzialne, co oznacza, 偶e przestrzegaj standard贸w etycznych i prawnych oraz mog by zaufane przez u偶ytkownik贸w i interesariuszy.

Testowanie bezpieczestwa to proces oceny bezpieczestwa systemu AI lub LLM poprzez identyfikacj i wykorzystywanie ich podatnoci. Mo偶e by wykonywane przez programist贸w, u偶ytkownik贸w lub audytor贸w zewntrznych, w zale偶noci od celu i zakresu testowania. Niekt贸re z najczstszych metod testowania bezpieczestwa dla system贸w AI i LLM to:

- **Sanityzacja danych**: Jest to proces usuwania lub anonimizacji wra偶liwych lub prywatnych informacji z danych treningowych lub danych wejciowych systemu AI lub LLM. Sanityzacja danych mo偶e pom贸c zapobiec wyciekowi danych i zoliwej manipulacji poprzez zmniejszenie ekspozycji poufnych lub osobistych danych.
- **Testy przeciwnikaskie**: Jest to proces generowania i stosowania przeciwnikaskich przykad贸w do danych wejciowych lub wyjciowych systemu AI lub LLM, aby oceni jego odporno i wytrzymao na ataki przeciwnikaskie. Testy przeciwnikaskie mog pom贸c zidentyfikowa i zagodzi podatnoci i saboci systemu AI lub LLM, kt贸re mog by wykorzystywane przez atakujcych.
- **Weryfikacja modelu**: Jest to proces weryfikacji poprawnoci i kompletnoci parametr贸w modelu lub architektury systemu AI lub LLM. Weryfikacja modelu mo偶e pom贸c wykrywa i zapobiega kradzie偶y modelu poprzez zapewnienie, 偶e model jest chroniony i uwierzytelniony.
- **Walidacja wynik贸w**: Jest to proces walidacji jakoci i niezawodnoci wynik贸w systemu AI lub LLM. Walidacja wynik贸w mo偶e pom贸c wykrywa i korygowa zoliwe manipulacje poprzez zapewnienie, 偶e wyniki s sp贸jne i dokadne.

OpenAI, lider w systemach AI, ustanowi seri _ocen bezpieczestwa_ w ramach swojej inicjatywy sieci red teamingu, majcej na celu testowanie wynik贸w system贸w AI w nadziei na przyczynienie si do bezpieczestwa AI.

> Oceny mog obejmowa proste testy typu pytanie-odpowied藕, jak i bardziej zo偶one symulacje. Jako konkretne przykady, oto przykadowe oceny opracowane przez OpenAI do oceny zachowa AI z wielu perspektyw:

#### Perswazja

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI mo偶e oszuka inny system AI, aby powiedzia tajne sowo?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI mo偶e przekona inny system AI do przekazania pienidzy?
- [Propozycja gosowania](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI mo偶e wpyn na poparcie innego systemu AI dla propozycji politycznej?

#### Steganografia (ukryte wiadomoci)

- [Steganografia](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI mo偶e przekazywa tajne wiadomoci bez bycia zapanym przez inny system AI?
- [Kompresja tekstu](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI mo偶e kompresowa i dekompresowa wiadomoci, aby umo偶liwi ukrywanie tajnych wiadomoci?
- [Punkt Schellinga](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI mo偶e koordynowa z innym systemem AI bez bezporedniej komunikacji?

### Bezpieczestwo AI

Konieczne jest, abymy d偶yli do ochrony system贸w AI przed zoliwymi atakami, nadu偶yciami lub niezamierzonymi konsekwencjami. Obejmuje to podjcie krok贸w w celu zapewnienia bezpieczestwa, niezawodnoci i wiarygodnoci system贸w AI, takich jak:

- Zabezpieczanie danych i algorytm贸w u偶ywanych do trenowania i uruchamiania modeli AI
- Zapobieganie nieautoryzowanemu dostpowi, manipulacji lub sabota偶owi system贸w AI
- Wykrywanie i agodzenie stronniczoci, dyskryminacji lub problem贸w etycznych w systemach AI
- Zapewnienie odpowiedzialnoci, przejrzystoci i wyjanialnoci decyzji i dziaa AI
- Dostosowanie cel贸w i wartoci system贸w AI do tych ludzi i spoeczestwa

Bezpieczestwo AI jest wa偶ne dla zapewnienia integralnoci, dostpnoci i poufnoci system贸w AI i danych. Niekt贸re z wyzwa i mo偶liwoci bezpieczestwa AI to:

- Mo偶liwo: Wczenie AI do strategii cyberbezpieczestwa, poniewa偶 mo偶e odegra kluczow rol w identyfikacji zagro偶e i poprawie czasu reakcji. AI mo偶e pom贸c automatyzowa i wzmacnia wykrywanie i agodzenie cyberatak贸w, takich jak phishing, malware czy ransomware.
- Wyzwanie: AI mo偶e by r贸wnie偶 u偶ywane przez przeciwnik贸w do przeprowadzania zaawansowanych atak贸w, takich jak generowanie faszywych lub wprowadzajcych w bd treci, podszywanie si pod u偶ytkownik贸w lub wykorzystywanie podatnoci w systemach AI. Dlatego programici AI maj unikaln odpowiedzialno za projektowanie system贸w, kt贸re s odporne i wytrzymae na nadu偶ycia.

### Ochrona danych

LLM mog stanowi zagro偶enie dla prywatnoci i bezpieczestwa danych, kt贸re wykorzystuj. Na przykad, LLM mog potencjalnie zapamitywa i ujawnia wra偶liwe informacje ze swoich danych treningowych, takie jak osobiste imiona, adresy, hasa czy numery kart kredytowych. Mog by r贸wnie偶 manipulowane lub atakowane przez zoliwe podmioty, kt贸re chc wykorzysta ich podatnoci lub stronniczo. Dlatego wa偶ne jest, aby by wiadomym tych zagro偶e i podj odpowiednie rodki w celu ochrony danych u偶ywanych z LLM. Istnieje kilka krok贸w, kt贸re mo偶esz podj, aby chroni dane u偶ywane z LLM. Kroki te obejmuj:

- **Ograniczanie iloci i rodzaju danych, kt贸re s udostpniane LLM**: Udostpniaj tylko dane, kt贸re s niezbdne i istotne dla zamierzonych cel贸w, i unikaj udostpniania danych, kt贸re s wra偶liwe, poufne lub osobiste. U偶ytkownicy powinni r贸wnie偶 anonimizowa lub szyfrowa dane, kt贸re udostpniaj LLM, na przykad poprzez usuwanie lub maskowanie informacji identyfikujcych, lub korzystanie z bezpiecznych kana贸w komunikacji.
- **Weryfikowanie danych generowanych przez LLM**: Zawsze sprawdzaj dokadno i jako wynik贸w generowanych przez LLM, aby upewni si, 偶e nie zawieraj one niepo偶danych lub nieodpowiednich informacji.
- **Zgaszanie i alarmowanie o wszelkich naruszeniach danych lub incydentach**: Bd藕 czujny na wszelkie podejrzane lub nienormalne dziaania lub zachowania ze strony LLM, takie jak generowanie tekst贸w, kt贸re s nieistotne, niedokadne, obra藕liwe lub szkodliwe. Mo偶e to by wskazaniem naruszenia danych lub incydentu bezpieczestwa.

Bezpieczestwo danych, zarzdzanie i zgodno s krytyczne dla ka偶dej organizacji, kt贸ra chce wykorzysta moc danych i AI w rodowisku wielu chmur. Zabezpieczanie i zarzdzanie wszystkimi swoimi danymi to zo偶one i wieloaspektowe przedsiwzicie. Musisz zabezpieczy i zarzdza r贸偶nymi typami danych (ustrukturyzowane, nieustrukturyzowane i dane generowane przez AI) w r贸偶nych lokalizacjach w wielu chmurach, i musisz uwzgldni istniejce i przysze przepisy dotyczce bezpieczestwa danych, zarzdzania i AI. Aby chroni swoje dane, musisz przyj pewne najlepsze praktyki i rodki ostro偶noci, takie jak:

- Korzystanie z usug lub platform chmurowych, kt贸re oferuj funkcje ochrony danych i prywatnoci.
- Korzystanie z narzdzi do jakoci i walidacji danych, aby sprawdza dane pod ktem bd贸w, niesp贸jnoci lub anomalii.
- Korzystanie z ram zarzdzania danymi i etyki, aby zapewni, 偶e dane s u偶ywane w odpowiedzialny i przejrzysty spos贸b.

### Emulowanie realnych zagro偶e - Red teaming w AI

Emulowanie realnych zagro偶e jest obecnie uwa偶ane za standardow praktyk w budowaniu odpornych system贸w AI poprzez stosowanie podobnych narzdzi, taktyk, procedur do identyfikacji ryzyka dla system贸w i testowania odpowiedzi obroc贸w.

> Praktyka red teamingu w AI ewoluowaa, aby przybra bardziej rozszerzone znaczenie: obejmuje nie tylko sondowanie podatnoci bezpieczestwa, ale tak偶e sondowanie innych awarii systemu, takich jak generowanie potencjalnie szkodliwych treci. Systemy AI wi偶 si z nowymi ryzykami, a red teaming jest kluczowy dla zrozumienia tych nowych ryzyk, takich jak wstrzykiwanie prompt贸w i tworzenie treci bez podstaw. - [Microsoft AI Red Team buduje przyszo bezpieczniejszej AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Wskaz贸wki i zasoby do red teamingu](../../images/13-AI-red-team.png?WT.mc_id=academic-105485-koreyst)]()

Poni偶ej znajduj si kluczowe spostrze偶enia, kt贸re uksztatoway program Microsoft AI Red Team.

1. **Szeroki zakres Red Teamingu AI:**
   Red teaming AI obejmuje teraz zar贸wno bezpieczestwo, jak i wyniki Odpowiedzialnej AI (RAI). Tradycyjnie, red teaming koncentrowa si na aspektach bezpieczestwa, traktujc model jako wektor (np. kradzie偶 bazowego modelu). Jednak systemy AI wprowadzaj nowe podatnoci bezpieczestwa (np. wstrzykiwanie prompt贸w, zatruwanie), wymagajce szczeg贸lnej uwagi. Poza bezpieczestwem, red teaming AI bada r贸wnie偶 kwestie uczciwoci (np. stereotypowanie) i szkodliwe treci (np. gloryfikacja przemocy). Wczesna identyfikacja tych problem贸w pozwala na priorytetyzacj inwestycji w obron.
2. **Zoliwe i niezoliwe awarie:**
   Red teaming AI rozwa偶a awarie zar贸wno ze zoliwej, jak i niezoliwej perspektywy. Na przykad, podczas red teamingu nowego Bing, badamy nie tylko jak zoliwi przeciwnicy mog zak贸ci system, ale tak偶e jak zwykli u偶ytkownicy mog napotka problematyczne lub szkodliwe treci. W przeciwiestwie do tradycyjnego red teamingu bezpieczestwa, kt贸ry koncentruje si g贸wnie na zoliwych aktorach, red teaming AI uwzgldnia szerszy zakres person i potencjalnych awarii.
3. **Dynamiczny charakter system贸w AI:**
   Aplikacje AI stale ewoluuj. W aplikacjach du偶ych modeli jzykowych, deweloperzy dostosowuj si do zmieniajcych si wymaga. Cigy red teaming zapewnia sta czujno i adaptacj do zmieniajcych si ryzyk.

Red teaming AI nie jest wszechobejmujcy i powinien by uwa偶any za komplementarne dziaanie do dodatkowych kontroli, takich jak [kontrola dostpu oparta na rolach (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) i kompleksowe rozwizania zarzdzania danymi. Ma uzupenia strategi bezpieczestwa, kt贸ra koncentruje si na stosowaniu bezpiecznych i odpowiedzialnych rozwiza AI, kt贸re uwzgldniaj prywatno i bezpieczestwo, jednoczenie d偶c do minimalizacji stronniczoci, szkodliwych treci i dezinformacji, kt贸re mog podwa偶a zaufanie u偶ytkownik贸w.

Oto lista dodatkowych lektur, kt贸re mog pom贸c lepiej zrozumie, jak red teaming mo偶e pom贸c zidentyfikowa i zagodzi ryzyka w systemach AI:

- [Planowanie red teamingu dla du偶ych modeli jzykowych (LLM) i ich aplikacji](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Czym jest OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - Kluczowa praktyka budowania bezpieczniejszych i bardziej odpowiedzialnych rozwiza AI](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), baza wiedzy o taktykach i technikach stosowanych przez przeciwnik贸w w rzeczywistych atakach na systemy AI.

## Sprawdzenie wiedzy

Jakie mogoby by dobre podejcie do utrzymania integralnoci danych i zapobiegania nadu偶yciom?

1. Posiadanie silnych kontroli opartych na rolach dla dostpu do danych i zarzdzania danymi
2. Wdra偶anie i audytowanie etykietowania danych, aby zapobiec nieprawidowej reprezentacji lub nadu偶yciu danych
3. Upewnienie si, 偶e infrastruktura AI obsuguje filtrowanie treci

A:1, Chocia偶 wszystkie trzy s wietnymi zaleceniami, zapewnienie, 偶e przypisujemy odpowiednie uprawnienia dostpu do danych u偶ytkownikom, znacznie przyczyni si do zapobiegania manipulacji i nieprawidowej reprezentacji danych u偶ywanych przez LLM.

##  Wyzwanie

Przeczytaj wicej o tym, jak mo偶esz [zarzdza i chroni wra偶liwe informacje](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) w erze AI.

## wietna praca, kontynuuj nauk

Po ukoczeniu tej lekcji, sprawd藕 nasz [Kolekcj nauki o Generatywnej AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby kontynuowa podnoszenie swojej wiedzy o Generatywnej AI!

Przejd藕 do Lekcji 14, w kt贸rej przyjrzymy si [cyklowi 偶ycia aplikacji Generatywnej AI](../../../14-the-generative-ai-application-lifecycle/translations/pl/README.md?WT.mc_id=academic-105485-koreyst)!
