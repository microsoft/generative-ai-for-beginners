<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-07-09T16:58:54+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "fi"
}
-->
# Johdanto neuroverkkoihin: Perceptroni

Yksi ensimm√§isist√§ yrityksist√§ toteuttaa jotain nykyaikaisen neuroverkon kaltaista tehtiin vuonna 1957 Frank Rosenblattin toimesta Cornell Aeronautical Laboratoryssa. Se oli laitteistopohjainen toteutus nimelt√§ "Mark-1", joka oli suunniteltu tunnistamaan primitiivisi√§ geometrisia kuvioita, kuten kolmioita, neli√∂it√§ ja ympyr√∂it√§.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> Kuvia Wikipediasta

Sy√∂tekuva esitettiin 20x20 valosolutaulukolla, joten neuroverkolla oli 400 sy√∂tett√§ ja yksi bin√§√§rinen l√§ht√∂. Yksinkertainen verkko sis√§lsi yhden neuronin, jota kutsutaan my√∂s **kynnyslogiikkayksik√∂ksi**. Neuroverkon painot toimivat kuin potentiometrit, joita s√§√§dettiin manuaalisesti harjoitteluvaiheessa.

> ‚úÖ Potentiometri on laite, joka mahdollistaa piirin vastuksen s√§√§t√§misen k√§ytt√§j√§n toimesta.

> The New York Times kirjoitti tuolloin perceptronista: *elektronisen tietokoneen alkio, jonka [laivasto] odottaa pystyv√§n k√§velem√§√§n, puhumaan, n√§kem√§√§n, kirjoittamaan, lis√§√§ntym√§√§n ja tiedostamaan olemassaolonsa.*

## Perceptron-malli

Oletetaan, ett√§ mallissamme on N ominaisuutta, jolloin sy√∂tevektori on kooltaan N. Perceptron on **bin√§√§riluokittelumalli**, eli se pystyy erottamaan kaksi sy√∂teluokkaa toisistaan. Oletamme, ett√§ jokaiselle sy√∂tevektorille x perceptronin l√§ht√∂ on joko +1 tai -1 luokasta riippuen. L√§ht√∂ lasketaan kaavalla:

y(x) = f(w<sup>T</sup>x)

miss√§ f on askelaktivaatiofunktio

## Perceptronin opettaminen

Perceptronin opettamiseksi meid√§n t√§ytyy l√∂yt√§√§ painovektori w, joka luokittelee suurimman osan arvoista oikein, eli tuottaa pienimm√§n **virheen**. T√§m√§ virhe m√§√§ritell√§√§n **perceptronin kriteerin√§** seuraavasti:

E(w) = -‚àëw<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

miss√§:

* summa lasketaan niille harjoitusdatan pisteille i, jotka johtavat v√§√§r√§√§n luokitteluun
* x<sub>i</sub> on sy√∂tedata ja t<sub>i</sub> on joko -1 tai +1 negatiivisille ja positiivisille esimerkeille vastaavasti.

T√§t√§ kriteeri√§ pidet√§√§n painojen w funktiona, ja meid√§n t√§ytyy minimoida se. Usein k√§ytet√§√§n menetelm√§√§ nimelt√§ **gradienttilasku**, jossa aloitetaan jollain alkuarvolla w<sup>(0)</sup> ja p√§ivitet√§√§n painoja jokaisella askeleella kaavan mukaisesti:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Œ∑‚àáE(w)

T√§ss√§ Œ∑ on niin kutsuttu **oppimisnopeus** ja ‚àáE(w) tarkoittaa E:n **gradienttia**. Kun gradientti on laskettu, p√§√§dymme muotoon

w<sup>(t+1)</sup> = w<sup>(t)</sup> + ‚àëŒ∑x<sub>i</sub>t<sub>i</sub>

Algoritmi Pythonilla n√§ytt√§√§ t√§lt√§:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Yhteenveto

T√§ss√§ oppitunnissa opit perceptronista, joka on bin√§√§riluokittelumalli, ja miten sit√§ opetetaan k√§ytt√§m√§ll√§ painovektoria.

## üöÄ Haaste

Jos haluat kokeilla oman perceptronin rakentamista, kokeile t√§t√§ Microsoft Learn -labraa, joka k√§ytt√§√§ Azure ML designeria


## Kertaus & Itsen√§inen opiskelu

N√§hd√§ksesi, miten perceptronia voidaan k√§ytt√§√§ leikkimielisen ongelman sek√§ todellisten ongelmien ratkaisuun ja jatkaaksesi oppimista - siirry Perceptron-muistikirjaan.

T√§ss√§ on my√∂s mielenkiintoinen artikkeli perceptroneista.

## Teht√§v√§

T√§ss√§ oppitunnissa olemme toteuttaneet perceptronin bin√§√§riluokitteluteht√§v√§√§n ja k√§ytt√§neet sit√§ kahden k√§sinkirjoitetun numeron luokitteluun. T√§ss√§ labrassa sinun tulee ratkaista numeroluokittelun ongelma kokonaisuudessaan, eli m√§√§ritt√§√§, mik√§ numero todenn√§k√∂isimmin vastaa annettua kuvaa.

* Ohjeet
* Muistikirja

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattik√§√§nn√∂ksiss√§ saattaa esiinty√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§ist√§ asiakirjaa sen alkuper√§iskielell√§ tulee pit√§√§ virallisena l√§hteen√§. T√§rkeiss√§ tiedoissa suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§ aiheutuvista v√§√§rinymm√§rryksist√§ tai tulkinnoista.