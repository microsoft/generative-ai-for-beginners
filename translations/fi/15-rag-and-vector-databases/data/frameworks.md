<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:33:34+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "fi"
}
-->
# Neuroverkkojen kehykset

Kuten olemme jo oppineet, neuroverkkojen tehokkaaseen kouluttamiseen tarvitsemme kaksi asiaa:

* Kyvyn k√§sitell√§ tensoreita, esimerkiksi kertoa, laskea yhteen ja suorittaa funktioita kuten sigmoid tai softmax
* Kyvyn laskea kaikkien lausekkeiden gradientit gradienttilaskennan suorittamiseksi

Vaikka `numpy`-kirjasto pystyy hoitamaan ensimm√§isen osan, tarvitsemme mekanismin gradienttien laskemiseen. Kehyksess√§mme, jonka kehitimme edellisess√§ osassa, jouduimme ohjelmoimaan kaikki derivaatat manuaalisesti `backward`-metodiin, joka suorittaa takaisinkytkenn√§n. Ihanteellisesti kehys antaisi mahdollisuuden laskea gradientit *mille tahansa lausekkeelle*, jonka voimme m√§√§ritell√§.

Toinen t√§rke√§ asia on kyky suorittaa laskutoimituksia GPU:lla tai muilla erikoistuneilla laskentayksik√∂ill√§, kuten TPU:lla. Syvien neuroverkkojen koulutus vaatii *paljon* laskentaa, ja laskentojen rinnakkaistaminen GPU:illa on eritt√§in t√§rke√§√§.

> ‚úÖ Termi 'rinnakkaistaa' tarkoittaa laskentojen jakamista useille laitteille.

T√§ll√§ hetkell√§ kaksi suosituimpaa neuroverkkokehyst√§ ovat TensorFlow ja PyTorch. Molemmat tarjoavat matalan tason API:n tensorien k√§sittelyyn sek√§ CPU:lla ett√§ GPU:lla. Matalan tason API:n p√§√§ll√§ on my√∂s korkean tason API, nimelt√§√§n vastaavasti Keras ja PyTorch Lightning.

Matalan tason API | TensorFlow | PyTorch
-----------------|------------|---------
Korkean tason API| Keras      | PyTorch

**Matalan tason API:t** molemmissa kehyksiss√§ mahdollistavat ns. **laskentakaavioiden** rakentamisen. T√§m√§ kaavio m√§√§rittelee, miten lasketaan l√§ht√∂arvo (yleens√§ h√§vi√∂funktio) annetuilla sy√∂teparametreilla, ja se voidaan suorittaa GPU:lla, jos sellainen on k√§ytett√§viss√§. On olemassa funktioita, joilla t√§t√§ laskentakaaviota voidaan derivoida ja laskea gradientit, joita voidaan k√§ytt√§√§ mallin parametrien optimointiin.

**Korkean tason API:t** k√§sittelev√§t neuroverkkoja k√§yt√§nn√∂ss√§ **kerrosten sarjana**, mik√§ helpottaa useimpien neuroverkkojen rakentamista huomattavasti. Mallin koulutus vaatii yleens√§ datan valmistelun ja sitten `fit`-funktion kutsumisen ty√∂n suorittamiseksi.

Korkean tason API:n avulla voit rakentaa tyypillisi√§ neuroverkkoja nopeasti ilman, ett√§ sinun tarvitsee huolehtia monista yksityiskohdista. Samaan aikaan matalan tason API tarjoaa paljon enemm√§n kontrollia koulutusprosessiin, ja siksi sit√§ k√§ytet√§√§n paljon tutkimuksessa, kun ty√∂skennell√§√§n uusien neuroverkkorakenteiden parissa.

On my√∂s t√§rke√§√§ ymm√§rt√§√§, ett√§ molempia API:ita voi k√§ytt√§√§ yhdess√§, esimerkiksi voit kehitt√§√§ oman verkon kerrosarkkitehtuurin matalan tason API:lla ja k√§ytt√§√§ sit√§ sitten suuremmassa verkossa, joka on rakennettu ja koulutettu korkean tason API:lla. Tai voit m√§√§ritell√§ verkon korkean tason API:lla kerrosten sarjana ja k√§ytt√§√§ omaa matalan tason koulutussilmukkaasi optimointiin. Molemmat API:t perustuvat samoihin perusk√§sitteisiin ja on suunniteltu toimimaan hyvin yhdess√§.

## Oppiminen

T√§ss√§ kurssissa tarjoamme suurimman osan sis√§ll√∂st√§ sek√§ PyTorchille ett√§ TensorFlow:lle. Voit valita mieluisimman kehyksen ja k√§yd√§ l√§pi vain siihen liittyv√§t muistikirjat. Jos et ole varma, kumpaa kehyst√§ valita, lue netist√§ keskusteluja aiheesta **PyTorch vs. TensorFlow**. Voit my√∂s tutustua molempiin kehyksiin saadaksesi paremman k√§sityksen.

Miss√§ mahdollista, k√§yt√§mme yksinkertaisuuden vuoksi korkean tason API:ita. Uskomme kuitenkin, ett√§ on t√§rke√§√§ ymm√§rt√§√§ neuroverkkojen toiminta alusta alkaen, joten aluksi ty√∂skentelemme matalan tason API:n ja tensorien kanssa. Jos haluat kuitenkin p√§√§st√§ nopeasti alkuun etk√§ halua k√§ytt√§√§ paljon aikaa n√§iden yksityiskohtien oppimiseen, voit hyp√§t√§ suoraan korkean tason API:n muistikirjoihin.

## ‚úçÔ∏è Harjoitukset: Kehykset

Jatka oppimista seuraavissa muistikirjoissa:

Matalan tason API | TensorFlow+Keras -muistikirja | PyTorch
-----------------|-------------------------------|---------
Korkean tason API| Keras                         | *PyTorch Lightning*

Kun olet hallinnut kehykset, kerrataan viel√§ ylisovittamisen k√§site.

# Ylisovittaminen

Ylisovittaminen on eritt√§in t√§rke√§ k√§site koneoppimisessa, ja sen ymm√§rt√§minen oikein on ratkaisevan t√§rke√§√§!

Tarkastellaan seuraavaa ongelmaa, jossa pyrit√§√§n approksimoimaan 5 pistett√§ (joita on merkitty `x` alla olevissa kaavioissa):

!linear | overfit
-------------------------|--------------------------
**Lineaarinen malli, 2 parametria** | **Ei-lineaarinen malli, 7 parametria**
Koulutuksen virhe = 5.3 | Koulutuksen virhe = 0
Validointivirhe = 5.1 | Validointivirhe = 20

* Vasemmalla n√§emme hyv√§n suoran viivan approksimaation. Koska parametrien m√§√§r√§ on sopiva, malli ymm√§rt√§√§ pisteiden jakauman oikein.
* Oikealla malli on liian voimakas. Koska meill√§ on vain 5 pistett√§ ja mallilla on 7 parametria, se voi sovittaa kaikki pisteet tarkasti, jolloin koulutuksen virhe on 0. T√§m√§ est√§√§ mallia ymm√§rt√§m√§st√§ datan oikeaa kaavaa, mink√§ vuoksi validointivirhe on hyvin korkea.

On eritt√§in t√§rke√§√§ l√∂yt√§√§ oikea tasapaino mallin rikkauden (parametrien m√§√§r√§) ja koulutusn√§ytteiden m√§√§r√§n v√§lill√§.

## Miksi ylisovittaminen tapahtuu

  * Liian v√§h√§n koulutusdataa
  * Liian voimakas malli
  * Liikaa kohinaa sy√∂tteess√§

## Miten havaita ylisovittaminen

Kuten yll√§ olevasta kaaviosta n√§kyy, ylisovittaminen voidaan havaita hyvin matalasta koulutusvirheest√§ ja korkeasta validointivirheest√§. Tavallisesti koulutuksen aikana sek√§ koulutus- ett√§ validointivirheet alkavat pienenty√§, mutta jossain vaiheessa validointivirhe saattaa lakata pienentym√§st√§ ja alkaa kasvaa. T√§m√§ on merkki ylisovittamisesta ja osoitus siit√§, ett√§ koulutus kannattaa todenn√§k√∂isesti lopettaa t√§ss√§ vaiheessa (tai ainakin tallentaa mallin tilanne).

ylisovittaminen

## Miten est√§√§ ylisovittamista

Jos huomaat ylisovittamista, voit tehd√§ seuraavia asioita:

 * Lis√§t√§ koulutusdatan m√§√§r√§√§
 * V√§hent√§√§ mallin monimutkaisuutta
 * K√§ytt√§√§ jonkinlaista regularisointitekniikkaa, kuten Dropoutia, jota k√§sittelemme my√∂hemmin.

## Ylisovittaminen ja bias-varianssi-vaihtokauppa

Ylisovittaminen on itse asiassa osa laajempaa tilastotieteellist√§ ongelmaa, jota kutsutaan bias-varianssi-vaihtokaupaksi. Kun tarkastelemme mallin virhel√§hteit√§, voimme erottaa kaksi virhetyyppi√§:

* **Bias-virheet** johtuvat siit√§, ett√§ algoritmimme ei pysty mallintamaan koulutusdatan ja mallin v√§list√§ suhdetta oikein. T√§m√§ voi johtua siit√§, ett√§ mallimme ei ole tarpeeksi voimakas (**alisovittaminen**).
* **Varianssivirheet** johtuvat siit√§, ett√§ malli sovittaa kohinaa sy√∂tteess√§ merkityksellisen suhteen sijaan (**ylisovittaminen**).

Koulutuksen aikana bias-virhe pienenee (kun malli oppii approksimoimaan dataa), mutta varianssivirhe kasvaa. On t√§rke√§√§ lopettaa koulutus joko manuaalisesti (kun havaitsemme ylisovittamisen) tai automaattisesti (ottamalla k√§ytt√∂√∂n regularisointi) ylisovittamisen est√§miseksi.

## Yhteenveto

T√§ss√§ oppitunnissa opit eroja kahden suosituimman teko√§lykehyksen, TensorFlow:n ja PyTorchin, eri API:iden v√§lill√§. Lis√§ksi opit eritt√§in t√§rke√§st√§ aiheesta, ylisovittamisesta.

## üöÄ Haaste

Mukana olevissa muistikirjoissa l√∂yd√§t teht√§vi√§ sivun alareunasta; k√§y muistikirjat l√§pi ja suorita teht√§v√§t.

## Kertaus & Itsen√§inen opiskelu

Tutki seuraavia aiheita:

- TensorFlow
- PyTorch
- Ylisovittaminen

Kysy itselt√§si seuraavat kysymykset:

- Mik√§ on ero TensorFlow:n ja PyTorchin v√§lill√§?
- Mik√§ on ero ylisovittamisen ja alisovittamisen v√§lill√§?

## Teht√§v√§

T√§ss√§ laboratoriossa sinun tulee ratkaista kaksi luokitteluongelmaa k√§ytt√§en yksikerroksisia ja monikerroksisia t√§ysin yhdistettyj√§ verkkoja PyTorchilla tai TensorFlow:lla.

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattik√§√§nn√∂ksiss√§ saattaa esiinty√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§ist√§ asiakirjaa sen alkuper√§iskielell√§ tulee pit√§√§ virallisena l√§hteen√§. T√§rkeiss√§ tiedoissa suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§ aiheutuvista v√§√§rinymm√§rryksist√§ tai tulkinnoista.