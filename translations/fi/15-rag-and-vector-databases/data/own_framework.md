<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:48:09+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "fi"
}
-->
# Johdanto neuroverkkoihin. Monikerroksinen perceptroni

EdellisessÃ¤ osassa opit yksinkertaisimmasta neuroverkkomallista â€“ yksikerroksisesta perceptronista, joka on lineaarinen kahden luokan luokittelumalli.

TÃ¤ssÃ¤ osassa laajennamme tÃ¤tÃ¤ mallia joustavammaksi rakenteeksi, joka mahdollistaa:

* **moniluokkaluokittelun** kahden luokan lisÃ¤ksi
* **regressio-ongelmien** ratkaisemisen luokittelun lisÃ¤ksi
* luokkien erottamisen, jotka eivÃ¤t ole lineaarisesti erotettavissa

KehitÃ¤mme myÃ¶s oman modulaarisen kehyksen Pythonilla, jonka avulla voimme rakentaa erilaisia neuroverkkorakenteita.

## Koneoppimisen formalisaatio

Aloitetaan koneoppimisongelman formalisoimisesta. Oletetaan, ettÃ¤ meillÃ¤ on opetusdatajoukko **X** ja siihen liittyvÃ¤t tunnisteet **Y**, ja meidÃ¤n tÃ¤ytyy rakentaa malli *f*, joka tekee mahdollisimman tarkkoja ennusteita. Ennusteiden laatua mitataan **hÃ¤viÃ¶funktiolla** â„’. Seuraavia hÃ¤viÃ¶funktioita kÃ¤ytetÃ¤Ã¤n usein:

* Regressio-ongelmassa, jossa ennustetaan lukuarvoa, voimme kÃ¤yttÃ¤Ã¤ **absoluuttista virhettÃ¤** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| tai **neliÃ¶virhettÃ¤** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Luokittelussa kÃ¤ytÃ¤mme **0-1 hÃ¤viÃ¶tÃ¤** (joka vastaa mallin **tarkkuutta**) tai **logistista hÃ¤viÃ¶tÃ¤**.

Yksikerroksisessa perceptronissa funktio *f* mÃ¤Ã¤riteltiin lineaarisena funktiona *f(x)=wx+b* (tÃ¤ssÃ¤ *w* on painomatriisi, *x* on syÃ¶teominaisuuksien vektori ja *b* on bias-vektori). Eri neuroverkkorakenteissa tÃ¤mÃ¤ funktio voi olla monimutkaisempi.

> Luokittelussa on usein toivottavaa saada verkon ulostulona luokkien todennÃ¤kÃ¶isyydet. Muuttaaksemme mielivaltaiset luvut todennÃ¤kÃ¶isyyksiksi (esim. normalisoidaksemme ulostulon), kÃ¤ytÃ¤mme usein **softmax**-funktiota Ïƒ, jolloin funktio *f* muuttuu muotoon *f(x)=Ïƒ(wx+b)*

EdellÃ¤ mÃ¤Ã¤ritellyssÃ¤ funktiossa *f* parametreja *w* ja *b* kutsutaan **parametreiksi** Î¸=âŸ¨*w,b*âŸ©. Kun datasetti âŸ¨**X**,**Y**âŸ© on annettu, voimme laskea kokonaisvirheen koko datasetille parametrien Î¸ funktiona.

> âœ… **Neuroverkon koulutuksen tavoitteena on minimoida virhe muuttamalla parametreja Î¸**

## Gradienttilaskun optimointi

Tunnettu funktioiden optimointimenetelmÃ¤ on **gradienttilasku**. Ajatus on, ettÃ¤ voimme laskea hÃ¤viÃ¶funktion derivaatan (moniulotteisessa tapauksessa **gradientin**) parametreihin nÃ¤hden ja muuttaa parametreja siten, ettÃ¤ virhe pienenee. TÃ¤mÃ¤ voidaan formalisoida seuraavasti:

* Alustetaan parametrit satunnaisilla arvoilla w<sup>(0)</sup>, b<sup>(0)</sup>
* Toistetaan seuraava askel monta kertaa:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup> - Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup> - Î·âˆ‚â„’/âˆ‚b

Koulutuksen aikana optimointiaskeleet lasketaan yleensÃ¤ koko datasetin perusteella (muista, ettÃ¤ hÃ¤viÃ¶ lasketaan kaikkien opetusnÃ¤ytteiden summana). KÃ¤ytÃ¤nnÃ¶ssÃ¤ otamme kuitenkin pieniÃ¤ osajoukkoja datasta, joita kutsutaan **minierÃ¤joukoiksi** (minibatches), ja laskemme gradientit nÃ¤iden osajoukkojen perusteella. Koska osajoukko valitaan satunnaisesti joka kerta, tÃ¤tÃ¤ menetelmÃ¤Ã¤ kutsutaan **stokastiseksi gradienttilaskuksi** (SGD).

## Monikerroksiset perceptronit ja takaisinkytkentÃ¤ (backpropagation)

Yksikerroksinen verkko, kuten yllÃ¤ nÃ¤htiin, pystyy luokittelemaan lineaarisesti erotettavat luokat. Rakentaaksemme monipuolisemman mallin voimme yhdistÃ¤Ã¤ useita verkon kerroksia. Matemaattisesti tÃ¤mÃ¤ tarkoittaa, ettÃ¤ funktio *f* saa monimutkaisemman muodon ja lasketaan useassa vaiheessa:
* z<sub>1</sub> = w<sub>1</sub>x + b<sub>1</sub>
* z<sub>2</sub> = w<sub>2</sub>Î±(z<sub>1</sub>) + b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

TÃ¤ssÃ¤ Î± on **ei-lineaarinen aktivointifunktio**, Ïƒ on softmax-funktio ja parametrit Î¸ = âŸ¨*w<sub>1</sub>, b<sub>1</sub>, w<sub>2</sub>, b<sub>2</sub>*âŸ©.

Gradienttilasku pysyy samana, mutta gradienttien laskeminen on monimutkaisempaa. KetjusÃ¤Ã¤nnÃ¶n avulla voimme laskea derivaatat seuraavasti:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… KetjusÃ¤Ã¤ntÃ¶Ã¤ kÃ¤ytetÃ¤Ã¤n hÃ¤viÃ¶funktion derivaattojen laskemiseen parametrien suhteen.

Huomaa, ettÃ¤ kaikkien nÃ¤iden lausekkeiden vasemmanpuoleinen osa on sama, joten voimme tehokkaasti laskea derivaatat aloittaen hÃ¤viÃ¶funktiosta ja kulkemalla "taaksepÃ¤in" laskentakaavion lÃ¤pi. TÃ¤tÃ¤ monikerroksisen perceptronin koulutusmenetelmÃ¤Ã¤ kutsutaan **takaisinkytkennÃ¤ksi** eli backpropagationiksi.

> TODO: kuvan lÃ¤hde

> âœ… KÃ¤ymme backpropagationin tarkemmin lÃ¤pi esimerkkimuistiossa.

## Yhteenveto

TÃ¤ssÃ¤ oppitunnissa rakensimme oman neuroverkkokirjaston ja kÃ¤ytimme sitÃ¤ yksinkertaisen kaksidimensioisen luokittelutehtÃ¤vÃ¤n ratkaisemiseen.

## ğŸš€ Haaste

Mukana olevassa muistiossa toteutat oman kehyksesi monikerroksisten perceptronien rakentamiseen ja kouluttamiseen. NÃ¤et yksityiskohtaisesti, miten nykyaikaiset neuroverkot toimivat.

Siirry OwnFramework-muistioon ja kÃ¤y se lÃ¤pi.

## Kertaus & ItsenÃ¤inen opiskelu

TakaisinkytkentÃ¤ on yleinen algoritmi tekoÃ¤lyssÃ¤ ja koneoppimisessa, jota kannattaa opiskella tarkemmin.

## TehtÃ¤vÃ¤

TÃ¤ssÃ¤ laboratoriossa sinun tulee kÃ¤yttÃ¤Ã¤ tÃ¤ssÃ¤ oppitunnissa rakentamaasi kehystÃ¤ ratkaistaksesi MNIST-kÃ¤sinkirjoitettujen numeroiden luokittelutehtÃ¤vÃ¤.

* Ohjeet
* Muistio

**Vastuuvapauslauseke**:  
TÃ¤mÃ¤ asiakirja on kÃ¤Ã¤nnetty kÃ¤yttÃ¤mÃ¤llÃ¤ tekoÃ¤lypohjaista kÃ¤Ã¤nnÃ¶spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ettÃ¤ automaattikÃ¤Ã¤nnÃ¶ksissÃ¤ saattaa esiintyÃ¤ virheitÃ¤ tai epÃ¤tarkkuuksia. AlkuperÃ¤istÃ¤ asiakirjaa sen alkuperÃ¤iskielellÃ¤ tulee pitÃ¤Ã¤ virallisena lÃ¤hteenÃ¤. TÃ¤rkeissÃ¤ asioissa suositellaan ammattimaista ihmiskÃ¤Ã¤nnÃ¶stÃ¤. Emme ole vastuussa tÃ¤mÃ¤n kÃ¤Ã¤nnÃ¶ksen kÃ¤ytÃ¶stÃ¤ aiheutuvista vÃ¤Ã¤rinymmÃ¤rryksistÃ¤ tai tulkinnoista.