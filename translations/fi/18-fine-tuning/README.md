<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "807f0d9fc1747e796433534e1be6a98a",
  "translation_date": "2025-10-17T19:45:12+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "fi"
}
-->
[![Avoimet l√§hdemallit](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.fi.png)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# LLM:n hienos√§√§t√∂

Suurten kielimallien k√§ytt√∂ generatiivisten teko√§lysovellusten rakentamiseen tuo mukanaan uusia haasteita. Yksi keskeinen ongelma on varmistaa mallin tuottaman sis√§ll√∂n vastausten laatu (tarkkuus ja osuvuus) k√§ytt√§j√§n antamaan pyynt√∂√∂n. Aiemmissa oppitunneissa k√§sittelimme tekniikoita, kuten kehotteen suunnittelua ja hakuun perustuvaa generointia, jotka pyrkiv√§t ratkaisemaan ongelman _muokkaamalla kehotteen sy√∂tett√§_ olemassa olevaan malliin.

T√§m√§n p√§iv√§n oppitunnilla k√§sittelemme kolmatta tekniikkaa, **hienos√§√§t√∂√§**, joka pyrkii ratkaisemaan haasteen _kouluttamalla mallia uudelleen_ lis√§datan avulla. Sukelletaan yksityiskohtiin.

## Oppimistavoitteet

T√§m√§ oppitunti esittelee hienos√§√§d√∂n k√§sitteen esikoulutetuille kielimalleille, tutkii t√§m√§n l√§hestymistavan etuja ja haasteita sek√§ tarjoaa ohjeita siit√§, milloin ja miten hienos√§√§t√∂√§ kannattaa k√§ytt√§√§ generatiivisten teko√§lymallien suorituskyvyn parantamiseksi.

Oppitunnin lopussa sinun pit√§isi pysty√§ vastaamaan seuraaviin kysymyksiin:

- Mit√§ hienos√§√§t√∂ kielimalleille tarkoittaa?
- Milloin ja miksi hienos√§√§t√∂ on hy√∂dyllist√§?
- Kuinka voin hienos√§√§t√§√§ esikoulutettua mallia?
- Mitk√§ ovat hienos√§√§d√∂n rajoitukset?

Valmis? Aloitetaan.

## Havainnollistettu opas

Haluatko saada yleiskuvan siit√§, mit√§ k√§sittelemme ennen kuin sukellamme syvemm√§lle? Tutustu t√§h√§n havainnollistettuun oppaaseen, joka kuvaa oppimismatkaa t√§m√§n oppitunnin aikana - ydinajatuksista ja hienos√§√§d√∂n motivaatiosta prosessin ja parhaiden k√§yt√§nt√∂jen ymm√§rt√§miseen hienos√§√§t√∂teht√§v√§n suorittamiseksi. T√§m√§ on kiehtova aihe tutkittavaksi, joten √§l√§ unohda tarkistaa [Resurssit](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) -sivua saadaksesi lis√§linkkej√§ itseohjautuvaan oppimismatkaasi!

![Havainnollistettu opas kielimallien hienos√§√§t√∂√∂n](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.fi.png)

## Mit√§ hienos√§√§t√∂ kielimalleille tarkoittaa?

M√§√§ritelm√§n mukaan suuret kielimallit ovat _esikoulutettuja_ suurilla m√§√§rill√§ teksti√§, jotka on ker√§tty monipuolisista l√§hteist√§, kuten internetist√§. Kuten olemme oppineet aiemmilla oppitunneilla, tarvitsemme tekniikoita, kuten _kehotteen suunnittelu_ ja _hakuun perustuva generointi_, parantaaksemme mallin vastausten laatua k√§ytt√§j√§n kysymyksiin ("kehotteisiin").

Yksi suosittu kehotteen suunnittelutekniikka sis√§lt√§√§ mallille enemm√§n ohjeita siit√§, mit√§ vastaukselta odotetaan, joko antamalla _ohjeita_ (selke√§t ohjeet) tai _muutamia esimerkkej√§_ (ep√§suorat ohjeet). T√§t√§ kutsutaan _few-shot learningiksi_, mutta sill√§ on kaksi rajoitusta:

- Mallin token-rajoitukset voivat rajoittaa annettavien esimerkkien m√§√§r√§√§ ja vaikuttavuutta.
- Mallin token-kustannukset voivat tehd√§ esimerkkien lis√§√§misest√§ jokaiseen kehotteeseen kallista ja rajoittaa joustavuutta.

Hienos√§√§t√∂ on yleinen k√§yt√§nt√∂ koneoppimisj√§rjestelmiss√§, jossa otamme esikoulutetun mallin ja koulutamme sen uudelleen uudella datalla parantaaksemme sen suorituskyky√§ tiettyyn teht√§v√§√§n. Kielimallien kontekstissa voimme hienos√§√§t√§√§ esikoulutetun mallin _huolellisesti valitulla esimerkkijoukolla tietty√§ teht√§v√§√§ tai sovellusaluetta varten_ luodaksemme **r√§√§t√§l√∂idyn mallin**, joka voi olla tarkempi ja osuvampi kyseiselle teht√§v√§lle tai alueelle. Hienos√§√§d√∂n sivuhy√∂tyn√§ on my√∂s se, ett√§ se voi v√§hent√§√§ tarvittavien esimerkkien m√§√§r√§√§ few-shot learningissa - v√§hent√§en tokenien k√§ytt√∂√§ ja siihen liittyvi√§ kustannuksia.

## Milloin ja miksi meid√§n pit√§isi hienos√§√§t√§√§ malleja?

T√§ss√§ kontekstissa, kun puhumme hienos√§√§d√∂st√§, viittaamme **valvottuun** hienos√§√§t√∂√∂n, jossa uudelleenkoulutus tehd√§√§n **lis√§√§m√§ll√§ uutta dataa**, joka ei ollut osa alkuper√§ist√§ koulutusdatakokonaisuutta. T√§m√§ eroaa valvomattomasta hienos√§√§t√∂menetelm√§st√§, jossa mallia koulutetaan uudelleen alkuper√§isell√§ datalla, mutta eri hyperparametreilla.

Keskeinen asia muistaa on, ett√§ hienos√§√§t√∂ on edistynyt tekniikka, joka vaatii tietyn tason asiantuntemusta haluttujen tulosten saavuttamiseksi. Jos se tehd√§√§n v√§√§rin, se ei v√§ltt√§m√§tt√§ tuota odotettuja parannuksia ja voi jopa heikent√§√§ mallin suorituskyky√§ kohdealueellasi.

Joten ennen kuin opit "kuinka" hienos√§√§t√§√§ kielimalleja, sinun t√§ytyy tiet√§√§ "miksi" sinun pit√§isi valita t√§m√§ reitti ja "milloin" aloittaa hienos√§√§t√∂prosessi. Aloita kysym√§ll√§ itselt√§si n√§m√§ kysymykset:

- **K√§ytt√∂tapaus**: Mik√§ on hienos√§√§d√∂n _k√§ytt√∂tapauksesi_? Mit√§ osaa nykyisest√§ esikoulutetusta mallista haluat parantaa?
- **Vaihtoehdot**: Oletko kokeillut _muita tekniikoita_ haluttujen tulosten saavuttamiseksi? K√§yt√§ niit√§ vertailukohtana.
  - Kehotteen suunnittelu: Kokeile tekniikoita, kuten few-shot-kehotteita, joissa on esimerkkej√§ asiaankuuluvista kehotusvastauksista. Arvioi vastausten laatu.
  - Hakuun perustuva generointi: Kokeile kehotteiden t√§ydent√§mist√§ hakutuloksilla, jotka on haettu etsim√§ll√§ dataasi. Arvioi vastausten laatu.
- **Kustannukset**: Oletko tunnistanut hienos√§√§d√∂n kustannukset?
  - S√§√§dett√§vyys - onko esikoulutettu malli saatavilla hienos√§√§t√∂√∂n?
  - Ty√∂ - koulutusdatan valmistelu, mallin arviointi ja hienos√§√§t√∂.
  - Laskenta - hienos√§√§t√∂teht√§vien suorittaminen ja hienos√§√§detyn mallin k√§ytt√∂√∂notto.
  - Data - riitt√§v√§n laadukkaiden esimerkkien saatavuus hienos√§√§t√∂vaikutuksen saavuttamiseksi.
- **Hy√∂dyt**: Oletko vahvistanut hienos√§√§d√∂n hy√∂dyt?
  - Laatu - ylitt√§√§k√∂ hienos√§√§detty malli vertailukohdan?
  - Kustannukset - v√§hent√§√§k√∂ se tokenien k√§ytt√∂√§ yksinkertaistamalla kehotteita?
  - Laajennettavuus - voiko perusmallia k√§ytt√§√§ uudelleen uusille alueille?

Vastaamalla n√§ihin kysymyksiin sinun pit√§isi pysty√§ p√§√§tt√§m√§√§n, onko hienos√§√§t√∂ oikea l√§hestymistapa k√§ytt√∂tapauksessasi. Ihanteellisesti l√§hestymistapa on perusteltu vain, jos hy√∂dyt ylitt√§v√§t kustannukset. Kun p√§√§t√§t jatkaa, on aika mietti√§ _kuinka_ voit hienos√§√§t√§√§ esikoulutettua mallia.

Haluatko lis√§tietoja p√§√§t√∂ksentekoprosessista? Katso [Hienos√§√§t√§√§k√∂ vai ei](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Kuinka voimme hienos√§√§t√§√§ esikoulutettua mallia?

Hienos√§√§t√§√§ksesi esikoulutettua mallia tarvitset:

- esikoulutetun mallin hienos√§√§t√∂√§ varten
- datakokonaisuuden hienos√§√§t√∂√∂n
- koulutusymp√§rist√∂n hienos√§√§t√∂teht√§v√§n suorittamiseen
- is√§nn√∂intialustan hienos√§√§detyn mallin k√§ytt√∂√∂nottoon

## Hienos√§√§t√∂ k√§yt√§nn√∂ss√§

Seuraavat resurssit tarjoavat vaiheittaisia opetusohjelmia, jotka opastavat sinut todellisen esimerkin l√§pi valitun mallin ja huolellisesti valitun datakokonaisuuden avulla. N√§iden opetusohjelmien l√§pik√§ymiseen tarvitset tilin kyseisell√§ palveluntarjoajalla sek√§ p√§√§syn asiaankuuluvaan malliin ja datakokonaisuuksiin.

| Palveluntarjoaja | Opetusohjelma                                                                                                                                                                       | Kuvaus                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ----------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI           | [Kuinka hienos√§√§t√§√§ chat-malleja](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                | Opettele hienos√§√§t√§m√§√§n `gpt-35-turbo` tietty√§ aluetta varten ("reseptiassistentti") valmistamalla koulutusdataa, suorittamalla hienos√§√§t√∂teht√§v√§ ja k√§ytt√§m√§ll√§ hienos√§√§detty√§ mallia p√§√§ttelyyn.                                                                                                                                                                                                                                              |
| Azure OpenAI     | [GPT 3.5 Turbo hienos√§√§t√∂opetusohjelma](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | Opettele hienos√§√§t√§m√§√§n `gpt-35-turbo-0613` -mallia **Azurella** ottamalla askeleet koulutusdatan luomiseen ja lataamiseen, hienos√§√§t√∂teht√§v√§n suorittamiseen. Ota k√§ytt√∂√∂n ja k√§yt√§ uutta mallia.                                                                                                                                                                                                                                                                 |
| Hugging Face     | [Hienos√§√§t√∂ LLM:ille Hugging Facen avulla](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | T√§m√§ blogikirjoitus opastaa sinut hienos√§√§t√§m√§√§n _avointa LLM:√§√§_ (esim. `CodeLlama 7B`) k√§ytt√§m√§ll√§ [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) -kirjastoa ja [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) avoimilla [datakokonaisuuksilla](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) Hugging Facessa. |
|                   |                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ü§ó AutoTrain     | [Hienos√§√§t√∂ LLM:ille AutoTrainin avulla](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                         | AutoTrain (tai AutoTrain Advanced) on Hugging Facen kehitt√§m√§ Python-kirjasto, joka mahdollistaa hienos√§√§d√∂n monille eri teht√§ville, mukaan lukien LLM-hienos√§√§t√∂. AutoTrain on kooditon ratkaisu, ja hienos√§√§t√∂ voidaan tehd√§ omassa pilvess√§, Hugging Face Spacesissa tai paikallisesti. Se tukee sek√§ verkkopohjaista k√§ytt√∂liittym√§√§, CLI:t√§ ett√§ koulutusta yaml-konfiguraatiotiedostojen avulla.                                                                               |
|                   |                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Teht√§v√§

Valitse yksi yll√§ olevista opetusohjelmista ja k√§y se l√§pi. _Saatamme kopioida version n√§ist√§ opetusohjelmista Jupyter Notebooks -tiedostoihin t√§ss√§ repossa vain viitteeksi. K√§yt√§ alkuper√§isi√§ l√§hteit√§ saadaksesi uusimmat versiot_.

## Hienoa ty√∂t√§! Jatka oppimista.

T√§m√§n oppitunnin j√§lkeen tutustu [Generatiivisen teko√§lyn oppimiskokoelmaan](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) jatkaaksesi generatiivisen teko√§lyn tiet√§myksesi kehitt√§mist√§!

Onnittelut!! Olet suorittanut t√§m√§n kurssin v2-sarjan viimeisen oppitunnin! √Ñl√§ lopeta oppimista ja rakentamista. \*\*Tutustu [RESURSSIT](RESOURCES.md?WT.mc_id=academic-105485-koreyst) -sivuun saadaksesi lis√§ehdotuksia juuri t√§st√§ aiheesta.

My√∂s v1-sarjan oppitunteja on p√§ivitetty lis√§√§ teht√§vill√§ ja k√§sitteill√§. Joten ota hetki aikaa p√§ivitt√§√§ksesi tietosi - ja ole hyv√§ [ja jaa kysymyksesi ja palautteesi](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) auttaaksesi meit√§ parantamaan n√§it√§ oppitunteja yhteis√∂lle.

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. T√§rkeiss√§ tiedoissa suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.