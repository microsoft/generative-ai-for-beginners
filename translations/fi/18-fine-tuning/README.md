<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-07-09T17:45:07+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "fi"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.fi.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# LLM-mallisi hienos√§√§t√∂

Suuriin kielimalleihin perustuvien generatiivisten teko√§lysovellusten rakentaminen tuo mukanaan uusia haasteita. Keskeinen kysymys on varmistaa mallin tuottamien vastausten laatu (tarkkuus ja merkityksellisyys) k√§ytt√§j√§n pyynn√∂n perusteella. Aiemmissa oppitunneissa k√§sittelimme tekniikoita, kuten promptin suunnittelua ja hakua hy√∂dynt√§v√§√§ generointia, jotka pyrkiv√§t ratkaisemaan ongelman _muokkaamalla mallille annettavaa sy√∂tett√§_.

T√§m√§n p√§iv√§n oppitunnilla k√§sittelemme kolmatta tekniikkaa, **hienos√§√§t√∂√§**, joka pyrkii ratkaisemaan haasteen _kouluttamalla mallia uudelleen_ lis√§aineistolla. Sukelletaanpa yksityiskohtiin.

## Oppimistavoitteet

T√§m√§ oppitunti esittelee hienos√§√§d√∂n k√§sitteen esikoulutetuille kielimalleille, tutkii t√§m√§n l√§hestymistavan etuja ja haasteita sek√§ antaa ohjeita siit√§, milloin ja miten hienos√§√§t√∂√§ kannattaa k√§ytt√§√§ generatiivisten teko√§lymalliesi suorituskyvyn parantamiseksi.

Oppitunnin lopussa sinun pit√§isi osata vastata seuraaviin kysymyksiin:

- Mit√§ hienos√§√§t√∂ tarkoittaa kielimalleille?
- Milloin ja miksi hienos√§√§t√∂ on hy√∂dyllist√§?
- Miten voin hienos√§√§t√§√§ esikoulutetun mallin?
- Mitk√§ ovat hienos√§√§d√∂n rajoitukset?

Valmis? Aloitetaan.

## Kuvitettu opas

Haluatko saada kokonaiskuvan siit√§, mit√§ k√§sittelemme ennen syvemp√§√§ sukellusta? Tutustu t√§h√§n kuvitettuun oppaaseen, joka kuvaa oppimismatkan t√§m√§n oppitunnin aiheisiin ‚Äì ydink√§sitteiden ja hienos√§√§d√∂n motivaation oppimisesta prosessin ja parhaiden k√§yt√§nt√∂jen ymm√§rt√§miseen hienos√§√§t√∂teht√§v√§n suorittamiseksi. T√§m√§ on kiehtova aihe, joten muista my√∂s tutustua [Resurssit](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) -sivuun, josta l√∂yd√§t lis√§linkkej√§ itseopiskelun tueksi!

![Kuvitettu opas kielimallien hienos√§√§t√∂√∂n](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.fi.png)

## Mit√§ hienos√§√§t√∂ tarkoittaa kielimalleille?

M√§√§ritelm√§n mukaan suuret kielimallit ovat _esikoulutettuja_ suurilla tekstim√§√§rill√§, jotka on ker√§tty monista eri l√§hteist√§, mukaan lukien internet. Kuten olemme oppineet aiemmissa oppitunneissa, tarvitsemme tekniikoita kuten _promptin suunnittelua_ ja _hakua hy√∂dynt√§v√§√§ generointia_ parantaaksemme mallin vastausten laatua k√§ytt√§j√§n kysymyksiin ("prompteihin").

Yksi suosittu promptin suunnittelutekniikka on antaa mallille enemm√§n ohjeita siit√§, mit√§ vastauksessa odotetaan, joko antamalla _ohjeita_ (selke√§√§ ohjausta) tai _muutamia esimerkkej√§_ (ep√§suoraa ohjausta). T√§t√§ kutsutaan _few-shot-oppimiseksi_, mutta siin√§ on kaksi rajoitusta:

- Mallin token-rajoitukset voivat rajoittaa annettavien esimerkkien m√§√§r√§√§ ja heikent√§√§ tehokkuutta.
- Mallin token-kustannukset voivat tehd√§ esimerkkien lis√§√§misest√§ jokaiseen promptiin kallista ja rajoittaa joustavuutta.

Hienos√§√§t√∂ on yleinen k√§yt√§nt√∂ koneoppimisj√§rjestelmiss√§, jossa otetaan esikoulutettu malli ja koulutetaan sit√§ uudelleen uudella aineistolla parantaaksemme sen suorituskyky√§ tietyss√§ teht√§v√§ss√§. Kielimallien yhteydess√§ voimme hienos√§√§t√§√§ esikoulutettua mallia _valikoidulla esimerkkiaineistolla tietty√§ teht√§v√§√§ tai sovellusaluetta varten_ luodaksemme **r√§√§t√§l√∂idyn mallin**, joka voi olla tarkempi ja merkityksellisempi juuri kyseiseen teht√§v√§√§n tai alueeseen. Hienos√§√§d√∂n sivuvaikutuksena on my√∂s se, ett√§ se voi v√§hent√§√§ few-shot-oppimiseen tarvittavien esimerkkien m√§√§r√§√§ ‚Äì v√§hent√§en tokenien k√§ytt√∂√§ ja siihen liittyvi√§ kustannuksia.

## Milloin ja miksi hienos√§√§t√§√§ malleja?

T√§ss√§ yhteydess√§, kun puhumme hienos√§√§d√∂st√§, tarkoitamme **valvottua** hienos√§√§t√∂√§, jossa uudelleenkoulutus tehd√§√§n **lis√§√§m√§ll√§ uutta dataa**, jota ei ollut alkuper√§isess√§ koulutusdatassa. T√§m√§ eroaa valvomattomasta hienos√§√§d√∂st√§, jossa mallia koulutetaan uudelleen alkuper√§isell√§ datalla, mutta eri hyperparametreilla.

T√§rke√§√§ on muistaa, ett√§ hienos√§√§t√∂ on edistynyt tekniikka, joka vaatii tietyn tason asiantuntemusta haluttujen tulosten saavuttamiseksi. Jos hienos√§√§t√∂ tehd√§√§n v√§√§rin, se ei v√§ltt√§m√§tt√§ paranna mallin suorituskyky√§ odotetusti, ja voi jopa heikent√§√§ mallin toimintaa kohdealueellasi.

Ennen kuin opit "miten" hienos√§√§t√§√§ kielimalleja, sinun t√§ytyy tiet√§√§ "miksi" valita t√§m√§ l√§hestymistapa ja "milloin" aloittaa hienos√§√§t√∂prosessi. Aloita kysym√§ll√§ itselt√§si n√§m√§ kysymykset:

- **K√§ytt√∂tapaus**: Mik√§ on sinun _k√§ytt√∂tapauksesi_ hienos√§√§d√∂lle? Mit√§ nykyisen esikoulutetun mallin osa-aluetta haluat parantaa?
- **Vaihtoehdot**: Oletko kokeillut _muita tekniikoita_ haluttujen tulosten saavuttamiseksi? K√§yt√§ niit√§ vertailupohjana.
  - Promptin suunnittelu: Kokeile few-shot-promptteja, joissa on esimerkkej√§ relevantista vastauksesta. Arvioi vastausten laatua.
  - Hakua hy√∂dynt√§v√§ generointi: Kokeile t√§ydent√§√§ promptteja hakutuloksilla, jotka haetaan datastasi. Arvioi vastausten laatua.
- **Kustannukset**: Oletko tunnistanut hienos√§√§d√∂n kustannukset?
  - S√§√§dett√§vyys ‚Äì onko esikoulutettu malli saatavilla hienos√§√§t√∂√∂n?
  - Ty√∂m√§√§r√§ ‚Äì koulutusdatan valmistelu, mallin arviointi ja hienos√§√§t√∂
  - Laskentateho ‚Äì hienos√§√§t√∂ty√∂n suorittaminen ja hienos√§√§detyn mallin k√§ytt√∂√∂notto
  - Data ‚Äì riitt√§v√§n laadukkaiden esimerkkien saatavuus hienos√§√§d√∂n vaikutuksen saavuttamiseksi
- **Hy√∂dyt**: Oletko varmistanut hienos√§√§d√∂n hy√∂dyt?
  - Laatu ‚Äì ylittik√∂ hienos√§√§detty malli vertailutason?
  - Kustannukset ‚Äì v√§hent√§√§k√∂ se tokenien k√§ytt√∂√§ yksinkertaistamalla promptteja?
  - Laajennettavuus ‚Äì voiko perusmallia k√§ytt√§√§ uudelleen uusilla alueilla?

Vastaamalla n√§ihin kysymyksiin sinun pit√§isi pysty√§ p√§√§tt√§m√§√§n, onko hienos√§√§t√∂ oikea l√§hestymistapa k√§ytt√∂tapaukseesi. Ihanteellisesti l√§hestymistapa on perusteltu vain, jos hy√∂dyt ylitt√§v√§t kustannukset. Kun p√§√§t√§t jatkaa, on aika mietti√§ _miten_ voit hienos√§√§t√§√§ esikoulutettua mallia.

Haluatko lis√§√§ n√§kemyksi√§ p√§√§t√∂ksentekoprosessiin? Katso [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Miten hienos√§√§t√§√§ esikoulutettua mallia?

Hienos√§√§t√∂√§ varten tarvitset:

- esikoulutetun mallin, jota hienos√§√§t√§√§
- aineiston, jota k√§ytet√§√§n hienos√§√§t√∂√∂n
- koulutusymp√§rist√∂n hienos√§√§t√∂ty√∂n suorittamiseen
- is√§nt√§ymp√§rist√∂n hienos√§√§detyn mallin k√§ytt√∂√∂nottoon

## Hienos√§√§t√∂ k√§yt√§nn√∂ss√§

Seuraavat resurssit tarjoavat vaiheittaiset ohjeet, jotka opastavat sinut l√§pi k√§yt√§nn√∂n esimerkin valitun mallin ja valikoidun aineiston avulla. N√§iden ohjeiden l√§pik√§yntiin tarvitset tilin kyseisell√§ palveluntarjoajalla sek√§ p√§√§syn asiaankuuluviin malleihin ja aineistoihin.

| Palveluntarjoaja | Opas                                                                                                                                                                         | Kuvaus                                                                                                                                                                                                                                                                                                                                                                                                                           |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI           | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)              | Opettele hienos√§√§t√§m√§√§n `gpt-35-turbo` tiettyyn sovellusalueeseen ("reseptiavustaja") valmistamalla koulutusdata, suorittamalla hienos√§√§t√∂ty√∂ ja k√§ytt√§m√§ll√§ hienos√§√§detty√§ mallia ennustamiseen.                                                                                                                                                                                                                                   |
| Azure OpenAI     | [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | Opettele hienos√§√§t√§m√§√§n `gpt-35-turbo-0613` -mallia **Azurella** luomalla ja lataamalla koulutusdata, suorittamalla hienos√§√§t√∂ty√∂ sek√§ ottamalla uusi malli k√§ytt√∂√∂n.                                                                                                                                                                                                                                                               |
| Hugging Face     | [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                             | T√§m√§ blogikirjoitus opastaa hienos√§√§t√§m√§√§n _avoimen LLM:n_ (esim. `CodeLlama 7B`) k√§ytt√§en [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) -kirjastoa ja [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) -ty√∂kaluja avoimilla [aineistoilla](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) Hugging Facessa. |
|                  |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| ü§ó AutoTrain     | [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                       | AutoTrain (tai AutoTrain Advanced) on Hugging Facen kehitt√§m√§ Python-kirjasto, joka mahdollistaa hienos√§√§d√∂n moniin eri teht√§viin, mukaan lukien LLM-hienos√§√§t√∂. AutoTrain on kooditon ratkaisu, ja hienos√§√§t√∂ voidaan tehd√§ omassa pilvess√§, Hugging Face Spacesissa tai paikallisesti. Se tukee web-pohjaista k√§ytt√∂liittym√§√§, komentorivity√∂kaluja ja koulutusta yaml-konfiguraatiotiedostoilla.                                                                                 |
|                  |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                 |

## Teht√§v√§

Valitse yll√§ olevista oppaista yksi ja k√§y se l√§pi. _Saatamme toteuttaa n√§ist√§ oppaista version Jupyter Notebookeina t√§ss√§ repossa vain viitteeksi. K√§yt√§ alkuper√§isi√§ l√§hteit√§ saadaksesi uusimmat versiot._

## Hienoa ty√∂t√§! Jatka oppimista.

Oppitunnin suorittamisen j√§lkeen tutustu [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) -kokoelmaan jatkaaksesi generatiivisen teko√§lyn osaamisesi kehitt√§mist√§!

Onnittelut!! Olet suorittanut t√§m√§n kurssin v2-sarjan viimeisen oppitunnin! √Ñl√§ lopeta oppimista ja rakentamista. \*\*Tutustu [RESOURCES](RESOURCES.md?WT.mc_id=academic-105485-koreyst) -sivuun, josta l√∂yd√§t lis√§ehdotuksia juuri t√§h√§n aiheeseen.

My√∂s v1-sarjan oppitunteja on p√§ivitetty lis√§√§m√§ll√§ uusia teht√§vi√§ ja k√§sitteit√§. K√§yt√§ hetki p√§ivitt√§√§ksesi tietosi ‚Äì ja ole hyv√§ ja [jaa kysymyksesi ja palautteesi](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) auttaaksesi meit√§ parantamaan n√§it√§ oppitunteja yhteis√∂n hyv√§ksi.

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattik√§√§nn√∂ksiss√§ saattaa esiinty√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§ist√§ asiakirjaa sen alkuper√§iskielell√§ tulee pit√§√§ virallisena l√§hteen√§. T√§rkeiss√§ tiedoissa suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§ aiheutuvista v√§√§rinymm√§rryksist√§ tai tulkinnoista.