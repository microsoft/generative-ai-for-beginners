{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rakentaminen Meta-perheen mallien kanssa\n",
    "\n",
    "## Johdanto\n",
    "\n",
    "Tässä osiossa käsitellään:\n",
    "\n",
    "- Tutustutaan kahteen päämalliin Meta-perheestä – Llama 3.1 ja Llama 3.2\n",
    "- Ymmärretään kunkin mallin käyttötarkoitukset ja tilanteet\n",
    "- Koodiesimerkki, joka havainnollistaa kummankin mallin erityispiirteitä\n",
    "\n",
    "## Meta-malliperhe\n",
    "\n",
    "Tässä osiossa tutustumme kahteen Meta-perheen eli \"Llama-lauman\" malliin – Llama 3.1 ja Llama 3.2\n",
    "\n",
    "Näistä malleista on saatavilla eri versioita, ja ne löytyvät Github Model -markkinapaikalta. Lisätietoa Github-mallien käytöstä [AI-mallien prototyyppien rakentamiseen](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Malliversiot:\n",
    "- Llama 3.1 – 70B Instruct\n",
    "- Llama 3.1 – 405B Instruct\n",
    "- Llama 3.2 – 11B Vision Instruct\n",
    "- Llama 3.2 – 90B Vision Instruct\n",
    "\n",
    "*Huom: Llama 3 on myös saatavilla Github-malleissa, mutta sitä ei käsitellä tässä osiossa*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "405 miljardin parametrin Llama 3.1 kuuluu avoimen lähdekoodin LLM-malleihin.\n",
    "\n",
    "Tämä malli on päivitys aiempaan Llama 3 -versioon ja tarjoaa:\n",
    "\n",
    "- Suuremman kontekstin ikkunan – 128k tokenia vs 8k tokenia\n",
    "- Suuremman maksimimäärän ulostulotokeneita – 4096 vs 2048\n",
    "- Parempi monikielinen tuki – johtuen suuremmasta määrästä opetustokeneita\n",
    "\n",
    "Näiden ansiosta Llama 3.1 pystyy käsittelemään monimutkaisempia käyttötapauksia GenAI-sovelluksissa, kuten:\n",
    "- Luontainen funktiokutsuminen – mahdollisuus kutsua ulkoisia työkaluja ja funktioita LLM-työnkulun ulkopuolella\n",
    "- Parempi RAG-suorituskyky – suuremman kontekstin ikkunan ansiosta\n",
    "- Synteettisen datan generointi – mahdollisuus luoda tehokasta dataa esimerkiksi hienosäätöä varten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Function Calling\n",
    "\n",
    "Llama 3.1 on hienosäädetty toimimaan tehokkaammin funktio- ja työkalukutsujen kanssa. Mallissa on myös kaksi sisäänrakennettua työkalua, jotka se osaa tunnistaa käyttäjän kehotteen perusteella tarpeellisiksi. Nämä työkalut ovat:\n",
    "\n",
    "- **Brave Search** – Voidaan käyttää ajantasaisen tiedon, kuten sään, hakemiseen verkosta\n",
    "- **Wolfram Alpha** – Soveltuu monimutkaisempiin matemaattisiin laskelmiin, joten omien funktioiden kirjoittaminen ei ole välttämätöntä.\n",
    "\n",
    "Voit myös luoda omia mukautettuja työkaluja, joita LLM voi kutsua.\n",
    "\n",
    "Alla olevassa koodiesimerkissä:\n",
    "\n",
    "- Määrittelemme käytettävissä olevat työkalut (brave_search, wolfram_alpha) järjestelmäkehotteessa.\n",
    "- Lähetämme käyttäjän kehotteen, jossa kysytään tietyn kaupungin säästä.\n",
    "- LLM vastaa työkalukutsulla Brave Search -työkalulle, joka näyttää tältä: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Huom: Tämä esimerkki tekee ainoastaan työkalukutsun. Jos haluat saada tulokset, sinun täytyy luoda ilmainen tili Brave API -sivulla ja määritellä itse funktio.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Vaikka Llama 3.1 on suuri kielimalli, sen rajoituksiin kuuluu multimodaalisuuden puute. Eli se ei pysty käyttämään erilaisia syötteitä, kuten kuvia, kehotteina ja antamaan niihin vastauksia. Tämä ominaisuus on yksi Llama 3.2:n tärkeimmistä uudistuksista. Näihin ominaisuuksiin kuuluvat myös:\n",
    "\n",
    "- Multimodaalisuus – pystyy käsittelemään sekä teksti- että kuvakehotteita\n",
    "- Pienet ja keskikokoiset mallit (11B ja 90B) – tarjoaa joustavia käyttöönotto­vaihtoehtoja\n",
    "- Vain tekstiin perustuvat mallit (1B ja 3B) – mahdollistaa mallin käytön reunalaitteissa ja mobiililaitteissa sekä tarjoaa matalan viiveen\n",
    "\n",
    "Multimodaalituki on merkittävä edistysaskel avoimen lähdekoodin mallien maailmassa. Alla olevassa esimerkissä sekä kuva että tekstikehote annetaan Llama 3.2 90B -mallille, joka analysoi kuvan.\n",
    "\n",
    "### Multimodaalituki Llama 3.2:n kanssa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppiminen ei lopu tähän – jatka matkaa\n",
    "\n",
    "Kun olet suorittanut tämän oppitunnin, tutustu [Generative AI Learning -kokoelmaamme](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ja jatka Generatiivisen tekoälyn osaamisesi kehittämistä!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Vastuuvapauslauseke**:  \nTämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Pyrimme tarkkuuteen, mutta huomioithan, että automaattiset käännökset saattavat sisältää virheitä tai epätarkkuuksia. Alkuperäistä asiakirjaa sen alkuperäisellä kielellä tulee pitää ensisijaisena lähteenä. Kriittisissä tapauksissa suosittelemme ammattimaisen ihmiskääntäjän käyttöä. Emme ole vastuussa tämän käännöksen käytöstä mahdollisesti aiheutuvista väärinkäsityksistä tai tulkinnoista.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:44:53+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "fi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}