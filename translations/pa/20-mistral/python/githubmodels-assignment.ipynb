{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ਮਿਸਟਰਲ ਮਾਡਲਾਂ ਨਾਲ ਬਣਾਉਣਾ\n",
    "\n",
    "## ਜਾਣੂ ਕਰਵਾਉਣਾ\n",
    "\n",
    "ਇਸ ਪਾਠ ਵਿੱਚ ਤੁਸੀਂ ਸਿੱਖੋਗੇ:\n",
    "- ਵੱਖ-ਵੱਖ ਮਿਸਟਰਲ ਮਾਡਲਾਂ ਦੀ ਜਾਂਚ ਕਰਨਾ\n",
    "- ਹਰ ਮਾਡਲ ਲਈ ਵਰਤੋਂ ਦੇ ਕੇਸ ਤੇ ਹਾਲਾਤ ਸਮਝਣਾ\n",
    "- ਕੋਡ ਉਦਾਹਰਨਾਂ ਰਾਹੀਂ ਹਰ ਮਾਡਲ ਦੀਆਂ ਵਿਲੱਖਣ ਖਾਸੀਤਾਂ ਵੇਖਣਾ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਮਿਸਟਰਲ ਮਾਡਲ\n",
    "\n",
    "ਇਸ ਪਾਠ ਵਿੱਚ, ਅਸੀਂ 3 ਵੱਖ-ਵੱਖ ਮਿਸਟਰਲ ਮਾਡਲਾਂ ਦੀ ਜਾਂਚ ਕਰਾਂਗੇ:  \n",
    "**ਮਿਸਟਰਲ ਲਾਰਜ**, **ਮਿਸਟਰਲ ਸਮਾਲ** ਅਤੇ **ਮਿਸਟਰਲ ਨੀਮੋ**।\n",
    "\n",
    "ਇਹਨਾਂ ਵਿੱਚੋਂ ਹਰ ਮਾਡਲ Github Model marketplace 'ਤੇ ਮੁਫ਼ਤ ਉਪਲਬਧ ਹਨ। ਇਸ ਨੋਟਬੁੱਕ ਵਿੱਚ ਦਿੱਤਾ ਕੋਡ ਇਨ੍ਹਾਂ ਮਾਡਲਾਂ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਚਲਾਇਆ ਜਾਵੇਗਾ। Github Models ਦੀ ਵਰਤੋਂ ਕਰਕੇ [AI ਮਾਡਲਾਂ ਨਾਲ ਪ੍ਰੋਟੋਟਾਈਪ ਬਣਾਉਣ](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) ਬਾਰੇ ਹੋਰ ਜਾਣਕਾਰੀ ਇੱਥੇ ਮਿਲ ਸਕਦੀ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਮਿਸਟਰਲ ਲਾਰਜ 2 (2407)\n",
    "ਮਿਸਟਰਲ ਲਾਰਜ 2 ਇਸ ਵੇਲੇ ਮਿਸਟਰਲ ਵੱਲੋਂ ਸਭ ਤੋਂ ਵਧੀਆ ਮਾਡਲ ਹੈ ਅਤੇ ਇਹ ਵਪਾਰਕ ਵਰਤੋਂ ਲਈ ਬਣਾਇਆ ਗਿਆ ਹੈ।\n",
    "\n",
    "ਇਹ ਮਾਡਲ ਅਸਲੀ ਮਿਸਟਰਲ ਲਾਰਜ ਤੋਂ ਅੱਪਗ੍ਰੇਡ ਹੈ, ਜਿਸ ਵਿੱਚ ਇਹ ਸੁਧਾਰ ਹਨ:\n",
    "- ਵੱਡਾ ਕਾਂਟੈਕਸਟ ਵਿੰਡੋ - 128k ਵਲੋਂ 32k ਨਾਲ ਤੁਲਨਾ ਕਰਕੇ\n",
    "- ਗਣਿਤ ਅਤੇ ਕੋਡਿੰਗ ਟਾਸਕਾਂ 'ਤੇ ਵਧੀਆ ਕਾਰਗੁਜ਼ਾਰੀ - 76.9% ਔਸਤ ਸਹੀ ਜਵਾਬ, 60.4% ਨਾਲ ਤੁਲਨਾ ਕਰਕੇ\n",
    "- ਬਹੁਭਾਸ਼ਾਈ ਕਾਰਗੁਜ਼ਾਰੀ ਵਿੱਚ ਵਾਧਾ - ਭਾਸ਼ਾਵਾਂ ਵਿੱਚ ਸ਼ਾਮਲ ਹਨ: ਅੰਗਰੇਜ਼ੀ, ਫਰਾਂਸੀਸੀ, ਜਰਮਨ, ਸਪੇਨੀ, ਇਤਾਲਵੀ, ਪੁਰਤਗਾਲੀ, ਡੱਚ, ਰੂਸੀ, ਚੀਨੀ, ਜਾਪਾਨੀ, ਕੋਰੀਅਨ, ਅਰਬੀ ਅਤੇ ਹਿੰਦੀ।\n",
    "\n",
    "ਇਨ੍ਹਾਂ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਨਾਲ, ਮਿਸਟਰਲ ਲਾਰਜ ਖਾਸ ਕਰਕੇ ਇਨ੍ਹਾਂ ਵਿੱਚ ਮਹਿਰਤ ਰੱਖਦਾ ਹੈ:\n",
    "- *ਰੀਟਰੀਵਲ ਆਗਮੈਂਟਡ ਜਨਰੇਸ਼ਨ (RAG)* - ਵੱਡੇ ਕਾਂਟੈਕਸਟ ਵਿੰਡੋ ਕਰਕੇ\n",
    "- *ਫੰਕਸ਼ਨ ਕਾਲਿੰਗ* - ਇਸ ਮਾਡਲ ਵਿੱਚ ਨੈਟਿਵ ਫੰਕਸ਼ਨ ਕਾਲਿੰਗ ਹੈ, ਜਿਸ ਨਾਲ ਬਾਹਰੀ ਟੂਲਾਂ ਅਤੇ API ਨਾਲ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਹੋ ਸਕਦੀ ਹੈ। ਇਹ ਕਾਲਾਂ ਇਕੱਠੀਆਂ ਜਾਂ ਲੜੀਵਾਰ ਤਰੀਕੇ ਨਾਲ ਇੱਕ-ਇੱਕ ਕਰਕੇ ਕੀਤੀਆਂ ਜਾ ਸਕਦੀਆਂ ਹਨ।\n",
    "- *ਕੋਡ ਜਨਰੇਸ਼ਨ* - ਇਹ ਮਾਡਲ Python, Java, TypeScript ਅਤੇ C++ ਕੋਡ ਬਣਾਉਣ ਵਿੱਚ ਬਹੁਤ ਵਧੀਆ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ਇਸ ਉਦਾਹਰਨ ਵਿੱਚ, ਅਸੀਂ Mistral Large 2 ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਇੱਕ RAG ਪੈਟਰਨ ਨੂੰ ਇੱਕ ਲਿਖਤੀ ਦਸਤਾਵੇਜ਼ 'ਤੇ ਚਲਾ ਰਹੇ ਹਾਂ। ਸਵਾਲ ਕੋਰੀਅਨ ਵਿੱਚ ਲਿਖਿਆ ਗਿਆ ਹੈ ਅਤੇ ਲੇਖਕ ਦੀਆਂ ਕਾਲਜ ਤੋਂ ਪਹਿਲਾਂ ਦੀਆਂ ਸਰਗਰਮੀਆਂ ਬਾਰੇ ਪੁੱਛਦਾ ਹੈ।\n",
    "\n",
    "ਇਹ Cohere Embeddings Model ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਲਿਖਤੀ ਦਸਤਾਵੇਜ਼ ਅਤੇ ਸਵਾਲ ਦੋਵਾਂ ਦੇ embeddings ਬਣਾਉਂਦਾ ਹੈ। ਇਸ ਨਮੂਨੇ ਲਈ, ਇਹ faiss Python ਪੈਕੇਜ ਨੂੰ ਇੱਕ ਵੈਕਟਰ ਸਟੋਰ ਵਜੋਂ ਵਰਤਦਾ ਹੈ।\n",
    "\n",
    "Mistral ਮਾਡਲ ਨੂੰ ਭੇਜੇ ਜਾਣ ਵਾਲੇ ਪ੍ਰਾਂਪਟ ਵਿੱਚ ਸਵਾਲ ਅਤੇ ਉਹ ਚੰਕ ਵੀ ਸ਼ਾਮਲ ਹੁੰਦੇ ਹਨ ਜੋ ਸਵਾਲ ਨਾਲ ਮਿਲਦੇ-ਜੁਲਦੇ ਹੁੰਦੇ ਹਨ। ਮਾਡਲ ਫਿਰ ਕੁਦਰਤੀ ਭਾਸ਼ਾ ਵਿੱਚ ਜਵਾਬ ਦਿੰਦਾ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /home/codespace/.python/current/lib/python3.12/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author primarily engaged in two activities before college: writing and programming. In terms of writing, they wrote short stories, albeit not very good ones, with minimal plot and characters expressing strong feelings. For programming, they started writing programs on the IBM 1401 used for data processing during their 9th grade, at the age of 13 or 14. They used an early version of Fortran and typed programs on punch cards, later loading them into the card reader to run the program.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference import EmbeddingsClient\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-large\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text\n",
    "\n",
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "len(chunks)\n",
    "\n",
    "embed_model_name = \"cohere-embed-v3-multilingual\" \n",
    "\n",
    "embed_client = EmbeddingsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(token)\n",
    ")\n",
    "\n",
    "embed_response = embed_client.embed(\n",
    "    input=chunks,\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "text_embeddings = []\n",
    "for item in embed_response.data:\n",
    "    length = len(item.embedding)\n",
    "    text_embeddings.append(item.embedding)\n",
    "text_embeddings = np.array(text_embeddings)\n",
    "\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)\n",
    "\n",
    "question = \"저자가 대학에 오기 전에 주로 했던 두 가지 일은 무엇이었나요?？\"\n",
    "\n",
    "question_embedding = embed_client.embed(\n",
    "    input=[question],\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "question_embeddings = np.array(question_embedding.data[0].embedding)\n",
    "\n",
    "\n",
    "D, I = index.search(question_embeddings.reshape(1, -1), k=2) # distance, index\n",
    "retrieved_chunks = [chunks[i] for i in I.tolist()[0]]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunks}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=prompt),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਮਿਸਟਰਲ ਸਮਾਲ\n",
    "ਮਿਸਟਰਲ ਸਮਾਲ ਮਿਸਟਰਲ ਪਰਿਵਾਰ ਦੇ ਮਾਡਲਾਂ ਵਿੱਚੋਂ ਇੱਕ ਹੋਰ ਮਾਡਲ ਹੈ ਜੋ ਪ੍ਰੀਮੀਅਰ/ਐਂਟਰਪ੍ਰਾਈਜ਼ ਸ਼੍ਰੇਣੀ ਹੇਠਾਂ ਆਉਂਦਾ ਹੈ। ਨਾਂ ਤੋਂ ਹੀ ਪਤਾ ਲੱਗਦਾ ਹੈ ਕਿ ਇਹ ਇੱਕ ਛੋਟਾ ਭਾਸ਼ਾ ਮਾਡਲ (SLM) ਹੈ। ਮਿਸਟਰਲ ਸਮਾਲ ਵਰਤਣ ਦੇ ਫਾਇਦੇ ਇਹ ਹਨ:\n",
    "- ਲਾਗਤ ਵਿੱਚ ਬਚਤ - ਮਿਸਟਰਲ ਲਾਰਜ ਜਾਂ NeMo ਵਰਗੇ ਮਿਸਟਰਲ LLMs ਨਾਲੋਂ 80% ਤੱਕ ਘੱਟ ਕੀਮਤ\n",
    "- ਘੱਟ ਲੈਟੈਂਸੀ - ਮਿਸਟਰਲ ਦੇ LLMs ਨਾਲੋਂ ਤੇਜ਼ ਜਵਾਬ\n",
    "- ਲਚਕੀਲਾ - ਵੱਖ-ਵੱਖ ਮਾਹੌਲਾਂ ਵਿੱਚ ਘੱਟ ਸਰੋਤਾਂ ਦੀ ਲੋੜ ਨਾਲ ਆਸਾਨੀ ਨਾਲ ਲਾਗੂ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "ਮਿਸਟਰਲ ਸਮਾਲ ਇਨ੍ਹਾਂ ਲਈ ਬਿਹਤਰ ਹੈ:\n",
    "- ਟੈਕਸਟ ਆਧਾਰਤ ਕੰਮਾਂ ਲਈ, ਜਿਵੇਂ ਕਿ ਸੰਖੇਪ, ਭਾਵਨਾ ਵਿਸ਼ਲੇਸ਼ਣ ਅਤੇ ਅਨੁਵਾਦ\n",
    "- ਐਪਲੀਕੇਸ਼ਨਾਂ ਲਈ ਜਿੱਥੇ ਵਾਰ-ਵਾਰ ਬੇਨਤੀਆਂ ਆਉਂਦੀਆਂ ਹਨ, ਕਿਉਂਕਿ ਇਹ ਲਾਗਤ-ਅਸਰਦਾਰ ਹੈ\n",
    "- ਘੱਟ ਲੈਟੈਂਸੀ ਵਾਲੇ ਕੋਡ ਕੰਮਾਂ ਲਈ, ਜਿਵੇਂ ਕਿ ਕੋਡ ਸਮੀਖਿਆ ਅਤੇ ਸੁਝਾਵ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਮਿਸਟਰਲ ਸਮਾਲ ਅਤੇ ਮਿਸਟਰਲ ਲਾਰਜ ਦੀ ਤੁਲਨਾ\n",
    "\n",
    "ਮਿਸਟਰਲ ਸਮਾਲ ਅਤੇ ਲਾਰਜ ਵਿਚਕਾਰ ਲੈਟੈਂਸੀ ਦੇ ਫਰਕ ਨੂੰ ਵੇਖਾਉਣ ਲਈ, ਹੇਠਾਂ ਦਿੱਤੇ ਸੈੱਲ ਚਲਾਓ।\n",
    "\n",
    "ਤੁਸੀਂ 3-5 ਸਕਿੰਟ ਦੇ ਵਿਚਕਾਰ ਜਵਾਬ ਦੇ ਸਮੇਂ ਵਿੱਚ ਫਰਕ ਦੇਖ ਸਕਦੇ ਹੋ। ਇਸਦੇ ਨਾਲ ਹੀ, ਇੱਕੋ ਪ੍ਰੰਪਟ ਉੱਤੇ ਜਵਾਬ ਦੀ ਲੰਬਾਈ ਅਤੇ ਅੰਦਾਜ਼ ਵੀ ਨੋਟ ਕਰੋ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-small\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-large\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਮਿਸਟਰਾਲ ਨੀਮੋ\n",
    "\n",
    "ਇਸ ਪਾਠ ਵਿੱਚ ਚਰਚਾ ਕੀਤੇ ਹੋਰ ਦੋ ਮਾਡਲਾਂ ਨਾਲੋਂ, ਮਿਸਟਰਾਲ ਨੀਮੋ ਹੀ ਇੱਕੋ ਇਕ ਮੁਫ਼ਤ ਮਾਡਲ ਹੈ ਜਿਸਦੀ Apache2 ਲਾਇਸੰਸ ਹੈ।\n",
    "\n",
    "ਇਸਨੂੰ ਮਿਸਟਰਾਲ ਦੇ ਪਹਿਲੇ ਖੁੱਲ੍ਹੇ ਸਰੋਤ LLM, ਮਿਸਟਰਾਲ 7B, ਦਾ ਅੱਪਗਰੇਡ ਮੰਨਿਆ ਜਾਂਦਾ ਹੈ।\n",
    "\n",
    "ਨੀਮੋ ਮਾਡਲ ਦੀਆਂ ਕੁਝ ਹੋਰ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਹਨ:\n",
    "\n",
    "- *ਹੋਰ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਟੋਕਨਾਈਜ਼ੇਸ਼ਨ:* ਇਹ ਮਾਡਲ ਆਮ ਤੌਰ 'ਤੇ ਵਰਤੇ ਜਾਂਦੇ tiktoken ਦੀ ਥਾਂ Tekken tokenizer ਵਰਤਦਾ ਹੈ। ਇਸ ਨਾਲ ਵੱਧ ਭਾਸ਼ਾਵਾਂ ਅਤੇ ਕੋਡ 'ਤੇ ਵਧੀਆ ਕਾਰਗੁਜ਼ਾਰੀ ਮਿਲਦੀ ਹੈ।\n",
    "\n",
    "- *ਫਾਈਨਟਿਊਨਿੰਗ:* ਬੇਸ ਮਾਡਲ ਫਾਈਨਟਿਊਨ ਕਰਨ ਲਈ ਉਪਲਬਧ ਹੈ। ਇਸ ਨਾਲ ਉਹਨਾਂ ਸਥਿਤੀਆਂ ਲਈ ਵਧੇਰੇ ਲਚੀਲਾਪਨ ਮਿਲਦਾ ਹੈ ਜਿੱਥੇ ਫਾਈਨਟਿਊਨਿੰਗ ਦੀ ਲੋੜ ਹੋ ਸਕਦੀ ਹੈ।\n",
    "\n",
    "- *ਨੇਟਿਵ ਫੰਕਸ਼ਨ ਕਾਲਿੰਗ* - ਮਿਸਟਰਾਲ ਲਾਰਜ ਵਾਂਗ, ਇਹ ਮਾਡਲ ਫੰਕਸ਼ਨ ਕਾਲਿੰਗ 'ਤੇ ਟ੍ਰੇਨ ਕੀਤਾ ਗਿਆ ਹੈ। ਇਹਨੂੰ ਖਾਸ ਬਣਾਉਂਦਾ ਹੈ, ਕਿਉਂਕਿ ਇਹ ਪਹਿਲੇ ਖੁੱਲ੍ਹੇ ਸਰੋਤ ਮਾਡਲਾਂ ਵਿੱਚੋਂ ਇੱਕ ਹੈ ਜੋ ਇਹ ਕਰ ਸਕਦਾ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਮਿਸਟਰਾਲ ਨੀਮੋ\n",
    "\n",
    "ਇਸ ਪਾਠ ਵਿੱਚ ਚਰਚਾ ਕੀਤੇ ਹੋਰ ਦੋ ਮਾਡਲਾਂ ਨਾਲੋਂ, ਮਿਸਟਰਾਲ ਨੀਮੋ ਹੀ ਇਕਲੌਤਾ ਮੁਫ਼ਤ ਮਾਡਲ ਹੈ ਜਿਸਦੀ Apache2 ਲਾਇਸੈਂਸ ਹੈ।\n",
    "\n",
    "ਇਸਨੂੰ ਮਿਸਟਰਾਲ ਦੇ ਪਹਿਲੇ ਖੁੱਲ੍ਹੇ ਸਰੋਤ LLM, ਮਿਸਟਰਾਲ 7B, ਦਾ ਅੱਪਗਰੇਡ ਮੰਨਿਆ ਜਾਂਦਾ ਹੈ।\n",
    "\n",
    "ਨੀਮੋ ਮਾਡਲ ਦੀਆਂ ਕੁਝ ਹੋਰ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਹਨ:\n",
    "\n",
    "- *ਹੋਰ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਟੋਕਨਾਈਜ਼ੇਸ਼ਨ:* ਇਹ ਮਾਡਲ ਆਮ ਤੌਰ 'ਤੇ ਵਰਤੇ ਜਾਂਦੇ tiktoken ਦੀ ਥਾਂ Tekken tokenizer ਵਰਤਦਾ ਹੈ। ਇਸ ਨਾਲ ਵੱਧ ਭਾਸ਼ਾਵਾਂ ਅਤੇ ਕੋਡ 'ਤੇ ਵਧੀਆ ਕਾਰਗੁਜ਼ਾਰੀ ਮਿਲਦੀ ਹੈ।\n",
    "\n",
    "- *ਫਾਈਨਟਿਊਨਿੰਗ:* ਬੇਸ ਮਾਡਲ ਫਾਈਨਟਿਊਨਿੰਗ ਲਈ ਉਪਲਬਧ ਹੈ। ਇਸ ਨਾਲ ਉਹਨਾਂ ਸਥਿਤੀਆਂ ਲਈ ਵਧੇਰੇ ਲਚੀਲਾਪਨ ਮਿਲਦਾ ਹੈ ਜਿੱਥੇ ਫਾਈਨਟਿਊਨਿੰਗ ਦੀ ਲੋੜ ਹੋ ਸਕਦੀ ਹੈ।\n",
    "\n",
    "- *ਨੇਟਿਵ ਫੰਕਸ਼ਨ ਕਾਲਿੰਗ* - ਮਿਸਟਰਾਲ ਲਾਰਜ ਵਾਂਗ, ਇਹ ਮਾਡਲ ਫੰਕਸ਼ਨ ਕਾਲਿੰਗ 'ਤੇ ਟ੍ਰੇਨ ਕੀਤਾ ਗਿਆ ਹੈ। ਇਹਨੂੰ ਖਾਸ ਬਣਾਉਂਦਾ ਹੈ, ਕਿਉਂਕਿ ਇਹ ਪਹਿਲੇ ਖੁੱਲ੍ਹੇ ਸਰੋਤ ਮਾਡਲਾਂ ਵਿੱਚੋਂ ਇੱਕ ਹੈ ਜੋ ਇਹ ਕਰਦਾ ਹੈ।\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ਟੋਕਨਾਈਜ਼ਰ ਦੀ ਤੁਲਨਾ\n",
    "\n",
    "ਇਸ ਉਦਾਹਰਨ ਵਿੱਚ, ਅਸੀਂ ਵੇਖਾਂਗੇ ਕਿ Mistral NeMo ਟੋਕਨਾਈਜ਼ੇਸ਼ਨ ਨੂੰ Mistral Large ਨਾਲੋਂ ਕਿਵੇਂ ਹੈਂਡਲ ਕਰਦਾ ਹੈ।\n",
    "\n",
    "ਦੋਵੇਂ ਉਦਾਹਰਨਾਂ ਵਿੱਚ ਇੱਕੋ ਜਿਹਾ ਪ੍ਰੋੰਪਟ ਵਰਤਿਆ ਗਿਆ ਹੈ ਪਰ ਤੁਸੀਂ ਵੇਖੋਗੇ ਕਿ NeMo ਵੱਲੋਂ ਵਾਪਸ ਆਉਣ ਵਾਲੇ ਟੋਕਨ Mistral Large ਨਾਲੋਂ ਘੱਟ ਹਨ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistral-common\n",
      "  Downloading mistral_common-1.4.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (4.23.0)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from mistral-common) (2.9.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (2.32.3)\n",
      "Collecting sentencepiece==0.2.0 (from mistral-common)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from mistral-common)\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from mistral-common) (4.12.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (0.20.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.1->mistral-common) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.1->mistral-common) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (2024.8.30)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<0.8.0,>=0.7.0->mistral-common)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Downloading mistral_common-1.4.4-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, regex, tiktoken, mistral-common\n",
      "Successfully installed mistral-common-1.4.4 regex-2024.9.11 sentencepiece-0.2.0 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mistral-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"open-mistral-nemo\t\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"mistral-large-latest\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਸਿੱਖਣਾ ਇੱਥੇ ਖਤਮ ਨਹੀਂ ਹੁੰਦਾ, ਆਪਣਾ ਸਫਰ ਜਾਰੀ ਰੱਖੋ\n",
    "\n",
    "ਇਹ ਪਾਠ ਪੂਰਾ ਕਰਨ ਤੋਂ ਬਾਅਦ, ਆਪਣਾ Generative AI ਗਿਆਨ ਹੋਰ ਵਧਾਉਣ ਲਈ ਸਾਡੀ [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ਵੀ ਜ਼ਰੂਰ ਵੇਖੋ!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ਅਸਵੀਕਰਨ**:  \nਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਅਸੀਂ ਯਥਾਸੰਭਵ ਸਹੀ ਅਨੁਵਾਦ ਕਰਨ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਪਰ ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਵਿੱਚ ਰੱਖੋ ਕਿ ਆਟੋਮੈਟਿਕ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਣਪਛਾਤੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਹੀ ਅਧਿਕਾਰਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫ਼ਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਹੋਣ ਵਾਲੀਆਂ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "coopTranslator": {
   "original_hash": "03bc2561665a8b411acfc24a8aa0d442",
   "translation_date": "2025-08-25T22:19:41+00:00",
   "source_file": "20-mistral/python/githubmodels-assignment.ipynb",
   "language_code": "pa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}