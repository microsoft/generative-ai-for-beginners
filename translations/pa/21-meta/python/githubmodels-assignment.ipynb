{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ਮੈਟਾ ਪਰਿਵਾਰ ਮਾਡਲਾਂ ਨਾਲ ਕੰਮ ਕਰਨਾ\n",
    "\n",
    "## ਜਾਣੂ ਕਰਵਾਉਣਾ\n",
    "\n",
    "ਇਸ ਪਾਠ ਵਿੱਚ ਤੁਸੀਂ ਸਿੱਖੋਗੇ:\n",
    "\n",
    "- ਮੈਟਾ ਪਰਿਵਾਰ ਦੇ ਦੋ ਮੁੱਖ ਮਾਡਲਾਂ - Llama 3.1 ਅਤੇ Llama 3.2 ਦੀ ਜਾਂਚ ਕਰਨਾ\n",
    "- ਹਰ ਮਾਡਲ ਲਈ ਵਰਤੋਂ ਦੇ ਕੇਸ ਅਤੇ ਸਥਿਤੀਆਂ ਨੂੰ ਸਮਝਣਾ\n",
    "- ਹਰ ਮਾਡਲ ਦੀਆਂ ਵਿਲੱਖਣ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਦਿਖਾਉਣ ਲਈ ਕੋਡ ਉਦਾਹਰਨ\n",
    "\n",
    "## ਮੈਟਾ ਪਰਿਵਾਰ ਦੇ ਮਾਡਲ\n",
    "\n",
    "ਇਸ ਪਾਠ ਵਿੱਚ ਅਸੀਂ ਮੈਟਾ ਪਰਿਵਾਰ ਜਾਂ \"Llama Herd\" ਦੇ 2 ਮਾਡਲਾਂ - Llama 3.1 ਅਤੇ Llama 3.2 ਦੀ ਜਾਂਚ ਕਰਾਂਗੇ\n",
    "\n",
    "ਇਹ ਮਾਡਲ ਵੱਖ-ਵੱਖ ਰੂਪਾਂ ਵਿੱਚ ਆਉਂਦੇ ਹਨ ਅਤੇ Github Model marketplace ਉੱਤੇ ਉਪਲਬਧ ਹਨ। Github Models ਦੀ ਵਰਤੋਂ ਕਰਕੇ [AI ਮਾਡਲਾਂ ਨਾਲ ਪ੍ਰੋਟੋਟਾਈਪ ਬਣਾਉਣ] ਲਈ ਹੋਰ ਜਾਣਕਾਰੀ ਇੱਥੇ ਮਿਲ ਸਕਦੀ ਹੈ: (https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "ਮਾਡਲ ਰੂਪ:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*ਨੋਟ: Llama 3 ਵੀ Github Models ਉੱਤੇ ਉਪਲਬਧ ਹੈ ਪਰ ਇਹ ਪਾਠ ਇਸ ਬਾਰੇ ਨਹੀਂ ਹੋਵੇਗਾ*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਲਾਮਾ 3.1\n",
    "\n",
    "405 ਬਿਲੀਅਨ ਪੈਰਾਮੀਟਰਾਂ ਨਾਲ, ਲਾਮਾ 3.1 ਖੁੱਲ੍ਹੇ ਸਰੋਤ LLM ਸ਼੍ਰੇਣੀ ਵਿੱਚ ਆਉਂਦੀ ਹੈ।\n",
    "\n",
    "ਇਹ ਮਾਡਲ ਪਹਿਲਾਂ ਆਏ ਲਾਮਾ 3 ਵਿੱਚੋਂ ਅੱਪਗਰੇਡ ਹੈ, ਜੋ ਕਿ ਇਹ ਸੁਧਾਰ ਲਿਆਉਂਦੀ ਹੈ:\n",
    "\n",
    "- ਵੱਡਾ ਕਾਂਟੈਕਸਟ ਵਿੰਡੋ - 128k ਟੋਕਨ ਬਨਾਮ 8k ਟੋਕਨ\n",
    "- ਵੱਧ ਤੋਂ ਵੱਧ ਆਉਟਪੁੱਟ ਟੋਕਨ - 4096 ਬਨਾਮ 2048\n",
    "- ਬਿਹਤਰ ਬਹੁਭਾਸ਼ਾਈ ਸਹਿਯੋਗ - ਟ੍ਰੇਨਿੰਗ ਟੋਕਨ ਵਧਣ ਕਰਕੇ\n",
    "\n",
    "ਇਹ ਸੁਧਾਰ ਲਾਮਾ 3.1 ਨੂੰ ਹੋਰ ਪੇਚੀਦੇ ਕੇਸਾਂ ਨੂੰ ਹੱਲ ਕਰਨ ਯੋਗ ਬਣਾਉਂਦੇ ਹਨ, ਜਦੋਂ ਤੁਸੀਂ GenAI ਐਪਲੀਕੇਸ਼ਨ ਬਣਾਉਂਦੇ ਹੋ, ਜਿਸ ਵਿੱਚ ਸ਼ਾਮਲ ਹਨ:\n",
    "- ਨੈਟਿਵ ਫੰਕਸ਼ਨ ਕਾਲਿੰਗ - LLM ਵਰਕਫਲੋ ਤੋਂ ਬਾਹਰ ਬਾਹਰੀ ਟੂਲਾਂ ਅਤੇ ਫੰਕਸ਼ਨਾਂ ਨੂੰ ਕਾਲ ਕਰਨ ਦੀ ਸਮਰੱਥਾ\n",
    "- ਬਿਹਤਰ RAG ਕਾਰਗੁਜ਼ਾਰੀ - ਵੱਡੇ ਕਾਂਟੈਕਸਟ ਵਿੰਡੋ ਕਰਕੇ\n",
    "- ਸਿੰਥੈਟਿਕ ਡਾਟਾ ਜਨਰੇਸ਼ਨ - ਐਸੇ ਕੰਮਾਂ ਲਈ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਡਾਟਾ ਬਣਾਉਣ ਦੀ ਸਮਰੱਥਾ, ਜਿਵੇਂ ਕਿ ਫਾਈਨ-ਟਿਊਨਿੰਗ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ਨੇਟਿਵ ਫੰਕਸ਼ਨ ਕਾਲਿੰਗ\n",
    "\n",
    "Llama 3.1 ਨੂੰ ਹੋਰ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਤਰੀਕੇ ਨਾਲ ਫੰਕਸ਼ਨ ਜਾਂ ਟੂਲ ਕਾਲ ਕਰਨ ਲਈ ਵਧੀਆ ਤਰੀਕੇ ਨਾਲ ਟਿਊਨ ਕੀਤਾ ਗਿਆ ਹੈ। ਇਸ ਵਿੱਚ ਦੋ ਇਨ-ਬਿਲਟ ਟੂਲ ਵੀ ਹਨ, ਜਿਨ੍ਹਾਂ ਨੂੰ ਮਾਡਲ ਯੂਜ਼ਰ ਵੱਲੋਂ ਆਏ ਪ੍ਰੰਪਟ ਦੇ ਆਧਾਰ 'ਤੇ ਵਰਤਣ ਦੀ ਲੋੜ ਸਮਝ ਸਕਦਾ ਹੈ। ਇਹ ਟੂਲ ਹਨ:\n",
    "\n",
    "- **Brave Search** - ਵੈੱਬ ਖੋਜ ਕਰਕੇ ਮੌਸਮ ਵਰਗੀਆਂ ਤਾਜ਼ਾ ਜਾਣਕਾਰੀਆਂ ਲੈਣ ਲਈ ਵਰਤਿਆ ਜਾ ਸਕਦਾ ਹੈ\n",
    "- **Wolfram Alpha** - ਵਧੇਰੇ ਜਟਿਲ ਗਣਿਤੀ ਹਿਸਾਬਾਂ ਲਈ ਵਰਤਿਆ ਜਾ ਸਕਦਾ ਹੈ, ਤਾਂ ਜੋ ਤੁਹਾਨੂੰ ਆਪਣੇ ਫੰਕਸ਼ਨ ਲਿਖਣ ਦੀ ਲੋੜ ਨਾ ਪਵੇ।\n",
    "\n",
    "ਤੁਸੀਂ ਆਪਣੇ ਮਨਪਸੰਦ ਟੂਲ ਵੀ ਬਣਾ ਸਕਦੇ ਹੋ, ਜਿਨ੍ਹਾਂ ਨੂੰ LLM ਕਾਲ ਕਰ ਸਕਦਾ ਹੈ।\n",
    "\n",
    "ਹੇਠਾਂ ਦਿੱਤੇ ਕੋਡ ਉਦਾਹਰਨ ਵਿੱਚ:\n",
    "\n",
    "- ਅਸੀਂ ਉਪਲਬਧ ਟੂਲ (brave_search, wolfram_alpha) ਨੂੰ ਸਿਸਟਮ ਪ੍ਰੰਪਟ ਵਿੱਚ ਪਰਿਭਾਸ਼ਿਤ ਕਰਦੇ ਹਾਂ।\n",
    "- ਯੂਜ਼ਰ ਪ੍ਰੰਪਟ ਭੇਜਦੇ ਹਾਂ ਜੋ ਕਿਸੇ ਨਿਸ਼ਚਿਤ ਸ਼ਹਿਰ ਦੇ ਮੌਸਮ ਬਾਰੇ ਪੁੱਛਦਾ ਹੈ।\n",
    "- LLM Brave Search ਟੂਲ ਨੂੰ ਕਾਲ ਕਰੇਗਾ, ਜੋ ਕੁਝ ਇਸ ਤਰ੍ਹਾਂ ਦਿਸੇਗਾ `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*ਨੋਟ: ਇਹ ਉਦਾਹਰਨ ਸਿਰਫ ਟੂਲ ਕਾਲ ਕਰਦੀ ਹੈ, ਜੇ ਤੁਸੀਂ ਨਤੀਜੇ ਲੈਣਾ ਚਾਹੁੰਦੇ ਹੋ, ਤਾਂ ਤੁਹਾਨੂੰ Brave API ਪੇਜ 'ਤੇ ਮੁਫ਼ਤ ਅਕਾਊਂਟ ਬਣਾਉਣਾ ਪਵੇਗਾ ਅਤੇ ਫੰਕਸ਼ਨ ਨੂੰ ਖੁਦ ਪਰਿਭਾਸ਼ਿਤ ਕਰਨਾ ਪਵੇਗਾ*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ਲਾਮਾ 3.2\n",
    "\n",
    "ਇੱਕ LLM ਹੋਣ ਦੇ ਬਾਵਜੂਦ, ਲਾਮਾ 3.1 ਦੀ ਇੱਕ ਸੀਮਿਤੀ ਮਲਟੀਮੋਡੈਲਿਟੀ ਹੈ। ਇਹ ਹੈ, ਵੱਖ-ਵੱਖ ਕਿਸਮ ਦੇ ਇਨਪੁੱਟ ਵਰਤਣ ਦੀ ਸਮਰੱਥਾ, ਜਿਵੇਂ ਕਿ ਚਿੱਤਰਾਂ ਨੂੰ ਪ੍ਰੋੰਪਟ ਵਜੋਂ ਵਰਤਣਾ ਅਤੇ ਜਵਾਬ ਦੇਣਾ। ਇਹ ਸਮਰੱਥਾ ਲਾਮਾ 3.2 ਦੀਆਂ ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਵਿੱਚੋਂ ਇੱਕ ਹੈ। ਇਹ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਵਿੱਚ ਸ਼ਾਮਲ ਹਨ:\n",
    "\n",
    "- ਮਲਟੀਮੋਡੈਲਿਟੀ - ਟੈਕਸਟ ਅਤੇ ਚਿੱਤਰ ਦੋਹਾਂ ਪ੍ਰੋੰਪਟਾਂ ਦਾ ਮੁਲਾਂਕਣ ਕਰਨ ਦੀ ਸਮਰੱਥਾ\n",
    "- ਛੋਟੇ ਤੋਂ ਮੱਧਮ ਆਕਾਰ ਦੇ ਵੈਰੀਐਂਟ (11B ਅਤੇ 90B) - ਇਹ ਲਚਕੀਲੇ ਡਿਪਲੋਇਮੈਂਟ ਵਿਕਲਪ ਦਿੰਦੇ ਹਨ,\n",
    "- ਸਿਰਫ਼ ਟੈਕਸਟ ਵਾਲੇ ਵੈਰੀਐਂਟ (1B ਅਤੇ 3B) - ਇਹ ਮਾਡਲ ਨੂੰ ਐਜ / ਮੋਬਾਈਲ ਡਿਵਾਈਸਾਂ 'ਤੇ ਡਿਪਲੋਇ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦੇ ਹਨ ਅਤੇ ਘੱਟ ਲੈਟੈਂਸੀ ਮੁਹੱਈਆ ਕਰਦੇ ਹਨ\n",
    "\n",
    "ਮਲਟੀਮੋਡਲ ਸਹਿਯੋਗ ਖੁੱਲ੍ਹੇ ਸਰੋਤ ਮਾਡਲਾਂ ਦੀ ਦੁਨੀਆ ਵਿੱਚ ਇੱਕ ਵੱਡਾ ਕਦਮ ਹੈ। ਹੇਠਾਂ ਦਿੱਤਾ ਕੋਡ ਉਦਾਹਰਨ ਇੱਕ ਚਿੱਤਰ ਅਤੇ ਟੈਕਸਟ ਪ੍ਰੋੰਪਟ ਦੋਹਾਂ ਲੈਂਦਾ ਹੈ, ਤਾਂ ਜੋ ਲਾਮਾ 3.2 90B ਤੋਂ ਚਿੱਤਰ ਦੀ ਵਿਸ਼ਲੇਸ਼ਣ ਮਿਲ ਸਕੇ।\n",
    "\n",
    "### ਲਾਮਾ 3.2 ਨਾਲ ਮਲਟੀਮੋਡਲ ਸਹਿਯੋਗ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ਸਿੱਖਣਾ ਇੱਥੇ ਖਤਮ ਨਹੀਂ ਹੁੰਦਾ, ਆਪਣਾ ਸਫਰ ਜਾਰੀ ਰੱਖੋ\n",
    "\n",
    "ਇਹ ਪਾਠ ਪੂਰਾ ਕਰਨ ਤੋਂ ਬਾਅਦ, ਆਪਣਾ Generative AI ਗਿਆਨ ਹੋਰ ਵਧਾਉਣ ਲਈ ਸਾਡੀ [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ਵੀ ਜ਼ਰੂਰ ਵੇਖੋ!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ਅਸਵੀਕਰਨ**:  \nਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਅਸੀਂ ਯਥਾਸੰਭਵ ਸਹੀ ਅਨੁਵਾਦ ਕਰਨ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਪਰ ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਵਿੱਚ ਰੱਖੋ ਕਿ ਆਟੋਮੈਟਿਕ ਅਨੁਵਾਦ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਣਪੁਰੀ ਸੂਚਨਾ ਹੋ ਸਕਦੀ ਹੈ। ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਲਿਖਿਆ ਦਸਤਾਵੇਜ਼ ਹੀ ਅਧਿਕਾਰਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫ਼ਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਹੋਣ ਵਾਲੀਆਂ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਅਰਥ ਲਗਾਉਣ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:41:22+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "pa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}