{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Build App Wey Dey Generate Image\n",
    "\n",
    "LLMs no be only for text generation. Dem fit also use am generate image from text description. To get image as one modality fit dey very useful for plenty areas like MedTech, architecture, tourism, game development and more. For dis chapter, we go look two popular image generation models, DALL-E and Midjourney.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "For dis lesson, we go talk about:\n",
    "\n",
    "- How image generation dey work and why e dey useful.\n",
    "- DALL-E and Midjourney, wetin dem be and how dem dey work.\n",
    "- How you fit take build app wey dey generate image.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "After you finish dis lesson, you go sabi:\n",
    "\n",
    "- Build app wey dey generate image.\n",
    "- Set boundary for your app with meta prompts.\n",
    "- Work with DALL-E and Midjourney.\n",
    "\n",
    "## Why You Go Build App Wey Dey Generate Image?\n",
    "\n",
    "App wey dey generate image na better way to test wetin Generative AI fit do. You fit use am for example:\n",
    "\n",
    "- **Image editing and synthesis**. You fit generate image for different use cases like editing image or creating new one.  \n",
    "\n",
    "- **Fit work for different industries**. You fit use am generate image for industries like MedTech, Tourism, Game development and more.\n",
    "\n",
    "## Scenario: Edu4All\n",
    "\n",
    "For dis lesson, we go still dey work with our startup, Edu4All. Students go dey create images for their assignments. Wetin dem go create na their choice, e fit be illustration for their own fairytale, new character for their story or help dem visualize their ideas and concepts.\n",
    "\n",
    "Example, if Edu4All students dey work for class on monuments, dem fit generate something like dis:\n",
    "\n",
    "![Edu4All startup, class on monuments, Eifel Tower](../../../../translated_images/startup.94d6b79cc4bb3f5a.pcm.png)\n",
    "\n",
    "using prompt like:\n",
    "\n",
    "> \"Dog next to Eiffel Tower in early morning sunlight\"\n",
    "\n",
    "## Wetin Be DALL-E and Midjourney?\n",
    "\n",
    "[DALL-E](https://openai.com/dall-e-2?WT.mc_id=academic-105485-koreyst) and [Midjourney](https://www.midjourney.com/?WT.mc_id=academic-105485-koreyst) na two popular image generation models wey dey allow you use prompt generate image.\n",
    "\n",
    "### DALL-E\n",
    "\n",
    "Make we start with DALL-E, e be Generative AI model wey dey generate image from text description.\n",
    "\n",
    "> [DALL-E na combination of two models, CLIP and diffused attention](https://towardsdatascience.com/openais-dall-e-and-clip-101-a-brief-introduction-3a4367280d4e?WT.mc_id=academic-105485-koreyst).  \n",
    "\n",
    "- **CLIP**, na model wey dey generate embeddings, wey be numerical representation of data, from image and text.  \n",
    "\n",
    "- **Diffused attention**, na model wey dey generate image from embeddings. DALL-E dey trained with dataset of images and text, e fit generate image from text description. Example, DALL-E fit generate image of cat wey wear hat or dog wey get mohawk.\n",
    "\n",
    "### Midjourney\n",
    "\n",
    "Midjourney dey work like DALL-E, e dey generate image from text prompt. Midjourney fit also generate image with prompt like “cat wey wear hat” or “dog wey get mohawk”.\n",
    "\n",
    "![Image generated by Midjourney, mechanical pigeon](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Rupert_Breheny_mechanical_dove_eca144e7-476d-4976-821d-a49c408e4f36.png/440px-Rupert_Breheny_mechanical_dove_eca144e7-476d-4976-821d-a49c408e4f36.png?WT.mc_id=academic-105485-koreyst)\n",
    "\n",
    "*Image cred Wikipedia, image generated by Midjourney*\n",
    "\n",
    "## How DALL-E and Midjourney Dey Work\n",
    "\n",
    "First, [DALL-E](https://arxiv.org/pdf/2102.12092.pdf?WT.mc_id=academic-105485-koreyst). DALL-E na Generative AI model wey dey use transformer architecture with *autoregressive transformer*.  \n",
    "\n",
    "*Autoregressive transformer* dey define how model dey generate image from text description. E dey generate one pixel at a time, then e go use the pixel wey e don generate to create the next one. E go pass through plenty layers for neural network until the image complete.\n",
    "\n",
    "With dis process, DALL-E fit control attributes, objects, characteristics and more for the image wey e generate. But DALL-E 2 and 3 get more control over the image wey dem dey generate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How you go take build your first image generation app\n",
    "\n",
    "So wetin e go take to build image generation app? You go need dis libraries:\n",
    "\n",
    "- **python-dotenv**, e good well make you use dis library to keep your secrets for *.env* file comot from the code.\n",
    "- **openai**, na dis library you go use to connect with OpenAI API.\n",
    "- **pillow**, to work with images for Python.\n",
    "- **requests**, e go help you make HTTP requests.\n",
    "\n",
    "1. Create one file *.env* wey get dis content:\n",
    "\n",
    "    ```text\n",
    "    AZURE_OPENAI_ENDPOINT=<your endpoint>\n",
    "    AZURE_OPENAI_API_KEY=<your key>\n",
    "    ```\n",
    "\n",
    "    You fit find dis information for Azure Portal for your resource under \"Keys and Endpoint\" section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gather all di libraries wey dey up for one file wey dem go call *requirements.txt* like dis:\n",
    "\n",
    "    ```text\n",
    "    python-dotenv\n",
    "    openai\n",
    "    pillow\n",
    "    requests\n",
    "    ```\n",
    "\n",
    "\n",
    "1. After dat, make virtual environment and install di libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# create virtual env\n",
    "! python3 -m venv venv\n",
    "# activate environment\n",
    "! source venv/bin/activate\n",
    "# install libraries\n",
    "# pip install -r requirements.txt, if using a requirements.txt file \n",
    "! pip install python-dotenv openai pillow requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> For Windows, use dis commands to create and activate your virtual environment:\n",
    "\n",
    "    ```bash\n",
    "    python3 -m venv venv\n",
    "    venv\\Scripts\\activate.bat\n",
    "    ````\n",
    "\n",
    "1. Add dis code for file wey dem call *app.py*:\n",
    "\n",
    "    ```python\n",
    "    import openai\n",
    "    import os\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    import dotenv\n",
    "    \n",
    "    # import dotenv\n",
    "    dotenv.load_dotenv()\n",
    "    \n",
    "    # Get endpoint and key from environment variables\n",
    "    openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "    openai.api_key = os.environ['AZURE_OPENAI_API_KEY']     \n",
    "    \n",
    "    # Assign the API version (DALL-E is currently supported for the 2023-06-01-preview API version only)\n",
    "    openai.api_version = '2023-06-01-preview'\n",
    "    openai.api_type = 'azure'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Create an image by using the image generation API\n",
    "        generation_response = openai.Image.create(\n",
    "            prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n",
    "            size='1024x1024',\n",
    "            n=2,\n",
    "            temperature=0,\n",
    "        )\n",
    "        # Set the directory for the stored image\n",
    "        image_dir = os.path.join(os.curdir, 'images')\n",
    "    \n",
    "        # If the directory doesn't exist, create it\n",
    "        if not os.path.isdir(image_dir):\n",
    "            os.mkdir(image_dir)\n",
    "    \n",
    "        # Initialize the image path (note the filetype should be png)\n",
    "        image_path = os.path.join(image_dir, 'generated-image.png')\n",
    "    \n",
    "        # Retrieve the generated image\n",
    "        image_url = generation_response[\"data\"][0][\"url\"]  # extract image URL from response\n",
    "        generated_image = requests.get(image_url).content  # download the image\n",
    "        with open(image_path, \"wb\") as image_file:\n",
    "            image_file.write(generated_image)\n",
    "    \n",
    "        # Display the image in the default image viewer\n",
    "        image = Image.open(image_path)\n",
    "        image.show()\n",
    "    \n",
    "    # catch exceptions\n",
    "    except openai.InvalidRequestError as err:\n",
    "        print(err)\n",
    "\n",
    "    ```\n",
    "\n",
    "Make we explain dis code:\n",
    "\n",
    "- First, we go import di libraries wey we need, including di OpenAI library, di dotenv library, di requests library, and di Pillow library.\n",
    "\n",
    "    ```python\n",
    "    import openai\n",
    "    import os\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    import dotenv\n",
    "    ```\n",
    "\n",
    "- Next, we go load di environment variables from di *.env* file.\n",
    "\n",
    "    ```python\n",
    "    # import dotenv\n",
    "    dotenv.load_dotenv()\n",
    "    ```\n",
    "\n",
    "- After dat, we go set di endpoint, key for di OpenAI API, version and type.\n",
    "\n",
    "    ```python\n",
    "    # Get endpoint and key from environment variables\n",
    "    openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "    openai.api_key = os.environ['AZURE_OPENAI_API_KEY'] \n",
    "\n",
    "    # add version and type, Azure specific\n",
    "    openai.api_version = '2023-06-01-preview'\n",
    "    openai.api_type = 'azure'\n",
    "    ```\n",
    "\n",
    "- Next, we go generate di image:\n",
    "\n",
    "    ```python\n",
    "    # Create an image by using the image generation API\n",
    "    generation_response = openai.Image.create(\n",
    "        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n",
    "        size='1024x1024',\n",
    "        n=2,\n",
    "        temperature=0,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    Di code wey dey up go respond with JSON object wey get di URL of di image wey dem generate. We fit use di URL download di image and save am for file.\n",
    "\n",
    "- Last last, we go open di image and use di standard image viewer to show am:\n",
    "\n",
    "    ```python\n",
    "    image = Image.open(image_path)\n",
    "    image.show()\n",
    "    ```\n",
    "    \n",
    "### More details on how to generate di image\n",
    "\n",
    "Make we look di code wey dey generate di image well well:\n",
    "\n",
    "```python\n",
    "generation_response = openai.Image.create(\n",
    "        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n",
    "        size='1024x1024',\n",
    "        n=2,\n",
    "        temperature=0,\n",
    "    )\n",
    "```\n",
    "\n",
    "- **prompt**, na di text wey dem go use generate di image. For dis case, we dey use di prompt \"Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils\".\n",
    "- **size**, na di size of di image wey dem go generate. For dis case, we dey generate image wey be 1024x1024 pixels.\n",
    "- **n**, na di number of images wey dem go generate. For dis case, we dey generate two images.\n",
    "- **temperature**, na one parameter wey dey control how random di output of Generative AI model go be. Di temperature na value wey dey between 0 and 1, where 0 mean say di output go dey sure sure and 1 mean say di output go dey random. Di default value na 0.7.\n",
    "\n",
    "Plenty things dey wey you fit do with images, we go talk about am for di next section.\n",
    "\n",
    "## Extra things wey image generation fit do\n",
    "\n",
    "You don see how we fit generate image with small Python code. But plenty other things dey wey you fit do with images.\n",
    "\n",
    "You fit also do dis ones:\n",
    "\n",
    "- **Make edits**. If you provide one image wey don dey already, one mask and one prompt, you fit change di image. For example, you fit add something for one part of di image. Imagine di bunny image, you fit add hat for di bunny head. How you go do am na to provide di image, one mask (wey go show di part wey you wan change) and one text prompt to talk wetin dem go do.\n",
    "\n",
    "    ```python\n",
    "    response = openai.Image.create_edit(\n",
    "      image=open(\"base_image.png\", \"rb\"),\n",
    "      mask=open(\"mask.png\", \"rb\"),\n",
    "      prompt=\"An image of a rabbit with a hat on its head.\",\n",
    "      n=1,\n",
    "      size=\"1024x1024\"\n",
    "    )\n",
    "    image_url = response['data'][0]['url']\n",
    "    ```\n",
    "\n",
    "    Di base image go only get di rabbit but di final image go get di hat for di rabbit head.\n",
    "    \n",
    "- **Create variations**. \n",
    "    Check our [OpenAI notebook for more gist](./oai-assignment.ipynb?WT.mc_id=academic-105485-koreyst).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:  \nDis dokyument don use AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator) do di translation. Even as we dey try make am correct, abeg sabi say machine translation fit get mistake or no dey accurate well. Di original dokyument for im native language na di main source wey you go trust. For important information, e better make professional human translator check am. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translation.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "d5c5bc8d857e5cb63e313beac9fa6402",
   "translation_date": "2025-11-12T09:19:21+00:00",
   "source_file": "09-building-image-applications/python/aoai-assignment.ipynb",
   "language_code": "pcm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}