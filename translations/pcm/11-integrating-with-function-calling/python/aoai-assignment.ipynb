{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Dis lesson go cover:  \n",
    "- Wetin be function calling and wetin e dey use do  \n",
    "- How you fit create function call wit Azure OpenAI  \n",
    "- How you fit put function call inside app  \n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "After you finish dis lesson, you go sabi how to and understand:  \n",
    "\n",
    "- Why e good to use function calling  \n",
    "- How to setup Function Call wit Azure Open AI Service  \n",
    "- How to design better function calls wey go fit your app use case  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand How Function Call Work\n",
    "\n",
    "For dis lesson, we wan build one feature for our education startup wey go allow users use chatbot to find technical courses. We go recommend courses wey match dia skill level, current role, and technology wey dem dey interested in.\n",
    "\n",
    "To finish dis work, we go use combination of:\n",
    " - `Azure Open AI` to create chat experience for di user\n",
    " - `Microsoft Learn Catalog API` to help users find courses based on wetin dem request\n",
    " - `Function Calling` to take di user's query and send am go one function wey go make di API request.\n",
    "\n",
    "To start, make we look why we go wan use function calling in di first place:\n",
    "\n",
    "print(\"Messages wey dey next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get new response from GPT wey fit see di function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Function Calling\n",
    "\n",
    "If you don learn any oda lesson for dis course, you go don sabi di power wey Large Language Models (LLMs) get. We dey hope say you fit also see some of di wahala wey dem get. \n",
    "\n",
    "Function Calling na one feature for Azure Open AI Service wey dey help solve dis kind wahala: \n",
    "1) Make response format dey consistent \n",
    "2) Make e possible to use data from oda sources for one application inside chat context \n",
    "\n",
    "Before Function Calling, di response wey LLM dey give no dey structured and e no dey consistent. Developers go need write plenty complex validation code to make sure say dem fit handle di different ways wey response fit take show. \n",
    "\n",
    "Users no fit get answer like \"Wetn be di current weather for Stockholm?\". Dis na because di models dey limited to di time wey dem train di data. \n",
    "\n",
    "Make we look di example wey dey below wey go show dis problem: \n",
    "\n",
    "Make we say we wan create one database of student data so we fit suggest di correct course to dem. Below, we get two descriptions of students wey dey very similar for di data wey dem carry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wan send dis one go LLM make e fit arrange di data. Later, we fit use am for our app to send am go API or keep am for database.\n",
    "\n",
    "Make we create two prompts wey be di same wey go tell di LLM di kind information wey we dey find:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wan send dis one to LLM make e fit check di parts wey dey important to our product. So we fit create two same prompts to tell di LLM wetin to do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afta we don create dis two prompts, we go send dem to di LLM by usin `openai.ChatCompletion`. We go store di prompt for di `messages` variable and give di role to `user`. Dis na to mimic message wey user dey write to chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit send both requests to di LLM and check di response we go receive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though di prompts dey same and di descriptions dey similar, we fit get different formats for di `Grades` property.\n",
    "\n",
    "If you run di cell wey dey up many times, di format fit be `3.7` or `3.7 GPA`.\n",
    "\n",
    "Dis na because di LLM dey take unstructured data wey dey inside di written prompt and e dey return unstructured data too. We need structured format so we go sabi wetin to expect when we wan store or use di data.\n",
    "\n",
    "If we use functional calling, we fit make sure say we go dey receive structured data back. When we dey use function calling, di LLM no dey actually call or run any functions. Instead, we go create structure wey di LLM go follow for e responses. We go then use di structured responses to sabi which function we go run for our applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Function Calling Flow Diagram](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.pcm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit take wetin de return from di function send am back to di LLM. Di LLM go then respond using natural language to ansa di user question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases for using function calls \n",
    "\n",
    "**Call External Tools**  \n",
    "Chatbots dey good for ansa di questions wey users dey ask. Wit function calling, di chatbots fit use di message wey user send to do some kain task. For example, student fit tell chatbot say \"Send email to my instructor say I need more help for dis subject\". Dis one fit make di chatbot call function wey be `send_email(to: string, body: string)`.\n",
    "\n",
    "**Make API or Database Queries**  \n",
    "Users fit find information wit natural language wey go turn to formatted query or API request. Example na teacher wey fit ask \"Who be di students wey finish di last assignment\" wey fit make di chatbot call function wey be `get_completed(student_name: string, assignment: int, current_status: string)`.\n",
    "\n",
    "**Create Structured Data**  \n",
    "Users fit take one block of text or CSV and use di LLM to comot di important information wey dey inside. For example, student fit change one Wikipedia article about peace agreements to make AI flash cards. Dis one fit happen wit function wey be `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How to Make Your First Function Call\n",
    "\n",
    "To make function call, e get 3 main steps wey you go follow:  \n",
    "1. Use Chat Completions API take call your list of functions plus user message.  \n",
    "2. Check wetin the model respond so you fit do action like run function or API Call.  \n",
    "3. Call Chat Completions API again with wetin your function respond so you fit use am create reply for the user.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flow of a Function Call](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.pcm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements of a function call \n",
    "\n",
    "#### Users Input \n",
    "\n",
    "Di first step na to create user message. You fit assign am dynamic by using di value wey dey text input or you fit put value for here. If na your first time wey you dey work with di Chat Completions API, we go need define di `role` and di `content` of di message. \n",
    "\n",
    "Di `role` fit be `system` (to set rules), `assistant` (di model) or `user` (di end-user). For function calling, we go set am as `user` and example question. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create functions.\n",
    "\n",
    "Next thing we go do na to define one function and di parameters wey dey inside di function. We go use only one function here wey dem call `search_courses` but you fit create plenty functions.\n",
    "\n",
    "**Important**: Functions dey inside di system message to di LLM and dem go dey count inside di number of tokens wey you get available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitions** \n",
    "\n",
    "`name` - Na di name of di function wey we wan make e run. \n",
    "\n",
    "`description` - Dis na di description of how di function dey work. For here, e dey important to make am clear and straight. \n",
    "\n",
    "`parameters` - List of values and format wey you wan make di model produce for im response. \n",
    "\n",
    "`type` - Di data type wey di properties go dey store inside. \n",
    "\n",
    "`properties` - List of di specific values wey di model go use for im response. \n",
    "\n",
    "`name` - Di name of di property wey di model go use for im formatted response. \n",
    "\n",
    "`type` - Di data type of dis property. \n",
    "\n",
    "`description` - Description of di specific property. \n",
    "\n",
    "**Optional**\n",
    "\n",
    "`required` - Di property wey dem need for di function call to complete. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to call di function\n",
    "Afta we don define di function, di next step na to put am inside di call wey go go Chat Completion API. We go do dis one by adding `functions` to di request. For dis case, `functions=functions`.\n",
    "\n",
    "E get option wey fit make us set `function_call` to `auto`. Dis one mean say we go allow di LLM choose which function e go call based on wetin di user message talk, instead of us choosing am by ourselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make we check di response and see how dem arrange am:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "You fit see say di name of di function dey call, and from wetin di user talk, di LLM fit find di data wey go match di arguments for di function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to Put Function Call Inside App\n",
    "\n",
    "Afta we don test di formatted response wey come from di LLM, na time to put am inside app.\n",
    "\n",
    "### How to Manage di Flow\n",
    "\n",
    "To put dis one inside our app, make we follow dis steps:\n",
    "\n",
    "First, make we call di Open AI services and keep di message for one variable wey we go call `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go define di function wey go call di Microsoft Learn API to get list of courses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As e good make we do am well, we go first check if di model wan call one function. After dat, we go create one of di functions wey dey available and match am to di function wey dem dey call.  \n",
    "We go then carry di arguments of di function and map dem to di arguments wey come from di LLM.  \n",
    "\n",
    "Finally, we go add di function call message and di values wey di `search_courses` message return. Dis one go give di LLM all di information wey e need to take reply di user wit natural language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go send di updated message to di LLM so we fit get natural language response instead of API JSON formatted response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge\n",
    "\n",
    "Good job! To continue to dey learn about Azure Open AI Function Calling, you fit build: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - Add more parameters for di function wey go fit help learners find more courses. You fit see di available API parameters for here: \n",
    " - Create another function call wey go collect more information from di learner like dia native language \n",
    " - Add error handling for when di function call and/or API call no return any suitable courses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:  \nDis dokyument don use AI translet service [Co-op Translator](https://github.com/Azure/co-op-translator) do di translet. Even as we dey try make am correct, abeg sabi say AI translet fit get mistake or no dey accurate well. Di original dokyument for im native language na di one wey you go take as di correct source. For important mata, e good make professional human translet am. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translet.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-11-12T09:10:55+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "pcm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}