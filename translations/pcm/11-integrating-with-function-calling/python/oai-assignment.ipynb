{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Dis lesson go cover:  \n",
    "- Wetin be function calling and wetin e fit do  \n",
    "- How you fit create function call wit OpenAI  \n",
    "- How you go fit put function call inside one application  \n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "After you finish dis lesson, you go sabi how to and understand:  \n",
    "\n",
    "- Why e good to use function calling  \n",
    "- How to setup Function Call wit OpenAI Service  \n",
    "- How to design better function calls wey go work well for your application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Function Calls \n",
    "\n",
    "For dis lesson, we wan build one feature for our education startup wey go allow users use chatbot to find technical courses. We go recommend courses wey match dia skill level, current role and technology wey dem dey interested in. \n",
    "\n",
    "To complete dis, we go use combination of: \n",
    " - `OpenAI` to create chat experience for di user\n",
    " - `Microsoft Learn Catalog API` to help users find courses based on wetin di user request\n",
    " - `Function Calling` to take di user's query and send am to one function to make di API request. \n",
    "\n",
    "To start, make we look why we go wan use function calling in di first place: \n",
    "\n",
    "print(\"Messages wey dey next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get new response from GPT wey fit see di function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Function Calling\n",
    "\n",
    "If you don do any oda lesson for dis course, you fit don sabi di power wey dey for Large Language Models (LLMs). E sure say you fit don see some of di wahala wey dey follow dem too.\n",
    "\n",
    "Function Calling na one feature wey OpenAI Service design to solve dis kind wahala:\n",
    "\n",
    "Inconsistent Response Formatting:\n",
    "- Before function calling, di response wey dey come from large language model no dey structured well and e dey inconsistent. Developers go need write plenty validation code to fit handle di different ways wey di output dey show.\n",
    "\n",
    "Limited Integration with External Data:\n",
    "- Before dis feature, e no easy to carry data from oda parts of di application put for chat context.\n",
    "\n",
    "By making response formats standard and making am easy to connect with external data, function calling dey make development simple and e dey reduce di need for extra validation logic.\n",
    "\n",
    "Users no fit get answer like \"Wetn be di current weather for Stockholm?\". Dis na because di models dey limited to di time wey dem train di data.\n",
    "\n",
    "Make we look di example below wey go show dis problem:\n",
    "\n",
    "Make we say we wan create one database of student data so we fit suggest di correct course to dem. Below, we get two descriptions of students wey dey very similar for di data wey dem carry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wan send dis one go LLM make e fit arrange di data. Later, we fit use am for our app to send am go API or keep am for database.\n",
    "\n",
    "Make we create two prompts wey be di same wey go tell di LLM di kind information wey we dey look for:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wan send dis one to LLM make e fit check di parts wey dey important to our product. So we fit create two same prompts to tell di LLM wetin to do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afta we don create dis two prompts, we go send dem to di LLM by usin `openai.ChatCompletion`. We go store di prompt for di `messages` variable and give di role to `user`. Dis na to mimic message wey user dey write to chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit send both requests to di LLM and check di response we go receive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though di prompts dey same and di descriptions dey similar, we fit get different formats for di `Grades` property.\n",
    "\n",
    "If you run di cell wey dey up many times, di format fit be `3.7` or `3.7 GPA`.\n",
    "\n",
    "Dis na because di LLM dey take unstructured data wey dey inside di written prompt and e dey return unstructured data too. We need make we get structured format so we go sabi wetin to expect when we dey store or use dis data.\n",
    "\n",
    "If we use functional calling, we fit make sure say we go dey receive structured data back. When we dey use function calling, di LLM no dey actually call or run any functions. Instead, we dey create structure wey di LLM go follow for im responses. We go then use di structured responses to sabi which function we go run for our applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Function Calling Flow Diagram](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.pcm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit take wetin de return from di function send am back to di LLM. Di LLM go then respond wit natural language to ansa di user question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases for using function calls\n",
    "\n",
    "**Call External Tools**  \n",
    "Chatbots dey good for ansa di questions wey users dey ask. Wit function calling, di chatbots fit use di message wey user send to do some kain task. Example, student fit tell chatbot say \"Send email to my instructor say I need more help for dis subject\". Dis one fit make function call to `send_email(to: string, body: string)`\n",
    "\n",
    "**Make API or Database Queries**  \n",
    "Users fit find information wit natural language wey go turn to formatted query or API request. Example, teacher fit ask \"Who be di students wey finish di last assignment\" wey fit call function wey dem name `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Make Structured Data**  \n",
    "Users fit take one block of text or CSV and use di LLM to comot di important information wey dey inside. Example, student fit change one Wikipedia article about peace agreements to make AI flash cards. Dis one fit happen wit function wey dem call `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How to Make Your First Function Call\n",
    "\n",
    "To make function call, e get 3 main steps wey you go follow:  \n",
    "1. Use Chat Completions API take call list of your functions plus user message.  \n",
    "2. Check wetin the model respond, so you fit do action like run function or API Call.  \n",
    "3. Call Chat Completions API again with wetin your function respond, so you fit use am take reply the user.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flow of a Function Call](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.pcm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things wey dey inside function call\n",
    "\n",
    "#### User Input\n",
    "\n",
    "Di first step na to create user message. You fit use di value wey dey text input take assign am or you fit just put value for here. If na your first time wey you dey work with Chat Completions API, we go need define di `role` and di `content` of di message.\n",
    "\n",
    "Di `role` fit be `system` (to set rules), `assistant` (di model) or `user` (di end-user). For function calling, we go set am as `user` and example question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create functions.\n",
    "\n",
    "Next thing we go do na to define one function and the parameters wey dey inside the function. We go use just one function for here wey we go call `search_courses`, but you fit create plenty functions if you want.\n",
    "\n",
    "**Important**: Functions dey inside the system message wey dey go LLM and dem go dey count for the amount of tokens wey you get available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitions**\n",
    "\n",
    "Di way wey function dey structured get plenty levels, and each level get im own properties. Dis na di breakdown of di nested structure:\n",
    "\n",
    "**Top Level Function Properties:**\n",
    "\n",
    "`name` - Di name of di function wey we wan make dem call.\n",
    "\n",
    "`description` - Dis na di explanation of how di function dey work. E good make e dey clear and specific.\n",
    "\n",
    "`parameters` - List of values and format wey you wan make di model produce for im response.\n",
    "\n",
    "**Parameters Object Properties:**\n",
    "\n",
    "`type` - Di data type of di parameters object (normally \"object\").\n",
    "\n",
    "`properties` - List of di specific values wey di model go use for im response.\n",
    "\n",
    "**Individual Parameter Properties:**\n",
    "\n",
    "`name` - E dey defined by di property key (like \"role\", \"product\", \"level\").\n",
    "\n",
    "`type` - Di data type of dis specific parameter (like \"string\", \"number\", \"boolean\").\n",
    "\n",
    "`description` - Explanation of di specific parameter.\n",
    "\n",
    "**Optional Properties:**\n",
    "\n",
    "`required` - Array wey dey list di parameters wey dem need to complete di function call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to take call for function\n",
    "Afta we don define function, na im we go need put am inside the call wey go go Chat Completion API. We go do am by adding `functions` for the request. For dis case, `functions=functions`.\n",
    "\n",
    "E still get option to set `function_call` to `auto`. Dis one mean say we go allow the LLM choose which function e go call based on wetin user talk, instead of us picking am by ourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make we check di response and see how dem arrange am:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "You fit see say di name of di function wey dem call dey dere, and from wetin di user talk, di LLM fit find di data wey go match di arguments for di function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How to Put Function Call Inside App\n",
    "\n",
    "Afta we don test the formatted response wey come from LLM, na im we go fit put am inside app.\n",
    "\n",
    "### How to Arrange the Flow\n",
    "\n",
    "To put am inside our app, make we follow dis steps:\n",
    "\n",
    "First, make we call OpenAI services and keep the message for one variable wey we go name `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go define di function wey go call di Microsoft Learn API to get list of courses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As e good practice, we go first check if di model wan call one function. After dat, we go create one of di functions wey dey available and match am to di function wey dem dey call.  \n",
    "We go then carry di arguments of di function and map dem to di arguments wey come from di LLM.\n",
    "\n",
    "Finally, we go add di function call message and di values wey di `search_courses` message return. Dis one go give di LLM all di information wey e need to take reply di user wit natural language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go send di updated message to di LLM so we fit get natural language response instead of API JSON formatted response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge \n",
    "\n",
    "Good job! To continue to dey learn about OpenAI Function Calling, you fit build: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - Add more parameters for the function wey go fit help learners find more courses. You fit see the API parameters wey dey available here: \n",
    " - Create another function call wey go collect more information from the learner like their native language \n",
    " - Add error handling for when the function call and/or API call no return any course wey dey suitable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:  \nDis dokyument don translate wit AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). Even as we dey try make am accurate, abeg sabi say machine translation fit get mistake or no dey correct well. Di original dokyument for im native language na di main source wey you go trust. For important mata, e better make professional human translation dey use. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translation.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-11-12T09:11:53+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "pcm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}