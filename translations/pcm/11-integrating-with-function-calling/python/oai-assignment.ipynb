{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Dis lesson go cover: \n",
    "- Wetin be function calling and how e dey useful \n",
    "- How to create function call using OpenAI \n",
    "- How to put function call inside application \n",
    "\n",
    "## Learning Goals \n",
    "\n",
    "After you finish dis lesson, you go sabi how to and understand: \n",
    "\n",
    "-  Why you go use function calling \n",
    "- How to setup Function Call using OpenAI Service \n",
    "- How to design beta function calls for how your application go take use am\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Function Calls \n",
    "\n",
    "For dis lesson, we wan build one feature for our education startup wey go allow users use chatbot to find technical courses. We go recommend courses wey fit their skill level, current role and technology wey dem dey interested for. \n",
    "\n",
    "To complete dis, we go use combination of: \n",
    " - `OpenAI` to create chat experience for the user\n",
    " - `Microsoft Learn Catalog API` to help users find courses based on wetin the user request \n",
    " - `Function Calling` to carry the user's query come send am go function to make the API request. \n",
    "\n",
    "To start, make we look why we go want use function calling for the first place: \n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get new response from GPT wey fit see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Function Calling \n",
    "\n",
    "If you don complete any oda lesson for dis course, you fit don sabi di power wey dey for using Large Language Models (LLMs). Hope say you fit also see some of dia limitations too. \n",
    "\n",
    "Function Calling na one feature of di OpenAI Service wey dem design to solve dis kain wahala:\n",
    "\n",
    "Inconsistent Response Formatting:\n",
    "- Before function calling, responses from one large language model no get structure and dem no consistent. Developers for write complex validation code to handle every variation wey dey for di output.\n",
    "\n",
    "Limited Integration with External Data:\n",
    "- Before dis feature, e hard to put data from oda parts of one application inside chat context.\n",
    "\n",
    "By standardizing response formats and making e easy to join external data, function calling dey make development simple and e reduce di need for extra validation logic.\n",
    "\n",
    "Users no fit get answers like \"Wetin be di current weather for Stockholm?\". Dis na because models dey limited to di time wey dem train di data.\n",
    "\n",
    "Make we look di example wey dey below wey show dis problem:\n",
    "\n",
    "Make we talk say we wan create one database of student data so dat we fit suggest di correct course to dem. Below we get two descriptions of students wey similar well well for di data wey dem get.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wan send dis to an LLM make e parse di data. Dis one fit later dey use for our application to send am go API or store am for database.\n",
    "\n",
    "Make we create two identical prompts wey we go tell di LLM wetin kind information we dey interested for:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wan send dis to one LLM make e fit parse di parts wey important to our product. So we fit create two identical prompts to instruct di LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afta we don create dis two prompts, we go send dem go LLM by using `openai.ChatCompletion`. We dey store di prompt for di `messages` variable and assign di role to `user`. Dis na to mimic message wey user dey write go chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit send both requests go the LLM and check the response we receive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even tho di prompts na di same and di descriptions dey similar, we fit get different formats for di `Grades` property.\n",
    "\n",
    "If you run di cell wey dey above many times, di format fit be `3.7` or `3.7 GPA`.\n",
    "\n",
    "Dis na because di LLM dey take unstructured data for di form of di written prompt and e go return unstructured data too. We need make we get structured format so dat we go sabi wetin to expect when we dey store or use dis data.\n",
    "\n",
    "By using functional calling, we fit make sure say we go receive structured data back. When we dey use function calling, di LLM no really dey call or run any functions. Instead, we dey create structure for di LLM to follow for im responses. We go then use those structured responses to sabi which function to run for our applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Function Calling Flow Diagram](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.pcm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit den take wetin di function return come send am back to di LLM. Di LLM go den respond wit natural language to answer di user query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases for using function calls \n",
    "\n",
    "**Calling External Tools**  \n",
    "Chatbots dey very good for giving answers to questions wey users ask. By using function calling, the chatbots fit use messages from users to complete certain tasks. For example, one student fit ask the chatbot to \"Send email to my instructor saying I need more assistance with this subject\". Dis one fit make function call to `send_email(to: string, body: string)`\n",
    "\n",
    "\n",
    "**Create API or Database Queries**  \n",
    "Users fit find information by using natural language wey go convert into formatted query or API request. Example be say one teacher fit ask \"Who are the students that completed the last assignment\" wey fit call function wey dem name `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "\n",
    "**Creating Structured Data**  \n",
    "Users fit take one block of text or CSV and use the LLM to extract important information from am. For example, one student fit convert Wikipedia article about peace agreements to create AI flash cards. Dis one fit happen by using function wey dem call `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Your First Function Call \n",
    "\n",
    "Di process wey you go take create function call get 3 main steps: \n",
    "1. Call di Chat Completions API wit list of your functions and user message \n",
    "2. Read di model response to perform action like execute function or API Call \n",
    "3. Make another call to Chat Completions API wit di response from your function to use dat info create response to di user. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flow of a Function Call](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.pcm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements of a function call \n",
    "\n",
    "#### Users Input \n",
    "\n",
    "Di first step na to create user message. Dis fit dey dynamically assigned by taking di value of text input or you fit assign value here. If na your first time to dey work with Chat Completions API, we need to define di `role` and di `content` of di message. \n",
    "\n",
    "Di `role` fit be either `system` (wey dey create rules), `assistant` (di model) or `user` (di end-user). For function calling, we go assign am as `user` plus example question. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions. \n",
    "\n",
    "Next we go define one function and the parameters of that function. We go use just one function here wey dem call `search_courses` but you fit create plenty functions.\n",
    "\n",
    "**Important** : Functions dey inside the system message to the LLM and dem go count for the amount of tokens wey you get.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitions** \n",
    "\n",
    "Di function definition structure get plenti levels, each get im own properties. Dis na di breakdown of di nested structure:\n",
    "\n",
    "**Top Level Function Properties:**\n",
    "\n",
    "`name` -  Di name of di function wey we want make e dey called. \n",
    "\n",
    "`description` - Dis na di description of how di function dey work. Here e important to dey specific and clear \n",
    "\n",
    "`parameters` - List of values and format wey you want make di model produce for im response \n",
    "\n",
    "**Parameters Object Properties:**\n",
    "\n",
    "`type` -  Di data type of di parameters object (usually \"object\")\n",
    "\n",
    "`properties` - List of di specific values wey di model go use for im response \n",
    "\n",
    "**Individual Parameter Properties:**\n",
    "\n",
    "`name` - Implicitly defined by di property key (e.g., \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Di data type of dis specific parameter (e.g., \"string\", \"number\", \"boolean\") \n",
    "\n",
    "`description` - Description of di specific parameter \n",
    "\n",
    "**Optional Properties:**\n",
    "\n",
    "`required` - Array wey dey list which parameters dey required for di function call to complete \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the function call \n",
    "After we don define function, now we need put am for the call to the Chat Completion API. We dey do dis by adding `functions` to the request. For dis case, na `functions=functions`. \n",
    "\n",
    "E still get option to set `function_call` to `auto`. Dis one mean say we go allow the LLM decide which function suppose call based on the user message instead make we assign am ourselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make we look di response and see how e take format: \n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "You fit see say di name of di function don call and from di user message, di LLM fit find di data wey go match di arguments of di function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Integrating Function Calls into an Application. \n",
    "\n",
    "\n",
    "After we don test the formatted response from the LLM, now we fit integrate dis into an application. \n",
    "\n",
    "### Managing the flow \n",
    "\n",
    "To integrate dis into our application, make we follow dis steps: \n",
    "\n",
    "First, make we call the OpenAI services and store the message for one variable wey dem call `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go define di function wey go call di Microsoft Learn API to get list of courses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As best practice, we go den check if di model wan call function. After dat, we go create one of di available functions and match am to di function wey dem dey call.  \n",
    "We go den take di arguments of di function and map dem to arguments from di LLM.\n",
    "\n",
    "Lastly, we go add di function call message and di values wey di `search_courses` message return. Dis one go give di LLM all di information wey e need to  \n",
    "respond to di user using natural language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go send di updated message go di LLM so we fit receive natural language response instead of API JSON formatted response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge \n",
    "\n",
    "Great work! To continue your learning of OpenAI Function Calling you fit build: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - More parameters of the function wey fit help learners find more courses. You fit find the available API parameters here: \n",
    " - Create another function call wey go take more information from the learner like their native language \n",
    " - Create error handling wen the function call and/or API call no return any suitable courses \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:\nDis document na AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator) wey translate am. Even though we dey try make am correct, abeg sabi say automated translation fit get some mistakes or no too correct. The original document wey e dey for im own language na im be the correct one. If na serious matter, e better make person wey sabi translate am well do am. We no go responsible for any wahala or wrong understanding wey fit happen because of this translation.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T12:22:26+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "pcm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}