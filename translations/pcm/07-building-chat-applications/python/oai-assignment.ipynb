{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Chapter 7: How to Build Chat Apps\n",
    "## OpenAI API Quickstart\n",
    "\n",
    "Dis notebook na from [Azure OpenAI Samples Repository](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst) wey get notebooks wey dey use [Azure OpenAI](notebook-azure-openai.ipynb) services.\n",
    "\n",
    "Python OpenAI API dey work with Azure OpenAI Models too, but e get small changes. You fit learn more about di difference for here: [How to switch between OpenAI and Azure OpenAI endpoints with Python](https://learn.microsoft.com/azure/ai-services/openai/how-to/switching-endpoints?WT.mc_id=academic-109527-jasmineg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Overview  \n",
    "\"Big language models na functions wey dey change text to text. If dem give am one text as input, big language model go try predict di text wey go follow\"(1). Dis \"quickstart\" notebook go show users di main ideas about LLM, di main package wey dem need to start with AML, small intro to how to design prompt, and some short examples of different ways wey person fit use am.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Table of Contents  \n",
    "\n",
    "[Overview](../../../../07-building-chat-applications/python)  \n",
    "[How to use OpenAI Service](../../../../07-building-chat-applications/python)  \n",
    "[1. How you go take create your OpenAI Service](../../../../07-building-chat-applications/python)  \n",
    "[2. How you go install am](../../../../07-building-chat-applications/python)    \n",
    "[3. How you go take set your credentials](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Use Cases](../../../../07-building-chat-applications/python)    \n",
    "[1. How to summarize text](../../../../07-building-chat-applications/python)  \n",
    "[2. How to classify text](../../../../07-building-chat-applications/python)  \n",
    "[3. How to generate new product names](../../../../07-building-chat-applications/python)  \n",
    "[4. How to fine-tune classifier](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[References](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Build your first prompt  \n",
    "Dis small exercise go show you basic intro on how to submit prompt to OpenAI model for one simple task wey be \"summarization\".\n",
    "\n",
    "**Steps**:  \n",
    "1. Install OpenAI library for your python environment  \n",
    "2. Load standard helper libraries and set your OpenAI security credentials for the OpenAI Service wey you don create  \n",
    "3. Choose model for your task  \n",
    "4. Create one simple prompt for the model  \n",
    "5. Submit your request to the model API!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Install OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 2. Import helper libraries and set up credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\")\n",
    "assert API_KEY, \"ERROR: OpenAI Key is missing\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. How to find di correct model  \n",
    "Di GPT-3.5-turbo or GPT-4 models sabi understand and fit generate natural language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Prompt Design  \n",
    "\n",
    "\"Di magic wey dey large language models be say as dem dey train am to reduce di prediction error for plenty text, di models go learn concepts wey go help dem make di predictions. For example, dem go learn concepts like\"(1):\n",
    "\n",
    "* how to spell\n",
    "* how grammar dey work\n",
    "* how to paraphrase\n",
    "* how to answer questions\n",
    "* how to hold conversation\n",
    "* how to write for plenty languages\n",
    "* how to code\n",
    "* etc.\n",
    "\n",
    "#### How to control large language model  \n",
    "\"Among all di inputs wey dey enter large language model, na di text prompt dey influence am pass(1).\n",
    "\n",
    "Large language models fit dey prompted to produce output in different ways:\n",
    "\n",
    "Instruction: Tell di model wetin you want\n",
    "Completion: Make di model complete di beginning of wetin you want\n",
    "Demonstration: Show di model wetin you want, with either:\n",
    "Few examples for di prompt\n",
    "Plenty hundreds or thousands of examples for fine-tuning training dataset\"\n",
    "\n",
    "\n",
    "\n",
    "#### Three basic guidelines dey to create prompts:\n",
    "\n",
    "**Show and tell**. Make am clear wetin you want either through instructions, examples, or mix di two. If you want di model to arrange list of items for alphabetical order or to classify paragraph by sentiment, show am say na wetin you want.\n",
    "\n",
    "**Provide quality data**. If you dey try build classifier or make di model follow pattern, make sure say you get enough examples. Make sure say you proofread your examples â€” di model dey usually smart enough to see through basic spelling mistakes and give you response, but e fit also assume say na intentional and e fit affect di response.\n",
    "\n",
    "**Check your settings.** Di temperature and top_p settings dey control how deterministic di model go dey to generate response. If you dey ask am for response wey get only one correct answer, you go want set dem low. If you dey look for more diverse responses, you fit set dem high. Di number one mistake wey people dey make with these settings na to assume say dem be \"cleverness\" or \"creativity\" controls.\n",
    "\n",
    "\n",
    "Source: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 5. Submit!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Repeat di same call, how di results take compare?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Summarize Text  \n",
    "#### Challenge  \n",
    "Make text short by adding 'tl;dr:' for di end of di text. See as di model sabi do plenty tasks witout extra instructions. You fit try use more descriptive prompts pass tl;dr to change how di model go behave and make di summarization fit wetin you want(3).  \n",
    "\n",
    "Recent work don show big improvement for plenty NLP tasks and benchmarks by pre-training wit big text corpus then fine-tune am for specific task. Even though di architecture no dey tied to any task, dis method still need task-specific fine-tuning datasets wey get thousands or tens of thousands of examples. But humans fit do new language task wit just few examples or simple instructions - somtin wey current NLP systems still dey struggle to do. For here, we show say if we make language models bigger, e go improve task-agnostic, few-shot performance well well, sometimes e fit even match di state-of-the-art fine-tuning methods wey dey before.  \n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Exercises for different use cases  \n",
    "1. Summarize Text  \n",
    "2. Classify Text  \n",
    "3. Generate New Product Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Classify Text  \n",
    "#### Challenge  \n",
    "Put items for inside categories wey dem give during inference time. For dis example wey dey below, we go provide both the categories and the text wey we wan classify for the prompt (*playground_reference). \n",
    "\n",
    "Customer Inquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\n",
    "\n",
    "Classified category:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Generate New Product Names\n",
    "#### Challenge\n",
    "Make product names from example words. For here, we dey put info about di product wey we wan generate name for inside di prompt. We go also show one similar example to show di pattern we dey expect. We don set di temperature value high so dat randomness go dey and we go get more creative answers.\n",
    "\n",
    "Product description: Machine wey dey make milkshake for house  \n",
    "Seed words: fast, healthy, compact.  \n",
    "Product names: HomeShaker, Fit Shaker, QuickShake, Shake Maker  \n",
    "\n",
    "Product description: Pair of shoes wey fit any foot size.  \n",
    "Seed words: adaptable, fit, omni-fit.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Referens  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [OpenAI Studio Examples](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Best practices for fine-tuning GPT-3 to classify text](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# For More Help  \n",
    "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Contributors\n",
    "* Louis Li  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:  \nDis dokyument don use AI transleto service [Co-op Translator](https://github.com/Azure/co-op-translator) do di translation. Even as we dey try make am accurate, abeg sabi say machine translation fit get mistake or no dey correct well. Di original dokyument wey dey for im native language na di main source wey you go trust. For important informate, e good make professional human transleto check am. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translation.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "1854bdde1dd56b366f1f6c9122f7d304",
   "translation_date": "2025-11-12T09:18:08+00:00",
   "source_file": "07-building-chat-applications/python/oai-assignment.ipynb",
   "language_code": "pcm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}