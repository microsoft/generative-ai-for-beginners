# Resources For Self-Guided Learning

Dis lesson na di one wey dem build wit plenty core resources from OpenAI and Azure OpenAI as reference for di terms and tutorials. Here na list wey no complete, wey fit help you for your own self-guided learning waka.

## 1. Primary Resources

| Title/Link                                                                                                                                                                                                                   | Description                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning with OpenAI Models](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Fine-tuning dey improve few-shot learning by training wit plenty examples wey no fit enter di prompt, e go save cost, make response better, and make requests fast. **Get overview of fine-tuning from OpenAI.**                                                                                    |
| [What is Fine-Tuning with Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Understand **wetin fine-tuning be (concept)**, why e dey important (di problem wey e dey solve), wetin data you go use (training) and how to measure di quality.                                                                                                                                                                           |
| [Customize a model with fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Azure OpenAI Service go help you adjust di models to fit your own datasets using fine-tuning. Learn **how to fine-tune (process)** di models using Azure AI Studio, Python SDK or REST API.                                                                                                                                |
| [Recommendations for LLM fine-tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLMs fit no perform well for some domains, tasks, or datasets, or fit give wrong or misleading outputs. **When you go consider fine-tuning** as di solution to dis kind problem?                                                                                                                                  |
| [Continuous Fine Tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Continuous fine-tuning na di process wey you go dey use already fine-tuned model as base model and **fine-tune am more** wit new training examples.                                                                                                                                                     |
| [Fine-tuning and function calling](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Fine-tuning your model **wit function calling examples** fit make di model output better by giving more accurate and consistent responses - wit similar format and cost-savings.                                                                                                                                        |
| [Fine-tuning Models: Azure OpenAI Guidance](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Check dis table to understand **which models fit fine-tune** for Azure OpenAI, and di regions wey dem dey available. Check di token limits and training data expiry dates if you need am.                                                                                                                            |
| [To Fine Tune or Not To Fine Tune? That is the Question](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Dis 30-min **Oct 2023** episode of di AI Show dey talk about di benefits, drawbacks and practical insights wey go help you decide.                                                                                                                                                                                        |
| [Getting Started With LLM Fine-Tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Dis **AI Playbook** resource go show you how to prepare data, format am, fine-tune hyperparameters and di challenges/limitations wey you need to sabi.                                                                                                                                                                         |
| **Tutorial**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Learn how to create sample fine-tuning dataset, prepare for fine-tuning, create fine-tuning job, and deploy di fine-tuned model for Azure.                                                                                                                                                                                    |
| **Tutorial**: [Fine-tune a Llama 2 model in Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio go help you adjust large language models to fit your own datasets _using UI-based workflow wey dey good for low-code developers_. See dis example.                                                                                                                                                               |
| **Tutorial**:[Fine-tune Hugging Face models for a single GPU on Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Dis article dey explain how to fine-tune Hugging Face model wit Hugging Face transformers library on single GPU wit Azure DataBricks + Hugging Face Trainer libraries.                                                                                                                                                |
| **Training:** [Fine-tune a foundation model with Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Di model catalog for Azure Machine Learning get plenty open source models wey you fit fine-tune for your specific task. Try dis module wey dey [from di AzureML Generative AI Learning Path](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Tutorial:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | Fine-tuning GPT-3.5 or GPT-4 models for Microsoft Azure using W&B go help you track and analyze model performance well. Dis guide dey expand di concepts from di OpenAI Fine-Tuning guide wit specific steps and features for Azure OpenAI.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Secondary Resources

Dis section get extra resources wey dey important to check, but we no fit cover am for dis lesson. We fit talk about dem for future lesson, or as secondary assignment option later. For now, use dem to build your own knowledge and sabi for dis topic.

| Title/Link                                                                                                                                                                                                            | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Data preparation and analysis for chat model fine-tuning](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Dis notebook na tool to preprocess and analyze di chat dataset wey dem dey use for fine-tuning chat model. E dey check format errors, give basic statistics, and estimate token counts for fine-tuning costs. See: [Fine-tuning method for gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning for Retrieval Augmented Generation (RAG) with Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Di aim of dis notebook na to show example of how to fine-tune OpenAI models for Retrieval Augmented Generation (RAG). Dem go also use Qdrant and Few-Shot Learning to make model performance better and reduce wrong answers.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT with Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) na AI developer platform, wey get tools for training models, fine-tuning models, and using foundation models. Read their [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst) guide first, then try di Cookbook exercise.                                                                                                                                                                                                                  |
| **Community Tutorial** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - fine-tuning for Small Language Models                                                   | Meet [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), Microsoftâ€™s new small model, wey dey powerful but compact. Dis tutorial go show you how to fine-tune Phi-2, how to build unique dataset and fine-tune model using QLoRA.                                                                                                                                                                       |
| **Hugging Face Tutorial** [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Dis blog post dey show how to fine-tune open LLMs using Hugging Face TRL, Transformers & datasets for 2024. You go define use case, setup dev environment, prepare dataset, fine-tune di model, test-evaluate am, then deploy am to production.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | E dey make training and deployments of [state-of-the-art machine learning models](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst) faster and easier. Di repo get Colab-friendly tutorials wit YouTube video guidance, for fine-tuning. **E reflect di recent [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst) update**. Read di [AutoTrain documentation](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Disclaimer**:  
Dis dokyument don use AI transleto service [Co-op Translator](https://github.com/Azure/co-op-translator) do di translation. Even as we dey try make am correct, abeg sabi say machine translation fit get mistake or no dey accurate well. Di original dokyument wey dey for im native language na di main source wey you go fit trust. For important mata, e better make professional human transleto check am. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->