{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Build Wit Meta Family Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dis lesson go cover:\n",
    "\n",
    "- How to check di two main Meta family models - Llama 3.1 and Llama 3.2\n",
    "- How to sabi di use-cases and di kind situations wey fit each model\n",
    "- Code example wey go show di special features wey each model get\n",
    "\n",
    "## Di Meta Family of Models\n",
    "\n",
    "For dis lesson, we go check 2 models wey dey di Meta family or \"Llama Herd\" - Llama 3.1 and Llama 3.2.\n",
    "\n",
    "Dis models get different types and dem dey for di Github Model marketplace. You fit find more info about how to use Github Models to [prototype wit AI models](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Model Types:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Note: Llama 3 dey available for Github Models too but we no go talk about am for dis lesson.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "E get 405 Billion Parameters, Llama 3.1 dey fall inside di open source LLM category.\n",
    "\n",
    "Dis mode na upgrade to di earlier release wey dem call Llama 3, e dey offer:\n",
    "\n",
    "- Bigger context window - 128k tokens instead of 8k tokens\n",
    "- Bigger Max Output Tokens - 4096 instead of 2048\n",
    "- Better Multilingual Support - because dem increase di training tokens\n",
    "\n",
    "Dis one make Llama 3.1 fit handle more complex use cases wen people dey build GenAI applications like:\n",
    "\n",
    "- Native Function Calling - e fit call external tools and functions outside di LLM workflow\n",
    "- Better RAG Performance - because di context window don big pass before\n",
    "- Synthetic Data Generation - e fit create better data for tasks like fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Function Calling \n",
    "\n",
    "Llama 3.1 don beta well well to sabi how to use function or tool call. E still get two tools wey dey inside am wey the model fit sabi say e need use based on wetin the user talk. Dis tools na: \n",
    "\n",
    "- **Brave Search** - Fit use am to find correct information like weather by doing web search \n",
    "- **Wolfram Alpha** - Fit use am for strong mathematical calculation so you no go need write your own functions. \n",
    "\n",
    "You fit still create your own tools wey LLM go fit call. \n",
    "\n",
    "For the code example wey dey below: \n",
    "\n",
    "- We go define the tools wey dey available (brave_search, wolfram_alpha) for the system prompt. \n",
    "- Send user prompt wey dey ask about weather for one city. \n",
    "- The LLM go reply with tool call to Brave Search tool wey go look like dis `<|python_tag|>brave_search.call(query=\"Stockholm weather\")` \n",
    "\n",
    "*Note: Dis example na only the tool call e dey do, if you wan get the result, you go need create free account for Brave API page and define the function itself`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Even though Llama 3.1 na LLM, e get one wahala wey e get, wey be say e no sabi do multimodality. Dat one mean say e no fit use different kain input like image as prompt take give answer. Dis ability na one of di main features wey Llama 3.2 get. Di features wey e get na:\n",
    "\n",
    "- Multimodality - e fit check both text and image prompts\n",
    "- Small to Medium size variations (11B and 90B) - dis one dey make am easy to deploy anyhow\n",
    "- Text-only variations (1B and 3B) - dis one dey make di model fit work for edge / mobile devices and e dey fast\n",
    "\n",
    "Di multimodal support na big step for di world of open source models. Di code example wey dey below go use both image and text prompt take get analysis of di image from Llama 3.2 90B.\n",
    "\n",
    "### Multimodal Support with Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning no dey stop for here, continue di Journey\n",
    "\n",
    "Afta you don finish dis lesson, go check our [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) to continue to sabi more about Generative AI!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:  \nDis dokyument don use AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator) do di translation. Even though we dey try make am accurate, abeg make you sabi say machine translation fit get mistake or no dey correct well. Di original dokyument wey dey for im native language na di main source wey you go trust. For important information, e better make professional human translator check am. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translation.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-11-12T09:16:29+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "pcm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}