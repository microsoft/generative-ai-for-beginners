<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3772dcd23a98e2010f53ce8b9c583631",
  "translation_date": "2026-01-18T17:13:34+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "tw"
}
-->
[![Open Source Models](../../../../../translated_images/zh-TW/18-lesson-banner.f30176815b1a5074.webp)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# 微調您的大型語言模型（LLM）

使用大型語言模型來構建生成式 AI 應用會帶來新的挑戰。一個關鍵問題是確保模型針對特定用戶請求生成的內容的回應質量（準確性和相關性）。在之前的課程中，我們討論了像提示工程和檢索增強生成這樣的技術，這些技術嘗試通過_修改現有模型的提示輸入_來解決該問題。

在今天的課程中，我們將討論第三種技術，**微調（Fine-Tuning）**，它試圖通過_使用額外數據重新訓練模型本身_來解決挑戰。讓我們深入了解細節。

## 學習目標

本課程介紹了預訓練語言模型微調的概念，探討了這種方法的優點和挑戰，並提供了何時及如何使用微調來提升生成式 AI 模型性能的指導。

完成本課程後，您應該能夠回答以下問題：

- 語言模型的微調是什麼？
- 何時以及為何微調是有用的？
- 如何微調預訓練模型？
- 微調有哪些限制？

準備好了嗎？讓我們開始吧。

## 圖解指南

想先掌握我們將涵蓋的大致內容嗎？請參閱這份圖解指南，說明了本課程的學習路徑——從了解微調的核心概念與動機，到理解微調任務的流程與最佳實踐。這是一個令人著迷的探索話題，別忘了查看[資源](./RESOURCES.md?WT.mc_id=academic-105485-koreyst)頁面，獲取更多支持您自主學習旅程的連結！

![Illustrated Guide to Fine Tuning Language Models](../../../../../translated_images/zh-TW/18-fine-tuning-sketchnote.11b21f9ec8a70346.webp)

## 什麼是語言模型的微調？

從定義來看，大型語言模型是基於來自多元來源（包括網際網路）的大量文本進行_預訓練_的。正如我們在前面的課程中所學，我們需要像_提示工程_和_檢索增強生成_這樣的技術來提升模型對用戶問題（「提示」）的回應質量。

一種流行的提示工程技術是通過提供_指示_（明確指引）或_給出幾個範例_（隱性指引）來給模型更多關於期望回應內容的指導。這被稱為_少量學習（few-shot learning）_，但有兩個限制：

- 模型的 token 限制會限制您能提供的範例數量，從而限制效果。
- 模型的 token 成本會使每個提示添加範例變得昂貴，並限制靈活性。

微調是在機器學習系統中常見的做法，我們會使用新的數據重新訓練一個預訓練模型，以改進其在特定任務上的表現。在語言模型的範疇中，我們可以用_針對特定任務或應用領域策劃的一組範例_來微調預訓練模型，從而創建一個**定制模型**，該模型可能在特定任務或領域中更加準確和相關。微調的額外好處是它也可以減少少量學習所需的範例數量——從而降低 token 使用和相關成本。

## 何時以及為何應該微調模型？

在_本_文脈中，我們所說的微調指的是**監督式**微調，即通過**加入原始訓練數據集之外的新數據**進行重新訓練。這有別於非監督式微調，後者是用不同的超參數在原始數據上重新訓練模型。

關鍵要記得的是，微調是一種高階技術，需要一定程度的專業知識才能達到預期結果。若操作不當，可能無法帶來預期提升，甚至會降低模型在目標領域的表現。

因此，在您學習「如何」微調語言模型之前，您需要先了解「為何」應該採取這條路，以及「何時」啟動微調流程。先問自己這些問題：

- **用例**：您的微調_用例_是什麼？您想改進當前預訓練模型的哪些方面？
- **替代方案**：您是否嘗試過_其他技術_達成目標？利用這些技術建立基線便於比較。
  - 提示工程：嘗試像使用範例的少量提示等技術。評估回應品質。
  - 檢索增強生成（RAG）：嘗試用檢索到的數據結果增強提示。評估回應品質。
- **成本**：您是否已識別微調的成本？
  - 可調性——該預訓練模型是否支持微調？
  - 工作量——準備訓練數據、評估和細化模型所需的工作量。
  - 計算資源——運行微調作業及部署微調後模型所需的算力。
  - 數據——是否擁有足夠且質量合適的範例進行微調。
- **效益**：您是否已確認微調的效益？
  - 品質——微調模型是否超越了基線？
  - 成本——是否透過簡化提示降低 token 使用？
  - 擴展性——是否可將基礎模型重新運用於新領域？

透過回答這些問題，您應該能決定微調是否適合您的用例。理想情況下，只有當效益超過成本時，該方法才是合理的。一旦決定繼續，就該思考_如何_微調預訓練模型。

想獲得決策流程的更多洞見？觀看 [要微調還是不微調](https://www.youtube.com/watch?v=0Jo-z-MFxJs)。

## 如何微調預訓練模型？

要微調預訓練模型，您需要：

- 一個可供微調的預訓練模型
- 一個用於微調的數據集
- 一個運行微調任務的訓練環境
- 一個部署微調後模型的主機環境

## 微調實作

以下資源提供循序漸進的教學，引導您使用特定模型和策劃數據集的實際示例。要完成這些教學，您需要在相應提供者註冊帳號，並擁有相關模型與數據集的存取權。

| 供應商       | 教學                                                                                                                                                                          | 說明                                                                                                                                                                                                                                                                                                                                                                                                                                |
| ------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [如何微調聊天模型](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                           | 學習如何針對特定領域（「食譜助理」）微調 `gpt-35-turbo` ，包含準備訓練數據、執行微調作業，並使用微調後模型進行推斷。                                                                                                                                                                                                                                                                                                                     |
| Azure OpenAI | [GPT 3.5 Turbo 微調教學](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst)           | 學習如何在 **Azure** 上微調 `gpt-35-turbo-0613` 模型，包含創建和上傳訓練數據、執行微調作業，部署並使用新模型。                                                                                                                                                                                                                                                                                                                          |
| Hugging Face | [使用 Hugging Face 微調大型語言模型](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                            | 本文介紹如何使用 [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) 庫和 [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) 在 Hugging Face 上使用開放數據集對 _開放大型語言模型_（例：`CodeLlama 7B`）進行微調。                                                                                                                                    |
|              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| 🤗 AutoTrain | [使用 AutoTrain 微調大型語言模型](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                        | AutoTrain（或 AutoTrain Advanced）是 Hugging Face 開發的 Python 庫，支持多種任務的微調，包括大型語言模型微調。AutoTrain 是無需程式碼解決方案，可在您自己的雲端、Hugging Face Spaces 或本地執行微調。它同時支持基於 Web 的 GUI、命令行介面（CLI）和 YAML 配置文件的訓練。                                                                                                                                                  |
|              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| 🦥 Unsloth   | [使用 Unsloth 微調大型語言模型](https://github.com/unslothai/unsloth)                                                                                                        | Unsloth 是一個開源框架，支持大型語言模型的微調與強化學習（RL）。Unsloth 簡化了本地訓練、評估和部署過程，並附帶即用的[筆記本](https://github.com/unslothai/notebooks)。它同時支持文字轉語音（TTS）、BERT 及多模態模型。要開始使用，請詳讀其逐步[大型語言模型微調指南](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide)。                                                                                                                                               |
|              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                     |
## 作業

選擇上述一個教學並自行操作實作。_我們可能會在本倉庫的 Jupyter 筆記本中複製這些教學的版本供參考，請直接使用原始資源以獲取最新版本_。

## 做得好！繼續學習吧。

完成本課程後，請造訪我們的[生成式 AI 學習合集](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，繼續提升您的生成式 AI 能力！

恭喜您！您已完成本課程 v2 系列的最後一課！別停止學習和構建。**請查看[資源](RESOURCES.md?WT.mc_id=academic-105485-koreyst)頁面，獲取本主題的更多推薦資料。

我們的 v1 系列課程也已更新，增加了更多作業和概念。花點時間重新溫習知識，並請[分享您的問題與反饋](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst)，幫助我們為社區改進這些課程。

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**免責聲明**：  
本文件係使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於翻譯的準確性，但請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應視為權威來源。對於關鍵資訊，建議採用專業人工翻譯。我們不對因使用本翻譯而產生的任何誤解或誤譯承擔責任。
<!-- CO-OP TRANSLATOR DISCLAIMER END -->