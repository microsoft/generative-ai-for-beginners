{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Generation Application \n",
    "\n",
    "LLMs aren't just for generating text. You can also create images from text descriptions. Having images as another way to interact can be extremely helpful in fields like MedTech, architecture, tourism, game development, and more. In this chapter, we'll explore the two most popular image generation models: DALL-E and Midjourney.\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In this lesson, we'll cover:\n",
    "\n",
    "- What image generation is and why it's useful.\n",
    "- What DALL-E and Midjourney are, and how they work.\n",
    "- How to build an image generation app.\n",
    "\n",
    "## Learning Goals \n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Build an image generation application.\n",
    "- Set boundaries for your app using meta prompts.\n",
    "- Work with DALL-E and Midjourney.\n",
    "\n",
    "## Why build an image generation application?\n",
    "\n",
    "Image generation apps are a great way to see what Generative AI can do. They can be used for things like:  \n",
    "\n",
    "- **Image editing and synthesis**. You can create images for many different purposes, such as editing or combining images.  \n",
    "\n",
    "- **Useful across many industries**. They can also be used to generate images for industries like MedTech, tourism, game development, and more. \n",
    "\n",
    "## Scenario: Edu4All \n",
    "\n",
    "In this lesson, we'll keep working with our startup, Edu4All. The students will create images for their assignments. What kind of images is up to them—they might make illustrations for their own fairy tales, design a new character for their story, or create visuals to help explain their ideas and concepts. \n",
    "\n",
    "For example, if Edu4All's students are working on a lesson about monuments, they could generate something like this:\n",
    "\n",
    "![Edu4All startup, class on monuments, Eifel Tower](../../../../translated_images/startup.94d6b79cc4bb3f5afbf6e2ddfcf309aa5d1e256b5f30cc41d252024eaa9cc5dc.en.png)\n",
    "\n",
    "using a prompt like \n",
    "\n",
    "> \"Dog next to Eiffel Tower in early morning sunlight\"\n",
    "\n",
    "## What is DALL-E and Midjourney? \n",
    "\n",
    "[DALL-E](https://openai.com/dall-e-2?WT.mc_id=academic-105485-koreyst) and [Midjourney](https://www.midjourney.com/?WT.mc_id=academic-105485-koreyst) are two of the most popular image generation models. They let you use prompts to create images.\n",
    "\n",
    "### DALL-E\n",
    "\n",
    "Let's start with DALL-E, which is a Generative AI model that creates images from text descriptions. \n",
    "\n",
    "> [DALL-E is a combination of two models, CLIP and diffused attention](https://towardsdatascience.com/openais-dall-e-and-clip-101-a-brief-introduction-3a4367280d4e?WT.mc_id=academic-105485-koreyst).  \n",
    "\n",
    "- **CLIP** is a model that creates embeddings, which are numerical representations of data, from both images and text.  \n",
    "\n",
    "- **Diffused attention** is a model that generates images from those embeddings. DALL-E is trained on a dataset of images and text, and can generate images from text descriptions. For example, DALL-E can create an image of a cat in a hat, or a dog with a mohawk. \n",
    "\n",
    "### Midjourney\n",
    " \n",
    "Midjourney works similarly to DALL-E—it generates images from text prompts. You can use Midjourney to create images with prompts like “a cat in a hat” or “a dog with a mohawk”. \n",
    "\n",
    " \n",
    "\n",
    "![Image generated by Midjourney, mechanical pigeon](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Rupert_Breheny_mechanical_dove_eca144e7-476d-4976-821d-a49c408e4f36.png/440px-Rupert_Breheny_mechanical_dove_eca144e7-476d-4976-821d-a49c408e4f36.png?WT.mc_id=academic-105485-koreyst)\n",
    "\n",
    "*Image credit: Wikipedia, image generated by Midjourney*\n",
    "\n",
    "## How does DALL-E and Midjourney Work \n",
    "\n",
    "First, [DALL-E](https://arxiv.org/pdf/2102.12092.pdf?WT.mc_id=academic-105485-koreyst). DALL-E is a Generative AI model based on the transformer architecture with an *autoregressive transformer*.  \n",
    "\n",
    "An *autoregressive transformer* is a way for a model to generate images from text descriptions. It creates one pixel at a time, then uses the pixels it has already made to generate the next one. This process goes through multiple layers in a neural network until the image is finished.  \n",
    "\n",
    "With this approach, DALL-E can control the attributes, objects, characteristics, and more in the images it creates. However, DALL-E 2 and 3 offer even more control over the generated images,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your first image generation application\n",
    "\n",
    "So what do you need to build an image generation application? You’ll need the following libraries:\n",
    "\n",
    "- **python-dotenv**: It’s highly recommended to use this library to keep your secrets in a *.env* file, separate from your code.\n",
    "- **openai**: This is the library you’ll use to interact with the OpenAI API.\n",
    "- **pillow**: For working with images in Python.\n",
    "- **requests**: To help you make HTTP requests.\n",
    "\n",
    "1. Create a *.env* file with the following content:\n",
    "\n",
    "    ```text\n",
    "    AZURE_OPENAI_ENDPOINT=<your endpoint>\n",
    "    AZURE_OPENAI_API_KEY=<your key>\n",
    "    ```\n",
    "\n",
    "    You can find this information in the Azure Portal for your resource under the \"Keys and Endpoint\" section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gather the above libraries in a file named *requirements.txt* as follows:\n",
    "\n",
    "    ```text\n",
    "    python-dotenv\n",
    "    openai\n",
    "    pillow\n",
    "    requests\n",
    "    ```\n",
    "\n",
    "1. Next, create a virtual environment and install the libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# create virtual env\n",
    "! python3 -m venv venv\n",
    "# activate environment\n",
    "! source venv/bin/activate\n",
    "# install libraries\n",
    "# pip install -r requirements.txt, if using a requirements.txt file \n",
    "! pip install python-dotenv openai pillow requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> For Windows, use the following commands to create and activate your virtual environment:\n",
    "\n",
    "    ```bash\n",
    "    python3 -m venv venv\n",
    "    venv\\Scripts\\activate.bat\n",
    "    ```\n",
    "\n",
    "1. Add the following code in a file called *app.py*:\n",
    "\n",
    "    ```python\n",
    "    import openai\n",
    "    import os\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    import dotenv\n",
    "    \n",
    "    # import dotenv\n",
    "    dotenv.load_dotenv()\n",
    "    \n",
    "    # Get endpoint and key from environment variables\n",
    "    openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "    openai.api_key = os.environ['AZURE_OPENAI_API_KEY']     \n",
    "    \n",
    "    # Assign the API version (DALL-E is currently supported for the 2023-06-01-preview API version only)\n",
    "    openai.api_version = '2023-06-01-preview'\n",
    "    openai.api_type = 'azure'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Create an image by using the image generation API\n",
    "        generation_response = openai.Image.create(\n",
    "            prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n",
    "            size='1024x1024',\n",
    "            n=2,\n",
    "            temperature=0,\n",
    "        )\n",
    "        # Set the directory for the stored image\n",
    "        image_dir = os.path.join(os.curdir, 'images')\n",
    "    \n",
    "        # If the directory doesn't exist, create it\n",
    "        if not os.path.isdir(image_dir):\n",
    "            os.mkdir(image_dir)\n",
    "    \n",
    "        # Initialize the image path (note the filetype should be png)\n",
    "        image_path = os.path.join(image_dir, 'generated-image.png')\n",
    "    \n",
    "        # Retrieve the generated image\n",
    "        image_url = generation_response[\"data\"][0][\"url\"]  # extract image URL from response\n",
    "        generated_image = requests.get(image_url).content  # download the image\n",
    "        with open(image_path, \"wb\") as image_file:\n",
    "            image_file.write(generated_image)\n",
    "    \n",
    "        # Display the image in the default image viewer\n",
    "        image = Image.open(image_path)\n",
    "        image.show()\n",
    "    \n",
    "    # catch exceptions\n",
    "    except openai.InvalidRequestError as err:\n",
    "        print(err)\n",
    "\n",
    "    ```\n",
    "\n",
    "Let's break down this code:\n",
    "\n",
    "- First, we import the necessary libraries, including the OpenAI library, the dotenv library, the requests library, and the Pillow library.\n",
    "\n",
    "    ```python\n",
    "    import openai\n",
    "    import os\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    import dotenv\n",
    "    ```\n",
    "\n",
    "- Next, we load the environment variables from the *.env* file.\n",
    "\n",
    "    ```python\n",
    "    # import dotenv\n",
    "    dotenv.load_dotenv()\n",
    "    ```\n",
    "\n",
    "- After that, we set the endpoint, key for the OpenAI API, version, and type.\n",
    "\n",
    "    ```python\n",
    "    # Get endpoint and key from environment variables\n",
    "    openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "    openai.api_key = os.environ['AZURE_OPENAI_API_KEY'] \n",
    "\n",
    "    # add version and type, Azure specific\n",
    "    openai.api_version = '2023-06-01-preview'\n",
    "    openai.api_type = 'azure'\n",
    "    ```\n",
    "\n",
    "- Next, we generate the image:\n",
    "\n",
    "    ```python\n",
    "    # Create an image by using the image generation API\n",
    "    generation_response = openai.Image.create(\n",
    "        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n",
    "        size='1024x1024',\n",
    "        n=2,\n",
    "        temperature=0,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    The code above returns a JSON object that contains the URL of the generated image. We can use this URL to download the image and save it to a file.\n",
    "\n",
    "- Finally, we open the image and use the default image viewer to display it:\n",
    "\n",
    "    ```python\n",
    "    image = Image.open(image_path)\n",
    "    image.show()\n",
    "    ```\n",
    "\n",
    "### More details on generating the image\n",
    "\n",
    "Let's take a closer look at the code that generates the image:\n",
    "\n",
    "```python\n",
    "generation_response = openai.Image.create(\n",
    "        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n",
    "        size='1024x1024',\n",
    "        n=2,\n",
    "        temperature=0,\n",
    "    )\n",
    "```\n",
    "\n",
    "- **prompt** is the text prompt used to generate the image. In this example, we're using the prompt \"Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils\".\n",
    "- **size** is the size of the generated image. Here, we're generating an image that is 1024x1024 pixels.\n",
    "- **n** is the number of images to generate. In this case, we're generating two images.\n",
    "- **temperature** is a parameter that controls the randomness of the output from a Generative AI model. The temperature is a value between 0 and 1, where 0 means the output is deterministic and 1 means the output is random. The default value is 0.7.\n",
    "\n",
    "There are more things you can do with images, which we'll cover in the next section.\n",
    "\n",
    "## Additional capabilities of image generation\n",
    "\n",
    "So far, you've seen how we can generate an image with just a few lines of Python. But there are more things you can do with images.\n",
    "\n",
    "You can also:\n",
    "\n",
    "- **Edit images**. By providing an existing image, a mask, and a prompt, you can modify an image. For example, you can add something to a specific part of an image. Imagine our bunny image—you could add a hat to the bunny. To do this, you provide the image, a mask (to identify the area to change), and a text prompt describing what should be done.\n",
    "\n",
    "    ```python\n",
    "    response = openai.Image.create_edit(\n",
    "      image=open(\"base_image.png\", \"rb\"),\n",
    "      mask=open(\"mask.png\", \"rb\"),\n",
    "      prompt=\"An image of a rabbit with a hat on its head.\",\n",
    "      n=1,\n",
    "      size=\"1024x1024\"\n",
    "    )\n",
    "    image_url = response['data'][0]['url']\n",
    "    ```\n",
    "\n",
    "    The base image would only have the rabbit, but the final image would show the rabbit with a hat.\n",
    "\n",
    "- **Create variations**.\n",
    "    Check out our [OpenAI notebook for more information](./oai-assignment.ipynb?WT.mc_id=academic-105485-koreyst).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:\nThis document has been translated using the AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "d5c5bc8d857e5cb63e313beac9fa6402",
   "translation_date": "2025-08-25T19:05:27+00:00",
   "source_file": "09-building-image-applications/python/aoai-assignment.ipynb",
   "language_code": "en"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}