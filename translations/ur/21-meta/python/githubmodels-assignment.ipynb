{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# میٹا فیملی ماڈلز کے ساتھ تعمیر کرنا\n",
    "\n",
    "## تعارف\n",
    "\n",
    "اس سبق میں آپ سیکھیں گے:\n",
    "\n",
    "- میٹا فیملی کے دو اہم ماڈلز - لاما 3.1 اور لاما 3.2 کا جائزہ\n",
    "- ہر ماڈل کے استعمال کے مقاصد اور حالات کو سمجھنا\n",
    "- ہر ماڈل کی منفرد خصوصیات کو ظاہر کرنے کے لیے کوڈ کی مثال\n",
    "\n",
    "## میٹا فیملی کے ماڈلز\n",
    "\n",
    "اس سبق میں، ہم میٹا فیملی یا \"لاما ہرڈ\" کے دو ماڈلز - لاما 3.1 اور لاما 3.2 کو دریافت کریں گے\n",
    "\n",
    "یہ ماڈلز مختلف ورژنز میں دستیاب ہیں اور گٹ ہب ماڈل مارکیٹ پلیس پر موجود ہیں۔ یہاں گٹ ہب ماڈلز کو [AI ماڈلز کے ساتھ پروٹوٹائپ بنانے](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) کے بارے میں مزید تفصیلات ہیں۔\n",
    "\n",
    "ماڈل ورژنز:\n",
    "- لاما 3.1 - 70B انسٹرکٹ\n",
    "- لاما 3.1 - 405B انسٹرکٹ\n",
    "- لاما 3.2 - 11B وژن انسٹرکٹ\n",
    "- لاما 3.2 - 90B وژن انسٹرکٹ\n",
    "\n",
    "*نوٹ: لاما 3 بھی گٹ ہب ماڈلز پر دستیاب ہے لیکن اس سبق میں شامل نہیں ہے*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## لاما 3.1\n",
    "\n",
    "405 ارب پیرا میٹرز کے ساتھ، لاما 3.1 اوپن سورس ایل ایل ایم کیٹیگری میں آتا ہے۔\n",
    "\n",
    "یہ ماڈل پہلے جاری ہونے والے لاما 3 کا اپ گریڈ ہے اور اس میں یہ خصوصیات شامل ہیں:\n",
    "\n",
    "- بڑا کانٹیکسٹ ونڈو - 128k ٹوکنز بمقابلہ 8k ٹوکنز\n",
    "- زیادہ سے زیادہ آؤٹ پٹ ٹوکنز - 4096 بمقابلہ 2048\n",
    "- بہتر کثیر لسانی سپورٹ - تربیتی ٹوکنز میں اضافے کی وجہ سے\n",
    "\n",
    "ان خصوصیات کی بدولت لاما 3.1 زیادہ پیچیدہ استعمال کے کیسز کو سنبھال سکتا ہے، خاص طور پر جب آپ GenAI ایپلیکیشنز بنا رہے ہوں، جیسے کہ:\n",
    "- نیٹو فنکشن کالنگ - ایل ایل ایم ورک فلو سے باہر ایکسٹرنل ٹولز اور فنکشنز کو کال کرنے کی صلاحیت\n",
    "- بہتر RAG پرفارمنس - زیادہ بڑے کانٹیکسٹ ونڈو کی وجہ سے\n",
    "- مصنوعی ڈیٹا جنریشن - ایسے ٹاسکس کے لیے مؤثر ڈیٹا بنانے کی صلاحیت، جیسے فائن ٹیوننگ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### نیٹو فنکشن کالنگ\n",
    "\n",
    "Llama 3.1 کو اس طرح بہتر بنایا گیا ہے کہ یہ فنکشن یا ٹول کالز زیادہ مؤثر طریقے سے کر سکے۔ اس میں دو بلٹ اِن ٹولز بھی شامل ہیں جنہیں ماڈل صارف کی پرامپٹ کے مطابق استعمال کرنے کی ضرورت کو پہچان سکتا ہے۔ یہ ٹولز یہ ہیں:\n",
    "\n",
    "- **Brave Search** - ویب سرچ کے ذریعے تازہ ترین معلومات جیسے موسم معلوم کرنے کے لیے استعمال کیا جا سکتا ہے\n",
    "- **Wolfram Alpha** - زیادہ پیچیدہ ریاضیاتی حسابات کے لیے استعمال کیا جا سکتا ہے، اس طرح آپ کو اپنے فنکشنز لکھنے کی ضرورت نہیں رہتی۔\n",
    "\n",
    "آپ اپنی مرضی کے مطابق ٹولز بھی بنا سکتے ہیں جنہیں LLM کال کر سکے۔\n",
    "\n",
    "نیچے دیے گئے کوڈ کی مثال میں:\n",
    "\n",
    "- ہم دستیاب ٹولز (brave_search, wolfram_alpha) کو سسٹم پرامپٹ میں بیان کرتے ہیں۔\n",
    "- صارف کی طرف سے ایک پرامپٹ بھیجتے ہیں جس میں کسی مخصوص شہر کے موسم کے بارے میں پوچھا جاتا ہے۔\n",
    "- LLM Brave Search ٹول کو کال کرنے کے ساتھ جواب دے گا، جو اس طرح نظر آئے گا `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*نوٹ: یہ مثال صرف ٹول کال کرتی ہے، اگر آپ نتائج حاصل کرنا چاہتے ہیں تو آپ کو Brave API پیج پر مفت اکاؤنٹ بنانا ہوگا اور فنکشن کو خود ڈیفائن کرنا ہوگا*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### لاما 3.2\n",
    "\n",
    "اگرچہ لاما 3.1 ایک ایل ایل ایم ہے، اس کی ایک بڑی کمی ملٹی موڈیلٹی ہے۔ یعنی، مختلف قسم کے ان پٹ جیسے تصاویر کو پرامپٹ کے طور پر استعمال کرنا اور ان کے جوابات دینا۔ یہ صلاحیت لاما 3.2 کی اہم خصوصیات میں سے ایک ہے۔ ان خصوصیات میں شامل ہیں:\n",
    "\n",
    "- ملٹی موڈیلٹی - یہ ماڈل ٹیکسٹ اور امیج پرامپٹ دونوں کو سمجھنے کی صلاحیت رکھتا ہے\n",
    "- چھوٹے سے درمیانے سائز کی مختلف اقسام (11B اور 90B) - اس سے ماڈل کو مختلف طریقوں سے ڈپلائے کرنا آسان ہو جاتا ہے،\n",
    "- صرف ٹیکسٹ پر مبنی اقسام (1B اور 3B) - اس سے ماڈل کو ایج یا موبائل ڈیوائسز پر چلانا ممکن ہوتا ہے اور کم تاخیر کے ساتھ نتائج ملتے ہیں\n",
    "\n",
    "ملٹی موڈل سپورٹ اوپن سورس ماڈلز کی دنیا میں ایک بڑا قدم ہے۔ نیچے دی گئی کوڈ کی مثال میں ایک تصویر اور ایک ٹیکسٹ پرامپٹ دونوں کو استعمال کیا گیا ہے تاکہ لاما 3.2 90B سے تصویر کا تجزیہ حاصل کیا جا سکے۔\n",
    "\n",
    "### لاما 3.2 کے ساتھ ملٹی موڈل سپورٹ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سیکھنا یہاں ختم نہیں ہوتا، سفر جاری رکھیں\n",
    "\n",
    "اس سبق کو مکمل کرنے کے بعد، ہماری [جنریٹیو اے آئی لرننگ کلیکشن](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) دیکھیں تاکہ آپ اپنی جنریٹیو اے آئی کی معلومات کو مزید بہتر بنا سکیں!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**اعلانِ دستبرداری**:  \nیہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کی پوری کوشش کرتے ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا عدم درستگی ہو سکتی ہے۔ اصل دستاویز کو اس کی اصل زبان میں مستند ماخذ سمجھا جانا چاہیے۔ اہم معلومات کے لیے پیشہ ور انسانی ترجمہ تجویز کیا جاتا ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کی ذمہ داری ہم قبول نہیں کرتے۔\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:37:40+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "ur"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}