{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral ماڈلز کے ساتھ تعمیر کرنا\n",
    "\n",
    "## تعارف\n",
    "\n",
    "اس سبق میں شامل ہے:\n",
    "- مختلف Mistral ماڈلز کا جائزہ لینا\n",
    "- ہر ماڈل کے استعمال کے مقاصد اور مواقع کو سمجھنا\n",
    "- کوڈ کی مثالیں جو ہر ماڈل کی منفرد خصوصیات کو ظاہر کرتی ہیں\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مسٹرال ماڈلز\n",
    "\n",
    "اس سبق میں، ہم تین مختلف مسٹرال ماڈلز کا جائزہ لیں گے:\n",
    "**مسٹرال لارج**، **مسٹرال اسمال** اور **مسٹرال نیمو**۔\n",
    "\n",
    "یہ تمام ماڈلز گٹ ہب ماڈل مارکیٹ پلیس پر مفت دستیاب ہیں۔ اس نوٹ بک میں موجود کوڈ انہی ماڈلز کو استعمال کرے گا۔ مزید معلومات کے لیے یہاں دیکھیں کہ گٹ ہب ماڈلز کے ذریعے [اے آئی ماڈلز کے ساتھ پروٹوٹائپ کیسے بنائیں](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst)۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مسٹرال لارج 2 (2407)\n",
    "مسٹرال لارج 2 اس وقت مسٹرال کا سب سے جدید ماڈل ہے اور اسے کاروباری اداروں کے لیے تیار کیا گیا ہے۔\n",
    "\n",
    "یہ ماڈل اصل مسٹرال لارج کا اپ گریڈ ہے اور اس میں یہ خصوصیات شامل ہیں:\n",
    "-  زیادہ بڑا کانٹیکسٹ ونڈو - 128k بمقابلہ 32k\n",
    "-  ریاضی اور کوڈنگ کے کاموں میں بہتر کارکردگی - اوسط درستگی 76.9% بمقابلہ 60.4%\n",
    "-  مختلف زبانوں میں بہتر کارکردگی - شامل زبانیں: انگریزی، فرانسیسی، جرمن، ہسپانوی، اطالوی، پرتگالی، ڈچ، روسی، چینی، جاپانی، کوریائی، عربی اور ہندی۔\n",
    "\n",
    "ان خصوصیات کے ساتھ، مسٹرال لارج ان کاموں میں بہترین ہے:\n",
    "- *ریٹریول آگمینٹڈ جنریشن (RAG)* - بڑے کانٹیکسٹ ونڈو کی وجہ سے\n",
    "- *فنکشن کالنگ* - اس ماڈل میں فنکشن کالنگ کی صلاحیت موجود ہے جس سے بیرونی ٹولز اور APIs کے ساتھ انضمام ممکن ہے۔ یہ کالز ایک ساتھ یا ترتیب وار کی جا سکتی ہیں۔\n",
    "- *کوڈ جنریشن* - یہ ماڈل Python، Java، TypeScript اور C++ میں کوڈ بنانے میں بہترین ہے۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### مسترال لارج 2 کا استعمال کرتے ہوئے آر اے جی کی مثال\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اس مثال میں، ہم Mistral Large 2 کا استعمال کر رہے ہیں تاکہ ایک ٹیکسٹ ڈاکیومنٹ پر RAG پیٹرن چلایا جا سکے۔ سوال کوریائی زبان میں لکھا گیا ہے اور مصنف کی کالج میں داخلے سے پہلے کی سرگرمیوں کے بارے میں پوچھتا ہے۔\n",
    "\n",
    "یہ Cohere Embeddings Model کا استعمال کرتا ہے تاکہ ٹیکسٹ ڈاکیومنٹ اور سوال دونوں کی ایمبیڈنگز بنائی جا سکیں۔ اس نمونے میں، faiss Python پیکیج کو وییکٹر اسٹور کے طور پر استعمال کیا گیا ہے۔\n",
    "\n",
    "جو پرامپٹ Mistral ماڈل کو بھیجا جاتا ہے، اس میں سوالات اور وہ چنکس شامل ہوتے ہیں جو سوال سے ملتے جلتے ہیں۔ اس کے بعد ماڈل قدرتی زبان میں جواب فراہم کرتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /home/codespace/.python/current/lib/python3.12/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author primarily engaged in two activities before college: writing and programming. In terms of writing, they wrote short stories, albeit not very good ones, with minimal plot and characters expressing strong feelings. For programming, they started writing programs on the IBM 1401 used for data processing during their 9th grade, at the age of 13 or 14. They used an early version of Fortran and typed programs on punch cards, later loading them into the card reader to run the program.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference import EmbeddingsClient\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-large\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text\n",
    "\n",
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "len(chunks)\n",
    "\n",
    "embed_model_name = \"cohere-embed-v3-multilingual\" \n",
    "\n",
    "embed_client = EmbeddingsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(token)\n",
    ")\n",
    "\n",
    "embed_response = embed_client.embed(\n",
    "    input=chunks,\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "text_embeddings = []\n",
    "for item in embed_response.data:\n",
    "    length = len(item.embedding)\n",
    "    text_embeddings.append(item.embedding)\n",
    "text_embeddings = np.array(text_embeddings)\n",
    "\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)\n",
    "\n",
    "question = \"저자가 대학에 오기 전에 주로 했던 두 가지 일은 무엇이었나요?？\"\n",
    "\n",
    "question_embedding = embed_client.embed(\n",
    "    input=[question],\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "question_embeddings = np.array(question_embedding.data[0].embedding)\n",
    "\n",
    "\n",
    "D, I = index.search(question_embeddings.reshape(1, -1), k=2) # distance, index\n",
    "retrieved_chunks = [chunks[i] for i in I.tolist()[0]]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunks}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=prompt),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مسٹرال اسمال  \n",
    "مسٹرال اسمال، مسٹرال فیملی کے ماڈلز میں سے ایک اور ماڈل ہے جو پریمیئر/انٹرپرائز کیٹیگری میں شامل ہے۔ جیسا کہ نام سے ظاہر ہے، یہ ایک اسمال لینگویج ماڈل (SLM) ہے۔ مسٹرال اسمال استعمال کرنے کے فائدے یہ ہیں:  \n",
    "- لاگت میں بچت، مسٹرال کے بڑے ماڈلز جیسے مسٹرال لارج اور نی مو کے مقابلے میں - 80% تک قیمت میں کمی  \n",
    "- کم تاخیر - مسٹرال کے بڑے ماڈلز کے مقابلے میں تیز جواب  \n",
    "- لچکدار - مختلف ماحول میں کم وسائل کے ساتھ آسانی سے ڈپلائے کیا جا سکتا ہے۔  \n",
    "\n",
    "مسٹرال اسمال ان کاموں کے لیے بہترین ہے:  \n",
    "- ٹیکسٹ پر مبنی کام جیسے خلاصہ بنانا، جذبات کا تجزیہ اور ترجمہ  \n",
    "- ایسی ایپلیکیشنز جہاں کم لاگت کی وجہ سے بار بار درخواستیں کی جاتی ہیں  \n",
    "- کم تاخیر والے کوڈ کے کام جیسے ریویو اور کوڈ کی تجاویز\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Small اور Mistral Large کا موازنہ\n",
    "\n",
    "Mistral Small اور Large کے درمیان تاخیر میں فرق دکھانے کے لیے، نیچے دیے گئے سیلز چلائیں۔\n",
    "\n",
    "آپ کو جواب کے وقت میں 3 سے 5 سیکنڈ کا فرق نظر آنا چاہیے۔ اسی پرامپٹ پر جوابات کی لمبائی اور انداز کا بھی مشاہدہ کریں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-small\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-large\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## میسترال نی مو\n",
    "\n",
    "اس سبق میں زیر بحث دیگر دو ماڈلز کے مقابلے میں، میسترال نی مو واحد مفت ماڈل ہے جس کے پاس اپاچی2 لائسنس ہے۔\n",
    "\n",
    "اسے میسترال کے پہلے اوپن سورس ایل ایل ایم، میسترال 7B، کا اپ گریڈ سمجھا جاتا ہے۔\n",
    "\n",
    "نی مو ماڈل کی کچھ اور خصوصیات یہ ہیں:\n",
    "\n",
    "- *زیادہ مؤثر ٹوکنائزیشن:* یہ ماڈل عام طور پر استعمال ہونے والے tiktoken کی بجائے Tekken ٹوکنائزر استعمال کرتا ہے۔ اس سے زیادہ زبانوں اور کوڈ پر بہتر کارکردگی حاصل ہوتی ہے۔\n",
    "\n",
    "- *فائن ٹیوننگ:* بیس ماڈل فائن ٹیوننگ کے لیے دستیاب ہے۔ اس سے ان صورتوں میں زیادہ لچک ملتی ہے جہاں فائن ٹیوننگ کی ضرورت ہو سکتی ہے۔\n",
    "\n",
    "- *نیٹو فنکشن کالنگ* - میسترال لارج کی طرح، اس ماڈل کو فنکشن کالنگ پر تربیت دی گئی ہے۔ یہ اسے منفرد بناتا ہے کیونکہ یہ اوپن سورس ماڈلز میں سے ایک ہے جس نے یہ صلاحیت حاصل کی۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## میسترال نی مو\n",
    "\n",
    "اس سبق میں زیر بحث دیگر دو ماڈلز کے مقابلے میں، میسترال نی مو واحد مفت ماڈل ہے جس کے پاس اپاچی2 لائسنس ہے۔\n",
    "\n",
    "اسے میسترال کے پہلے اوپن سورس ایل ایل ایم، میسترال 7B، کا اپ گریڈ سمجھا جاتا ہے۔\n",
    "\n",
    "نی مو ماڈل کی کچھ اور خصوصیات یہ ہیں:\n",
    "\n",
    "- *زیادہ مؤثر ٹوکنائزیشن:* یہ ماڈل عام طور پر استعمال ہونے والے tiktoken کی بجائے Tekken ٹوکنائزر استعمال کرتا ہے۔ اس سے زیادہ زبانوں اور کوڈ پر بہتر کارکردگی حاصل ہوتی ہے۔\n",
    "\n",
    "- *فائن ٹیوننگ:* بیس ماڈل فائن ٹیوننگ کے لیے دستیاب ہے۔ اس سے ان صورتوں میں زیادہ لچک ملتی ہے جہاں فائن ٹیوننگ کی ضرورت ہو سکتی ہے۔\n",
    "\n",
    "- *نیٹو فنکشن کالنگ* - میسترال لارج کی طرح، اس ماڈل کو فنکشن کالنگ پر تربیت دی گئی ہے۔ یہ اسے منفرد بناتا ہے کیونکہ یہ اوپن سورس ماڈلز میں سے ایک ہے جس نے یہ خصوصیت سب سے پہلے حاصل کی۔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ٹوکنائزرز کا موازنہ\n",
    "\n",
    "اس مثال میں، ہم دیکھیں گے کہ Mistral NeMo ٹوکنائزیشن کو کس طرح ہینڈل کرتا ہے، Mistral Large کے مقابلے میں۔\n",
    "\n",
    "دونوں مثالیں ایک ہی پرامپٹ لیتی ہیں، لیکن آپ دیکھیں گے کہ NeMo کم ٹوکنز واپس کرتا ہے جبکہ Mistral Large زیادہ ٹوکنز دیتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistral-common\n",
      "  Downloading mistral_common-1.4.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (4.23.0)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from mistral-common) (2.9.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (2.32.3)\n",
      "Collecting sentencepiece==0.2.0 (from mistral-common)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from mistral-common)\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from mistral-common) (4.12.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (0.20.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.1->mistral-common) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.1->mistral-common) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (2024.8.30)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<0.8.0,>=0.7.0->mistral-common)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Downloading mistral_common-1.4.4-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, regex, tiktoken, mistral-common\n",
      "Successfully installed mistral-common-1.4.4 regex-2024.9.11 sentencepiece-0.2.0 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mistral-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"open-mistral-nemo\t\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"mistral-large-latest\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سیکھنا یہاں ختم نہیں ہوتا، سفر جاری رکھیں\n",
    "\n",
    "اس سبق کو مکمل کرنے کے بعد، ہماری [جنریٹیو اے آئی لرننگ کلیکشن](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) دیکھیں تاکہ آپ اپنی جنریٹیو اے آئی کی معلومات کو مزید بہتر بنا سکیں!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**اعلانِ دستبرداری**:  \nیہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کی پوری کوشش کرتے ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا عدم درستگی ہو سکتی ہے۔ اصل دستاویز کو اس کی اصل زبان میں مستند ماخذ سمجھا جانا چاہیے۔ اہم معلومات کے لیے پیشہ ور انسانی ترجمہ تجویز کیا جاتا ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کی ذمہ داری ہم پر نہیں ہوگی۔\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "coopTranslator": {
   "original_hash": "03bc2561665a8b411acfc24a8aa0d442",
   "translation_date": "2025-08-25T22:13:32+00:00",
   "source_file": "20-mistral/python/githubmodels-assignment.ipynb",
   "language_code": "ur"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}