{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung\n",
    "\n",
    "In dieser Lektion werden folgende Themen behandelt:\n",
    "- Was ist Function Calling und wofür wird es verwendet\n",
    "- Wie man einen Function Call mit OpenAI erstellt\n",
    "- Wie man einen Function Call in eine Anwendung integriert\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "Nach Abschluss dieser Lektion wissen und verstehen Sie:\n",
    "\n",
    "- Den Zweck der Verwendung von Function Calling\n",
    "- Einrichtung eines Function Calls mit dem OpenAI Service\n",
    "- Effektive Gestaltung von Function Calls für den Anwendungsfall Ihrer Anwendung\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verständnis von Funktionsaufrufen\n",
    "\n",
    "In dieser Lektion möchten wir eine Funktion für unser Bildungs-Startup entwickeln, mit der Nutzer über einen Chatbot technische Kurse finden können. Wir empfehlen Kurse, die zu ihrem Kenntnisstand, ihrer aktuellen Rolle und ihrer bevorzugten Technologie passen.\n",
    "\n",
    "Dafür nutzen wir eine Kombination aus:\n",
    " - `OpenAI`, um eine Chat-Erfahrung für die Nutzer zu schaffen\n",
    " - `Microsoft Learn Catalog API`, um Nutzern passende Kurse basierend auf ihrer Anfrage zu zeigen\n",
    " - `Function Calling`, um die Anfrage des Nutzers an eine Funktion weiterzuleiten, die dann die API-Abfrage ausführt\n",
    "\n",
    "Schauen wir uns zunächst an, warum wir überhaupt Funktionsaufrufe verwenden sollten:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # Eine neue Antwort von GPT erhalten, bei der die Funktionsergebnisse berücksichtigt werden können\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warum Function Calling\n",
    "\n",
    "Wenn du bereits eine andere Lektion in diesem Kurs abgeschlossen hast, kennst du wahrscheinlich die Leistungsfähigkeit von Large Language Models (LLMs). Hoffentlich hast du aber auch einige ihrer Einschränkungen bemerkt.\n",
    "\n",
    "Function Calling ist eine Funktion des OpenAI Service, die entwickelt wurde, um folgende Herausforderungen zu lösen:\n",
    "\n",
    "Inkonsistente Antwortformate:\n",
    "- Vor Function Calling waren die Antworten eines Large Language Models unstrukturiert und uneinheitlich. Entwickler mussten komplexe Validierungscodes schreiben, um jede Variation im Output zu verarbeiten.\n",
    "\n",
    "Eingeschränkte Integration externer Daten:\n",
    "- Vor dieser Funktion war es schwierig, Daten aus anderen Teilen einer Anwendung in einen Chat-Kontext einzubinden.\n",
    "\n",
    "Durch die Standardisierung der Antwortformate und die nahtlose Integration externer Daten vereinfacht Function Calling die Entwicklung und reduziert den Bedarf an zusätzlicher Validierungslogik.\n",
    "\n",
    "Nutzer konnten keine Antworten wie „Wie ist das aktuelle Wetter in Stockholm?“ erhalten. Das liegt daran, dass die Modelle auf den Zeitpunkt beschränkt waren, zu dem die Daten trainiert wurden.\n",
    "\n",
    "Schauen wir uns das folgende Beispiel an, das dieses Problem verdeutlicht:\n",
    "\n",
    "Angenommen, wir möchten eine Datenbank mit Studentendaten erstellen, um ihnen den passenden Kurs vorzuschlagen. Unten haben wir zwei Beschreibungen von Studierenden, die sich inhaltlich sehr ähnlich sind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir möchten dies an ein LLM senden, um die Daten zu analysieren. Später kann dies in unserer Anwendung verwendet werden, um es an eine API zu senden oder in einer Datenbank zu speichern.\n",
    "\n",
    "Lass uns zwei identische Prompts erstellen, mit denen wir das LLM anweisen, welche Informationen uns interessieren:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir möchten dies an ein LLM senden, um die für unser Produkt wichtigen Teile zu analysieren. So können wir zwei identische Prompts erstellen, um das LLM anzuleiten:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir diese beiden Prompts erstellt haben, senden wir sie mit `openai.ChatCompletion` an das LLM. Wir speichern den Prompt in der Variable `messages` und weisen die Rolle `user` zu. Dies dient dazu, eine Nachricht von einem Benutzer an einen Chatbot zu simulieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch wenn die Prompts gleich sind und die Beschreibungen ähnlich, können wir unterschiedliche Formate für die Eigenschaft `Grades` erhalten.\n",
    "\n",
    "Wenn du die obenstehende Zelle mehrmals ausführst, kann das Format entweder `3.7` oder `3.7 GPA` sein.\n",
    "\n",
    "Das liegt daran, dass das LLM unstrukturierte Daten in Form des geschriebenen Prompts entgegennimmt und ebenfalls unstrukturierte Daten zurückgibt. Wir brauchen jedoch ein strukturiertes Format, damit wir wissen, was uns beim Speichern oder Verwenden dieser Daten erwartet.\n",
    "\n",
    "Durch die Verwendung von funktionalem Aufruf können wir sicherstellen, dass wir strukturierte Daten zurückbekommen. Beim funktionalen Aufruf ruft das LLM tatsächlich keine Funktionen auf oder führt sie aus. Stattdessen erstellen wir eine Struktur, der das LLM bei seinen Antworten folgen soll. Diese strukturierten Antworten nutzen wir dann, um zu wissen, welche Funktion in unseren Anwendungen ausgeführt werden soll.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Funktionsaufruf-Flussdiagramm](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.de.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anwendungsfälle für die Nutzung von Funktionsaufrufen\n",
    "\n",
    "**Externe Tools aufrufen**  \n",
    "Chatbots eignen sich hervorragend, um Nutzerfragen zu beantworten. Mithilfe von Funktionsaufrufen können Chatbots Nachrichten von Nutzern nutzen, um bestimmte Aufgaben zu erledigen. Zum Beispiel kann ein*e Student*in den Chatbot bitten: „Sende eine E-Mail an meine Lehrkraft und teile mit, dass ich mehr Unterstützung bei diesem Thema benötige.“ Dies kann einen Funktionsaufruf an `send_email(to: string, body: string)` auslösen.\n",
    "\n",
    "**API- oder Datenbankabfragen erstellen**  \n",
    "Nutzer*innen können Informationen in natürlicher Sprache suchen, die dann in eine formatierte Abfrage oder API-Anfrage umgewandelt wird. Ein Beispiel wäre eine Lehrkraft, die fragt: „Welche Schüler*innen haben die letzte Aufgabe abgeschlossen?“ – dies könnte eine Funktion namens `get_completed(student_name: string, assignment: int, current_status: string)` aufrufen.\n",
    "\n",
    "**Strukturierte Daten erstellen**  \n",
    "Nutzer*innen können einen Textblock oder eine CSV-Datei nehmen und das LLM nutzen, um wichtige Informationen daraus zu extrahieren. Zum Beispiel kann ein*e Student*in einen Wikipedia-Artikel über Friedensabkommen verwenden, um daraus KI-Lernkarten zu erstellen. Dies kann mit einer Funktion wie `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)` geschehen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Erstellen Ihres ersten Funktionsaufrufs\n",
    "\n",
    "Der Prozess zum Erstellen eines Funktionsaufrufs umfasst drei Hauptschritte:\n",
    "1. Aufruf der Chat Completions API mit einer Liste Ihrer Funktionen und einer Benutzernachricht\n",
    "2. Die Antwort des Modells auslesen, um eine Aktion auszuführen, z. B. eine Funktion oder einen API-Aufruf ausführen\n",
    "3. Einen weiteren Aufruf an die Chat Completions API machen, diesmal mit der Antwort Ihrer Funktion, um diese Informationen zu nutzen und eine Antwort für den Benutzer zu erstellen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ablauf eines Funktionsaufrufs](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.de.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elemente eines Funktionsaufrufs\n",
    "\n",
    "#### Benutzereingabe\n",
    "\n",
    "Der erste Schritt ist, eine Benutzernachricht zu erstellen. Diese kann dynamisch zugewiesen werden, indem man den Wert eines Texteingabefelds übernimmt, oder man kann hier einen Wert festlegen. Wenn du zum ersten Mal mit der Chat Completions API arbeitest, müssen wir die `role` und den `content` der Nachricht definieren.\n",
    "\n",
    "Die `role` kann entweder `system` (Regeln erstellen), `assistant` (das Modell) oder `user` (der Endnutzer) sein. Für Funktionsaufrufe setzen wir dies auf `user` und geben eine Beispiel-Frage an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen erstellen.\n",
    "\n",
    "Als Nächstes definieren wir eine Funktion und deren Parameter. Wir verwenden hier nur eine Funktion namens `search_courses`, aber du kannst auch mehrere Funktionen erstellen.\n",
    "\n",
    "**Wichtig**: Funktionen werden in die Systemnachricht an das LLM aufgenommen und zählen zu den verfügbaren Tokens, die dir zur Verfügung stehen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitionen**\n",
    "\n",
    "Die Struktur der Funktionsdefinition hat mehrere Ebenen, von denen jede ihre eigenen Eigenschaften besitzt. Hier ist eine Übersicht über die verschachtelte Struktur:\n",
    "\n",
    "**Eigenschaften der Funktion auf oberster Ebene:**\n",
    "\n",
    "`name` – Der Name der Funktion, die aufgerufen werden soll.\n",
    "\n",
    "`description` – Dies ist die Beschreibung, wie die Funktion funktioniert. Hier ist es wichtig, spezifisch und klar zu sein.\n",
    "\n",
    "`parameters` – Eine Liste von Werten und dem Format, das das Modell in seiner Antwort liefern soll.\n",
    "\n",
    "**Eigenschaften des Parameters-Objekts:**\n",
    "\n",
    "`type` – Der Datentyp des Parameters-Objekts (meistens \"object\")\n",
    "\n",
    "`properties` – Liste der spezifischen Werte, die das Modell für seine Antwort verwendet\n",
    "\n",
    "**Eigenschaften einzelner Parameter:**\n",
    "\n",
    "`name` – Implizit durch den Eigenschaftsschlüssel definiert (z. B. \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` – Der Datentyp dieses spezifischen Parameters (z. B. \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` – Beschreibung des jeweiligen Parameters\n",
    "\n",
    "**Optionale Eigenschaften:**\n",
    "\n",
    "`required` – Ein Array, das auflistet, welche Parameter für den Funktionsaufruf erforderlich sind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Den Funktionsaufruf durchführen\n",
    "Nachdem wir eine Funktion definiert haben, müssen wir sie nun im Aufruf der Chat Completion API einbinden. Das machen wir, indem wir `functions` zur Anfrage hinzufügen. In diesem Fall `functions=functions`.\n",
    "\n",
    "Es gibt außerdem die Möglichkeit, `function_call` auf `auto` zu setzen. Das bedeutet, dass wir dem LLM überlassen, basierend auf der Nutzeranfrage zu entscheiden, welche Funktion aufgerufen werden soll, anstatt dies selbst festzulegen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt schauen wir uns die Antwort an und sehen, wie sie formatiert ist:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Du siehst, dass der Name der Funktion aufgerufen wird und das LLM anhand der Benutzernachricht die Daten finden konnte, um die Argumente der Funktion auszufüllen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integration von Funktionsaufrufen in eine Anwendung.\n",
    "\n",
    "Nachdem wir die formatierte Antwort vom LLM getestet haben, können wir diese nun in eine Anwendung integrieren.\n",
    "\n",
    "### Steuerung des Ablaufs\n",
    "\n",
    "Um dies in unsere Anwendung zu integrieren, gehen wir wie folgt vor:\n",
    "\n",
    "Zuerst rufen wir die OpenAI-Services auf und speichern die Nachricht in einer Variable namens `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werden wir die Funktion definieren, die die Microsoft Learn API aufruft, um eine Liste von Kursen zu erhalten:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als bewährte Methode prüfen wir zunächst, ob das Modell eine Funktion aufrufen möchte. Danach erstellen wir eine der verfügbaren Funktionen und ordnen sie der aufgerufenen Funktion zu.  \n",
    "Anschließend nehmen wir die Argumente der Funktion und ordnen sie den Argumenten aus dem LLM zu.\n",
    "\n",
    "Zum Schluss fügen wir die Nachricht zum Funktionsaufruf und die Werte hinzu, die von der `search_courses`-Nachricht zurückgegeben wurden. So erhält das LLM alle Informationen, die es benötigt, um dem Nutzer in natürlicher Sprache zu antworten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge \n",
    "\n",
    "Super gemacht! Um dein Wissen über OpenAI Function Calling weiter zu vertiefen, kannst du folgendes umsetzen: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - Füge der Funktion weitere Parameter hinzu, die Lernenden helfen könnten, mehr passende Kurse zu finden. Die verfügbaren API-Parameter findest du hier: \n",
    " - Erstelle einen weiteren Funktionsaufruf, der zusätzliche Informationen vom Lernenden abfragt, zum Beispiel die Muttersprache \n",
    " - Baue eine Fehlerbehandlung ein, falls der Funktionsaufruf und/oder der API-Aufruf keine passenden Kurse zurückliefert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Haftungsausschluss**:  \nDieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ausgangssprache gilt als maßgebliche Quelle. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T20:42:07+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}