{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung\n",
    "\n",
    "Diese Lektion behandelt:  \n",
    "- Was Funktionsaufrufe sind und wofür sie verwendet werden  \n",
    "- Wie man einen Funktionsaufruf mit OpenAI erstellt  \n",
    "- Wie man einen Funktionsaufruf in eine Anwendung integriert  \n",
    "\n",
    "## Lernziele\n",
    "\n",
    "Nach Abschluss dieser Lektion wissen Sie, wie man Folgendes macht und verstehen:  \n",
    "\n",
    "- Den Zweck der Verwendung von Funktionsaufrufen  \n",
    "- Einrichtung von Funktionsaufrufen mit dem OpenAI-Dienst  \n",
    "- Gestaltung effektiver Funktionsaufrufe für den Anwendungsfall Ihrer Anwendung\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verständnis von Funktionsaufrufen\n",
    "\n",
    "Für diese Lektion möchten wir eine Funktion für unser Bildungs-Startup entwickeln, die es Nutzern ermöglicht, einen Chatbot zu verwenden, um technische Kurse zu finden. Wir werden Kurse empfehlen, die ihrem Fähigkeitsniveau, ihrer aktuellen Rolle und der interessierenden Technologie entsprechen.\n",
    "\n",
    "Um dies zu erreichen, verwenden wir eine Kombination aus:\n",
    " - `OpenAI`, um eine Chat-Erfahrung für den Nutzer zu schaffen\n",
    " - `Microsoft Learn Catalog API`, um Nutzern zu helfen, Kurse basierend auf der Anfrage des Nutzers zu finden\n",
    " - `Function Calling`, um die Anfrage des Nutzers zu erfassen und an eine Funktion zu senden, die die API-Anfrage durchführt.\n",
    "\n",
    "Um zu beginnen, schauen wir uns an, warum wir überhaupt Function Calling verwenden möchten:\n",
    "\n",
    "print(\"Nachrichten in der nächsten Anfrage:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # eine neue Antwort von GPT erhalten, bei der es die Funktionsantwort sehen kann\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warum Funktionsaufrufe\n",
    "\n",
    "Wenn Sie bereits eine andere Lektion in diesem Kurs abgeschlossen haben, verstehen Sie wahrscheinlich die Leistungsfähigkeit der Verwendung von Large Language Models (LLMs). Hoffentlich können Sie auch einige ihrer Einschränkungen erkennen.\n",
    "\n",
    "Funktionsaufrufe sind eine Funktion des OpenAI-Dienstes, die entwickelt wurde, um die folgenden Herausforderungen zu bewältigen:\n",
    "\n",
    "Inkonsistente Antwortformatierung:\n",
    "- Vor Funktionsaufrufen waren die Antworten eines großen Sprachmodells unstrukturiert und inkonsistent. Entwickler mussten komplexen Validierungscode schreiben, um jede Variation der Ausgabe zu handhaben.\n",
    "\n",
    "Begrenzte Integration mit externen Daten:\n",
    "- Vor dieser Funktion war es schwierig, Daten aus anderen Teilen einer Anwendung in einen Chat-Kontext einzubinden.\n",
    "\n",
    "Durch die Standardisierung von Antwortformaten und die nahtlose Integration mit externen Daten vereinfacht Funktionsaufrufe die Entwicklung und reduziert den Bedarf an zusätzlicher Validierungslogik.\n",
    "\n",
    "Benutzer konnten keine Antworten wie „Wie ist das aktuelle Wetter in Stockholm?“ erhalten. Dies liegt daran, dass Modelle auf die Zeit beschränkt waren, zu der die Daten trainiert wurden.\n",
    "\n",
    "Schauen wir uns das folgende Beispiel an, das dieses Problem veranschaulicht:\n",
    "\n",
    "Angenommen, wir möchten eine Datenbank mit Studentendaten erstellen, damit wir ihnen den richtigen Kurs vorschlagen können. Unten haben wir zwei Beschreibungen von Studenten, die in den enthaltenen Daten sehr ähnlich sind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir möchten dies an ein LLM senden, um die Daten zu analysieren. Dies kann später in unserer Anwendung verwendet werden, um es an eine API zu senden oder in einer Datenbank zu speichern.\n",
    "\n",
    "Lassen Sie uns zwei identische Eingabeaufforderungen erstellen, in denen wir das LLM anweisen, welche Informationen für uns von Interesse sind:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir möchten dies an ein LLM senden, um die für unser Produkt wichtigen Teile zu analysieren. So können wir zwei identische Eingabeaufforderungen erstellen, um das LLM zu instruieren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir diese beiden Eingabeaufforderungen erstellt haben, senden wir sie mit `openai.ChatCompletion` an das LLM. Wir speichern die Eingabeaufforderung in der Variablen `messages` und weisen die Rolle `user` zu. Dies soll eine Nachricht von einem Benutzer simulieren, die an einen Chatbot geschrieben wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir beide Anfragen an das LLM senden und die erhaltene Antwort untersuchen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obwohl die Eingabeaufforderungen gleich sind und die Beschreibungen ähnlich sind, können wir unterschiedliche Formate der Eigenschaft `Grades` erhalten.\n",
    "\n",
    "Wenn Sie die obige Zelle mehrmals ausführen, kann das Format `3.7` oder `3.7 GPA` sein.\n",
    "\n",
    "Das liegt daran, dass das LLM unstrukturierte Daten in Form der geschriebenen Eingabeaufforderung aufnimmt und ebenfalls unstrukturierte Daten zurückgibt. Wir benötigen jedoch ein strukturiertes Format, damit wir wissen, was zu erwarten ist, wenn wir diese Daten speichern oder verwenden.\n",
    "\n",
    "Durch die Verwendung von Funktionsaufrufen können wir sicherstellen, dass wir strukturierte Daten zurückerhalten. Beim Einsatz von Funktionsaufrufen ruft das LLM tatsächlich keine Funktionen auf oder führt sie aus. Stattdessen erstellen wir eine Struktur, der das LLM für seine Antworten folgen soll. Diese strukturierten Antworten verwenden wir dann, um zu wissen, welche Funktion in unseren Anwendungen ausgeführt werden soll.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Funktionsaufruf-Flussdiagramm](../../../../translated_images/Function-Flow.083875364af4f4bb.de.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können dann das, was von der Funktion zurückgegeben wird, nehmen und an das LLM zurücksenden. Das LLM wird dann in natürlicher Sprache antworten, um die Anfrage des Benutzers zu beantworten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anwendungsfälle für die Verwendung von Funktionsaufrufen\n",
    "\n",
    "**Externe Werkzeuge aufrufen**  \n",
    "Chatbots sind hervorragend darin, Antworten auf Fragen von Nutzern zu geben. Durch die Verwendung von Funktionsaufrufen können die Chatbots Nachrichten von Nutzern nutzen, um bestimmte Aufgaben zu erledigen. Zum Beispiel kann ein Student den Chatbot bitten: „Sende eine E-Mail an meinen Dozenten, in der steht, dass ich mehr Unterstützung bei diesem Thema benötige“. Dies kann einen Funktionsaufruf an `send_email(to: string, body: string)` auslösen.\n",
    "\n",
    "**API- oder Datenbankabfragen erstellen**  \n",
    "Nutzer können Informationen in natürlicher Sprache finden, die in eine formatierte Abfrage oder API-Anfrage umgewandelt wird. Ein Beispiel hierfür könnte ein Lehrer sein, der fragt: „Wer sind die Studenten, die die letzte Aufgabe abgeschlossen haben“, was eine Funktion namens `get_completed(student_name: string, assignment: int, current_status: string)` aufrufen könnte.\n",
    "\n",
    "**Strukturierte Daten erstellen**  \n",
    "Nutzer können einen Textblock oder eine CSV-Datei nehmen und das LLM verwenden, um wichtige Informationen daraus zu extrahieren. Zum Beispiel kann ein Student einen Wikipedia-Artikel über Friedensabkommen in KI-Lernkarten umwandeln. Dies kann durch die Verwendung einer Funktion namens `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)` erfolgen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Erstellen Ihres ersten Funktionsaufrufs\n",
    "\n",
    "Der Prozess zur Erstellung eines Funktionsaufrufs umfasst 3 Hauptschritte:  \n",
    "1. Aufrufen der Chat Completions API mit einer Liste Ihrer Funktionen und einer Benutzernachricht  \n",
    "2. Lesen der Antwort des Modells, um eine Aktion auszuführen, z. B. eine Funktion oder einen API-Aufruf auszuführen  \n",
    "3. Einen weiteren Aufruf an die Chat Completions API mit der Antwort Ihrer Funktion machen, um diese Informationen zu verwenden, um eine Antwort für den Benutzer zu erstellen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ablauf eines Funktionsaufrufs](../../../../translated_images/LLM-Flow.3285ed8caf4796d7.de.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elemente eines Funktionsaufrufs\n",
    "\n",
    "#### Benutzereingabe\n",
    "\n",
    "Der erste Schritt besteht darin, eine Benutzernachricht zu erstellen. Diese kann dynamisch zugewiesen werden, indem der Wert einer Texteingabe übernommen wird, oder Sie können hier einen Wert zuweisen. Wenn Sie zum ersten Mal mit der Chat Completions API arbeiten, müssen wir die `role` und den `content` der Nachricht definieren.\n",
    "\n",
    "Die `role` kann entweder `system` (Regeln erstellen), `assistant` (das Modell) oder `user` (der Endbenutzer) sein. Für den Funktionsaufruf weisen wir dies als `user` und eine Beispiel-Frage zu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen erstellen.\n",
    "\n",
    "Als Nächstes definieren wir eine Funktion und die Parameter dieser Funktion. Wir verwenden hier nur eine Funktion namens `search_courses`, aber Sie können mehrere Funktionen erstellen.\n",
    "\n",
    "**Wichtig**: Funktionen sind in der Systemnachricht an das LLM enthalten und werden auf die Anzahl der verfügbaren Tokens angerechnet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitionen** \n",
    "\n",
    "Die Funktionsdefinitionsstruktur hat mehrere Ebenen, jede mit eigenen Eigenschaften. Hier ist eine Aufschlüsselung der verschachtelten Struktur:\n",
    "\n",
    "**Eigenschaften der obersten Funktionsebene:**\n",
    "\n",
    "`name` - Der Name der Funktion, die aufgerufen werden soll. \n",
    "\n",
    "`description` - Dies ist die Beschreibung, wie die Funktion funktioniert. Hier ist es wichtig, spezifisch und klar zu sein. \n",
    "\n",
    "`parameters` - Eine Liste von Werten und Formaten, die das Modell in seiner Antwort erzeugen soll. \n",
    "\n",
    "**Eigenschaften des Parameterobjekts:**\n",
    "\n",
    "`type` - Der Datentyp des Parameterobjekts (normalerweise \"object\")\n",
    "\n",
    "`properties` - Liste der spezifischen Werte, die das Modell für seine Antwort verwenden wird. \n",
    "\n",
    "**Eigenschaften einzelner Parameter:**\n",
    "\n",
    "`name` - Implizit durch den Eigenschaftsschlüssel definiert (z. B. \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Der Datentyp dieses spezifischen Parameters (z. B. \"string\", \"number\", \"boolean\") \n",
    "\n",
    "`description` - Beschreibung des spezifischen Parameters \n",
    "\n",
    "**Optionale Eigenschaften:**\n",
    "\n",
    "`required` - Ein Array, das auflistet, welche Parameter für den Abschluss des Funktionsaufrufs erforderlich sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Den Funktionsaufruf machen  \n",
    "Nachdem wir eine Funktion definiert haben, müssen wir sie nun im Aufruf der Chat Completion API einbinden. Dies tun wir, indem wir `functions` zur Anfrage hinzufügen. In diesem Fall `functions=functions`.  \n",
    "\n",
    "Es gibt auch die Möglichkeit, `function_call` auf `auto` zu setzen. Das bedeutet, dass wir das LLM entscheiden lassen, welche Funktion basierend auf der Benutzernachricht aufgerufen werden soll, anstatt dies selbst zuzuweisen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schauen wir uns nun die Antwort an und sehen, wie sie formatiert ist:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Sie können sehen, dass der Name der Funktion aufgerufen wird und dass das LLM anhand der Benutzernachricht die Daten gefunden hat, um die Argumente der Funktion zu füllen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integration von Funktionsaufrufen in eine Anwendung. \n",
    "\n",
    "\n",
    "Nachdem wir die formatierte Antwort des LLM getestet haben, können wir diese nun in eine Anwendung integrieren. \n",
    "\n",
    "### Steuerung des Ablaufs \n",
    "\n",
    "Um dies in unsere Anwendung zu integrieren, gehen wir wie folgt vor: \n",
    "\n",
    "Zuerst rufen wir die OpenAI-Dienste auf und speichern die Nachricht in einer Variablen namens `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt definieren wir die Funktion, die die Microsoft Learn API aufruft, um eine Liste von Kursen zu erhalten:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als bewährte Methode werden wir dann prüfen, ob das Modell eine Funktion aufrufen möchte. Danach erstellen wir eine der verfügbaren Funktionen und ordnen sie der Funktion zu, die aufgerufen wird.  \n",
    "Anschließend nehmen wir die Argumente der Funktion und ordnen sie den Argumenten des LLM zu.\n",
    "\n",
    "Zuletzt fügen wir die Funktionsaufrufnachricht und die Werte, die durch die `search_courses`-Nachricht zurückgegeben wurden, an. Dies gibt dem LLM alle Informationen, die es benötigt, um in natürlicher Sprache auf den Benutzer zu antworten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt senden wir die aktualisierte Nachricht an das LLM, damit wir eine Antwort in natürlicher Sprache anstelle einer API JSON-formatierten Antwort erhalten können.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code-Herausforderung\n",
    "\n",
    "Großartige Arbeit! Um dein Lernen über OpenAI Function Calling fortzusetzen, kannst du Folgendes erstellen: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    " - Weitere Parameter der Funktion, die Lernenden helfen könnten, mehr Kurse zu finden. Die verfügbaren API-Parameter findest du hier:  \n",
    " - Erstelle einen weiteren Funktionsaufruf, der mehr Informationen vom Lernenden wie seine Muttersprache entgegennimmt  \n",
    " - Erstelle eine Fehlerbehandlung, wenn der Funktionsaufruf und/oder der API-Aufruf keine geeigneten Kurse zurückgibt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Haftungsausschluss**:  \nDieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ursprungssprache ist als maßgebliche Quelle zu betrachten. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die aus der Nutzung dieser Übersetzung entstehen.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T08:45:49+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}