# Retrieval Augmented Generation (RAG) und Vektor-Datenbanken

[![Retrieval Augmented Generation (RAG) und Vektor-Datenbanken](../../../translated_images/de/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

In der Lektion zu Suchanwendungen haben wir kurz gelernt, wie man eigene Daten in Large Language Models (LLMs) integriert. In dieser Lektion werden wir tiefer in die Konzepte des Groundings Ihrer Daten in Ihrer LLM-Anwendung eintauchen, die Mechanik des Prozesses und die Methoden zur Speicherung von Daten, einschlie√ülich sowohl Einbettungen als auch Text.

> **Video demn√§chst verf√ºgbar**

## Einf√ºhrung

In dieser Lektion behandeln wir Folgendes:

- Eine Einf√ºhrung in RAG, was es ist und warum es in der KI (k√ºnstliche Intelligenz) verwendet wird.

- Verst√§ndnis, was Vektor-Datenbanken sind und wie man eine f√ºr unsere Anwendung erstellt.

- Ein praktisches Beispiel, wie man RAG in eine Anwendung integriert.

## Lernziele

Nach Abschluss dieser Lektion werden Sie in der Lage sein:

- Die Bedeutung von RAG bei der Datenabfrage und -verarbeitung zu erkl√§ren.

- Eine RAG-Anwendung einzurichten und Ihre Daten an ein LLM zu binden.

- Effektive Integration von RAG und Vektor-Datenbanken in LLM-Anwendungen.

## Unser Szenario: Verbesserung unserer LLMs mit unseren eigenen Daten

F√ºr diese Lektion m√∂chten wir unsere eigenen Notizen in das Bildungs-Startup einf√ºgen, sodass der Chatbot mehr Informationen zu den verschiedenen Themen erh√§lt. Mit den Notizen, die wir haben, k√∂nnen Lernende besser lernen und die verschiedenen Themen besser verstehen, was das Wiederholen f√ºr ihre Pr√ºfungen erleichtert. Um unser Szenario zu erstellen, werden wir verwenden:

- `Azure OpenAI:` das LLM, das wir zur Erstellung unseres Chatbots nutzen

- `AI for beginners' lesson on Neural Networks`: dies wird die Datenbasis sein, auf der wir unser LLM aufbauen

- `Azure AI Search` und `Azure Cosmos DB:` Vektor-Datenbank zum Speichern unserer Daten und zur Erstellung eines Suchindex

Benutzer k√∂nnen √úbungsquiz aus ihren Notizen erstellen, Lernkarten zur Wiederholung anlegen und diese zu pr√§gnanten √úbersichten zusammenfassen. Um zu beginnen, betrachten wir, was RAG ist und wie es funktioniert:

## Retrieval Augmented Generation (RAG)

Ein von einem LLM unterst√ºtzter Chatbot verarbeitet Benutzeranfragen, um Antworten zu generieren. Er ist so gestaltet, dass er interaktiv ist und sich mit Benutzern √ºber eine Vielzahl von Themen austauscht. Seine Antworten sind jedoch auf den bereitgestellten Kontext und seine grundlegenden Trainingsdaten beschr√§nkt. Beispielsweise endet das Wissensfenster von GPT-4 im September 2021, das hei√üt, es fehlen Kenntnisse √ºber Ereignisse, die nach diesem Zeitpunkt stattgefunden haben. Au√üerdem schlie√üen die zum Trainieren von LLMs verwendeten Daten vertrauliche Informationen wie pers√∂nliche Notizen oder ein Produkt-Handbuch eines Unternehmens aus.

### Wie RAGs (Retrieval Augmented Generation) funktionieren

![Zeichnung, die zeigt, wie RAGs funktionieren](../../../translated_images/de/how-rag-works.f5d0ff63942bd3a6.webp)

Angenommen, Sie m√∂chten einen Chatbot bereitstellen, der Quizfragen aus Ihren Notizen erstellt, ben√∂tigen Sie eine Verbindung zur Wissensdatenbank. Hier kommt RAG ins Spiel. RAGs funktionieren wie folgt:

- **Wissensdatenbank:** Vor der Abfrage m√ºssen diese Dokumente aufgenommen und vorverarbeitet werden, typischerweise indem gro√üe Dokumente in kleinere Abschnitte (Chunks) zerlegt, in Text-Einbettungen umgewandelt und in einer Datenbank gespeichert werden.

- **Benutzeranfrage:** Der Benutzer stellt eine Frage

- **Abruf (Retrieval):** Wenn ein Benutzer eine Frage stellt, ruft das Einbettungsmodell relevante Informationen aus unserer Wissensdatenbank ab, um mehr Kontext zu liefern, der in die Eingabeaufforderung integriert wird.

- **Erweiterte Generierung:** Das LLM verbessert seine Antwort basierend auf den abgerufenen Daten. Dadurch basiert die generierte Antwort nicht nur auf vortrainierten Daten, sondern auch auf relevanten Informationen aus dem hinzugef√ºgten Kontext. Die abgerufenen Daten werden verwendet, um die Antworten des LLM zu erg√§nzen. Das LLM gibt dann eine Antwort auf die Frage des Benutzers zur√ºck.

![Zeichnung, die die RAG-Architektur zeigt](../../../translated_images/de/encoder-decode.f2658c25d0eadee2.webp)

Die Architektur von RAGs wird mit Transformern implementiert, die aus zwei Teilen bestehen: einem Encoder und einem Decoder. Wenn ein Benutzer zum Beispiel eine Frage stellt, wird der Eingabetext in Vektoren ‚Äûkodiert‚Äú, die die Bedeutung der W√∂rter erfassen, und die Vektoren werden ‚Äûdekodiert‚Äú in unseren Dokumentenindex und generieren neuen Text basierend auf der Benutzeranfrage. Das LLM verwendet ein Encoder-Decoder-Modell, um die Ausgabe zu erzeugen.

Laut dem vorgeschlagenen Paper: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) gibt es zwei Ans√§tze f√ºr die Implementierung von RAG:

- **_RAG-Sequence_** nutzt abgerufene Dokumente, um die bestm√∂gliche Antwort auf eine Benutzeranfrage vorherzusagen

- **RAG-Token** verwendet Dokumente, um das n√§chste Token zu generieren und ruft dann weitere ab, um die Benutzeranfrage zu beantworten

### Warum sollten Sie RAGs verwenden?¬†

- **Informationsf√ºlle:** stellt sicher, dass Textantworten aktuell und auf dem neuesten Stand sind. Es verbessert daher die Leistung bei dom√§nenspezifischen Aufgaben durch Zugriff auf die interne Wissensdatenbank.

- Reduziert Falschinformationen, indem es **√ºberpr√ºfbare Daten** in der Wissensdatenbank nutzt, um Kontext zu den Benutzeranfragen bereitzustellen.

- Es ist **kosteneffektiv**, da es g√ºnstiger ist als ein Feintuning eines LLM.

## Erstellen einer Wissensdatenbank

Unsere Anwendung basiert auf unseren pers√∂nlichen Daten, d.h. der Neural Network Lektion im AI For Beginners Curriculum.

### Vektor-Datenbanken

Eine Vektor-Datenbank ist, im Gegensatz zu traditionellen Datenbanken, eine spezialisierte Datenbank, die darauf ausgelegt ist, eingebettete Vektoren zu speichern, zu verwalten und zu durchsuchen. Sie speichert numerische Repr√§sentationen von Dokumenten. Die Aufschl√ºsselung von Daten in numerische Einbettungen macht es unserem KI-System leichter, die Daten zu verstehen und zu verarbeiten.

Wir speichern unsere Einbettungen in Vektor-Datenbanken, da LLMs eine Begrenzung der Anzahl an Token haben, die sie als Eingabe akzeptieren. Da Sie nicht die gesamten Einbettungen an ein LLM weitergeben k√∂nnen, m√ºssen wir sie in Abschnitte (Chunks) aufteilen, und wenn ein Benutzer eine Frage stellt, werden die Einbettungen zur√ºckgegeben, die der Frage am √§hnlichsten sind. Das Chunking senkt auch die Kosten f√ºr die Anzahl der Token, die durch ein LLM laufen.

Einige beliebte Vektor-Datenbanken sind Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant und DeepLake. Sie k√∂nnen ein Azure Cosmos DB-Modell mit Azure CLI mit folgendem Befehl erstellen:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Vom Text zu den Einbettungen

Bevor wir unsere Daten speichern, m√ºssen wir sie in Vektor-Einbettungen umwandeln, bevor sie in der Datenbank abgelegt werden. Wenn Sie mit gro√üen Dokumenten oder langen Texten arbeiten, k√∂nnen Sie diese entsprechend der erwarteten Anfragen in Abschnitte aufteilen. Das Chunking kann auf Satz- oder Absatzebene erfolgen. Da beim Chunking die Bedeutung auch aus den umliegenden W√∂rtern abgeleitet wird, k√∂nnen Sie einem Chunk weiteren Kontext hinzuf√ºgen, z.B. durch Hinzuf√ºgen des Dokumenttitels oder Einbeziehen von Text vor oder nach dem Chunk. Sie k√∂nnen die Daten folgenderma√üen chunkieren:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Wenn das letzte St√ºck nicht die Mindestl√§nge erreicht hat, f√ºge es trotzdem hinzu
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Nach dem Chunking k√∂nnen wir dann unseren Text mit verschiedenen Einbettungsmodellen einbetten. Einige Modelle, die Sie verwenden k√∂nnen, sind: word2vec, ada-002 von OpenAI, Azure Computer Vision und viele mehr. Die Auswahl eines Modells h√§ngt von den verwendeten Sprachen, dem codierten Inhaltstyp (Text/Bilder/Audio), der Eingabegr√∂√üe, die es kodieren kann, und der L√§nge der Einbettungsausgabe ab.

Ein Beispiel f√ºr eingbetteten Text mit OpenAIs `text-embedding-ada-002` Modell ist:
![eine Einbettung des Wortes Katze](../../../translated_images/de/cat.74cbd7946bc9ca38.webp)

## Retrieval und Vektorsuche

Wenn ein Benutzer eine Frage stellt, wandelt der Retriever sie mithilfe des Query-Encoders in einen Vektor um, durchsucht dann unseren Dokumentensuchindex nach relevanten Vektoren, die im Dokument mit der Eingabe zusammenh√§ngen. Nach Abschluss konvertiert er sowohl den Eingabevektor als auch die Dokumentenvektoren zur√ºck in Text und gibt sie an das LLM weiter.

### Abruf

Retrieval findet statt, wenn das System versucht, schnell die Dokumente aus dem Index zu finden, die die Suchkriterien erf√ºllen. Das Ziel des Retrievers ist es, Dokumente zu holen, die verwendet werden, um Kontext bereitzustellen und das LLM auf Ihre Daten zu gr√ºnden.

Es gibt verschiedene M√∂glichkeiten, innerhalb unserer Datenbank zu suchen, z.B.:

- **Schl√ºsselwortsuche** ‚Äì wird f√ºr Textsuchen verwendet

- **Vektorsuche** ‚Äì wandelt Dokumente von Text in Vektorrepr√§sentationen mit Einbettungsmodellen um, was eine **semantische Suche** auf Basis der Bedeutung der W√∂rter erm√∂glicht. Der Abruf erfolgt durch Abfragen der Dokumente, deren Vektorrepr√§sentationen dem Benutzerfragevektor am n√§chsten sind.

- **Hybrid** ‚Äì eine Kombination aus Schl√ºsselwort- und Vektorsuche.

Eine Herausforderung beim Abruf besteht darin, dass es keine √§hnliche Antwort auf die Anfrage in der Datenbank gibt. Das System gibt dann die bestm√∂glichen Informationen zur√ºck. Sie k√∂nnen jedoch Taktiken verwenden, wie das Einrichten einer maximalen Relevanzdistanz oder Nutzung der hybriden Suche, die Schl√ºsselwort- und Vektorsuche kombiniert. In dieser Lektion verwenden wir hybride Suche, eine Kombination aus Vektor- und Schl√ºsselwortsuche. Wir speichern unsere Daten in einem DataFrame mit Spalten, die die Chunks sowie die Einbettungen enthalten.

### Vektor√§hnlichkeit

Der Retriever durchsucht die Wissensdatenbank nach Einbettungen, die nahe beieinanderliegen, also den n√§chsten Nachbarn, da es sich um √§hnliche Texte handelt. Wenn ein Benutzer eine Anfrage stellt, wird diese zuerst eingebettet und dann mit √§hnlichen Einbettungen abgeglichen. Das gebr√§uchlichste Ma√ü zur Bestimmung der √Ñhnlichkeit zwischen Vektoren ist die Kosinus-√Ñhnlichkeit, die auf dem Winkel zwischen zwei Vektoren basiert.

Wir k√∂nnen die √Ñhnlichkeit auch mit anderen Alternativen messen, die wir verwenden k√∂nnen, wie dem euklidischen Abstand, der die Gerade zwischen den Endpunkten zweier Vektoren misst, oder dem Skalarprodukt, welches die Summe der Produkte der korrespondierenden Elemente zweier Vektoren angibt.

### Suchindex

Beim Abruf m√ºssen wir vor der Suche einen Suchindex f√ºr unsere Wissensdatenbank erstellen. Ein Index speichert unsere Einbettungen und kann selbst in einer gro√üen Datenbank die √§hnlichsten Chunks schnell abrufen. Wir k√∂nnen unseren Index lokal erstellen mit:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Erstellen Sie den Suchindex
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Um den Index abzufragen, k√∂nnen Sie die Methode kneighbors verwenden
distances, indices = nbrs.kneighbors(embeddings)
```

### Neu-Rangierung

Nachdem Sie die Datenbank abgefragt haben, m√ºssen Sie m√∂glicherweise die Ergebnisse nach Relevanz sortieren. Ein neu-rangierendes LLM nutzt maschinelles Lernen, um die Relevanz der Suchergebnisse zu verbessern, indem es die Ergebnisse nach der Wichtigkeit sortiert. Mit Azure AI Search wird die Neu-Rangierung automatisch f√ºr Sie durch einen semantischen Neu-Ranger erledigt. Ein Beispiel, wie Neu-Rangierung mit n√§chsten Nachbarn funktioniert:

```python
# Finde die √§hnlichsten Dokumente
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Drucke die √§hnlichsten Dokumente
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Alles zusammenf√ºgen

Der letzte Schritt ist, unser LLM einzubinden, um Antworten zu erhalten, die auf unseren Daten basieren. Wir k√∂nnen es wie folgt implementieren:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Konvertiere die Frage in einen Abfragevektor
    query_vector = create_embeddings(user_input)

    # Finde die √§hnlichsten Dokumente
    distances, indices = nbrs.kneighbors([query_vector])

    # F√ºge Dokumente zur Abfrage hinzu, um Kontext bereitzustellen
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # Kombiniere die Historie und die Benutzereingabe
    history.append(user_input)

    # Erstelle ein Nachrichtenobjekt
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # Nutze Chat Completion, um eine Antwort zu generieren
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluation unserer Anwendung

### Evaluationsmetriken

- Qualit√§t der gelieferten Antworten: Sie sollen nat√ºrlich, fl√ºssig und menschen√§hnlich klingen

- Groundedness der Daten: Bewertung, ob die Antwort aus den bereitgestellten Dokumenten stammt

- Relevanz: Bewertung, ob die Antwort zur gestellten Frage passt und in Beziehung steht

- Fl√ºssigkeit ‚Äì ob die Antwort grammatikalisch sinnvoll ist

## Anwendungsf√§lle f√ºr RAG (Retrieval Augmented Generation) und Vektor-Datenbanken

Es gibt viele verschiedene Anwendungsf√§lle, bei denen Funktionsaufrufe Ihre App verbessern k√∂nnen, wie zum Beispiel:

- Frage-Antwort-Systeme: Ihr Unternehmenswissen an einen Chat binden, der von Mitarbeitern zur Beantwortung von Fragen verwendet wird.

- Empfehlungssysteme: Sie k√∂nnen ein System erstellen, das die √§hnlichsten Werte z.B. Filme, Restaurants und viele weitere abgleicht.

- Chatbot-Dienste: Sie k√∂nnen den Chatverlauf speichern und das Gespr√§ch basierend auf Benutzerdaten personalisieren.

- Bildsuche basierend auf Vektoreinbettungen, n√ºtzlich bei der Bilderkennung und Anomalieerkennung.

## Zusammenfassung

Wir haben die grundlegenden Bereiche von RAG abgedeckt, von der Hinzuf√ºgung unserer Daten zur Anwendung √ºber die Benutzeranfrage bis hin zur Ausgabe. Zur Vereinfachung der Erstellung von RAG k√∂nnen Sie Frameworks wie Semanti Kernel, Langchain oder Autogen verwenden.

## Aufgabe

Um Ihr Wissen zu Retrieval Augmented Generation (RAG) weiter zu vertiefen, k√∂nnen Sie:

- Ein Frontend f√ºr die Anwendung unter Verwendung eines Frameworks Ihrer Wahl erstellen

- Ein Framework wie LangChain oder Semantic Kernel verwenden und Ihre Anwendung nachbauen.

Herzlichen Gl√ºckwunsch zum Abschluss der Lektion üëè.

## Lernen endet hier nicht, setzen Sie die Reise fort

Nach Abschluss dieser Lektion schauen Sie sich unsere [Generative AI Lernsammlung](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) an, um Ihre Kenntnisse in Generativer KI weiter auszubauen!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Zwar bem√ºhen wir uns um Genauigkeit, dennoch k√∂nnen automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten. Das Originaldokument in seiner Ursprungssprache ist als ma√ügebliche Quelle zu betrachten. F√ºr wichtige Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die durch die Nutzung dieser √úbersetzung entstehen.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->