<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:40:26+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "de"
}
-->
# EinfÃ¼hrung in Neuronale Netze. Mehrschichtiger Perzeptron

Im vorherigen Abschnitt hast du das einfachste Modell eines neuronalen Netzes kennengelernt â€“ den einlagigen Perzeptron, ein lineares Zwei-Klassen-Klassifikationsmodell.

In diesem Abschnitt erweitern wir dieses Modell zu einem flexibleren Rahmen, der es uns ermÃ¶glicht:

* neben der Zwei-Klassen-Klassifikation auch **Mehrklassenklassifikation** durchzufÃ¼hren
* neben Klassifikationsproblemen auch **Regressionsprobleme** zu lÃ¶sen
* Klassen zu trennen, die nicht linear separierbar sind

AuÃŸerdem entwickeln wir unser eigenes modulares Framework in Python, mit dem wir verschiedene Architekturen neuronaler Netze aufbauen kÃ¶nnen.

## Formalisierung des Machine Learning

Beginnen wir mit der Formalisierung des Machine-Learning-Problems. Angenommen, wir haben einen Trainingsdatensatz **X** mit Labels **Y** und mÃ¼ssen ein Modell *f* erstellen, das mÃ¶glichst genaue Vorhersagen trifft. Die QualitÃ¤t der Vorhersagen wird durch die **Loss-Funktion** â„’ gemessen. HÃ¤ufig verwendete Loss-Funktionen sind:

* FÃ¼r Regressionsprobleme, bei denen eine Zahl vorhergesagt werden soll, kÃ¶nnen wir den **absoluten Fehler** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| oder den **quadratischen Fehler** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> verwenden
* FÃ¼r Klassifikation nutzen wir die **0-1-Loss** (die im Grunde der **Genauigkeit** des Modells entspricht) oder die **logistische Loss**

FÃ¼r den einlagigen Perzeptron wurde die Funktion *f* als lineare Funktion *f(x)=wx+b* definiert (wobei *w* die Gewichtsmatrix, *x* der Vektor der Eingabemerkmale und *b* der Bias-Vektor ist). FÃ¼r verschiedene Architekturen neuronaler Netze kann diese Funktion komplexer sein.

> Im Fall der Klassifikation ist es oft wÃ¼nschenswert, Wahrscheinlichkeiten der jeweiligen Klassen als Netzwerkausgabe zu erhalten. Um beliebige Zahlen in Wahrscheinlichkeiten umzuwandeln (z.B. zur Normalisierung der Ausgabe), verwenden wir hÃ¤ufig die **softmax**-Funktion Ïƒ, wodurch die Funktion *f* zu *f(x)=Ïƒ(wx+b)* wird.

In der obigen Definition von *f* werden *w* und *b* als **Parameter** Î¸=âŸ¨*w,b*âŸ© bezeichnet. Gegeben den Datensatz âŸ¨**X**,**Y**âŸ©, kÃ¶nnen wir den Gesamtfehler Ã¼ber den gesamten Datensatz als Funktion der Parameter Î¸ berechnen.

> âœ… **Das Ziel des Trainings eines neuronalen Netzes ist es, den Fehler durch Variation der Parameter Î¸ zu minimieren**

## Gradient Descent Optimierung

Es gibt eine bekannte Methode zur Optimierung von Funktionen, die **Gradient Descent** genannt wird. Die Idee ist, dass wir die Ableitung (im mehrdimensionalen Fall den **Gradienten**) der Loss-Funktion bezÃ¼glich der Parameter berechnen kÃ¶nnen und die Parameter so anpassen, dass der Fehler abnimmt. Formal lÃ¤sst sich das so ausdrÃ¼cken:

* Initialisiere die Parameter mit zufÃ¤lligen Werten w<sup>(0)</sup>, b<sup>(0)</sup>
* Wiederhole den folgenden Schritt viele Male:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup> - Î· âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup> - Î· âˆ‚â„’/âˆ‚b

WÃ¤hrend des Trainings werden die Optimierungsschritte eigentlich unter BerÃ¼cksichtigung des gesamten Datensatzes berechnet (denk daran, dass der Loss als Summe Ã¼ber alle Trainingsbeispiele berechnet wird). In der Praxis nehmen wir jedoch kleine Teilmengen des Datensatzes, sogenannte **Minibatches**, und berechnen die Gradienten nur auf Basis dieser Teilmenge. Da die Teilmenge jedes Mal zufÃ¤llig gewÃ¤hlt wird, nennt man diese Methode **stochastischer Gradient Descent** (SGD).

## Mehrschichtige Perzeptrons und Backpropagation

Ein einlagiges Netzwerk, wie oben gezeigt, kann linear separierbare Klassen klassifizieren. Um ein komplexeres Modell zu bauen, kÃ¶nnen wir mehrere Schichten des Netzwerks kombinieren. Mathematisch bedeutet das, dass die Funktion *f* eine komplexere Form annimmt und in mehreren Schritten berechnet wird:
* z<sub>1</sub> = w<sub>1</sub>x + b<sub>1</sub>
* z<sub>2</sub> = w<sub>2</sub> Î±(z<sub>1</sub>) + b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Hierbei ist Î± eine **nichtlineare Aktivierungsfunktion**, Ïƒ die softmax-Funktion, und die Parameter sind Î¸ = âŸ¨*w<sub>1</sub>, b<sub>1</sub>, w<sub>2</sub>, b<sub>2</sub>*âŸ©.

Der Gradient-Descent-Algorithmus bleibt gleich, aber die Berechnung der Gradienten wird komplexer. Mithilfe der Kettenregel der Differentiation kÃ¶nnen wir die Ableitungen folgendermaÃŸen berechnen:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Die Kettenregel wird verwendet, um die Ableitungen der Loss-Funktion bezÃ¼glich der Parameter zu berechnen.

Beachte, dass der linkeste Teil all dieser AusdrÃ¼cke gleich ist, sodass wir die Ableitungen effektiv ausgehend von der Loss-Funktion â€rÃ¼ckwÃ¤rtsâ€œ durch den Berechnungsgraphen berechnen kÃ¶nnen. Daher wird die Methode zum Training eines mehrschichtigen Perzeptrons **Backpropagation** oder kurz â€Backpropâ€œ genannt.

> TODO: Bildquelle

> âœ… Wir werden Backpropagation in unserem Notebook-Beispiel noch viel ausfÃ¼hrlicher behandeln.

## Fazit

In dieser Lektion haben wir unsere eigene Bibliothek fÃ¼r neuronale Netze erstellt und sie fÃ¼r eine einfache zweidimensionale Klassifikationsaufgabe verwendet.

## ğŸš€ Herausforderung

Im begleitenden Notebook wirst du dein eigenes Framework zum Aufbau und Training mehrschichtiger Perzeptrons implementieren. Du wirst im Detail sehen, wie moderne neuronale Netze funktionieren.

Gehe zum OwnFramework-Notebook und arbeite es durch.

## RÃ¼ckblick & Selbststudium

Backpropagation ist ein gÃ¤ngiger Algorithmus in KI und ML, der es wert ist, genauer studiert zu werden.

## Aufgabe

In diesem Labor sollst du das in dieser Lektion erstellte Framework verwenden, um die Klassifikation handgeschriebener Ziffern aus dem MNIST-Datensatz zu lÃ¶sen.

* Anweisungen
* Notebook

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Ãœbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) Ã¼bersetzt. Obwohl wir uns um Genauigkeit bemÃ¼hen, beachten Sie bitte, dass automatisierte Ãœbersetzungen Fehler oder Ungenauigkeiten enthalten kÃ¶nnen. Das Originaldokument in seiner Ursprungssprache gilt als maÃŸgebliche Quelle. FÃ¼r wichtige Informationen wird eine professionelle menschliche Ãœbersetzung empfohlen. Wir Ã¼bernehmen keine Haftung fÃ¼r MissverstÃ¤ndnisse oder Fehlinterpretationen, die aus der Nutzung dieser Ãœbersetzung entstehen.