{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feinabstimmung von Open AI Modellen\n",
    "\n",
    "Dieses Notebook basiert auf den aktuellen Richtlinien in der [Feinabstimmung](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst) Dokumentation von Open AI.\n",
    "\n",
    "Feinabstimmung verbessert die Leistung von Basismodellen für Ihre Anwendung, indem es mit zusätzlichen Daten und Kontext, die für diesen speziellen Anwendungsfall oder dieses Szenario relevant sind, erneut trainiert wird. Beachten Sie, dass Prompt-Engineering-Techniken wie _few shot learning_ und _retrieval augmented generation_ es Ihnen ermöglichen, den Standard-Prompt mit relevanten Daten zu erweitern, um die Qualität zu verbessern. Diese Ansätze sind jedoch durch die maximale Token-Fenstergröße des Ziel-Basismodells begrenzt.\n",
    "\n",
    "Mit der Feinabstimmung trainieren wir das Modell effektiv selbst mit den erforderlichen Daten neu (was uns erlaubt, viel mehr Beispiele zu verwenden, als in das maximale Token-Fenster passen) – und setzen eine _benutzerdefinierte_ Version des Modells ein, die beim Inferenzzeitpunkt keine Beispiele mehr benötigt. Dies verbessert nicht nur die Effektivität unseres Prompt-Designs (wir haben mehr Flexibilität bei der Nutzung des Token-Fensters für andere Dinge), sondern verbessert potenziell auch unsere Kosten (indem die Anzahl der Tokens, die wir beim Inferenzzeitpunkt an das Modell senden müssen, reduziert wird).\n",
    "\n",
    "Die Feinabstimmung umfasst 4 Schritte:\n",
    "1. Bereiten Sie die Trainingsdaten vor und laden Sie sie hoch.\n",
    "1. Führen Sie den Trainingsjob aus, um ein feinabgestimmtes Modell zu erhalten.\n",
    "1. Bewerten Sie das feinabgestimmte Modell und iterieren Sie zur Qualitätsverbesserung.\n",
    "1. Setzen Sie das feinabgestimmte Modell für die Inferenz ein, wenn Sie zufrieden sind.\n",
    "\n",
    "Beachten Sie, dass nicht alle Basismodelle Feinabstimmung unterstützen – [prüfen Sie die OpenAI-Dokumentation](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned?WT.mc_id=academic-105485-koreyst) für die neuesten Informationen. Sie können auch ein zuvor feinabgestimmtes Modell erneut feinabstimmen. In diesem Tutorial verwenden wir `gpt-35-turbo` als unser Ziel-Basismodell für die Feinabstimmung.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1.1: Bereiten Sie Ihren Datensatz vor\n",
    "\n",
    "Lassen Sie uns einen Chatbot erstellen, der Ihnen hilft, das Periodensystem der Elemente zu verstehen, indem er Fragen zu einem Element mit einem Limerick beantwortet. In _diesem_ einfachen Tutorial erstellen wir nur einen Datensatz, um das Modell mit einigen Beispielantworten zu trainieren, die das erwartete Format der Daten zeigen. In einem realen Anwendungsfall müssten Sie einen Datensatz mit viel mehr Beispielen erstellen. Möglicherweise können Sie auch einen offenen Datensatz (für Ihre Anwendungsdomäne) verwenden, falls ein solcher existiert, und ihn für die Feinabstimmung umformatieren.\n",
    "\n",
    "Da wir uns auf `gpt-35-turbo` konzentrieren und eine Einzelantwort (Chat Completion) suchen, können wir Beispiele mit [diesem vorgeschlagenen Format](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?WT.mc_id=academic-105485-koreyst) erstellen, das den Anforderungen der OpenAI-Chat-Completion entspricht. Wenn Sie mehrstufige Konversationsinhalte erwarten, würden Sie das [Mehrstufige-Beispiel-Format](https://platform.openai.com/docs/guides/fine-tuning/multi-turn-chat-examples?WT.mc_id=academic-105485-koreyst) verwenden, das einen `weight`-Parameter enthält, um anzugeben, welche Nachrichten im Feinabstimmungsprozess verwendet werden sollen (oder nicht).\n",
    "\n",
    "Für unser Tutorial verwenden wir das einfachere Einzelantwort-Format. Die Daten liegen im [jsonl-Format](https://jsonlines.org/?WT.mc_id=academic-105485-koreyst) mit einem Datensatz pro Zeile vor, wobei jeder als JSON-formatiertes Objekt dargestellt wird. Der Ausschnitt unten zeigt 2 Datensätze als Beispiel – siehe [training-data.jsonl](../../../../../18-fine-tuning/python/openai/training-data.jsonl) für den vollständigen Beispielsatz (10 Beispiele), den wir für unser Feinabstimmungstutorial verwenden. **Hinweis:** Jeder Datensatz _muss_ in einer einzigen Zeile definiert sein (nicht über mehrere Zeilen verteilt, wie es in einer formatierten JSON-Datei üblich ist).\n",
    "\n",
    "```json\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Gallium\"}, {\"role\": \"assistant\", \"content\": \"Gallium, oh gallium, so light - Melts in your hand, oh what a sight - At 86 degrees - Its liquid with ease - And in semiconductors, it's out of sight\"}]}\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Hydrogen\"}, {\"role\": \"assistant\", \"content\": \"Hydrogen, the first in the line - The lightest of all, so divine - It's in water, you see - And in stars, it's the key - The universe's most common sign\"}]}\n",
    "```\n",
    "\n",
    "In einem realen Anwendungsfall benötigen Sie eine viel größere Beispielsammlung für gute Ergebnisse – der Kompromiss liegt zwischen der Qualität der Antworten und der Zeit/Kosten für die Feinabstimmung. Wir verwenden eine kleine Sammlung, damit wir die Feinabstimmung schnell abschließen können, um den Prozess zu veranschaulichen. Siehe [dieses OpenAI Cookbook-Beispiel](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst) für ein komplexeres Feinabstimmungstutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Schritt 1.2 Laden Sie Ihren Datensatz hoch\n",
    "\n",
    "Laden Sie die Daten mit der Files API [wie hier beschrieben](https://platform.openai.com/docs/guides/fine-tuning/upload-a-training-file) hoch. Beachten Sie, dass Sie, um diesen Code auszuführen, zuvor die folgenden Schritte durchgeführt haben müssen:\n",
    " - Das Python-Paket `openai` installiert haben (stellen Sie sicher, dass Sie eine Version >=0.28.0 für die neuesten Funktionen verwenden)\n",
    " - Die Umgebungsvariable `OPENAI_API_KEY` auf Ihren OpenAI-API-Schlüssel gesetzt haben\n",
    "Um mehr zu erfahren, siehe die [Setup-Anleitung](./../../../00-course-setup/02-setup-local.md?WT.mc_id=academic-105485-koreyst), die für den Kurs bereitgestellt wird.\n",
    "\n",
    "Führen Sie nun den Code aus, um eine Datei für den Upload aus Ihrer lokalen JSONL-Datei zu erstellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-JdAJcagdOTG6ACNlFWzuzmyV', bytes=4021, created_at=1715566183, filename='training-data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
      "Training File ID: file-JdAJcagdOTG6ACNlFWzuzmyV\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_file = client.files.create(\n",
    "  file=open(\"./training-data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(ft_file)\n",
    "print(\"Training File ID: \" + ft_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Schritt 2.1: Erstellen Sie den Fine-Tuning-Job mit dem SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', created_at=1715566184, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-EZ6ag0n0S6Zm8eV9BSWKmE6l', result_files=[], seed=830529052, status='validating_files', trained_tokens=None, training_file='file-JdAJcagdOTG6ACNlFWzuzmyV', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)\n",
      "Fine-tuning Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_filejob = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file.id, \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "print(ft_filejob)\n",
    "print(\"Fine-tuning Job ID: \" + ft_filejob.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Schritt 2.2: Überprüfen Sie den Status des Jobs\n",
    "\n",
    "Hier sind einige Dinge, die Sie mit der `client.fine_tuning.jobs` API tun können:\n",
    "- `client.fine_tuning.jobs.list(limit=<n>)` - Listet die letzten n Fine-Tuning-Jobs auf\n",
    "- `client.fine_tuning.jobs.retrieve(<job_id>)` - Ruft Details zu einem bestimmten Fine-Tuning-Job ab\n",
    "- `client.fine_tuning.jobs.cancel(<job_id>)` - Bricht einen Fine-Tuning-Job ab\n",
    "- `client.fine_tuning.jobs.list_events(fine_tuning_job_id=<job_id>, limit=<b>)` - Listet bis zu n Ereignisse des Jobs auf\n",
    "- `client.fine_tuning.jobs.create(model=\"gpt-35-turbo\", training_file=\"your-training-file.jsonl\", ...)`\n",
    "\n",
    "Der erste Schritt des Prozesses ist die _Validierung der Trainingsdatei_, um sicherzustellen, dass die Daten im richtigen Format vorliegen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-GkWiDgZmOsuv4q5cSTEGscY6', created_at=1715566184, level='info', message='Validating training file: file-JdAJcagdOTG6ACNlFWzuzmyV', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-3899xdVTO3LN7Q7LkKLMJUnb', created_at=1715566184, level='info', message='Created fine-tuning job: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_filejob.id, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n",
      "Status: running\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "# Once the training data is validated\n",
    "# Track the job status to see if it is running and when it is complete\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(\"Trained Tokens:\", response.trained_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Schritt 2.3: Ereignisse verfolgen, um den Fortschritt zu überwachen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 85/100: training loss=0.14\n",
      "Step 86/100: training loss=0.00\n",
      "Step 87/100: training loss=0.00\n",
      "Step 88/100: training loss=0.07\n",
      "Step 89/100: training loss=0.00\n",
      "Step 90/100: training loss=0.00\n",
      "Step 91/100: training loss=0.00\n",
      "Step 92/100: training loss=0.00\n",
      "Step 93/100: training loss=0.00\n",
      "Step 94/100: training loss=0.00\n",
      "Step 95/100: training loss=0.08\n",
      "Step 96/100: training loss=0.05\n",
      "Step 97/100: training loss=0.00\n",
      "Step 98/100: training loss=0.00\n",
      "Step 99/100: training loss=0.00\n",
      "Step 100/100: training loss=0.00\n",
      "Checkpoint created at step 80 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyyF2:ckpt-step-80\n",
      "Checkpoint created at step 90 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyzhK:ckpt-step-90\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "# You can also track progress in a more granular way by checking for events\n",
    "# Refresh this code till you get the `The job has successfully completed` message\n",
    "response = client.fine_tuning.jobs.list_events(ft_filejob.id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2.4: Status im OpenAI-Dashboard anzeigen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können den Status auch einsehen, indem Sie die OpenAI-Website besuchen und den Abschnitt _Fine-tuning_ der Plattform erkunden. Dort wird Ihnen der Status des aktuellen Jobs angezeigt und Sie können auch die Historie vorheriger Jobausführungen verfolgen. In diesem Screenshot sehen Sie, dass die vorherige Ausführung fehlgeschlagen ist und der zweite Lauf erfolgreich war. Zum Kontext: Dies geschah, als der erste Lauf eine JSON-Datei mit falsch formatierten Datensätzen verwendete – nachdem dies behoben wurde, wurde der zweite Lauf erfolgreich abgeschlossen und das Modell zur Nutzung freigegeben.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/de/fine-tuned-model-status.563271727bf7bfba.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie können die Statusmeldungen und Metriken auch anzeigen, indem Sie im visuellen Dashboard weiter nach unten scrollen, wie gezeigt:\n",
    "\n",
    "| Messages | Metrics |\n",
    "|:---|:---|\n",
    "| ![Messages](../../../../../translated_images/de/fine-tuned-messages-panel.4ed0c2da5ea1313b.webp) |  ![Metrics](../../../../../translated_images/de/fine-tuned-metrics-panel.700d7e4995a65229.webp)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Schritt 3.1: ID abrufen & Feinabgestimmtes Modell im Code testen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the identity of the fine-tuned model once ready\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "fine_tuned_model_id = response.fine_tuned_model\n",
    "print(\"Fine-tuned Model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Strontium, a metal so bright - It's in fireworks, a dazzling sight - It's in bones, you see - And in tea, it's the key - It's the fortieth, so pure, that's the right\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# You can then use that model to generate completions from the SDK as shown\n",
    "# Or you can load that model into the OpenAI Playground (in the UI) to validate it from there.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Elle, a factual chatbot that answers questions about elements in the periodic table with a limerick\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Strontium\"},\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Schritt 3.2: Feinabgestimmtes Modell im Playground laden und testen\n",
    "\n",
    "Sie können das feinabgestimmte Modell jetzt auf zwei Arten testen. Zuerst können Sie den Playground besuchen und im Dropdown-Menü „Models“ Ihr neu feinabgestimmtes Modell aus der Liste auswählen. Die andere Möglichkeit ist die Verwendung der Option „Playground“, die im Fine-tuning-Bereich angezeigt wird (siehe Screenshot oben). Diese startet die folgende _vergleichende_ Ansicht, die die Foundation- und die feinabgestimmte Modellversion nebeneinander für eine schnelle Bewertung anzeigt.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/de/fine-tuned-playground-compare.56e06f0ad8922016.webp)\n",
    "\n",
    "Füllen Sie einfach den im Training verwendeten Systemkontext aus und geben Sie Ihre Testfrage ein. Sie werden feststellen, dass auf beiden Seiten der identische Kontext und die Frage aktualisiert werden. Führen Sie den Vergleich aus und Sie sehen die Unterschiede in den Ausgaben zwischen den Modellen. _Beachten Sie, wie das feinabgestimmte Modell die Antwort im von Ihnen in den Beispielen vorgegebenen Format darstellt, während das Foundation-Modell einfach der Systemaufforderung folgt_.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/de/fine-tuned-playground-launch.5a26495c983c6350.webp)\n",
    "\n",
    "Sie werden feststellen, dass der Vergleich auch die Token-Anzahl für jedes Modell sowie die für die Inferenz benötigte Zeit anzeigt. **Dieses spezielle Beispiel ist ein einfaches, das den Prozess zeigen soll, aber keine reale Datensatz- oder Szenarioabbildung darstellt**. Sie werden bemerken, dass beide Beispiele die gleiche Anzahl an Tokens zeigen (Systemkontext und Benutzeraufforderung sind identisch), wobei das feinabgestimmte Modell mehr Zeit für die Inferenz benötigt (benutzerdefiniertes Modell).\n",
    "\n",
    "In realen Szenarien werden Sie kein Spielzeugbeispiel wie dieses verwenden, sondern gegen echte Daten feinabstimmen (z. B. Produktkatalog für den Kundenservice), bei denen die Qualität der Antwort viel deutlicher wird. In _diesem_ Kontext erfordert das Erreichen einer gleichwertigen Antwortqualität mit dem Foundation-Modell mehr individuelles Prompt-Engineering, was den Token-Verbrauch und potenziell die damit verbundene Verarbeitungszeit für die Inferenz erhöht. _Um dies auszuprobieren, sehen Sie sich die Fine-Tuning-Beispiele im OpenAI Cookbook an, um zu starten._\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Haftungsausschluss**:  \nDieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ursprungssprache gilt als maßgebliche Quelle. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die aus der Nutzung dieser Übersetzung entstehen.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "1725725c956564056baf895e6ca92aa5",
   "translation_date": "2025-12-19T08:46:30+00:00",
   "source_file": "18-fine-tuning/python/openai/oai-assignment.ipynb",
   "language_code": "de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}