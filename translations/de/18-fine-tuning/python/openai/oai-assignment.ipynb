{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feinabstimmung von Open AI Modellen\n",
    "\n",
    "Dieses Notebook basiert auf den aktuellen Empfehlungen in der [Fine Tuning](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst) Dokumentation von Open AI.\n",
    "\n",
    "Die Feinabstimmung verbessert die Leistung von Foundation-Modellen für Ihre Anwendung, indem das Modell mit zusätzlichen, für den jeweiligen Anwendungsfall oder das Szenario relevanten Daten und Kontexten nachtrainiert wird. Beachten Sie, dass Techniken wie _Few Shot Learning_ und _Retrieval Augmented Generation_ es ermöglichen, den Standard-Prompt mit relevanten Daten zu erweitern, um die Qualität zu steigern. Diese Ansätze sind jedoch durch die maximale Token-Fenstergröße des jeweiligen Foundation-Modells begrenzt.\n",
    "\n",
    "Mit der Feinabstimmung trainieren wir das Modell selbst mit den benötigten Daten nach (wodurch wir viel mehr Beispiele verwenden können, als in das maximale Token-Fenster passen) – und stellen eine _individuelle_ Version des Modells bereit, die zur Inferenzzeit keine Beispiele mehr benötigt. Das verbessert nicht nur die Effektivität unseres Prompt-Designs (wir haben mehr Flexibilität, das Token-Fenster für andere Dinge zu nutzen), sondern kann auch unsere Kosten senken (da wir zur Inferenzzeit weniger Tokens an das Modell senden müssen).\n",
    "\n",
    "Die Feinabstimmung besteht aus 4 Schritten:\n",
    "1. Die Trainingsdaten vorbereiten und hochladen.\n",
    "1. Den Trainingsjob ausführen, um ein feinabgestimmtes Modell zu erhalten.\n",
    "1. Das feinabgestimmte Modell evaluieren und für Qualität iterieren.\n",
    "1. Das feinabgestimmte Modell für die Inferenz bereitstellen, sobald Sie zufrieden sind.\n",
    "\n",
    "Beachten Sie, dass nicht alle Foundation-Modelle die Feinabstimmung unterstützen – [prüfen Sie die OpenAI-Dokumentation](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned?WT.mc_id=academic-105485-koreyst) für aktuelle Informationen. Sie können auch ein bereits feinabgestimmtes Modell weiter feinabstimmen. In diesem Tutorial verwenden wir `gpt-35-turbo` als unser Ziel-Foundation-Modell für die Feinabstimmung.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1.1: Bereite dein Datenset vor\n",
    "\n",
    "Wir bauen einen Chatbot, der dir hilft, das Periodensystem der Elemente zu verstehen, indem er Fragen zu einem Element mit einem Limerick beantwortet. In _diesem_ einfachen Tutorial erstellen wir nur ein Datenset, um das Modell mit einigen Beispielantworten zu trainieren, die das erwartete Format der Daten zeigen. In einem echten Anwendungsfall müsstest du ein Datenset mit deutlich mehr Beispielen erstellen. Möglicherweise kannst du auch ein offenes Datenset (für deinen Anwendungsbereich) verwenden, falls eines existiert, und es für das Fine-Tuning entsprechend umformatieren.\n",
    "\n",
    "Da wir uns auf `gpt-35-turbo` konzentrieren und eine Einzelantwort (Chat Completion) erwarten, können wir Beispiele mit [diesem vorgeschlagenen Format](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?WT.mc_id=academic-105485-koreyst) erstellen, das die Anforderungen für OpenAI Chat Completions widerspiegelt. Wenn du mehrstufige Konversationen erwartest, solltest du das [Multi-Turn-Beispielformat](https://platform.openai.com/docs/guides/fine-tuning/multi-turn-chat-examples?WT.mc_id=academic-105485-koreyst) verwenden, das einen `weight`-Parameter enthält, um anzugeben, welche Nachrichten im Fine-Tuning-Prozess verwendet werden sollen (oder nicht).\n",
    "\n",
    "Für unser Tutorial nutzen wir das einfachere Einzelantwort-Format. Die Daten liegen im [jsonl-Format](https://jsonlines.org/?WT.mc_id=academic-105485-koreyst) vor, mit jeweils einem Datensatz pro Zeile, wobei jeder als JSON-Objekt formatiert ist. Der folgende Ausschnitt zeigt 2 Datensätze als Beispiel – siehe [training-data.jsonl](../../../../../18-fine-tuning/python/openai/training-data.jsonl) für das vollständige Beispielset (10 Beispiele), das wir für unser Fine-Tuning-Tutorial verwenden. **Hinweis:** Jeder Datensatz _muss_ in einer einzigen Zeile definiert sein (nicht über mehrere Zeilen verteilt, wie es bei formatierten JSON-Dateien üblich ist)\n",
    "\n",
    "```json\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Gallium\"}, {\"role\": \"assistant\", \"content\": \"Gallium, oh gallium, so light - Melts in your hand, oh what a sight - At 86 degrees - Its liquid with ease - And in semiconductors, it's out of sight\"}]}\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Hydrogen\"}, {\"role\": \"assistant\", \"content\": \"Hydrogen, the first in the line - The lightest of all, so divine - It's in water, you see - And in stars, it's the key - The universe's most common sign\"}]}\n",
    "```\n",
    "\n",
    "In einem echten Anwendungsfall benötigst du für gute Ergebnisse eine deutlich größere Beispielsammlung – hier gibt es einen Kompromiss zwischen der Qualität der Antworten und dem Zeit-/Kostenaufwand für das Fine-Tuning. Wir verwenden ein kleines Set, damit wir das Fine-Tuning schnell abschließen und den Prozess veranschaulichen können. Sieh dir [dieses Beispiel aus dem OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst) für ein komplexeres Fine-Tuning-Tutorial an.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1.2 Laden Sie Ihr Datenset hoch\n",
    "\n",
    "Laden Sie die Daten mit der Files API [wie hier beschrieben](https://platform.openai.com/docs/guides/fine-tuning/upload-a-training-file) hoch. Beachten Sie, dass Sie, um diesen Code auszuführen, zuvor folgende Schritte erledigt haben müssen:\n",
    " - Das `openai` Python-Paket installiert haben (stellen Sie sicher, dass Sie eine Version >=0.28.0 für die neuesten Funktionen verwenden)\n",
    " - Die Umgebungsvariable `OPENAI_API_KEY` mit Ihrem OpenAI API-Schlüssel gesetzt haben\n",
    "Weitere Informationen finden Sie im [Setup-Leitfaden](./../../../00-course-setup/02-setup-local.md?WT.mc_id=academic-105485-koreyst), der für den Kurs bereitgestellt wurde.\n",
    "\n",
    "Führen Sie nun den Code aus, um aus Ihrer lokalen JSONL-Datei eine Datei zum Hochladen zu erstellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-JdAJcagdOTG6ACNlFWzuzmyV', bytes=4021, created_at=1715566183, filename='training-data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
      "Training File ID: file-JdAJcagdOTG6ACNlFWzuzmyV\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_file = client.files.create(\n",
    "  file=open(\"./training-data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(ft_file)\n",
    "print(\"Training File ID: \" + ft_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2.1: Erstellen Sie den Fine-Tuning-Job mit dem SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', created_at=1715566184, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-EZ6ag0n0S6Zm8eV9BSWKmE6l', result_files=[], seed=830529052, status='validating_files', trained_tokens=None, training_file='file-JdAJcagdOTG6ACNlFWzuzmyV', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)\n",
      "Fine-tuning Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_filejob = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file.id, \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "print(ft_filejob)\n",
    "print(\"Fine-tuning Job ID: \" + ft_filejob.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2.2: Überprüfe den Status des Jobs\n",
    "\n",
    "Hier sind einige Dinge, die du mit der `client.fine_tuning.jobs` API machen kannst:\n",
    "- `client.fine_tuning.jobs.list(limit=<n>)` – Zeigt die letzten n Fine-Tuning-Jobs an\n",
    "- `client.fine_tuning.jobs.retrieve(<job_id>)` – Zeigt Details zu einem bestimmten Fine-Tuning-Job an\n",
    "- `client.fine_tuning.jobs.cancel(<job_id>)` – Bricht einen Fine-Tuning-Job ab\n",
    "- `client.fine_tuning.jobs.list_events(fine_tuning_job_id=<job_id>, limit=<b>)` – Zeigt bis zu n Ereignisse des Jobs an\n",
    "- `client.fine_tuning.jobs.create(model=\"gpt-35-turbo\", training_file=\"your-training-file.jsonl\", ...)`\n",
    "\n",
    "Der erste Schritt im Prozess ist _die Validierung der Trainingsdatei_, um sicherzustellen, dass die Daten im richtigen Format vorliegen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-GkWiDgZmOsuv4q5cSTEGscY6', created_at=1715566184, level='info', message='Validating training file: file-JdAJcagdOTG6ACNlFWzuzmyV', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-3899xdVTO3LN7Q7LkKLMJUnb', created_at=1715566184, level='info', message='Created fine-tuning job: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_filejob.id, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n",
      "Status: running\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "# Once the training data is validated\n",
    "# Track the job status to see if it is running and when it is complete\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(\"Trained Tokens:\", response.trained_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2.3: Ereignisse verfolgen, um den Fortschritt zu überwachen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 85/100: training loss=0.14\n",
      "Step 86/100: training loss=0.00\n",
      "Step 87/100: training loss=0.00\n",
      "Step 88/100: training loss=0.07\n",
      "Step 89/100: training loss=0.00\n",
      "Step 90/100: training loss=0.00\n",
      "Step 91/100: training loss=0.00\n",
      "Step 92/100: training loss=0.00\n",
      "Step 93/100: training loss=0.00\n",
      "Step 94/100: training loss=0.00\n",
      "Step 95/100: training loss=0.08\n",
      "Step 96/100: training loss=0.05\n",
      "Step 97/100: training loss=0.00\n",
      "Step 98/100: training loss=0.00\n",
      "Step 99/100: training loss=0.00\n",
      "Step 100/100: training loss=0.00\n",
      "Checkpoint created at step 80 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyyF2:ckpt-step-80\n",
      "Checkpoint created at step 90 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyzhK:ckpt-step-90\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "# You can also track progress in a more granular way by checking for events\n",
    "# Refresh this code till you get the `The job has successfully completed` message\n",
    "response = client.fine_tuning.jobs.list_events(ft_filejob.id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2.4: Status im OpenAI-Dashboard anzeigen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kannst den Status auch einsehen, indem du die OpenAI-Website besuchst und im Bereich _Fine-tuning_ der Plattform nachschaust. Dort wird dir der Status des aktuellen Jobs angezeigt, und du kannst auch die Historie vorheriger Ausführungen verfolgen. In diesem Screenshot siehst du, dass die vorherige Ausführung fehlgeschlagen ist und der zweite Durchlauf erfolgreich war. Zur Erklärung: Dies geschah, weil beim ersten Durchlauf eine JSON-Datei mit falsch formatierten Einträgen verwendet wurde – nachdem das behoben war, wurde der zweite Durchlauf erfolgreich abgeschlossen und das Modell stand zur Nutzung bereit.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fine-tuned-model-status.563271727bf7bfba7e3f73a201f8712fae3cea1c08f7c7f12ca469c06d234122.de.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kannst die Statusmeldungen und Kennzahlen auch weiter unten im visuellen Dashboard sehen, wie hier gezeigt:\n",
    "\n",
    "| Nachrichten | Kennzahlen |\n",
    "|:---|:---|\n",
    "| ![Nachrichten](../../../../../translated_images/fine-tuned-messages-panel.4ed0c2da5ea1313b3a706a66f66bf5007c379cd9219cfb74cb30c0b04b90c4c8.de.png) |  ![Kennzahlen](../../../../../translated_images/fine-tuned-metrics-panel.700d7e4995a652299584ab181536a6cfb67691a897a518b6c7a2aa0a17f1a30d.de.png)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 3.1: ID abrufen & Feinabgestimmtes Modell im Code testen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the identity of the fine-tuned model once ready\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "fine_tuned_model_id = response.fine_tuned_model\n",
    "print(\"Fine-tuned Model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Strontium, a metal so bright - It's in fireworks, a dazzling sight - It's in bones, you see - And in tea, it's the key - It's the fortieth, so pure, that's the right\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# You can then use that model to generate completions from the SDK as shown\n",
    "# Or you can load that model into the OpenAI Playground (in the UI) to validate it from there.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Elle, a factual chatbot that answers questions about elements in the periodic table with a limerick\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Strontium\"},\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 3.2: Feinabgestimmtes Modell im Playground laden & testen\n",
    "\n",
    "Du kannst das feinabgestimmte Modell jetzt auf zwei Arten testen. Zum einen kannst du den Playground besuchen und im Dropdown-Menü „Models“ dein neu feinabgestimmtes Modell aus den aufgelisteten Optionen auswählen. Die andere Möglichkeit ist, die Option „Playground“ im Fine-tuning-Panel zu nutzen (siehe Screenshot oben). Damit öffnet sich die folgende _Vergleichsansicht_, in der die Foundation- und die feinabgestimmte Modellversion nebeneinander angezeigt werden, um sie schnell zu vergleichen.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fine-tuned-playground-compare.56e06f0ad8922016497d39ced3d84ea296eec89073503f2bf346ec9718f913b5.de.png)\n",
    "\n",
    "Trage einfach den Systemkontext ein, den du in deinen Trainingsdaten verwendet hast, und gib deine Testfrage ein. Du wirst sehen, dass auf beiden Seiten der identische Kontext und die gleiche Frage übernommen werden. Starte den Vergleich und du erkennst die Unterschiede in den Ausgaben der beiden Modelle. _Beachte, wie das feinabgestimmte Modell die Antwort im von dir vorgegebenen Format ausgibt, während das Foundation-Modell einfach dem System-Prompt folgt_.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fine-tuned-playground-launch.5a26495c983c6350c227e05700a47a89002d132949a56fa4ff37f266ebe997b2.de.png)\n",
    "\n",
    "Dir wird auffallen, dass der Vergleich auch die Token-Anzahl für jedes Modell sowie die benötigte Zeit für die Inferenz anzeigt. **Dieses spezielle Beispiel ist sehr einfach gehalten, um den Ablauf zu zeigen, spiegelt aber kein echtes Datenset oder Szenario wider**. Es kann sein, dass beide Beispiele die gleiche Token-Anzahl zeigen (da Systemkontext und Nutzer-Prompt identisch sind), das feinabgestimmte Modell aber mehr Zeit für die Inferenz benötigt (Custom-Modell).\n",
    "\n",
    "In der Praxis wirst du kein so einfaches Beispiel verwenden, sondern mit echten Daten feinabstimmen (z. B. Produktkatalog für den Kundenservice), wobei die Qualität der Antworten viel deutlicher wird. In _diesem_ Zusammenhang ist es mit dem Foundation-Modell aufwendiger, eine vergleichbare Antwortqualität zu erreichen – dazu ist mehr individuelles Prompt-Engineering nötig, was den Token-Verbrauch und möglicherweise auch die Verarbeitungszeit für die Inferenz erhöht. _Um das auszuprobieren, schau dir die Fine-tuning-Beispiele im OpenAI Cookbook an._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Haftungsausschluss**:  \nDieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ausgangssprache gilt als maßgebliche Quelle. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "69b527f1e605a10fb9c7e00ae841021d",
   "translation_date": "2025-08-25T21:29:41+00:00",
   "source_file": "18-fine-tuning/python/openai/oai-assignment.ipynb",
   "language_code": "de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}