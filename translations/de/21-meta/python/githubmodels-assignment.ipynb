{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbeiten mit den Meta-Familienmodellen\n",
    "\n",
    "## Einführung\n",
    "\n",
    "In dieser Lektion behandeln wir:\n",
    "\n",
    "- Die beiden Hauptmodelle der Meta-Familie – Llama 3.1 und Llama 3.2\n",
    "- Die Anwendungsfälle und Einsatzszenarien für jedes Modell\n",
    "- Ein Codebeispiel, das die besonderen Merkmale jedes Modells zeigt\n",
    "\n",
    "## Die Meta-Modellfamilie\n",
    "\n",
    "In dieser Lektion schauen wir uns zwei Modelle aus der Meta-Familie oder dem „Llama Herd“ an – Llama 3.1 und Llama 3.2\n",
    "\n",
    "Diese Modelle gibt es in verschiedenen Varianten und sie sind auf dem Github Model Marketplace verfügbar. Weitere Informationen zur Nutzung von Github Models zum [Prototyping mit KI-Modellen](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) findest du hier.\n",
    "\n",
    "Modellvarianten:\n",
    "- Llama 3.1 – 70B Instruct\n",
    "- Llama 3.1 – 405B Instruct\n",
    "- Llama 3.2 – 11B Vision Instruct\n",
    "- Llama 3.2 – 90B Vision Instruct\n",
    "\n",
    "*Hinweis: Llama 3 ist ebenfalls auf Github Models verfügbar, wird aber in dieser Lektion nicht behandelt*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Mit 405 Milliarden Parametern gehört Llama 3.1 zur Kategorie der Open-Source-LLMs.\n",
    "\n",
    "Das Modell ist ein Upgrade zur früheren Version Llama 3 und bietet:\n",
    "\n",
    "- Größeres Kontextfenster – 128k Token statt 8k Token\n",
    "- Höhere maximale Ausgabetoken – 4096 statt 2048\n",
    "- Bessere mehrsprachige Unterstützung – durch die erhöhte Anzahl an Trainingstoken\n",
    "\n",
    "Dadurch kann Llama 3.1 komplexere Anwendungsfälle bei der Entwicklung von GenAI-Anwendungen bewältigen, darunter:\n",
    "- Native Function Calling – die Möglichkeit, externe Tools und Funktionen außerhalb des LLM-Workflows aufzurufen\n",
    "- Bessere RAG-Performance – dank des größeren Kontextfensters\n",
    "- Generierung synthetischer Daten – die Fähigkeit, effektive Daten für Aufgaben wie das Finetuning zu erstellen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Function Calling\n",
    "\n",
    "Llama 3.1 wurde darauf optimiert, Funktionen oder Tools effektiver aufzurufen. Das Modell verfügt außerdem über zwei integrierte Tools, die es je nach Nutzeranfrage als notwendig erkennen kann. Diese Tools sind:\n",
    "\n",
    "- **Brave Search** – Kann verwendet werden, um aktuelle Informationen wie das Wetter über eine Websuche zu erhalten\n",
    "- **Wolfram Alpha** – Kann für komplexere mathematische Berechnungen genutzt werden, sodass eigene Funktionen nicht nötig sind.\n",
    "\n",
    "Du kannst auch eigene benutzerdefinierte Tools erstellen, die das LLM aufrufen kann.\n",
    "\n",
    "Im folgenden Codebeispiel:\n",
    "\n",
    "- Definieren wir die verfügbaren Tools (brave_search, wolfram_alpha) im System-Prompt.\n",
    "- Senden eine Nutzeranfrage, die nach dem Wetter in einer bestimmten Stadt fragt.\n",
    "- Das LLM antwortet mit einem Tool-Aufruf für Brave Search, der so aussieht: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Hinweis: Dieses Beispiel führt nur den Tool-Aufruf aus. Wenn du die Ergebnisse erhalten möchtest, musst du ein kostenloses Konto auf der Brave API-Seite erstellen und die Funktion selbst definieren.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Trotz der Tatsache, dass Llama 3.1 ein LLM ist, hat es eine Einschränkung im Bereich der Multimodalität. Das bedeutet, dass es nicht in der Lage ist, verschiedene Eingabetypen wie Bilder als Prompts zu verwenden und darauf zu antworten. Diese Fähigkeit ist eines der Hauptmerkmale von Llama 3.2. Zu diesen Funktionen gehören außerdem:\n",
    "\n",
    "- Multimodalität – kann sowohl Text- als auch Bild-Prompts auswerten\n",
    "- Varianten in kleiner bis mittlerer Größe (11B und 90B) – dies ermöglicht flexible Einsatzmöglichkeiten,\n",
    "- Nur-Text-Varianten (1B und 3B) – dadurch kann das Modell auf Edge- oder Mobilgeräten eingesetzt werden und bietet geringe Latenz\n",
    "\n",
    "Die multimodale Unterstützung stellt einen großen Fortschritt in der Welt der Open-Source-Modelle dar. Das folgende Codebeispiel nimmt sowohl ein Bild als auch einen Text-Prompt, um eine Analyse des Bildes von Llama 3.2 90B zu erhalten.\n",
    "\n",
    "### Multimodale Unterstützung mit Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lernen hört hier nicht auf, setze deine Reise fort\n",
    "\n",
    "Nachdem du diese Lektion abgeschlossen hast, schau dir unsere [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) an, um dein Wissen über Generative KI weiter auszubauen!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Haftungsausschluss**:  \nDieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ausgangssprache gilt als maßgebliche Quelle. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:36:21+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}