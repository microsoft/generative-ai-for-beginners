<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6b7629b8ee4d7d874a27213e903d86a7",
  "translation_date": "2025-10-17T22:56:27+00:00",
  "source_file": "02-exploring-and-comparing-different-llms/README.md",
  "language_code": "de"
}
-->
# Erkundung und Vergleich verschiedener LLMs

[![Erkundung und Vergleich verschiedener LLMs](../../../translated_images/02-lesson-banner.ef94c84979f97f60f07e27d905e708cbcbdf78707120553ccab27d91c947805b.de.png)](https://youtu.be/KIRUeDKscfI?si=8BHX1zvwzQBn-PlK)

> _Klicken Sie auf das Bild oben, um das Video zu dieser Lektion anzusehen_

In der vorherigen Lektion haben wir gesehen, wie Generative KI die Technologielandschaft ver√§ndert, wie Large Language Models (LLMs) funktionieren und wie ein Unternehmen ‚Äì wie unser Startup ‚Äì sie f√ºr seine Anwendungsf√§lle nutzen und wachsen kann! In diesem Kapitel wollen wir verschiedene Arten von gro√üen Sprachmodellen (LLMs) vergleichen, um ihre Vor- und Nachteile zu verstehen.

Der n√§chste Schritt auf der Reise unseres Startups besteht darin, die aktuelle Landschaft der LLMs zu erkunden und zu verstehen, welche f√ºr unseren Anwendungsfall geeignet sind.

## Einf√ºhrung

Diese Lektion behandelt:

- Verschiedene Arten von LLMs in der aktuellen Landschaft.
- Testen, Iterieren und Vergleichen verschiedener Modelle f√ºr Ihren Anwendungsfall in Azure.
- Wie man ein LLM bereitstellt.

## Lernziele

Nach Abschluss dieser Lektion k√∂nnen Sie:

- Das richtige Modell f√ºr Ihren Anwendungsfall ausw√§hlen.
- Verstehen, wie Sie die Leistung Ihres Modells testen, iterieren und verbessern k√∂nnen.
- Wissen, wie Unternehmen Modelle bereitstellen.

## Verschiedene Arten von LLMs verstehen

LLMs k√∂nnen basierend auf ihrer Architektur, ihren Trainingsdaten und ihrem Anwendungsfall in verschiedene Kategorien eingeteilt werden. Das Verst√§ndnis dieser Unterschiede hilft unserem Startup, das richtige Modell f√ºr das Szenario auszuw√§hlen und zu verstehen, wie man testet, iteriert und die Leistung verbessert.

Es gibt viele verschiedene Arten von LLM-Modellen, und Ihre Wahl h√§ngt davon ab, wof√ºr Sie sie verwenden m√∂chten, welche Daten Sie haben, wie viel Sie bereit sind zu zahlen und mehr.

Je nachdem, ob Sie die Modelle f√ºr Text-, Audio-, Video-, Bildgenerierung usw. verwenden m√∂chten, k√∂nnten Sie sich f√ºr eine andere Art von Modell entscheiden.

- **Audio- und Spracherkennung**. F√ºr diesen Zweck sind Modelle vom Typ Whisper eine ausgezeichnete Wahl, da sie universell einsetzbar und auf Spracherkennung ausgerichtet sind. Sie sind auf vielf√§ltige Audiodaten trainiert und k√∂nnen mehrsprachige Spracherkennung durchf√ºhren. Erfahren Sie mehr √ºber [Whisper-Modelle hier](https://platform.openai.com/docs/models/whisper?WT.mc_id=academic-105485-koreyst).

- **Bildgenerierung**. F√ºr die Bildgenerierung sind DALL-E und Midjourney zwei sehr bekannte Optionen. DALL-E wird von Azure OpenAI angeboten. [Lesen Sie hier mehr √ºber DALL-E](https://platform.openai.com/docs/models/dall-e?WT.mc_id=academic-105485-koreyst) und auch in Kapitel 9 dieses Lehrplans.

- **Textgenerierung**. Die meisten Modelle sind auf Textgenerierung trainiert, und es gibt eine gro√üe Auswahl von GPT-3.5 bis GPT-4. Sie haben unterschiedliche Kosten, wobei GPT-4 am teuersten ist. Es lohnt sich, einen Blick auf den [Azure OpenAI Playground](https://oai.azure.com/portal/playground?WT.mc_id=academic-105485-koreyst) zu werfen, um zu beurteilen, welche Modelle am besten zu Ihren Anforderungen in Bezug auf F√§higkeiten und Kosten passen.

- **Multimodalit√§t**. Wenn Sie mehrere Arten von Daten in Eingabe und Ausgabe verarbeiten m√∂chten, sollten Sie Modelle wie [gpt-4 turbo mit Vision oder gpt-4o](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models?WT.mc_id=academic-105485-koreyst) in Betracht ziehen ‚Äì die neuesten Ver√∂ffentlichungen von OpenAI-Modellen ‚Äì die in der Lage sind, nat√ºrliche Sprachverarbeitung mit visueller Wahrnehmung zu kombinieren und Interaktionen √ºber multimodale Schnittstellen zu erm√∂glichen.

Die Auswahl eines Modells bedeutet, dass Sie einige grundlegende Funktionen erhalten, die jedoch m√∂glicherweise nicht ausreichen. Oft haben Sie unternehmensspezifische Daten, die Sie dem LLM irgendwie mitteilen m√ºssen. Es gibt verschiedene Ans√§tze, wie man dies angehen kann, mehr dazu in den kommenden Abschnitten.

### Foundation Models versus LLMs

Der Begriff Foundation Model wurde von [Stanford-Forschern gepr√§gt](https://arxiv.org/abs/2108.07258?WT.mc_id=academic-105485-koreyst) und als ein KI-Modell definiert, das bestimmte Kriterien erf√ºllt, wie:

- **Sie werden mit un√ºberwachtem Lernen oder selbst√ºberwachtem Lernen trainiert**, was bedeutet, dass sie auf nicht gekennzeichneten multimodalen Daten trainiert werden und keine menschliche Annotation oder Kennzeichnung von Daten f√ºr ihren Trainingsprozess ben√∂tigen.
- **Es handelt sich um sehr gro√üe Modelle**, die auf sehr tiefen neuronalen Netzwerken basieren und auf Milliarden von Parametern trainiert werden.
- **Sie sind normalerweise dazu gedacht, als ‚ÄûGrundlage‚Äú f√ºr andere Modelle zu dienen**, was bedeutet, dass sie als Ausgangspunkt f√ºr andere Modelle verwendet werden k√∂nnen, die darauf aufbauen, was durch Feinabstimmung erfolgen kann.

![Foundation Models versus LLMs](../../../translated_images/FoundationModel.e4859dbb7a825c94b284f17eae1c186aabc21d4d8644331f5b007d809cf8d0f2.de.png)

Bildquelle: [Essential Guide to Foundation Models and Large Language Models | von Babar M Bhatti | Medium
](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404)

Um diesen Unterschied weiter zu verdeutlichen, nehmen wir ChatGPT als Beispiel. Um die erste Version von ChatGPT zu erstellen, diente ein Modell namens GPT-3.5 als Foundation Model. Das bedeutet, dass OpenAI einige chat-spezifische Daten verwendet hat, um eine abgestimmte Version von GPT-3.5 zu erstellen, die darauf spezialisiert ist, in Konversationsszenarien wie Chatbots gut zu funktionieren.

![Foundation Model](../../../translated_images/Multimodal.2c389c6439e0fc51b0b7b226d95d7d900d372ae66902d71b8ce5ec4951b8efbe.de.png)

Bildquelle: [2108.07258.pdf (arxiv.org)](https://arxiv.org/pdf/2108.07258.pdf?WT.mc_id=academic-105485-koreyst)

### Open Source versus Propriet√§re Modelle

Eine weitere M√∂glichkeit, LLMs zu kategorisieren, ist, ob sie Open Source oder propriet√§r sind.

Open-Source-Modelle sind Modelle, die der √ñffentlichkeit zug√§nglich gemacht werden und von jedem genutzt werden k√∂nnen. Sie werden oft von dem Unternehmen, das sie erstellt hat, oder von der Forschungsgemeinschaft bereitgestellt. Diese Modelle k√∂nnen inspiziert, modifiziert und f√ºr verschiedene Anwendungsf√§lle angepasst werden. Sie sind jedoch nicht immer f√ºr den Produktionseinsatz optimiert und m√∂glicherweise nicht so leistungsf√§hig wie propriet√§re Modelle. Au√üerdem kann die Finanzierung f√ºr Open-Source-Modelle begrenzt sein, und sie werden m√∂glicherweise nicht langfristig gepflegt oder mit den neuesten Forschungsergebnissen aktualisiert. Beispiele f√ºr beliebte Open-Source-Modelle sind [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html?WT.mc_id=academic-105485-koreyst), [Bloom](https://huggingface.co/bigscience/bloom) und [LLaMA](https://llama.meta.com).

Propriet√§re Modelle sind Modelle, die einem Unternehmen geh√∂ren und der √ñffentlichkeit nicht zug√§nglich gemacht werden. Diese Modelle sind oft f√ºr den Produktionseinsatz optimiert. Sie d√ºrfen jedoch nicht inspiziert, modifiziert oder f√ºr verschiedene Anwendungsf√§lle angepasst werden. Au√üerdem sind sie nicht immer kostenlos verf√ºgbar und erfordern m√∂glicherweise ein Abonnement oder eine Zahlung f√ºr die Nutzung. Nutzer haben auch keine Kontrolle √ºber die Daten, die zum Trainieren des Modells verwendet werden, was bedeutet, dass sie dem Modellbesitzer vertrauen m√ºssen, dass er sich zur Einhaltung des Datenschutzes und der verantwortungsvollen Nutzung von KI verpflichtet. Beispiele f√ºr beliebte propriet√§re Modelle sind [OpenAI-Modelle](https://platform.openai.com/docs/models/overview?WT.mc_id=academic-105485-koreyst), [Google Bard](https://sapling.ai/llm/bard?WT.mc_id=academic-105485-koreyst) oder [Claude 2](https://www.anthropic.com/index/claude-2?WT.mc_id=academic-105485-koreyst).

### Embedding versus Bildgenerierung versus Text- und Codegenerierung

LLMs k√∂nnen auch nach dem Output kategorisiert werden, den sie generieren.

Embeddings sind eine Gruppe von Modellen, die Text in eine numerische Form umwandeln k√∂nnen, die als Embedding bezeichnet wird und eine numerische Darstellung des Eingabetextes darstellt. Embeddings erleichtern Maschinen das Verst√§ndnis der Beziehungen zwischen W√∂rtern oder S√§tzen und k√∂nnen als Eingaben von anderen Modellen wie Klassifikationsmodellen oder Clustermodellen verwendet werden, die bei numerischen Daten eine bessere Leistung erzielen. Embedding-Modelle werden h√§ufig f√ºr Transfer-Learning verwendet, bei dem ein Modell f√ºr eine Ersatzaufgabe erstellt wird, f√ºr die es eine F√ºlle von Daten gibt, und dann die Modellgewichte (Embeddings) f√ºr andere nachgelagerte Aufgaben wiederverwendet werden. Ein Beispiel f√ºr diese Kategorie ist [OpenAI Embeddings](https://platform.openai.com/docs/models/embeddings?WT.mc_id=academic-105485-koreyst).

![Embedding](../../../translated_images/Embedding.c3708fe988ccf76073d348483dbb7569f622211104f073e22e43106075c04800.de.png)

Bildgenerierungsmodelle sind Modelle, die Bilder generieren. Diese Modelle werden h√§ufig f√ºr Bildbearbeitung, Bildsynthese und Bild√ºbersetzung verwendet. Bildgenerierungsmodelle werden oft auf gro√üen Bilddatens√§tzen wie [LAION-5B](https://laion.ai/blog/laion-5b/?WT.mc_id=academic-105485-koreyst) trainiert und k√∂nnen verwendet werden, um neue Bilder zu generieren oder bestehende Bilder mit Techniken wie Inpainting, Super-Resolution und Kolorierung zu bearbeiten. Beispiele sind [DALL-E-3](https://openai.com/dall-e-3?WT.mc_id=academic-105485-koreyst) und [Stable Diffusion Modelle](https://github.com/Stability-AI/StableDiffusion?WT.mc_id=academic-105485-koreyst).

![Bildgenerierung](../../../translated_images/Image.349c080266a763fd255b840a921cd8fc526ed78dc58708fa569ff1873d302345.de.png)

Text- und Codegenerierungsmodelle sind Modelle, die Text oder Code generieren. Diese Modelle werden h√§ufig f√ºr Textzusammenfassungen, √úbersetzungen und Fragebeantwortung verwendet. Textgenerierungsmodelle werden oft auf gro√üen Textdatens√§tzen wie [BookCorpus](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html?WT.mc_id=academic-105485-koreyst) trainiert und k√∂nnen verwendet werden, um neuen Text zu generieren oder Fragen zu beantworten. Codegenerierungsmodelle wie [CodeParrot](https://huggingface.co/codeparrot?WT.mc_id=academic-105485-koreyst) werden oft auf gro√üen Code-Datens√§tzen wie GitHub trainiert und k√∂nnen verwendet werden, um neuen Code zu generieren oder Fehler in bestehendem Code zu beheben.

![Text- und Codegenerierung](../../../translated_images/Text.a8c0cf139e5cc2a0cd3edaba8d675103774e6ddcb3c9fc5a98bb17c9a450e31d.de.png)

### Encoder-Decoder versus Decoder-only

Um die verschiedenen Arten von Architekturen von LLMs zu erkl√§ren, verwenden wir eine Analogie.

Stellen Sie sich vor, Ihr Vorgesetzter gibt Ihnen die Aufgabe, ein Quiz f√ºr die Sch√ºler zu erstellen. Sie haben zwei Kollegen; einer ist f√ºr die Erstellung des Inhalts zust√§ndig und der andere f√ºr die √úberpr√ºfung.

Der Inhaltsersteller ist wie ein Decoder-only-Modell, er kann sich das Thema ansehen und sehen, was Sie bereits geschrieben haben, und dann basierend darauf einen Kurs schreiben. Sie sind sehr gut darin, ansprechende und informative Inhalte zu schreiben, aber sie sind nicht sehr gut darin, das Thema und die Lernziele zu verstehen. Einige Beispiele f√ºr Decoder-Modelle sind die GPT-Familie, wie GPT-3.

Der Pr√ºfer ist wie ein Encoder-only-Modell, er schaut sich den geschriebenen Kurs und die Antworten an, erkennt die Beziehung zwischen ihnen und versteht den Kontext, aber er ist nicht gut darin, Inhalte zu generieren. Ein Beispiel f√ºr ein Encoder-only-Modell w√§re BERT.

Stellen Sie sich vor, wir h√§tten jemanden, der sowohl das Quiz erstellen als auch √ºberpr√ºfen k√∂nnte, das w√§re ein Encoder-Decoder-Modell. Einige Beispiele w√§ren BART und T5.

### Service versus Modell

Nun sprechen wir √ºber den Unterschied zwischen einem Service und einem Modell. Ein Service ist ein Produkt, das von einem Cloud-Service-Anbieter angeboten wird und oft eine Kombination aus Modellen, Daten und anderen Komponenten ist. Ein Modell ist die Kernkomponente eines Services und ist oft ein Foundation Model, wie ein LLM.

Services sind oft f√ºr den Produktionseinsatz optimiert und √ºber eine grafische Benutzeroberfl√§che einfacher zu nutzen als Modelle. Allerdings sind Services nicht immer kostenlos verf√ºgbar und erfordern m√∂glicherweise ein Abonnement oder eine Zahlung f√ºr die Nutzung, im Austausch f√ºr die Nutzung der Ausr√ºstung und Ressourcen des Serviceanbieters, die Optimierung der Kosten und die einfache Skalierung. Ein Beispiel f√ºr einen Service ist [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=academic-105485-koreyst), der einen Pay-as-you-go-Tarifplan anbietet, was bedeutet, dass Nutzer proportional zu ihrer Nutzung des Services abgerechnet werden. Au√üerdem bietet der Azure OpenAI Service Sicherheit auf Unternehmensniveau und ein verantwortungsvolles KI-Framework zus√§tzlich zu den F√§higkeiten der Modelle.

Modelle sind lediglich das neuronale Netzwerk mit den Parametern, Gewichten und anderen Komponenten. Sie erm√∂glichen es Unternehmen, lokal zu arbeiten, erfordern jedoch den Kauf von Ausr√ºstung, den Aufbau einer Struktur zur Skalierung und den Kauf einer Lizenz oder die Nutzung eines Open-Source-Modells. Ein Modell wie LLaMA ist verf√ºgbar und kann verwendet werden, erfordert jedoch Rechenleistung, um das Modell auszuf√ºhren.

## Wie man verschiedene Modelle testet und iteriert, um die Leistung auf Azure zu verstehen

Sobald unser Team die aktuelle LLM-Landschaft erkundet und einige gute Kandidaten f√ºr ihre Szenarien identifiziert hat, besteht der n√§chste Schritt darin, sie mit ihren Daten und Arbeitslasten zu testen. Dies ist ein iterativer Prozess, der durch Experimente und Messungen durchgef√ºhrt wird.
Die meisten der Modelle, die wir in den vorherigen Abs√§tzen erw√§hnt haben (OpenAI-Modelle, Open-Source-Modelle wie Llama2 und Hugging Face Transformers), sind im [Model Catalog](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview?WT.mc_id=academic-105485-koreyst) in [Azure AI Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreyst) verf√ºgbar.

[Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/what-is-ai-studio?WT.mc_id=academic-105485-koreyst) ist eine Cloud-Plattform, die f√ºr Entwickler konzipiert wurde, um generative KI-Anwendungen zu erstellen und den gesamten Entwicklungszyklus ‚Äì von der Experimentierung bis zur Bewertung ‚Äì zu verwalten, indem alle Azure AI-Dienste in einem einzigen Hub mit einer benutzerfreundlichen grafischen Oberfl√§che kombiniert werden. Der Model Catalog in Azure AI Studio erm√∂glicht es dem Benutzer:

- Das gew√ºnschte Foundation Model im Katalog zu finden ‚Äì entweder propriet√§r oder Open Source ‚Äì und nach Aufgabe, Lizenz oder Name zu filtern. Um die Suchbarkeit zu verbessern, sind die Modelle in Sammlungen organisiert, wie die Azure OpenAI Collection, Hugging Face Collection und mehr.

![Model catalog](../../../translated_images/AzureAIStudioModelCatalog.3cf8a499aa8ba0314f2c73d4048b3225d324165f547525f5b7cfa5f6c9c68941.de.png)

- Die Modellkarte zu √ºberpr√ºfen, einschlie√ülich einer detaillierten Beschreibung der vorgesehenen Verwendung und der Trainingsdaten, Codebeispiele und Bewertungsergebnisse aus der internen Evaluierungsbibliothek.

![Model card](../../../translated_images/ModelCard.598051692c6e400d681a713ba7717e8b6e5e65f08d12131556fcec0f1789459b.de.png)

- Benchmarks zwischen Modellen und Datens√§tzen vergleichen, die in der Branche verf√ºgbar sind, um zu beurteilen, welches Modell am besten zum Gesch√§ftsszenario passt, √ºber die [Model Benchmarks](https://learn.microsoft.com/azure/ai-studio/how-to/model-benchmarks?WT.mc_id=academic-105485-koreyst)-Ansicht.

![Model benchmarks](../../../translated_images/ModelBenchmarks.254cb20fbd06c03a4ca53994585c5ea4300a88bcec8eff0450f2866ee2ac5ff3.de.png)

- Das Modell mit benutzerdefinierten Trainingsdaten feinabstimmen, um die Leistung des Modells in einer bestimmten Arbeitslast zu verbessern, und dabei die Experimentier- und Verfolgungsfunktionen von Azure AI Studio nutzen.

![Model fine-tuning](../../../translated_images/FineTuning.aac48f07142e36fddc6571b1f43ea2e003325c9c6d8e3fc9d8834b771e308dbf.de.png)

- Das urspr√ºngliche vortrainierte Modell oder die feinabgestimmte Version f√ºr eine Remote-Echtzeitinferenz ‚Äì verwaltete Berechnung ‚Äì oder einen serverlosen API-Endpunkt ‚Äì [pay-as-you-go](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview#model-deployment-managed-compute-and-serverless-api-pay-as-you-go?WT.mc_id=academic-105485-koreyst) ‚Äì bereitstellen, um Anwendungen die Nutzung zu erm√∂glichen.

![Model deployment](../../../translated_images/ModelDeploy.890da48cbd0bccdb4abfc9257f3d884831e5d41b723e7d1ceeac9d60c3c4f984.de.png)

> [!NOTE]
> Nicht alle Modelle im Katalog sind derzeit f√ºr Feinabstimmung und/oder Pay-as-you-go-Bereitstellung verf√ºgbar. √úberpr√ºfen Sie die Modellkarte, um Details zu den F√§higkeiten und Einschr√§nkungen des Modells zu erfahren.

## Verbesserung der Ergebnisse von LLMs

Wir haben mit unserem Startup-Team verschiedene Arten von LLMs und eine Cloud-Plattform (Azure Machine Learning) untersucht, die es uns erm√∂glicht, verschiedene Modelle zu vergleichen, sie mit Testdaten zu bewerten, die Leistung zu verbessern und sie auf Inferenzendpunkten bereitzustellen.

Aber wann sollte man ein Modell feinabstimmen, anstatt ein vortrainiertes zu verwenden? Gibt es andere Ans√§tze, um die Leistung eines Modells f√ºr spezifische Arbeitslasten zu verbessern?

Es gibt mehrere Ans√§tze, die ein Unternehmen nutzen kann, um die gew√ºnschten Ergebnisse von einem LLM zu erzielen. Sie k√∂nnen verschiedene Arten von Modellen mit unterschiedlichen Trainingsgraden ausw√§hlen, wenn Sie ein LLM in der Produktion einsetzen, mit unterschiedlichen Komplexit√§ts-, Kosten- und Qualit√§tsstufen. Hier sind einige Ans√§tze:

- **Prompt Engineering mit Kontext**. Die Idee ist, beim Prompting gen√ºgend Kontext bereitzustellen, um die gew√ºnschten Antworten zu erhalten.

- **Retrieval Augmented Generation, RAG**. Ihre Daten k√∂nnten beispielsweise in einer Datenbank oder einem Web-Endpunkt existieren. Um sicherzustellen, dass diese Daten oder ein Teil davon zum Zeitpunkt des Promptings einbezogen werden, k√∂nnen Sie die relevanten Daten abrufen und in den Prompt einf√ºgen.

- **Feinabgestimmtes Modell**. Hier wird das Modell weiter auf Ihre eigenen Daten trainiert, was dazu f√ºhrt, dass das Modell genauer und besser auf Ihre Bed√ºrfnisse reagiert, aber m√∂glicherweise kostspielig ist.

![LLMs deployment](../../../translated_images/Deploy.18b2d27412ec8c02871386cbe91097c7f2190a8c6e2be88f66392b411609a48c.de.png)

Bildquelle: [Four Ways that Enterprises Deploy LLMs | Fiddler AI Blog](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)

### Prompt Engineering mit Kontext

Vortrainierte LLMs funktionieren sehr gut bei allgemeinen Aufgaben der nat√ºrlichen Sprachverarbeitung, selbst wenn sie mit einem kurzen Prompt aufgerufen werden, wie einem Satz, der vervollst√§ndigt werden soll, oder einer Frage ‚Äì dem sogenannten ‚ÄûZero-Shot‚Äú-Lernen.

Je genauer der Benutzer jedoch seine Anfrage formulieren kann, mit einer detaillierten Anfrage und Beispielen ‚Äì dem Kontext ‚Äì desto pr√§ziser und n√§her an den Erwartungen des Benutzers wird die Antwort sein. In diesem Fall sprechen wir von ‚ÄûOne-Shot‚Äú-Lernen, wenn der Prompt nur ein Beispiel enth√§lt, und von ‚ÄûFew-Shot-Lernen‚Äú, wenn er mehrere Beispiele enth√§lt. Prompt Engineering mit Kontext ist der kosteneffektivste Ansatz, um zu beginnen.

### Retrieval Augmented Generation (RAG)

LLMs haben die Einschr√§nkung, dass sie nur die Daten verwenden k√∂nnen, die w√§hrend ihres Trainings verwendet wurden, um eine Antwort zu generieren. Das bedeutet, dass sie nichts √ºber Ereignisse wissen, die nach ihrem Trainingsprozess stattgefunden haben, und sie k√∂nnen nicht auf nicht-√∂ffentliche Informationen (wie Unternehmensdaten) zugreifen. 

Dies kann durch RAG √ºberwunden werden, eine Technik, die den Prompt mit externen Daten in Form von Dokumentenfragmenten erweitert, wobei die L√§ngenbeschr√§nkungen des Prompts ber√ºcksichtigt werden. Dies wird durch Vektordatenbank-Tools (wie [Azure Vector Search](https://learn.microsoft.com/azure/search/vector-search-overview?WT.mc_id=academic-105485-koreyst)) unterst√ºtzt, die die n√ºtzlichen Fragmente aus verschiedenen vordefinierten Datenquellen abrufen und dem Prompt-Kontext hinzuf√ºgen.

Diese Technik ist sehr hilfreich, wenn ein Unternehmen nicht √ºber gen√ºgend Daten, Zeit oder Ressourcen verf√ºgt, um ein LLM feinabzustimmen, aber dennoch die Leistung f√ºr eine bestimmte Arbeitslast verbessern und das Risiko von Falschinformationen, d. h. Verf√§lschung der Realit√§t oder sch√§dlichen Inhalten, reduzieren m√∂chte.

### Feinabgestimmtes Modell

Feinabstimmung ist ein Prozess, der Transferlernen nutzt, um das Modell an eine nachgelagerte Aufgabe anzupassen oder ein spezifisches Problem zu l√∂sen. Im Gegensatz zum Few-Shot-Lernen und RAG f√ºhrt dies zur Erstellung eines neuen Modells mit aktualisierten Gewichten und Biases. Es erfordert eine Reihe von Trainingsbeispielen, die aus einem einzelnen Input (dem Prompt) und dem zugeh√∂rigen Output (der Vervollst√§ndigung) bestehen. 

Dies w√§re der bevorzugte Ansatz, wenn:

- **Verwendung von feinabgestimmten Modellen**. Ein Unternehmen m√∂chte weniger leistungsf√§hige, aber feinabgestimmte Modelle (wie Einbettungsmodelle) verwenden, anstatt hochleistungsf√§hige Modelle, was zu einer kosteng√ºnstigeren und schnelleren L√∂sung f√ºhrt.

- **Ber√ºcksichtigung der Latenz**. Die Latenz ist f√ºr einen bestimmten Anwendungsfall wichtig, sodass es nicht m√∂glich ist, sehr lange Prompts zu verwenden oder die Anzahl der Beispiele, die vom Modell gelernt werden sollen, nicht mit der L√§ngenbeschr√§nkung des Prompts √ºbereinstimmt.

- **Aktualit√§t bewahren**. Ein Unternehmen verf√ºgt √ºber viele hochwertige Daten und Ground-Truth-Labels sowie die Ressourcen, um diese Daten im Laufe der Zeit aktuell zu halten.

### Trainiertes Modell

Das Training eines LLM von Grund auf ist zweifellos der schwierigste und komplexeste Ansatz, der massive Datenmengen, qualifizierte Ressourcen und geeignete Rechenleistung erfordert. Diese Option sollte nur in einem Szenario in Betracht gezogen werden, in dem ein Unternehmen einen dom√§nenspezifischen Anwendungsfall und eine gro√üe Menge an dom√§nenspezifischen Daten hat.

## Wissenstest

Was k√∂nnte ein guter Ansatz sein, um die Ergebnisse der LLM-Vervollst√§ndigung zu verbessern?

1. Prompt Engineering mit Kontext  
1. RAG  
1. Feinabgestimmtes Modell  

A: 3, wenn Sie die Zeit, Ressourcen und hochwertige Daten haben, ist die Feinabstimmung die bessere Option, um auf dem neuesten Stand zu bleiben. Wenn Sie jedoch Verbesserungen anstreben und keine Zeit haben, lohnt es sich, zuerst RAG in Betracht zu ziehen.

## üöÄ Herausforderung

Lesen Sie mehr dar√ºber, wie Sie [RAG nutzen k√∂nnen](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview?WT.mc_id=academic-105485-koreyst) f√ºr Ihr Unternehmen.

## Gro√üartige Arbeit, setzen Sie Ihr Lernen fort

Nachdem Sie diese Lektion abgeschlossen haben, sehen Sie sich unsere [Generative AI Learning Collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) an, um Ihr Wissen √ºber generative KI weiter zu vertiefen!

Gehen Sie zu Lektion 3, in der wir uns ansehen, wie man [verantwortungsvoll mit generativer KI arbeitet](../03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.