<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2f686f2eb794941761252ac5e8e090b",
  "translation_date": "2025-05-19T09:22:18+00:00",
  "source_file": "02-exploring-and-comparing-different-llms/README.md",
  "language_code": "de"
}
-->
# Erkundung und Vergleich verschiedener LLMs

> _Klicken Sie auf das Bild oben, um das Video zu dieser Lektion anzusehen_

Mit der vorherigen Lektion haben wir gesehen, wie Generative KI die Technologielandschaft ver√§ndert, wie Gro√üe Sprachmodelle (LLMs) funktionieren und wie ein Unternehmen - wie unser Startup - sie auf seine Anwendungsf√§lle anwenden und wachsen kann! In diesem Kapitel wollen wir verschiedene Arten von gro√üen Sprachmodellen (LLMs) vergleichen, um ihre Vor- und Nachteile zu verstehen.

Der n√§chste Schritt auf der Reise unseres Startups besteht darin, die aktuelle Landschaft der LLMs zu erkunden und zu verstehen, welche f√ºr unseren Anwendungsfall geeignet sind.

## Einf√ºhrung

Diese Lektion behandelt:

- Verschiedene Arten von LLMs in der aktuellen Landschaft.
- Testen, Iterieren und Vergleichen verschiedener Modelle f√ºr Ihren Anwendungsfall in Azure.
- Wie man ein LLM bereitstellt.

## Lernziele

Nach Abschluss dieser Lektion werden Sie in der Lage sein:

- Das richtige Modell f√ºr Ihren Anwendungsfall auszuw√§hlen.
- Zu verstehen, wie Sie die Leistung Ihres Modells testen, iterieren und verbessern k√∂nnen.
- Zu wissen, wie Unternehmen Modelle bereitstellen.

## Verschiedene Arten von LLMs verstehen

LLMs k√∂nnen basierend auf ihrer Architektur, ihren Trainingsdaten und ihrem Anwendungsfall in verschiedene Kategorien unterteilt werden. Das Verst√§ndnis dieser Unterschiede wird unserem Startup helfen, das richtige Modell f√ºr das Szenario auszuw√§hlen und zu verstehen, wie man die Leistung testet, iteriert und verbessert.

Es gibt viele verschiedene Arten von LLM-Modellen, Ihre Wahl h√§ngt davon ab, wof√ºr Sie sie verwenden m√∂chten, welche Daten Sie haben, wie viel Sie bereit sind zu zahlen und mehr.

Je nachdem, ob Sie die Modelle f√ºr Text-, Audio-, Video-, Bilderzeugung usw. verwenden m√∂chten, k√∂nnten Sie sich f√ºr eine andere Art von Modell entscheiden.

- **Audio- und Spracherkennung**. F√ºr diesen Zweck sind Whisper-Modelle eine ausgezeichnete Wahl, da sie universell einsetzbar sind und auf Spracherkennung abzielen. Sie sind auf vielf√§ltige Audioinhalte trainiert und k√∂nnen mehrsprachige Spracherkennung durchf√ºhren. Erfahren Sie mehr √ºber [Whisper-Modelle hier](https://platform.openai.com/docs/models/whisper?WT.mc_id=academic-105485-koreyst).

- **Bildgenerierung**. F√ºr die Bildgenerierung sind DALL-E und Midjourney zwei sehr bekannte Optionen. DALL-E wird von Azure OpenAI angeboten. [Lesen Sie mehr √ºber DALL-E hier](https://platform.openai.com/docs/models/dall-e?WT.mc_id=academic-105485-koreyst) und auch in Kapitel 9 dieses Lehrplans.

- **Textgenerierung**. Die meisten Modelle sind auf Textgenerierung trainiert und Sie haben eine gro√üe Auswahl von GPT-3.5 bis GPT-4. Sie kommen zu unterschiedlichen Kosten, wobei GPT-4 das teuerste ist. Es lohnt sich, den [Azure OpenAI Playground](https://oai.azure.com/portal/playground?WT.mc_id=academic-105485-koreyst) zu erkunden, um zu bewerten, welche Modelle am besten zu Ihren Bed√ºrfnissen in Bezug auf F√§higkeiten und Kosten passen.

- **Multi-Modality**. Wenn Sie mehrere Arten von Daten in Eingabe und Ausgabe verarbeiten m√∂chten, k√∂nnten Sie sich Modelle wie [gpt-4 turbo mit Vision oder gpt-4o](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models?WT.mc_id=academic-105485-koreyst) ansehen - die neuesten Ver√∂ffentlichungen von OpenAI-Modellen - die in der Lage sind, nat√ºrliche Sprachverarbeitung mit visueller Wahrnehmung zu kombinieren und Interaktionen √ºber multimodale Schnittstellen zu erm√∂glichen.

Die Auswahl eines Modells bedeutet, dass Sie einige grundlegende F√§higkeiten erhalten, die jedoch m√∂glicherweise nicht ausreichen. Oft haben Sie unternehmensspezifische Daten, die Sie dem LLM irgendwie mitteilen m√ºssen. Es gibt einige verschiedene M√∂glichkeiten, wie Sie dies angehen k√∂nnen, mehr dazu in den kommenden Abschnitten.

### Foundation Models versus LLMs

Der Begriff Foundation Model wurde von [Stanford-Forschern gepr√§gt](https://arxiv.org/abs/2108.07258?WT.mc_id=academic-105485-koreyst) und definiert als ein KI-Modell, das einige Kriterien erf√ºllt, wie zum Beispiel:

- **Sie werden mit un√ºberwachtem Lernen oder selbst√ºberwachtem Lernen trainiert**, das bedeutet, dass sie auf unbeschrifteten multimodalen Daten trainiert werden und keine menschliche Annotation oder Beschriftung der Daten f√ºr ihren Trainingsprozess ben√∂tigen.
- **Sie sind sehr gro√üe Modelle**, basierend auf sehr tiefen neuronalen Netzwerken, die auf Milliarden von Parametern trainiert sind.
- **Sie sollen normalerweise als ‚ÄöFundament‚Äò f√ºr andere Modelle dienen**, das bedeutet, dass sie als Ausgangspunkt f√ºr andere Modelle verwendet werden k√∂nnen, die darauf aufgebaut werden k√∂nnen, was durch Feinabstimmung geschehen kann.

Um diese Unterscheidung weiter zu kl√§ren, nehmen wir ChatGPT als Beispiel. Um die erste Version von ChatGPT zu erstellen, diente ein Modell namens GPT-3.5 als Foundation Model. Das bedeutet, dass OpenAI einige chat-spezifische Daten verwendet hat, um eine abgestimmte Version von GPT-3.5 zu erstellen, die darauf spezialisiert war, in Konversationsszenarien, wie Chatbots, gut zu funktionieren.

### Open Source versus Proprietary Models

Eine weitere M√∂glichkeit, LLMs zu kategorisieren, ist, ob sie Open Source oder propriet√§r sind.

Open-Source-Modelle sind Modelle, die der √ñffentlichkeit zug√§nglich gemacht werden und von jedem genutzt werden k√∂nnen. Sie werden oft von dem Unternehmen, das sie erstellt hat, oder von der Forschungsgemeinschaft zur Verf√ºgung gestellt. Diese Modelle d√ºrfen inspiziert, modifiziert und f√ºr die verschiedenen Anwendungsf√§lle in LLMs angepasst werden. Sie sind jedoch nicht immer f√ºr den Produktionseinsatz optimiert und m√∂glicherweise nicht so leistungsf√§hig wie propriet√§re Modelle. Au√üerdem kann die Finanzierung f√ºr Open-Source-Modelle begrenzt sein, und sie werden m√∂glicherweise nicht langfristig gewartet oder mit den neuesten Forschungsergebnissen aktualisiert. Beispiele f√ºr beliebte Open-Source-Modelle sind [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html?WT.mc_id=academic-105485-koreyst), [Bloom](https://huggingface.co/bigscience/bloom) und [LLaMA](https://llama.meta.com).

Propriet√§re Modelle sind Modelle, die einem Unternehmen geh√∂ren und nicht der √ñffentlichkeit zug√§nglich gemacht werden. Diese Modelle sind oft f√ºr den Produktionseinsatz optimiert. Sie d√ºrfen jedoch nicht inspiziert, modifiziert oder f√ºr verschiedene Anwendungsf√§lle angepasst werden. Au√üerdem sind sie nicht immer kostenlos verf√ºgbar und k√∂nnen ein Abonnement oder eine Zahlung erfordern. Au√üerdem haben Benutzer keine Kontrolle √ºber die Daten, die zum Trainieren des Modells verwendet werden, was bedeutet, dass sie dem Modelleigent√ºmer vertrauen sollten, dass er sich zur Datensicherheit und zum verantwortungsvollen Einsatz von KI verpflichtet. Beispiele f√ºr beliebte propriet√§re Modelle sind [OpenAI-Modelle](https://platform.openai.com/docs/models/overview?WT.mc_id=academic-105485-koreyst), [Google Bard](https://sapling.ai/llm/bard?WT.mc_id=academic-105485-koreyst) oder [Claude 2](https://www.anthropic.com/index/claude-2?WT.mc_id=academic-105485-koreyst).

### Embedding versus Bildgenerierung versus Text- und Codegenerierung

LLMs k√∂nnen auch nach dem Output kategorisiert werden, den sie erzeugen.

Embeddings sind eine Gruppe von Modellen, die Text in eine numerische Form umwandeln k√∂nnen, genannt Embedding, was eine numerische Darstellung des Eingabetextes ist. Embeddings erleichtern es Maschinen, die Beziehungen zwischen W√∂rtern oder S√§tzen zu verstehen und k√∂nnen als Eingaben von anderen Modellen, wie Klassifikationsmodellen oder Clustermodellen, die eine bessere Leistung bei numerischen Daten haben, verwendet werden. Embedding-Modelle werden oft f√ºr Transferlernen verwendet, bei dem ein Modell f√ºr eine Ersatzaufgabe erstellt wird, f√ºr die es eine F√ºlle von Daten gibt, und dann die Modellgewichte (Embeddings) f√ºr andere nachgelagerte Aufgaben wiederverwendet werden. Ein Beispiel f√ºr diese Kategorie ist [OpenAI embeddings](https://platform.openai.com/docs/models/embeddings?WT.mc_id=academic-105485-koreyst).

Bildgenerierungsmodelle sind Modelle, die Bilder erzeugen. Diese Modelle werden oft f√ºr Bildbearbeitung, Bildsynthese und Bild√ºbersetzung verwendet. Bildgenerierungsmodelle werden oft auf gro√üen Datens√§tzen von Bildern trainiert, wie [LAION-5B](https://laion.ai/blog/laion-5b/?WT.mc_id=academic-105485-koreyst), und k√∂nnen verwendet werden, um neue Bilder zu erzeugen oder bestehende Bilder mit Inpainting-, Superaufl√∂sungs- und Kolorierungstechniken zu bearbeiten. Beispiele sind [DALL-E-3](https://openai.com/dall-e-3?WT.mc_id=academic-105485-koreyst) und [Stable Diffusion Modelle](https://github.com/Stability-AI/StableDiffusion?WT.mc_id=academic-105485-koreyst).

Text- und Codegenerierungsmodelle sind Modelle, die Text oder Code erzeugen. Diese Modelle werden oft f√ºr Textzusammenfassung, √úbersetzung und Fragenbeantwortung verwendet. Textgenerierungsmodelle werden oft auf gro√üen Datens√§tzen von Texten trainiert, wie [BookCorpus](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html?WT.mc_id=academic-105485-koreyst), und k√∂nnen verwendet werden, um neuen Text zu erzeugen oder Fragen zu beantworten. Codegenerierungsmodelle, wie [CodeParrot](https://huggingface.co/codeparrot?WT.mc_id=academic-105485-koreyst), werden oft auf gro√üen Datens√§tzen von Code, wie GitHub, trainiert und k√∂nnen verwendet werden, um neuen Code zu erzeugen oder Fehler in bestehendem Code zu beheben.

### Encoder-Decoder versus Decoder-only

Um √ºber die verschiedenen Arten von Architekturen von LLMs zu sprechen, verwenden wir eine Analogie.

Stellen Sie sich vor, Ihr Manager hat Ihnen die Aufgabe gegeben, ein Quiz f√ºr die Sch√ºler zu schreiben. Sie haben zwei Kollegen; einer ist f√ºr die Erstellung des Inhalts verantwortlich und der andere f√ºr die √úberpr√ºfung.

Der Inhaltsersteller ist wie ein Decoder-only Modell, er kann sich das Thema ansehen und sehen, was Sie bereits geschrieben haben, und dann einen Kurs darauf basierend schreiben. Sie sind sehr gut darin, ansprechende und informative Inhalte zu schreiben, aber sie sind nicht sehr gut darin, das Thema und die Lernziele zu verstehen. Einige Beispiele f√ºr Decoder-Modelle sind GPT-Familienmodelle, wie GPT-3.

Der Pr√ºfer ist wie ein Encoder-only Modell, er schaut sich den geschriebenen Kurs und die Antworten an, bemerkt die Beziehung zwischen ihnen und versteht den Kontext, aber er ist nicht gut darin, Inhalte zu erzeugen. Ein Beispiel f√ºr ein Encoder-only Modell w√§re BERT.

Stellen Sie sich vor, wir k√∂nnten auch jemanden haben, der das Quiz erstellen und √ºberpr√ºfen k√∂nnte, dies ist ein Encoder-Decoder Modell. Einige Beispiele w√§ren BART und T5.

### Service versus Modell

Nun, lassen Sie uns √ºber den Unterschied zwischen einem Service und einem Modell sprechen. Ein Service ist ein Produkt, das von einem Cloud-Dienstanbieter angeboten wird und oft eine Kombination aus Modellen, Daten und anderen Komponenten ist. Ein Modell ist die Kernkomponente eines Services und ist oft ein Foundation Model, wie ein LLM.

Services sind oft f√ºr den Produktionseinsatz optimiert und oft einfacher zu verwenden als Modelle, √ºber eine grafische Benutzeroberfl√§che. Allerdings sind Services nicht immer kostenlos verf√ºgbar und k√∂nnen ein Abonnement oder eine Zahlung erfordern, im Austausch daf√ºr, dass die Ausr√ºstung und Ressourcen des Serviceanbieters genutzt werden, was die Kosten optimiert und eine einfache Skalierung erm√∂glicht. Ein Beispiel f√ºr einen Service ist [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=academic-105485-koreyst), der einen Pay-as-you-go-Tarifplan bietet, was bedeutet, dass Benutzer proportional zu ihrer Nutzung des Services belastet werden. Au√üerdem bietet der Azure OpenAI Service Sicherheit auf Unternehmensniveau und einen verantwortungsvollen KI-Rahmen zus√§tzlich zu den F√§higkeiten der Modelle.

Modelle sind nur das neuronale Netzwerk, mit den Parametern, Gewichten und anderen. Unternehmen k√∂nnen sie lokal ausf√ºhren, m√ºssten jedoch Ausr√ºstung kaufen, eine Struktur zum Skalieren aufbauen und eine Lizenz erwerben oder ein Open-Source-Modell verwenden. Ein Modell wie LLaMA ist verf√ºgbar zur Nutzung und erfordert Rechenleistung, um das Modell auszuf√ºhren.

## Wie man mit verschiedenen Modellen testet und iteriert, um die Leistung in Azure zu verstehen

Sobald unser Team die aktuelle LLM-Landschaft erkundet und einige gute Kandidaten f√ºr ihre Szenarien identifiziert hat, besteht der n√§chste Schritt darin, sie auf ihren Daten und ihrer Arbeitslast zu testen. Dies ist ein iterativer Prozess, der durch Experimente und Messungen durchgef√ºhrt wird.
Die meisten der Modelle, die wir in den vorherigen Abs√§tzen erw√§hnt haben (OpenAI-Modelle, Open-Source-Modelle wie Llama2 und Hugging Face Transformers), sind im [Model Catalog](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview?WT.mc_id=academic-105485-koreyst) im [Azure AI Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreyst) verf√ºgbar.

[Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/what-is-ai-studio?WT.mc_id=academic-105485-koreyst) ist eine Cloud-Plattform, die f√ºr Entwickler entwickelt wurde, um generative KI-Anwendungen zu erstellen und den gesamten Entwicklungslebenszyklus zu verwalten - von Experimenten bis zur Bewertung - indem alle Azure AI-Dienste in einem einzigen Hub mit einer praktischen Benutzeroberfl√§che kombiniert werden. Der Model Catalog im Azure AI Studio erm√∂glicht es dem Benutzer:

- Finden Sie das Foundation Model von Interesse im Katalog - entweder propriet√§r oder Open Source, gefiltert nach Aufgabe, Lizenz oder Name. Um die Suchbarkeit zu verbessern, sind die Modelle in Sammlungen organisiert, wie Azure OpenAI-Sammlung, Hugging Face-Sammlung und mehr.

- √úberpr√ºfen Sie die Modellkarte, einschlie√ülich einer detaillierten Beschreibung der beabsichtigten Verwendung und Trainingsdaten, Codebeispiele und Bewertungsergebnisse in der internen Bewertungsbibliothek.
- Vergleichen Sie Benchmarks zwischen Modellen und Datens√§tzen, die in der Branche verf√ºgbar sind, um zu beurteilen, welches den Gesch√§ftsszenarien entspricht, √ºber das [Model Benchmarks](https://learn.microsoft.com/azure/ai-studio/how-to/model-benchmarks?WT.mc_id=academic-105485-koreyst) Fenster.

![Model benchmarks](../../../translated_images/ModelBenchmarks.b3b4182f762db04b59267af64ce77cc936d38adf40fb032f12acec9063578008.de.png)

- Passen Sie das Modell mit benutzerdefinierten Trainingsdaten an, um die Modellleistung in einer bestimmten Arbeitslast zu verbessern, und nutzen Sie die Experimentier- und Nachverfolgungsm√∂glichkeiten von Azure AI Studio.

![Model fine-tuning](../../../translated_images/FineTuning.f93db4ecbdc85b4a20ff1198fb82f5e2daa3a1ee328733b17d603727db20f5c0.de.png)

- Setzen Sie das urspr√ºngliche vortrainierte Modell oder die feinabgestimmte Version f√ºr eine Remote-Echtzeit-Inferenz - verwaltete Berechnung - oder einen serverlosen API-Endpunkt - [pay-as-you-go](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview#model-deployment-managed-compute-and-serverless-api-pay-as-you-go?WT.mc_id=academic-105485-koreyst) - ein, um Anwendungen den Zugriff darauf zu erm√∂glichen.

![Model deployment](../../../translated_images/ModelDeploy.7c78c2c5841567abf820d5da8354be454d3f20b62168905645aeac99e50c2562.de.png)

> [!NOTE]
> Nicht alle Modelle im Katalog sind derzeit f√ºr Feineinstellungen und/oder Pay-as-you-go-Bereitstellungen verf√ºgbar. √úberpr√ºfen Sie die Modellkarte f√ºr Details zu den F√§higkeiten und Einschr√§nkungen des Modells.

## Verbesserung der LLM-Ergebnisse

Wir haben mit unserem Startup-Team verschiedene Arten von LLMs und eine Cloud-Plattform (Azure Machine Learning) erkundet, die es uns erm√∂glicht, verschiedene Modelle zu vergleichen, sie auf Testdaten zu evaluieren, die Leistung zu verbessern und sie auf Inferenzendpunkten bereitzustellen.

Aber wann sollten sie in Betracht ziehen, ein Modell zu verfeinern, anstatt ein vortrainiertes zu verwenden? Gibt es andere Ans√§tze, um die Modellleistung bei bestimmten Arbeitslasten zu verbessern?

Es gibt mehrere Ans√§tze, die ein Unternehmen nutzen kann, um die gew√ºnschten Ergebnisse von einem LLM zu erzielen. Sie k√∂nnen verschiedene Arten von Modellen mit unterschiedlichen Trainingsgraden ausw√§hlen, wenn Sie ein LLM in der Produktion einsetzen, mit unterschiedlichen Komplexit√§ts-, Kosten- und Qualit√§tsniveaus. Hier sind einige verschiedene Ans√§tze:

- **Prompt-Engineering mit Kontext**. Die Idee ist, beim Prompten gen√ºgend Kontext bereitzustellen, um sicherzustellen, dass Sie die ben√∂tigten Antworten erhalten.

- **Retrieval Augmented Generation, RAG**. Ihre Daten k√∂nnten beispielsweise in einer Datenbank oder einem Web-Endpunkt existieren. Um sicherzustellen, dass diese Daten oder ein Teil davon zum Zeitpunkt des Promptens enthalten sind, k√∂nnen Sie die relevanten Daten abrufen und Teil des Benutzer-Prompts machen.

- **Feinabgestimmtes Modell**. Hier haben Sie das Modell weiter mit Ihren eigenen Daten trainiert, was dazu f√ºhrte, dass das Modell genauer und reaktionsf√§higer auf Ihre Bed√ºrfnisse ist, aber m√∂glicherweise kostspielig.

![LLMs deployment](../../../translated_images/Deploy.09224ecfe6a5ef47996fd0a44288772990139305451440c430662d43ac323ecd.de.png)

Bildquelle: [Four Ways that Enterprises Deploy LLMs | Fiddler AI Blog](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)

### Prompt-Engineering mit Kontext

Vortrainierte LLMs funktionieren sehr gut bei allgemeinen Aufgaben der nat√ºrlichen Sprachverarbeitung, selbst wenn sie mit einem kurzen Prompt aufgerufen werden, wie einem Satz zum Vervollst√§ndigen oder einer Frage ‚Äì das sogenannte ‚ÄûZero-Shot‚Äú-Lernen.

Je mehr der Benutzer jedoch seine Anfrage mit einer detaillierten Anfrage und Beispielen ‚Äì dem Kontext ‚Äì umrahmen kann, desto genauer und n√§her an den Erwartungen des Benutzers wird die Antwort sein. In diesem Fall sprechen wir von ‚ÄûOne-Shot‚Äú-Lernen, wenn der Prompt nur ein Beispiel enth√§lt, und ‚ÄûFew-Shot‚Äú-Lernen, wenn er mehrere Beispiele enth√§lt. Prompt-Engineering mit Kontext ist der kosteneffektivste Ansatz f√ºr den Einstieg.

### Retrieval Augmented Generation (RAG)

LLMs haben die Einschr√§nkung, dass sie nur die Daten verwenden k√∂nnen, die w√§hrend ihres Trainings verwendet wurden, um eine Antwort zu generieren. Das bedeutet, dass sie nichts √ºber die Fakten wissen, die nach ihrem Trainingsprozess passiert sind, und sie k√∂nnen nicht auf nicht-√∂ffentliche Informationen (wie Unternehmensdaten) zugreifen. Dies kann durch RAG √ºberwunden werden, eine Technik, die den Prompt mit externen Daten in Form von Dokumentenst√ºcken erg√§nzt, wobei die Prompt-L√§ngenbeschr√§nkungen ber√ºcksichtigt werden. Dies wird durch Vektordatenbank-Tools (wie [Azure Vector Search](https://learn.microsoft.com/azure/search/vector-search-overview?WT.mc_id=academic-105485-koreyst)) unterst√ºtzt, die die n√ºtzlichen St√ºcke aus verschiedenen vordefinierten Datenquellen abrufen und dem Prompt-Kontext hinzuf√ºgen.

Diese Technik ist sehr hilfreich, wenn ein Unternehmen nicht gen√ºgend Daten, Zeit oder Ressourcen hat, um ein LLM feinabzustimmen, aber dennoch die Leistung bei einer bestimmten Arbeitslast verbessern und das Risiko von F√§lschungen, d. h. Mystifikationen der Realit√§t oder sch√§dlichen Inhalten, reduzieren m√∂chte.

### Feinabgestimmtes Modell

Feinabstimmung ist ein Prozess, der Transferlernen nutzt, um das Modell an eine nachgelagerte Aufgabe anzupassen oder ein spezifisches Problem zu l√∂sen. Anders als Few-Shot-Lernen und RAG f√ºhrt dies zu einem neuen Modell mit aktualisierten Gewichten und Verzerrungen. Es erfordert eine Reihe von Trainingsbeispielen, die aus einem einzelnen Input (dem Prompt) und dem zugeh√∂rigen Output (der Vervollst√§ndigung) bestehen. Dies w√§re der bevorzugte Ansatz, wenn:

- **Verwendung feinabgestimmter Modelle**. Ein Unternehmen m√∂chte weniger leistungsf√§hige feinabgestimmte Modelle (wie Einbettungsmodelle) anstelle von Hochleistungsmodellen verwenden, was zu einer kosteng√ºnstigeren und schnelleren L√∂sung f√ºhrt.

- **Ber√ºcksichtigung der Latenz**. Latenz ist wichtig f√ºr einen bestimmten Anwendungsfall, daher ist es nicht m√∂glich, sehr lange Prompts zu verwenden oder die Anzahl der Beispiele, die vom Modell gelernt werden sollen, passt nicht zu den Prompt-L√§ngenbeschr√§nkungen.

- **Auf dem neuesten Stand bleiben**. Ein Unternehmen verf√ºgt √ºber viele hochwertige Daten und Ground-Truth-Labels sowie die Ressourcen, um diese Daten im Laufe der Zeit aktuell zu halten.

### Trainiertes Modell

Das Training eines LLM von Grund auf ist ohne Zweifel der schwierigste und komplexeste Ansatz, der massive Datenmengen, qualifizierte Ressourcen und angemessene Rechenleistung erfordert. Diese Option sollte nur in einem Szenario in Betracht gezogen werden, in dem ein Unternehmen einen dom√§nenspezifischen Anwendungsfall und eine gro√üe Menge dom√§nenzentrierter Daten hat.

## Wissenstest

Was k√∂nnte ein guter Ansatz sein, um die Ergebnisse der LLM-Vervollst√§ndigung zu verbessern?

1. Prompt-Engineering mit Kontext
2. RAG
3. Feinabgestimmtes Modell

A:3, wenn Sie die Zeit und Ressourcen sowie hochwertige Daten haben, ist die Feinabstimmung die bessere Option, um auf dem neuesten Stand zu bleiben. Wenn Sie jedoch Dinge verbessern m√∂chten und Ihnen die Zeit fehlt, lohnt es sich, zun√§chst RAG in Betracht zu ziehen.

## üöÄ Herausforderung

Lesen Sie mehr dar√ºber, wie Sie [RAG verwenden k√∂nnen](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview?WT.mc_id=academic-105485-koreyst) f√ºr Ihr Unternehmen.

## Gute Arbeit, setzen Sie Ihr Lernen fort

Nachdem Sie diese Lektion abgeschlossen haben, schauen Sie sich unsere [Generative AI Learning Sammlung](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) an, um Ihr Wissen √ºber Generative AI weiter zu vertiefen!

Gehen Sie zu Lektion 3, wo wir uns ansehen, wie man [verantwortungsbewusst mit Generative AI arbeitet](../03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)!

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir haften nicht f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.