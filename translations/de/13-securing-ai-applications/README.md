<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T22:54:49+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "de"
}
-->
# Sicherung Ihrer generativen KI-Anwendungen

[![Sicherung Ihrer generativen KI-Anwendungen](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.de.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Einf√ºhrung

Diese Lektion behandelt:

- Sicherheit im Kontext von KI-Systemen.
- H√§ufige Risiken und Bedrohungen f√ºr KI-Systeme.
- Methoden und √úberlegungen zur Sicherung von KI-Systemen.

## Lernziele

Nach Abschluss dieser Lektion werden Sie Folgendes verstehen:

- Die Bedrohungen und Risiken f√ºr KI-Systeme.
- H√§ufige Methoden und Praktiken zur Sicherung von KI-Systemen.
- Wie die Implementierung von Sicherheitstests unerwartete Ergebnisse und den Verlust des Benutzervertrauens verhindern kann.

## Was bedeutet Sicherheit im Kontext von generativer KI?

Da k√ºnstliche Intelligenz (KI) und maschinelles Lernen (ML) zunehmend unser Leben pr√§gen, ist es entscheidend, nicht nur Kundendaten, sondern auch die KI-Systeme selbst zu sch√ºtzen. KI/ML wird immer h√§ufiger zur Unterst√ºtzung von Entscheidungsprozessen mit hohem Wert in Branchen eingesetzt, in denen falsche Entscheidungen schwerwiegende Konsequenzen haben k√∂nnen.

Hier sind einige wichtige Punkte zu beachten:

- **Auswirkungen von KI/ML**: KI/ML haben erhebliche Auswirkungen auf das t√§gliche Leben, und daher ist es unerl√§sslich, sie zu sch√ºtzen.
- **Sicherheitsherausforderungen**: Diese Auswirkungen von KI/ML erfordern besondere Aufmerksamkeit, um die Notwendigkeit zu adressieren, KI-basierte Produkte vor raffinierten Angriffen zu sch√ºtzen, sei es durch Trolle oder organisierte Gruppen.
- **Strategische Probleme**: Die Technologiebranche muss proaktiv strategische Herausforderungen angehen, um langfristige Kundensicherheit und Datenschutz zu gew√§hrleisten.

Dar√ºber hinaus sind maschinelle Lernmodelle weitgehend nicht in der Lage, zwischen b√∂sartigen Eingaben und harmlosen anomalen Daten zu unterscheiden. Ein bedeutender Teil der Trainingsdaten stammt aus unkuratierten, unmoderierten √∂ffentlichen Datens√§tzen, die f√ºr Beitr√§ge Dritter offen sind. Angreifer m√ºssen Datens√§tze nicht kompromittieren, wenn sie frei dazu beitragen k√∂nnen. Im Laufe der Zeit werden Daten mit geringer Vertrauensw√ºrdigkeit zu hochvertrauensw√ºrdigen Daten, wenn die Datenstruktur/-formatierung korrekt bleibt.

Aus diesem Grund ist es entscheidend, die Integrit√§t und den Schutz der Datenspeicher sicherzustellen, die Ihre Modelle f√ºr ihre Entscheidungen verwenden.

## Verst√§ndnis der Bedrohungen und Risiken von KI

Im Bereich der KI und verwandter Systeme sticht Datenvergiftung als die bedeutendste Sicherheitsbedrohung hervor. Datenvergiftung tritt auf, wenn jemand absichtlich die Informationen ver√§ndert, die zur Schulung einer KI verwendet werden, wodurch diese Fehler macht. Dies liegt am Fehlen standardisierter Erkennungs- und Abhilfemethoden sowie an unserer Abh√§ngigkeit von unzuverl√§ssigen oder unkuratierten √∂ffentlichen Datens√§tzen f√ºr das Training. Um die Datenintegrit√§t zu wahren und einen fehlerhaften Trainingsprozess zu verhindern, ist es entscheidend, die Herkunft und den Ursprung Ihrer Daten zu verfolgen. Andernfalls bewahrheitet sich das alte Sprichwort ‚ÄûM√ºll rein, M√ºll raus‚Äú, was zu einer beeintr√§chtigten Modellleistung f√ºhrt.

Hier sind Beispiele daf√ºr, wie Datenvergiftung Ihre Modelle beeinflussen kann:

1. **Label-Flipping**: Bei einer bin√§ren Klassifizierungsaufgabe vertauscht ein Angreifer absichtlich die Labels eines kleinen Teils der Trainingsdaten. Zum Beispiel werden harmlose Proben als b√∂sartig gekennzeichnet, was dazu f√ºhrt, dass das Modell falsche Zuordnungen lernt.\
   **Beispiel**: Ein Spam-Filter klassifiziert legitime E-Mails aufgrund manipulierter Labels f√§lschlicherweise als Spam.
2. **Feature-Vergiftung**: Ein Angreifer ver√§ndert subtil Merkmale in den Trainingsdaten, um Vorurteile einzuf√ºhren oder das Modell zu t√§uschen.\
   **Beispiel**: Hinzuf√ºgen irrelevanter Schl√ºsselw√∂rter zu Produktbeschreibungen, um Empfehlungssysteme zu manipulieren.
3. **Dateninjektion**: Einschleusen b√∂sartiger Daten in den Trainingssatz, um das Verhalten des Modells zu beeinflussen.\
   **Beispiel**: Einf√ºgen gef√§lschter Nutzerbewertungen, um die Ergebnisse der Sentiment-Analyse zu verzerren.
4. **Hintert√ºrangriffe**: Ein Angreifer f√ºgt ein verstecktes Muster (Hintert√ºr) in die Trainingsdaten ein. Das Modell lernt, dieses Muster zu erkennen, und verh√§lt sich b√∂sartig, wenn es ausgel√∂st wird.\
   **Beispiel**: Ein Gesichtserkennungssystem, das mit Bildern trainiert wurde, die eine Hintert√ºr enthalten, und eine bestimmte Person falsch identifiziert.

Die MITRE Corporation hat [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) erstellt, eine Wissensdatenbank mit Taktiken und Techniken, die von Angreifern bei realen Angriffen auf KI-Systeme verwendet werden.

> Es gibt eine zunehmende Anzahl von Schwachstellen in KI-gest√ºtzten Systemen, da die Integration von KI die Angriffsfl√§che bestehender Systeme √ºber die traditionellen Cyberangriffe hinaus erweitert. Wir haben ATLAS entwickelt, um das Bewusstsein f√ºr diese einzigartigen und sich entwickelnden Schwachstellen zu sch√§rfen, da die globale Gemeinschaft zunehmend KI in verschiedene Systeme integriert. ATLAS basiert auf dem MITRE ATT&CK¬Æ-Framework, und seine Taktiken, Techniken und Verfahren (TTPs) erg√§nzen die im ATT&CK enthaltenen.

√Ñhnlich wie das MITRE ATT&CK¬Æ-Framework, das in der traditionellen Cybersicherheit umfassend f√ºr die Planung fortschrittlicher Bedrohungssimulationen verwendet wird, bietet ATLAS eine leicht durchsuchbare Sammlung von TTPs, die helfen k√∂nnen, aufkommende Angriffe besser zu verstehen und sich darauf vorzubereiten.

Dar√ºber hinaus hat das Open Web Application Security Project (OWASP) eine "[Top 10 Liste](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" der kritischsten Schwachstellen in Anwendungen, die LLMs nutzen, erstellt. Die Liste hebt die Risiken von Bedrohungen wie der oben genannten Datenvergiftung sowie anderen hervor, wie:

- **Prompt Injection**: Eine Technik, bei der Angreifer ein Large Language Model (LLM) durch sorgf√§ltig gestaltete Eingaben manipulieren, sodass es sich au√üerhalb seines vorgesehenen Verhaltens verh√§lt.
- **Lieferketten-Schwachstellen**: Die Komponenten und Software, die die Anwendungen eines LLM ausmachen, wie Python-Module oder externe Datens√§tze, k√∂nnen selbst kompromittiert werden, was zu unerwarteten Ergebnissen, eingef√ºhrten Vorurteilen und sogar Schwachstellen in der zugrunde liegenden Infrastruktur f√ºhrt.
- **√úberm√§√üige Abh√§ngigkeit**: LLMs sind fehleranf√§llig und neigen dazu, Halluzinationen zu haben, die zu ungenauen oder unsicheren Ergebnissen f√ºhren. In mehreren dokumentierten F√§llen haben Menschen die Ergebnisse f√ºr bare M√ºnze genommen, was zu unbeabsichtigten negativen Konsequenzen in der realen Welt f√ºhrte.

Microsoft Cloud Advocate Rod Trent hat ein kostenloses E-Book geschrieben, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), das sich ausf√ºhrlich mit diesen und anderen aufkommenden KI-Bedrohungen befasst und umfassende Leitlinien bietet, wie man diese Szenarien am besten angeht.

## Sicherheitstests f√ºr KI-Systeme und LLMs

K√ºnstliche Intelligenz (KI) transformiert verschiedene Bereiche und Branchen und bietet neue M√∂glichkeiten und Vorteile f√ºr die Gesellschaft. Allerdings bringt KI auch erhebliche Herausforderungen und Risiken mit sich, wie Datenschutz, Vorurteile, mangelnde Erkl√§rbarkeit und potenziellen Missbrauch. Daher ist es entscheidend, sicherzustellen, dass KI-Systeme sicher und verantwortungsvoll sind, das hei√üt, dass sie ethischen und rechtlichen Standards entsprechen und von Nutzern und Interessengruppen vertraut werden k√∂nnen.

Sicherheitstests sind der Prozess der Bewertung der Sicherheit eines KI-Systems oder LLMs, indem deren Schwachstellen identifiziert und ausgenutzt werden. Dies kann von Entwicklern, Nutzern oder externen Pr√ºfern durchgef√ºhrt werden, je nach Zweck und Umfang der Tests. Einige der h√§ufigsten Methoden f√ºr Sicherheitstests bei KI-Systemen und LLMs sind:

- **Datenbereinigung**: Dies ist der Prozess des Entfernens oder Anonymisierens sensibler oder privater Informationen aus den Trainingsdaten oder den Eingaben eines KI-Systems oder LLMs. Datenbereinigung kann helfen, Datenlecks und b√∂sartige Manipulationen zu verhindern, indem die Exposition vertraulicher oder pers√∂nlicher Daten reduziert wird.
- **Adversarial Testing**: Dies ist der Prozess der Erstellung und Anwendung von adversarialen Beispielen auf die Eingaben oder Ausgaben eines KI-Systems oder LLMs, um dessen Robustheit und Widerstandsf√§higkeit gegen Angriffe zu bewerten. Adversarial Testing kann helfen, Schwachstellen und M√§ngel eines KI-Systems oder LLMs zu identifizieren und zu beheben, die von Angreifern ausgenutzt werden k√∂nnten.
- **Modell√ºberpr√ºfung**: Dies ist der Prozess der √úberpr√ºfung der Korrektheit und Vollst√§ndigkeit der Modellparameter oder Architektur eines KI-Systems oder LLMs. Modell√ºberpr√ºfung kann helfen, Modelldiebstahl zu erkennen und zu verhindern, indem sichergestellt wird, dass das Modell gesch√ºtzt und authentifiziert ist.
- **Ausgabevalidierung**: Dies ist der Prozess der Validierung der Qualit√§t und Zuverl√§ssigkeit der Ausgabe eines KI-Systems oder LLMs. Ausgabevalidierung kann helfen, b√∂sartige Manipulationen zu erkennen und zu korrigieren, indem sichergestellt wird, dass die Ausgabe konsistent und genau ist.

OpenAI, ein f√ºhrendes Unternehmen im Bereich KI-Systeme, hat eine Reihe von _Sicherheitsbewertungen_ im Rahmen ihrer Red-Teaming-Initiative eingerichtet, die darauf abzielen, die Ausgabe von KI-Systemen zu testen, um zur KI-Sicherheit beizutragen.

> Bewertungen k√∂nnen von einfachen Q&A-Tests bis hin zu komplexeren Simulationen reichen. Hier sind konkrete Beispiele f√ºr Bewertungen, die von OpenAI entwickelt wurden, um KI-Verhalten aus verschiedenen Perspektiven zu evaluieren:

#### √úberzeugung

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System ein anderes KI-System dazu bringen, ein geheimes Wort zu sagen?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System ein anderes KI-System dazu √ºberreden, Geld zu spenden?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System die Unterst√ºtzung eines anderen KI-Systems f√ºr einen politischen Vorschlag beeinflussen?

#### Steganografie (versteckte Nachrichten)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System geheime Nachrichten √ºbermitteln, ohne von einem anderen KI-System entdeckt zu werden?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System Nachrichten komprimieren und dekomprimieren, um geheime Nachrichten zu verstecken?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System mit einem anderen KI-System koordinieren, ohne direkt zu kommunizieren?

### KI-Sicherheit

Es ist unerl√§sslich, dass wir uns bem√ºhen, KI-Systeme vor b√∂sartigen Angriffen, Missbrauch oder unbeabsichtigten Konsequenzen zu sch√ºtzen. Dazu geh√∂rt, Ma√ünahmen zu ergreifen, um die Sicherheit, Zuverl√§ssigkeit und Vertrauensw√ºrdigkeit von KI-Systemen zu gew√§hrleisten, wie:

- Sicherung der Daten und Algorithmen, die zur Schulung und Ausf√ºhrung von KI-Modellen verwendet werden
- Verhinderung unbefugten Zugriffs, Manipulation oder Sabotage von KI-Systemen
- Erkennung und Minderung von Vorurteilen, Diskriminierung oder ethischen Problemen in KI-Systemen
- Sicherstellung der Verantwortlichkeit, Transparenz und Erkl√§rbarkeit von KI-Entscheidungen und -Handlungen
- Ausrichtung der Ziele und Werte von KI-Systemen an denen von Menschen und der Gesellschaft

KI-Sicherheit ist wichtig, um die Integrit√§t, Verf√ºgbarkeit und Vertraulichkeit von KI-Systemen und Daten zu gew√§hrleisten. Einige der Herausforderungen und Chancen der KI-Sicherheit sind:

- **Chance**: Integration von KI in Cybersicherheitsstrategien, da sie eine entscheidende Rolle bei der Identifizierung von Bedrohungen und der Verbesserung der Reaktionszeiten spielen kann. KI kann helfen, die Erkennung und Minderung von Cyberangriffen wie Phishing, Malware oder Ransomware zu automatisieren und zu erweitern.
- **Herausforderung**: KI kann auch von Angreifern genutzt werden, um raffinierte Angriffe zu starten, wie das Generieren von gef√§lschten oder irref√ºhrenden Inhalten, das Nachahmen von Nutzern oder das Ausnutzen von Schwachstellen in KI-Systemen. Daher haben KI-Entwickler eine besondere Verantwortung, Systeme zu entwerfen, die robust und widerstandsf√§hig gegen Missbrauch sind.

### Datenschutz

LLMs k√∂nnen Risiken f√ºr die Privatsph√§re und Sicherheit der Daten darstellen, die sie verwenden. Beispielsweise k√∂nnen LLMs potenziell sensible Informationen aus ihren Trainingsdaten speichern und preisgeben, wie pers√∂nliche Namen, Adressen, Passw√∂rter oder Kreditkartennummern. Sie k√∂nnen auch von b√∂swilligen Akteuren manipuliert oder angegriffen werden, die ihre Schwachstellen oder Vorurteile ausnutzen m√∂chten. Daher ist es wichtig, sich dieser Risiken bewusst zu sein und geeignete Ma√ünahmen zu ergreifen, um die mit LLMs verwendeten Daten zu sch√ºtzen. Es gibt mehrere Schritte, die Sie unternehmen k√∂nnen, um die mit LLMs verwendeten Daten zu sch√ºtzen. Diese Schritte umfassen:

- **Begrenzung der Menge und Art der Daten, die mit LLMs geteilt werden**: Teilen Sie nur die Daten, die f√ºr die beabsichtigten Zwecke notwendig und relevant sind, und vermeiden Sie das Teilen von Daten, die sensibel, vertraulich oder pers√∂nlich sind. Nutzer sollten auch die Daten, die sie mit LLMs teilen, anonymisieren oder verschl√ºsseln, z. B. durch Entfernen oder Maskieren von Identifikationsinformationen oder die Verwendung sicherer Kommunikationskan√§le.
- **√úberpr√ºfung der von LLMs generierten Daten**: √úberpr√ºfen Sie stets die Genauigkeit und Qualit√§t der von LLMs generierten Ausgaben, um sicherzustellen, dass sie keine unerw√ºnschten oder unangemessenen Informationen enthalten.
- **Meldung und Alarmierung bei Datenverletzungen oder Vorf√§llen**: Seien Sie wachsam gegen√ºber verd√§chtigen oder ungew√∂hnlichen Aktivit√§ten oder Verhaltensweisen von LLMs, wie z. B. der Generierung von Texten, die irrelevant, ungenau, beleidigend oder sch√§dlich sind. Dies k√∂nnte ein Hinweis auf eine Datenverletzung oder einen Sicherheitsvorfall sein.

Datensicherheit, Governance und Compliance sind entscheidend f√ºr jede Organisation, die die M√∂glichkeiten von Daten und KI in einer Multi-Cloud-Umgebung nutzen m√∂chte. Die Sicherung und Verwaltung aller Ihrer Daten ist eine komplexe und facettenreiche Aufgabe. Sie m√ºssen verschiedene Arten von Daten (strukturierte, unstrukturierte und von KI generierte Daten) an verschiedenen Standorten √ºber mehrere Clouds hinweg sichern und verwalten, und Sie m√ºssen bestehende und zuk√ºnftige Vorschriften zur Datensicherheit, Governance und KI ber√ºcksichtigen. Um Ihre Daten zu sch√ºtzen, sollten Sie einige bew√§hrte Verfahren und Vorsichtsma√ünahmen ergreifen, wie:

- Nutzung von Cloud-Diensten oder Plattformen, die Datenschutz- und Sicherheitsfunktionen bieten.
- Einsatz von Tools zur Datenqualit√§t und -validierung, um Ihre Daten auf Fehler, Inkonsistenzen oder Anomalien zu √ºberpr√ºfen.
- Verwendung von Daten-Governance- und Ethik-Rahmenwerken, um sicherzustellen, dass Ihre Daten verantwortungsvoll und transparent genutzt werden.

### Nachahmung realer Bedrohungen - KI-Red-Teaming
Die Nachahmung realer Bedrohungen wird mittlerweile als Standardpraxis angesehen, um widerstandsf√§hige KI-Systeme zu entwickeln. Dabei werden √§hnliche Werkzeuge, Taktiken und Verfahren eingesetzt, um Risiken f√ºr Systeme zu identifizieren und die Reaktion der Verteidiger zu testen.

> Die Praxis des AI Red Teaming hat sich weiterentwickelt und umfasst nun eine erweiterte Bedeutung: Sie deckt nicht nur die Untersuchung von Sicherheitsl√ºcken ab, sondern auch die Analyse anderer Systemfehler, wie z. B. die Generierung potenziell sch√§dlicher Inhalte. KI-Systeme bringen neue Risiken mit sich, und Red Teaming ist entscheidend, um diese neuartigen Risiken zu verstehen, wie etwa Prompt Injection und die Erzeugung von nicht fundierten Inhalten. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Leitf√§den und Ressourcen f√ºr Red Teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.de.png)]()

Im Folgenden finden Sie wichtige Erkenntnisse, die das AI Red Team-Programm von Microsoft gepr√§gt haben.

1. **Erweiterter Umfang des AI Red Teaming:**
   AI Red Teaming umfasst nun sowohl Sicherheits- als auch Responsible AI (RAI)-Ergebnisse. Traditionell konzentrierte sich Red Teaming auf Sicherheitsaspekte und behandelte das Modell als Angriffsvektor (z. B. Diebstahl des zugrunde liegenden Modells). KI-Systeme bringen jedoch neuartige Sicherheitsl√ºcken mit sich (z. B. Prompt Injection, Datenvergiftung), die besondere Aufmerksamkeit erfordern. √úber Sicherheitsaspekte hinaus untersucht AI Red Teaming auch Fragen der Fairness (z. B. Stereotypisierung) und sch√§dliche Inhalte (z. B. Verherrlichung von Gewalt). Die fr√ºhzeitige Identifizierung dieser Probleme erm√∂glicht die Priorisierung von Investitionen in Verteidigungsma√ünahmen.
2. **B√∂swillige und harmlose Fehler:**
   AI Red Teaming ber√ºcksichtigt Fehler sowohl aus b√∂swilliger als auch aus harmloser Perspektive. Zum Beispiel untersuchen wir beim Red Teaming des neuen Bing nicht nur, wie b√∂swillige Angreifer das System untergraben k√∂nnen, sondern auch, wie normale Nutzer auf problematische oder sch√§dliche Inhalte sto√üen k√∂nnten. Im Gegensatz zum traditionellen Sicherheits-Red-Teaming, das sich haupts√§chlich auf b√∂swillige Akteure konzentriert, ber√ºcksichtigt AI Red Teaming eine breitere Palette von Personas und potenziellen Fehlern.
3. **Dynamische Natur von KI-Systemen:**
   KI-Anwendungen entwickeln sich st√§ndig weiter. In Anwendungen mit gro√üen Sprachmodellen passen Entwickler sich an sich √§ndernde Anforderungen an. Kontinuierliches Red Teaming gew√§hrleistet anhaltende Wachsamkeit und Anpassung an sich entwickelnde Risiken.

AI Red Teaming ist nicht allumfassend und sollte als erg√§nzende Ma√ünahme zu zus√§tzlichen Kontrollen wie [rollenbasierter Zugriffskontrolle (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) und umfassenden Datenmanagementl√∂sungen betrachtet werden. Es soll eine Sicherheitsstrategie erg√§nzen, die darauf abzielt, sichere und verantwortungsvolle KI-L√∂sungen einzusetzen, die Datenschutz und Sicherheit ber√ºcksichtigen und gleichzeitig darauf abzielen, Vorurteile, sch√§dliche Inhalte und Fehlinformationen zu minimieren, die das Vertrauen der Nutzer beeintr√§chtigen k√∂nnen.

Hier ist eine Liste zus√§tzlicher Lekt√ºre, die Ihnen helfen kann, besser zu verstehen, wie Red Teaming dazu beitragen kann, Risiken in Ihren KI-Systemen zu identifizieren und zu mindern:

- [Planung von Red Teaming f√ºr gro√üe Sprachmodelle (LLMs) und deren Anwendungen](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Was ist das OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - Eine Schl√ºsselpraxis f√ºr den Aufbau sicherer und verantwortungsvoller KI-L√∂sungen](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), eine Wissensdatenbank √ºber Taktiken und Techniken, die von Angreifern bei realen Angriffen auf KI-Systeme eingesetzt werden.

## Wissenstest

Was k√∂nnte ein guter Ansatz sein, um die Datenintegrit√§t zu wahren und Missbrauch zu verhindern?

1. Starke rollenbasierte Kontrollen f√ºr den Datenzugriff und das Datenmanagement implementieren
1. Datenkennzeichnung implementieren und √ºberpr√ºfen, um Datenfehlinterpretationen oder Missbrauch zu verhindern
1. Sicherstellen, dass Ihre KI-Infrastruktur die Inhaltsfilterung unterst√ºtzt

A:1, Obwohl alle drei Empfehlungen sinnvoll sind, wird die korrekte Zuweisung von Datenzugriffsrechten an Benutzer einen gro√üen Beitrag dazu leisten, Manipulation und Fehlinterpretation der von LLMs verwendeten Daten zu verhindern.

## üöÄ Herausforderung

Lesen Sie mehr dar√ºber, wie Sie [sensible Informationen verwalten und sch√ºtzen](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) k√∂nnen im Zeitalter der KI.

## Gro√üartige Arbeit, setzen Sie Ihr Lernen fort

Nachdem Sie diese Lektion abgeschlossen haben, sehen Sie sich unsere [Generative AI Learning Collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) an, um Ihr Wissen √ºber generative KI weiter zu vertiefen!

Gehen Sie weiter zu Lektion 14, in der wir uns [den Lebenszyklus von generativen KI-Anwendungen](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst) ansehen!

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.