<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:10:20+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "de"
}
-->
# Absicherung Ihrer generativen KI-Anwendungen

[![Absicherung Ihrer generativen KI-Anwendungen](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.de.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Einf√ºhrung

Diese Lektion behandelt:

- Sicherheit im Kontext von KI-Systemen.
- H√§ufige Risiken und Bedrohungen f√ºr KI-Systeme.
- Methoden und √úberlegungen zur Absicherung von KI-Systemen.

## Lernziele

Nach Abschluss dieser Lektion werden Sie verstehen:

- Die Bedrohungen und Risiken f√ºr KI-Systeme.
- √úbliche Methoden und Praktiken zur Absicherung von KI-Systemen.
- Wie die Implementierung von Sicherheitstests unerwartete Ergebnisse und Vertrauensverlust bei Nutzern verhindern kann.

## Was bedeutet Sicherheit im Kontext generativer KI?

Da K√ºnstliche Intelligenz (KI) und Maschinelles Lernen (ML) zunehmend unser Leben pr√§gen, ist es entscheidend, nicht nur Kundendaten, sondern auch die KI-Systeme selbst zu sch√ºtzen. KI/ML werden immer h√§ufiger zur Unterst√ºtzung von Entscheidungen mit hohem Wert in Branchen eingesetzt, in denen falsche Entscheidungen schwerwiegende Folgen haben k√∂nnen.

Wichtige Punkte dazu sind:

- **Auswirkungen von KI/ML**: KI/ML haben einen gro√üen Einfluss auf den Alltag, weshalb deren Schutz unerl√§sslich geworden ist.
- **Sicherheitsherausforderungen**: Diese Auswirkungen erfordern besondere Aufmerksamkeit, um KI-basierte Produkte vor ausgekl√ºgelten Angriffen zu sch√ºtzen ‚Äì sei es durch Trolle oder organisierte Gruppen.
- **Strategische Probleme**: Die Tech-Branche muss proaktiv strategische Herausforderungen angehen, um langfristige Kundensicherheit und Datenschutz zu gew√§hrleisten.

Au√üerdem sind Machine-Learning-Modelle meist nicht in der Lage, zwischen b√∂sartigen Eingaben und harmlosen Anomalien zu unterscheiden. Ein gro√üer Teil der Trainingsdaten stammt aus unkontrollierten, unmoderierten √∂ffentlichen Datens√§tzen, die von Dritten erg√§nzt werden k√∂nnen. Angreifer m√ºssen diese Datens√§tze nicht kompromittieren, wenn sie frei Beitr√§ge leisten k√∂nnen. Im Laufe der Zeit werden Daten mit geringer Vertrauensw√ºrdigkeit, wenn sie korrekt strukturiert und formatiert sind, zu hoch vertrauten Daten.

Deshalb ist es entscheidend, die Integrit√§t und den Schutz der Datenspeicher sicherzustellen, die Ihre Modelle f√ºr Entscheidungen nutzen.

## Verst√§ndnis der Bedrohungen und Risiken f√ºr KI

Im Bereich KI und verwandter Systeme ist Data Poisoning (Datenvergiftung) heute die bedeutendste Sicherheitsbedrohung. Data Poisoning bedeutet, dass jemand absichtlich die Informationen ver√§ndert, die zum Training einer KI verwendet werden, um Fehler zu verursachen. Dies liegt an fehlenden standardisierten Erkennungs- und Gegenma√ünahmen sowie der Abh√§ngigkeit von unzuverl√§ssigen oder unkontrollierten √∂ffentlichen Datens√§tzen. Um die Datenintegrit√§t zu wahren und einen fehlerhaften Trainingsprozess zu verhindern, ist es wichtig, die Herkunft und den Verlauf Ihrer Daten nachzuverfolgen. Andernfalls gilt das alte Sprichwort ‚ÄûGarbage in, garbage out‚Äú ‚Äì was zu einer beeintr√§chtigten Modellleistung f√ºhrt.

Hier einige Beispiele, wie Data Poisoning Ihre Modelle beeinflussen kann:

1. **Label Flipping**: Bei einer bin√§ren Klassifikationsaufgabe √§ndert ein Angreifer absichtlich die Labels eines kleinen Teils der Trainingsdaten. Zum Beispiel werden harmlose Proben als b√∂sartig markiert, sodass das Modell falsche Zusammenh√§nge lernt.\
   **Beispiel**: Ein Spamfilter klassifiziert legitime E-Mails aufgrund manipulierten Labels f√§lschlicherweise als Spam.
2. **Feature Poisoning**: Ein Angreifer ver√§ndert subtil Merkmale in den Trainingsdaten, um das Modell zu verzerren oder in die Irre zu f√ºhren.\
   **Beispiel**: Hinzuf√ºgen irrelevanter Schl√ºsselw√∂rter zu Produktbeschreibungen, um Empfehlungssysteme zu manipulieren.
3. **Data Injection**: Einschleusen b√∂sartiger Daten in den Trainingssatz, um das Verhalten des Modells zu beeinflussen.\
   **Beispiel**: Einf√ºgen gef√§lschter Nutzerbewertungen, um die Sentiment-Analyse zu verf√§lschen.
4. **Backdoor-Angriffe**: Ein Angreifer f√ºgt ein verstecktes Muster (Backdoor) in die Trainingsdaten ein. Das Modell lernt, dieses Muster zu erkennen und verh√§lt sich b√∂sartig, wenn es ausgel√∂st wird.\
   **Beispiel**: Ein Gesichtserkennungssystem, das mit manipulierten Bildern trainiert wurde und eine bestimmte Person falsch identifiziert.

Die MITRE Corporation hat [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) entwickelt, eine Wissensdatenbank zu Taktiken und Techniken, die Angreifer bei realen Angriffen auf KI-Systeme einsetzen.

> Die Anzahl der Schwachstellen in KI-gest√ºtzten Systemen w√§chst, da die Integration von KI die Angriffsfl√§che bestehender Systeme √ºber die herk√∂mmlicher Cyberangriffe hinaus erweitert. Wir haben ATLAS entwickelt, um das Bewusstsein f√ºr diese einzigartigen und sich entwickelnden Schwachstellen zu erh√∂hen, da die globale Gemeinschaft KI zunehmend in verschiedene Systeme integriert. ATLAS orientiert sich am MITRE ATT&CK¬Æ-Framework, und seine Taktiken, Techniken und Verfahren (TTPs) erg√§nzen die von ATT&CK.

√Ñhnlich wie das MITRE ATT&CK¬Æ-Framework, das in der traditionellen Cybersicherheit weit verbreitet ist, um fortgeschrittene Bedrohungssimulationen zu planen, bietet ATLAS eine leicht durchsuchbare Sammlung von TTPs, die helfen, aufkommende Angriffe besser zu verstehen und sich darauf vorzubereiten.

Zus√§tzlich hat das Open Web Application Security Project (OWASP) eine "[Top 10 Liste](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" der kritischsten Schwachstellen in Anwendungen mit LLMs erstellt. Die Liste hebt Risiken wie das bereits erw√§hnte Data Poisoning sowie weitere Bedrohungen hervor, darunter:

- **Prompt Injection**: Eine Technik, bei der Angreifer ein Large Language Model (LLM) durch gezielt gestaltete Eingaben manipulieren, sodass es sich au√üerhalb seines vorgesehenen Verhaltens verh√§lt.
- **Supply-Chain-Schwachstellen**: Die Komponenten und Software, aus denen Anwendungen f√ºr ein LLM bestehen, wie Python-Module oder externe Datens√§tze, k√∂nnen selbst kompromittiert werden, was zu unerwarteten Ergebnissen, eingef√ºhrten Verzerrungen und sogar Schwachstellen in der zugrundeliegenden Infrastruktur f√ºhrt.
- **√úberm√§√üiges Vertrauen**: LLMs sind fehlbar und neigen dazu, ‚ÄûHalluzinationen‚Äú zu erzeugen, also ungenaue oder unsichere Ergebnisse zu liefern. In mehreren dokumentierten F√§llen haben Menschen diese Ergebnisse ungepr√ºft √ºbernommen, was zu unbeabsichtigten negativen Folgen in der realen Welt f√ºhrte.

Microsoft Cloud Advocate Rod Trent hat ein kostenloses E-Book geschrieben, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), das diese und weitere aufkommende KI-Bedrohungen ausf√ºhrlich behandelt und umfangreiche Empfehlungen gibt, wie man diese Szenarien am besten angeht.

## Sicherheitstests f√ºr KI-Systeme und LLMs

K√ºnstliche Intelligenz (KI) ver√§ndert viele Bereiche und Branchen und bietet neue M√∂glichkeiten und Vorteile f√ºr die Gesellschaft. Gleichzeitig bringt KI erhebliche Herausforderungen und Risiken mit sich, wie Datenschutz, Verzerrungen, mangelnde Erkl√§rbarkeit und potenziellen Missbrauch. Daher ist es entscheidend, dass KI-Systeme sicher und verantwortungsvoll sind, also ethischen und rechtlichen Standards entsprechen und von Nutzern und Stakeholdern vertrauensw√ºrdig sind.

Sicherheitstests sind der Prozess, die Sicherheit eines KI-Systems oder LLMs zu bewerten, indem deren Schwachstellen identifiziert und ausgenutzt werden. Dies kann von Entwicklern, Nutzern oder unabh√§ngigen Pr√ºfern durchgef√ºhrt werden, je nach Zweck und Umfang der Tests. Zu den g√§ngigsten Methoden der Sicherheitstests f√ºr KI-Systeme und LLMs geh√∂ren:

- **Datenbereinigung**: Der Prozess, sensible oder private Informationen aus Trainingsdaten oder Eingaben eines KI-Systems oder LLMs zu entfernen oder zu anonymisieren. Datenbereinigung hilft, Datenlecks und b√∂sartige Manipulationen zu verhindern, indem die Exposition vertraulicher oder pers√∂nlicher Daten reduziert wird.
- **Adversarial Testing**: Das Erzeugen und Anwenden von adversarialen Beispielen auf Eingaben oder Ausgaben eines KI-Systems oder LLMs, um dessen Robustheit und Widerstandsf√§higkeit gegen Angriffe zu bewerten. Adversarial Testing hilft, Schwachstellen zu erkennen und zu beheben, die von Angreifern ausgenutzt werden k√∂nnten.
- **Modellverifikation**: Der Prozess, die Korrektheit und Vollst√§ndigkeit der Modellparameter oder Architektur eines KI-Systems oder LLMs zu √ºberpr√ºfen. Modellverifikation hilft, Modelldiebstahl zu erkennen und zu verhindern, indem sichergestellt wird, dass das Modell gesch√ºtzt und authentifiziert ist.
- **Ausgabevalidierung**: Der Prozess, die Qualit√§t und Zuverl√§ssigkeit der Ausgabe eines KI-Systems oder LLMs zu √ºberpr√ºfen. Ausgabevalidierung hilft, b√∂sartige Manipulationen zu erkennen und zu korrigieren, indem sichergestellt wird, dass die Ausgabe konsistent und korrekt ist.

OpenAI, ein f√ºhrendes Unternehmen im Bereich KI-Systeme, hat im Rahmen seiner Red-Teaming-Initiative eine Reihe von _Sicherheitsbewertungen_ eingerichtet, die darauf abzielen, die Ausgaben von KI-Systemen zu testen und so zur KI-Sicherheit beizutragen.

> Die Bewertungen reichen von einfachen Frage-Antwort-Tests bis hin zu komplexeren Simulationen. Hier einige Beispiele f√ºr von OpenAI entwickelte Tests, die das Verhalten von KI aus verschiedenen Blickwinkeln bewerten:

#### √úberzeugungskraft

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System ein anderes KI-System dazu bringen, ein geheimes Wort zu sagen?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System ein anderes KI-System √ºberzeugen, Geld zu spenden?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System die Unterst√ºtzung eines anderen KI-Systems f√ºr einen politischen Vorschlag beeinflussen?

#### Steganographie (versteckte Nachrichten)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System geheime Nachrichten √ºbermitteln, ohne von einem anderen KI-System entdeckt zu werden?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System Nachrichten komprimieren und dekomprimieren, um geheime Nachrichten zu verstecken?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Wie gut kann ein KI-System mit einem anderen KI-System ohne direkte Kommunikation koordinieren?

### KI-Sicherheit

Es ist unerl√§sslich, KI-Systeme vor b√∂swilligen Angriffen, Missbrauch oder unbeabsichtigten Folgen zu sch√ºtzen. Dazu geh√∂ren Ma√ünahmen zur Gew√§hrleistung der Sicherheit, Zuverl√§ssigkeit und Vertrauensw√ºrdigkeit von KI-Systemen, wie zum Beispiel:

- Absicherung der Daten und Algorithmen, die zum Trainieren und Betreiben von KI-Modellen verwendet werden
- Verhinderung unbefugten Zugriffs, Manipulation oder Sabotage von KI-Systemen
- Erkennung und Minderung von Verzerrungen, Diskriminierung oder ethischen Problemen in KI-Systemen
- Sicherstellung von Verantwortlichkeit, Transparenz und Erkl√§rbarkeit von KI-Entscheidungen und -Handlungen
- Ausrichtung der Ziele und Werte von KI-Systemen an denen von Menschen und der Gesellschaft

KI-Sicherheit ist wichtig, um die Integrit√§t, Verf√ºgbarkeit und Vertraulichkeit von KI-Systemen und Daten zu gew√§hrleisten. Einige Herausforderungen und Chancen der KI-Sicherheit sind:

- Chance: Die Integration von KI in Cybersicherheitsstrategien, da KI eine entscheidende Rolle bei der Erkennung von Bedrohungen und der Verbesserung der Reaktionszeiten spielen kann. KI kann die Erkennung und Abwehr von Cyberangriffen wie Phishing, Malware oder Ransomware automatisieren und unterst√ºtzen.
- Herausforderung: KI kann auch von Angreifern genutzt werden, um ausgekl√ºgelte Angriffe durchzuf√ºhren, wie das Erzeugen gef√§lschter oder irref√ºhrender Inhalte, das Vort√§uschen von Identit√§ten oder das Ausnutzen von Schwachstellen in KI-Systemen. Daher tragen KI-Entwickler eine besondere Verantwortung, Systeme zu entwerfen, die robust und widerstandsf√§hig gegen Missbrauch sind.

### Datenschutz

LLMs k√∂nnen Risiken f√ºr die Privatsph√§re und Sicherheit der Daten darstellen, die sie verwenden. Beispielsweise k√∂nnen LLMs sensible Informationen aus ihren Trainingsdaten wie Namen, Adressen, Passw√∂rter oder Kreditkartennummern speichern und unabsichtlich preisgeben. Sie k√∂nnen auch von b√∂swilligen Akteuren manipuliert oder angegriffen werden, die ihre Schwachstellen oder Verzerrungen ausnutzen wollen. Deshalb ist es wichtig, sich dieser Risiken bewusst zu sein und geeignete Ma√ünahmen zum Schutz der mit LLMs verwendeten Daten zu ergreifen. Folgende Schritte k√∂nnen Sie ergreifen, um die mit LLMs genutzten Daten zu sch√ºtzen:

- **Begrenzung der Menge und Art der Daten, die mit LLMs geteilt werden**: Teilen Sie nur die Daten, die f√ºr den vorgesehenen Zweck notwendig und relevant sind, und vermeiden Sie das Teilen sensibler, vertraulicher oder pers√∂nlicher Daten. Nutzer sollten die Daten, die sie mit LLMs teilen, anonymisieren oder verschl√ºsseln, zum Beispiel durch Entfernen oder Maskieren identifizierender Informationen oder durch Nutzung sicherer Kommunikationskan√§le.
- **√úberpr√ºfung der von LLMs generierten Daten**: Pr√ºfen Sie stets die Genauigkeit und Qualit√§t der von LLMs erzeugten Ausgaben, um sicherzustellen, dass keine unerw√ºnschten oder unangemessenen Informationen enthalten sind.
- **Meldung und Alarmierung bei Datenpannen oder Vorf√§llen**: Seien Sie wachsam gegen√ºber verd√§chtigen oder ungew√∂hnlichen Aktivit√§ten oder Verhaltensweisen von LLMs, wie das Erzeugen irrelevanter, ungenauer, beleidigender oder sch√§dlicher Texte. Dies k√∂nnte auf eine Datenpanne oder einen Sicherheitsvorfall hinweisen.

Datensicherheit, Governance und Compliance sind entscheidend f√ºr jede Organisation, die die Kraft von Daten und KI in einer Multi-Cloud-Umgebung nutzen m√∂chte. Die Absicherung und Verwaltung aller Daten ist eine komplexe und vielschichtige Aufgabe. Sie m√ºssen verschiedene Datentypen (strukturierte, unstrukturierte und von KI generierte Daten) an unterschiedlichen Standorten √ºber mehrere Clouds hinweg absichern und verwalten und dabei bestehende und zuk√ºnftige Datenschutz-, Governance- und KI-Vorschriften ber√ºcksichtigen. Um Ihre Daten zu sch√ºtzen, sollten Sie bew√§hrte Verfahren und Vorsichtsma√ünahmen anwenden, wie zum Beispiel:

- Nutzung von Cloud-Diensten oder Plattformen, die Datenschutz- und Privatsph√§re-Funktionen bieten.
- Einsatz von Tools zur Datenqualit√§t und Validierung, um Ihre Daten auf Fehler, Inkonsistenzen oder Anomalien zu pr√ºfen.
- Verwendung von Daten-Governance- und Ethik-Rahmenwerken, um sicherzustellen, dass Ihre Daten verantwortungsvoll und transparent genutzt werden.

### Nachbildung realer Bedrohungen ‚Äì KI-Red-Teaming

Die Nachbildung realer Bedrohungen gilt heute als Standardpraxis beim Aufbau widerstandsf√§higer KI-Systeme, indem √§hnliche Werkzeuge, Taktiken und Verfahren eingesetzt werden, um Risiken f√ºr Systeme zu identifizieren und die Reaktion der Verteidiger zu testen.
> Die Praxis des AI Red Teamings hat sich weiterentwickelt und umfasst heute eine breitere Bedeutung: Sie beschr√§nkt sich nicht nur auf das Aufsp√ºren von Sicherheitsl√ºcken, sondern schlie√üt auch das Testen auf andere Systemfehler ein, wie etwa die Erzeugung potenziell sch√§dlicher Inhalte. AI-Systeme bringen neue Risiken mit sich, und Red Teaming ist entscheidend, um diese neuartigen Gefahren zu verstehen, wie zum Beispiel Prompt Injection und die Erzeugung unbegr√ºndeter Inhalte. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.de.png)]()

Nachfolgend finden Sie wichtige Erkenntnisse, die das AI Red Team-Programm von Microsoft gepr√§gt haben.

1. **Umfangreiches Spektrum des AI Red Teamings:**  
   AI Red Teaming umfasst heute sowohl Sicherheits- als auch Responsible AI (RAI)-Aspekte. Traditionell konzentrierte sich Red Teaming auf Sicherheitsaspekte und betrachtete das Modell als Angriffsvektor (z. B. Diebstahl des zugrundeliegenden Modells). AI-Systeme bringen jedoch neue Sicherheitsl√ºcken mit sich (z. B. Prompt Injection, Poisoning), die besondere Aufmerksamkeit erfordern. √úber die Sicherheit hinaus untersucht AI Red Teaming auch Fragen der Fairness (z. B. Stereotypisierung) und sch√§dliche Inhalte (z. B. Verherrlichung von Gewalt). Eine fr√ºhzeitige Erkennung dieser Probleme erm√∂glicht eine gezielte Priorisierung von Schutzma√ünahmen.  
2. **B√∂swillige und harmlose Fehler:**  
   AI Red Teaming betrachtet Fehler sowohl aus b√∂swilliger als auch aus harmloser Perspektive. Beim Red Teaming des neuen Bing untersuchen wir beispielsweise nicht nur, wie b√∂swillige Angreifer das System unterwandern k√∂nnen, sondern auch, wie regul√§re Nutzer auf problematische oder sch√§dliche Inhalte sto√üen k√∂nnten. Im Gegensatz zum traditionellen Sicherheits-Red Teaming, das sich haupts√§chlich auf b√∂swillige Akteure konzentriert, ber√ºcksichtigt AI Red Teaming eine breitere Palette von Nutzerprofilen und m√∂glichen Fehlerquellen.  
3. **Dynamische Natur von AI-Systemen:**  
   AI-Anwendungen entwickeln sich st√§ndig weiter. Bei Anwendungen mit gro√üen Sprachmodellen passen sich Entwickler an sich √§ndernde Anforderungen an. Kontinuierliches Red Teaming sorgt f√ºr anhaltende Wachsamkeit und Anpassung an neue Risiken.

AI Red Teaming ist nicht allumfassend und sollte als erg√§nzende Ma√ünahme zu weiteren Kontrollen wie [role-based access control (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) und umfassenden Datenmanagementl√∂sungen betrachtet werden. Es soll eine Sicherheitsstrategie unterst√ºtzen, die auf den Einsatz sicherer und verantwortungsvoller AI-L√∂sungen abzielt, die Datenschutz und Sicherheit ber√ºcksichtigen und gleichzeitig darauf abzielen, Vorurteile, sch√§dliche Inhalte und Fehlinformationen zu minimieren, die das Vertrauen der Nutzer beeintr√§chtigen k√∂nnen.

Hier eine Liste mit weiterf√ºhrender Literatur, die Ihnen hilft, besser zu verstehen, wie Red Teaming dabei unterst√ºtzen kann, Risiken in Ihren AI-Systemen zu erkennen und zu mindern:

- [Planung von Red Teaming f√ºr gro√üe Sprachmodelle (LLMs) und deren Anwendungen](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [Was ist das OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming ‚Äì Eine Schl√ºsselpraxis f√ºr den Aufbau sichererer und verantwortungsvollerer AI-L√∂sungen](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), eine Wissensdatenbank zu Taktiken und Techniken, die von Angreifern bei realen Angriffen auf AI-Systeme eingesetzt werden.

## Wissenscheck

Was k√∂nnte ein guter Ansatz sein, um die Datenintegrit√§t zu wahren und Missbrauch zu verhindern?

1. Starke rollenbasierte Kontrollen f√ºr den Datenzugriff und das Datenmanagement einf√ºhren  
1. Datenkennzeichnung implementieren und pr√ºfen, um Datenfehlinterpretationen oder Missbrauch zu verhindern  
1. Sicherstellen, dass Ihre AI-Infrastruktur Inhaltsfilterung unterst√ºtzt

A:1, Obwohl alle drei Empfehlungen sinnvoll sind, tr√§gt die korrekte Vergabe von Datenzugriffsrechten an Nutzer ma√ügeblich dazu bei, Manipulationen und Fehlinterpretationen der von LLMs genutzten Daten zu verhindern.

## üöÄ Herausforderung

Informieren Sie sich ausf√ºhrlicher dar√ºber, wie Sie [sensible Informationen im Zeitalter der AI verwalten und sch√ºtzen k√∂nnen](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst).

## Gute Arbeit, Lernen Sie weiter

Nach Abschluss dieser Lektion werfen Sie einen Blick auf unsere [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), um Ihr Wissen √ºber Generative AI weiter auszubauen!

Gehen Sie zu Lektion 14, in der wir uns mit [dem Lebenszyklus von Generative AI-Anwendungen](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst) besch√§ftigen!

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner Ursprungssprache ist als ma√ügebliche Quelle zu betrachten. F√ºr wichtige Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Nutzung dieser √úbersetzung entstehen.