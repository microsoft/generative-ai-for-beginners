{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Kapitel 7: Chat-Anwendungen erstellen\n",
    "## Github Models API Schnellstart\n",
    "\n",
    "Dieses Notebook wurde aus dem [Azure OpenAI Samples Repository](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst) übernommen, das Notebooks enthält, die auf [Azure OpenAI](notebook-azure-openai.ipynb) Dienste zugreifen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Überblick  \n",
    "„Große Sprachmodelle sind Funktionen, die Text auf Text abbilden. Wenn sie eine Eingabezeichenkette erhalten, versuchen sie vorherzusagen, welcher Text als Nächstes kommt“(1). Dieses „Schnellstart“-Notebook führt Nutzer in grundlegende LLM-Konzepte ein, zeigt die wichtigsten Paketvoraussetzungen für den Einstieg mit AML, gibt eine sanfte Einführung in das Prompt-Design und enthält mehrere kurze Beispiele für verschiedene Anwendungsfälle.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Inhaltsverzeichnis  \n",
    "\n",
    "[Überblick](../../../../07-building-chat-applications/python)  \n",
    "[Wie man den OpenAI Service verwendet](../../../../07-building-chat-applications/python)  \n",
    "[1. Erstellen Ihres OpenAI Service](../../../../07-building-chat-applications/python)  \n",
    "[2. Installation](../../../../07-building-chat-applications/python)    \n",
    "[3. Zugangsdaten](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Anwendungsfälle](../../../../07-building-chat-applications/python)    \n",
    "[1. Text zusammenfassen](../../../../07-building-chat-applications/python)  \n",
    "[2. Text klassifizieren](../../../../07-building-chat-applications/python)  \n",
    "[3. Neue Produktnamen generieren](../../../../07-building-chat-applications/python)  \n",
    "[4. Einen Klassifikator feinabstimmen](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Referenzen](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Erstellen Sie Ihren ersten Prompt  \n",
    "Diese kurze Übung bietet eine grundlegende Einführung, wie Sie Prompts an ein Modell in Github Models für eine einfache Aufgabe wie die „Zusammenfassung“ senden können.\n",
    "\n",
    "**Schritte**:  \n",
    "1. Installieren Sie die Bibliothek `azure-ai-inference` in Ihrer Python-Umgebung, falls Sie dies noch nicht getan haben.  \n",
    "2. Laden Sie die Standard-Hilfsbibliotheken und richten Sie die Github Models-Anmeldedaten ein.  \n",
    "3. Wählen Sie ein Modell für Ihre Aufgabe aus  \n",
    "4. Erstellen Sie einen einfachen Prompt für das Modell  \n",
    "5. Senden Sie Ihre Anfrage an die Modell-API!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Installieren Sie `azure-ai-inference`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Das richtige Modell finden  \n",
    "Die Modelle GPT-3.5-turbo oder GPT-4 können natürliche Sprache verstehen und erzeugen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Prompt-Design  \n",
    "\n",
    "„Das Besondere an großen Sprachmodellen ist, dass sie durch das Minimieren des Vorhersagefehlers über riesige Mengen an Text letztlich Konzepte lernen, die für diese Vorhersagen nützlich sind. Zum Beispiel lernen sie Konzepte wie“(1):\n",
    "\n",
    "* wie man richtig schreibt\n",
    "* wie Grammatik funktioniert\n",
    "* wie man umformuliert\n",
    "* wie man Fragen beantwortet\n",
    "* wie man ein Gespräch führt\n",
    "* wie man in vielen Sprachen schreibt\n",
    "* wie man programmiert\n",
    "* usw.\n",
    "\n",
    "#### Wie man ein großes Sprachmodell steuert  \n",
    "„Von allen Eingaben für ein großes Sprachmodell ist der Text-Prompt mit Abstand der wichtigste Faktor(1).\n",
    "\n",
    "Große Sprachmodelle können auf verschiedene Arten dazu gebracht werden, eine Ausgabe zu erzeugen:\n",
    "\n",
    "Anweisung: Sage dem Modell, was du möchtest\n",
    "Vervollständigung: Bringe das Modell dazu, den Anfang dessen zu vervollständigen, was du möchtest\n",
    "Demonstration: Zeige dem Modell, was du möchtest, entweder durch:\n",
    "Einige Beispiele im Prompt\n",
    "Viele hunderte oder tausende Beispiele in einem Fine-Tuning-Trainingsdatensatz“\n",
    "\n",
    "\n",
    "\n",
    "#### Es gibt drei grundlegende Richtlinien für die Erstellung von Prompts:\n",
    "\n",
    "**Zeigen und erklären**. Mache klar, was du möchtest – entweder durch Anweisungen, Beispiele oder eine Kombination aus beidem. Wenn du möchtest, dass das Modell eine Liste alphabetisch sortiert oder einen Absatz nach Stimmung klassifiziert, zeige ihm, dass genau das gewünscht ist.\n",
    "\n",
    "**Gib hochwertige Daten vor**. Wenn du versuchst, einen Klassifizierer zu bauen oder das Modell einem Muster folgen zu lassen, stelle sicher, dass genügend Beispiele vorhanden sind. Überprüfe deine Beispiele sorgfältig – das Modell ist meist klug genug, um einfache Rechtschreibfehler zu erkennen und trotzdem zu antworten, aber es könnte auch annehmen, dass dies Absicht ist, was die Antwort beeinflussen kann.\n",
    "\n",
    "**Überprüfe deine Einstellungen.** Die Einstellungen für Temperatur und top_p steuern, wie vorhersehbar das Modell bei der Antwort ist. Wenn du eine Antwort erwartest, bei der es nur eine richtige Lösung gibt, solltest du diese Werte niedriger setzen. Wenn du vielfältigere Antworten möchtest, kannst du sie höher einstellen. Der häufigste Fehler bei diesen Einstellungen ist, zu glauben, dass sie die „Intelligenz“ oder „Kreativität“ des Modells steuern.\n",
    "\n",
    "\n",
    "Quelle: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Text zusammenfassen  \n",
    "#### Herausforderung  \n",
    "Fasse einen Text zusammen, indem du am Ende eines Textabschnitts ein 'tl;dr:' hinzufügst. Beachte, wie das Modell in der Lage ist, verschiedene Aufgaben ohne zusätzliche Anweisungen auszuführen. Du kannst mit ausführlicheren Prompts als tl;dr experimentieren, um das Verhalten des Modells zu beeinflussen und die Zusammenfassung nach deinen Wünschen anzupassen(3).  \n",
    "\n",
    "Aktuelle Arbeiten haben gezeigt, dass durch Vortraining auf einem großen Textkorpus und anschließendes Feintuning auf eine spezifische Aufgabe deutliche Fortschritte bei vielen NLP-Aufgaben und Benchmarks erzielt werden können. Obwohl diese Methode in der Regel aufgabenunabhängig in der Architektur ist, benötigt sie dennoch aufgabenspezifische Feintuning-Datensätze mit Tausenden oder Zehntausenden von Beispielen. Im Gegensatz dazu können Menschen in der Regel eine neue Sprachaufgabe schon mit wenigen Beispielen oder einfachen Anweisungen bewältigen – etwas, womit aktuelle NLP-Systeme noch weitgehend Schwierigkeiten haben. Hier zeigen wir, dass die Skalierung von Sprachmodellen die aufgabenunabhängige Few-Shot-Leistung deutlich verbessert und manchmal sogar mit bisherigen State-of-the-Art-Feintuning-Ansätzen konkurrieren kann.  \n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Übungen für verschiedene Anwendungsfälle  \n",
    "1. Text zusammenfassen  \n",
    "2. Text klassifizieren  \n",
    "3. Neue Produktnamen generieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Text klassifizieren  \n",
    "#### Herausforderung  \n",
    "Ordne Elemente in Kategorien ein, die zur Laufzeit vorgegeben werden. Im folgenden Beispiel geben wir sowohl die Kategorien als auch den zu klassifizierenden Text im Prompt an (*playground_reference).\n",
    "\n",
    "Kundenanfrage: Hallo, eine der Tasten auf meiner Laptop-Tastatur ist kürzlich kaputt gegangen und ich brauche einen Ersatz:\n",
    "\n",
    "Klassifizierte Kategorie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Neue Produktnamen generieren\n",
    "#### Herausforderung\n",
    "Erstelle Produktnamen aus Beispielwörtern. In der Aufgabenstellung geben wir Informationen über das Produkt, für das Namen generiert werden sollen. Außerdem zeigen wir ein ähnliches Beispiel, um das gewünschte Muster zu verdeutlichen. Die Temperatur wurde hoch eingestellt, um mehr Zufall und innovative Antworten zu ermöglichen.\n",
    "\n",
    "Produktbeschreibung: Ein Milchshake-Maker für zuhause\n",
    "Stichwörter: schnell, gesund, kompakt.\n",
    "Produktnamen: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Produktbeschreibung: Ein Paar Schuhe, das auf jede Fußgröße passt.\n",
    "Stichwörter: anpassbar, passgenau, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Referenzen  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [OpenAI Studio Beispiele](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Best Practices für das Fine-Tuning von GPT-3 zur Textklassifizierung](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Für weitere Hilfe  \n",
    "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Mitwirkende\n",
    "* [Chew-Yean Yam](https://www.linkedin.com/in/cyyam/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Haftungsausschluss**:  \nDieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ausgangssprache gilt als maßgebliche Quelle. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "3eeb8e5cad61b52a8366f6259a53ed49",
   "translation_date": "2025-08-25T17:20:45+00:00",
   "source_file": "07-building-chat-applications/python/githubmodels-assignment.ipynb",
   "language_code": "de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}