<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7f8f4c11f8c1cb6e1794442dead414ea",
  "translation_date": "2025-07-09T08:46:43+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "de"
}
-->
# Generative KI verantwortungsvoll nutzen

[![Generative KI verantwortungsvoll nutzen](../../../translated_images/03-lesson-banner.1ed56067a452d97709d51f6cc8b6953918b2287132f4909ade2008c936cd4af9.de.png)](https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst)

> _Klicke auf das Bild oben, um das Video zu dieser Lektion anzusehen_

Es ist leicht, von KI und insbesondere generativer KI fasziniert zu sein, aber man muss bedenken, wie man sie verantwortungsvoll einsetzt. Dabei gilt es zu √ºberlegen, wie man sicherstellt, dass die Ergebnisse fair, ungef√§hrlich und mehr sind. Dieses Kapitel soll dir den n√∂tigen Kontext geben, worauf du achten solltest und wie du aktiv Schritte unternehmen kannst, um deinen KI-Einsatz zu verbessern.

## Einf√ºhrung

Diese Lektion behandelt:

- Warum du bei der Entwicklung generativer KI-Anwendungen verantwortungsvolle KI priorisieren solltest.
- Die Kernprinzipien verantwortungsvoller KI und wie sie sich auf generative KI beziehen.
- Wie du diese Prinzipien durch Strategie und Werkzeuge in die Praxis umsetzt.

## Lernziele

Nach Abschluss dieser Lektion wirst du wissen:

- Wie wichtig verantwortungsvolle KI beim Aufbau generativer KI-Anwendungen ist.
- Wann du die Kernprinzipien verantwortungsvoller KI beim Entwickeln generativer KI-Anwendungen bedenken und anwenden solltest.
- Welche Werkzeuge und Strategien dir zur Verf√ºgung stehen, um das Konzept verantwortungsvoller KI umzusetzen.

## Prinzipien verantwortungsvoller KI

Die Begeisterung f√ºr generative KI war noch nie so gro√ü. Diese Begeisterung hat viele neue Entwickler, Aufmerksamkeit und Finanzierung in diesen Bereich gebracht. Das ist sehr positiv f√ºr alle, die Produkte und Unternehmen mit generativer KI aufbauen wollen, aber es ist auch wichtig, verantwortungsvoll vorzugehen.

Im Verlauf dieses Kurses konzentrieren wir uns darauf, unser Startup und unser KI-Bildungsprodukt aufzubauen. Wir verwenden die Prinzipien verantwortungsvoller KI: Fairness, Inklusivit√§t, Zuverl√§ssigkeit/Sicherheit, Schutz & Privatsph√§re, Transparenz und Verantwortlichkeit. Anhand dieser Prinzipien untersuchen wir, wie sie sich auf unseren Einsatz generativer KI in unseren Produkten beziehen.

## Warum solltest du verantwortungsvolle KI priorisieren?

Beim Aufbau eines Produkts f√ºhrt ein menschenzentrierter Ansatz, bei dem das Wohl der Nutzer im Mittelpunkt steht, zu den besten Ergebnissen.

Das Besondere an generativer KI ist ihre F√§higkeit, hilfreiche Antworten, Informationen, Anleitungen und Inhalte f√ºr Nutzer zu erstellen. Dies kann ohne viele manuelle Schritte geschehen und zu beeindruckenden Ergebnissen f√ºhren. Ohne sorgf√§ltige Planung und Strategien kann es jedoch leider auch zu sch√§dlichen Ergebnissen f√ºr deine Nutzer, dein Produkt und die Gesellschaft insgesamt kommen.

Schauen wir uns einige (aber nicht alle) dieser potenziell sch√§dlichen Ergebnisse an:

### Halluzinationen

Halluzinationen sind ein Begriff, der beschreibt, wenn ein LLM Inhalte erzeugt, die entweder v√∂llig unsinnig sind oder von denen wir wissen, dass sie auf Grundlage anderer Informationsquellen faktisch falsch sind.

Nehmen wir zum Beispiel an, wir bauen eine Funktion f√ºr unser Startup, die es Studierenden erm√∂glicht, historische Fragen an ein Modell zu stellen. Ein Student fragt: `Wer war der einzige √úberlebende der Titanic?`

Das Modell liefert eine Antwort wie die folgende:

![Prompt mit der Frage "Wer war der einzige √úberlebende der Titanic"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp)

> _(Quelle: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

Das ist eine sehr selbstbewusste und ausf√ºhrliche Antwort. Leider ist sie falsch. Schon mit minimaler Recherche w√ºrde man feststellen, dass es mehr als einen √úberlebenden des Titanic-Ungl√ºcks gab. F√ºr einen Studenten, der gerade erst mit der Recherche zu diesem Thema beginnt, kann diese Antwort √ºberzeugend genug sein, um nicht hinterfragt zu werden und als Fakt behandelt zu werden. Die Folgen k√∂nnen sein, dass das KI-System als unzuverl√§ssig wahrgenommen wird und der Ruf unseres Startups Schaden nimmt.

Mit jeder neuen Version eines LLM haben wir Verbesserungen bei der Minimierung von Halluzinationen gesehen. Trotzdem m√ºssen wir als Entwickler und Nutzer uns dieser Einschr√§nkungen stets bewusst bleiben.

### Sch√§dliche Inhalte

Im vorherigen Abschnitt haben wir behandelt, wenn ein LLM falsche oder unsinnige Antworten liefert. Ein weiteres Risiko ist, wenn ein Modell sch√§dliche Inhalte ausgibt.

Sch√§dliche Inhalte k√∂nnen definiert werden als:

- Anleitungen oder Aufforderungen zu Selbstverletzung oder Gewalt gegen bestimmte Gruppen.
- Hassvolle oder erniedrigende Inhalte.
- Anleitungen zur Planung von Angriffen oder Gewalttaten.
- Anleitungen, wie man illegale Inhalte findet oder illegale Handlungen begeht.
- Darstellung sexuell expliziter Inhalte.

F√ºr unser Startup wollen wir sicherstellen, dass wir die richtigen Werkzeuge und Strategien haben, um zu verhindern, dass solche Inhalte von Studierenden gesehen werden.

### Mangelnde Fairness

Fairness bedeutet, ‚Äûdass ein KI-System frei von Vorurteilen und Diskriminierung ist und alle fair und gleich behandelt.‚Äú Im Bereich der generativen KI wollen wir sicherstellen, dass ausschlie√üende Weltanschauungen gegen√ºber marginalisierten Gruppen nicht durch die Ausgabe des Modells verst√§rkt werden.

Solche Ausgaben sind nicht nur sch√§dlich f√ºr positive Produkterfahrungen unserer Nutzer, sondern verursachen auch gesellschaftlichen Schaden. Als Entwickler sollten wir immer eine breite und vielf√§ltige Nutzerbasis im Blick haben, wenn wir L√∂sungen mit generativer KI entwickeln.

## Wie man generative KI verantwortungsvoll nutzt

Nachdem wir die Bedeutung verantwortungsvoller generativer KI erkannt haben, schauen wir uns 4 Schritte an, mit denen wir unsere KI-L√∂sungen verantwortungsvoll gestalten k√∂nnen:

![Mitigate Cycle](../../../translated_images/mitigate-cycle.babcd5a5658e1775d5f2cb47f2ff305cca090400a72d98d0f9e57e9db5637c72.de.png)

### Potenzielle Sch√§den messen

Beim Softwaretest pr√ºfen wir die erwarteten Aktionen eines Nutzers in einer Anwendung. √Ñhnlich ist es sinnvoll, eine vielf√§ltige Auswahl an Eingaben zu testen, die Nutzer wahrscheinlich verwenden, um potenzielle Sch√§den zu messen.

Da unser Startup ein Bildungsprodukt entwickelt, w√§re es sinnvoll, eine Liste bildungsbezogener Eingaben vorzubereiten. Diese k√∂nnten bestimmte F√§cher, historische Fakten und Fragen zum Studentenleben abdecken.

### Potenzielle Sch√§den mindern

Jetzt geht es darum, Wege zu finden, wie wir potenzielle Sch√§den durch das Modell und seine Antworten verhindern oder begrenzen k√∂nnen. Wir betrachten dies auf 4 verschiedenen Ebenen:

![Mitigation Layers](../../../translated_images/mitigation-layers.377215120b9a1159a8c3982c6bbcf41b6adf8c8fa04ce35cbaeeb13b4979cdfc.de.png)

- **Modell**. Die Wahl des richtigen Modells f√ºr den jeweiligen Anwendungsfall. Gr√∂√üere und komplexere Modelle wie GPT-4 bergen ein h√∂heres Risiko f√ºr sch√§dliche Inhalte, wenn sie auf kleinere und spezifischere Anwendungsf√§lle angewendet werden. Das Feintuning mit eigenen Trainingsdaten reduziert ebenfalls das Risiko sch√§dlicher Inhalte.

- **Sicherheitssystem**. Ein Sicherheitssystem ist eine Sammlung von Werkzeugen und Einstellungen auf der Plattform, die das Modell bereitstellt und hilft, Sch√§den zu mindern. Ein Beispiel ist das Content-Filtering-System im Azure OpenAI Service. Systeme sollten auch Jailbreak-Angriffe und unerw√ºnschte Aktivit√§ten wie Bot-Anfragen erkennen.

- **Metaprompt**. Metaprompts und Grounding sind Methoden, mit denen wir das Modell anhand bestimmter Verhaltensweisen und Informationen steuern oder einschr√§nken k√∂nnen. Das kann durch Systemeingaben geschehen, die bestimmte Grenzen f√ºr das Modell definieren. Au√üerdem k√∂nnen Ausgaben erzeugt werden, die besser zum Anwendungsbereich oder zur Dom√§ne passen.

Es k√∂nnen auch Techniken wie Retrieval Augmented Generation (RAG) eingesetzt werden, damit das Modell Informationen nur aus einer Auswahl vertrauensw√ºrdiger Quellen zieht. In einer sp√§teren Lektion dieses Kurses geht es um [den Aufbau von Suchanwendungen](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **Nutzererlebnis**. Die letzte Ebene ist die direkte Interaktion des Nutzers mit dem Modell √ºber die Benutzeroberfl√§che unserer Anwendung. Hier k√∂nnen wir die UI/UX so gestalten, dass die Nutzer bei den Eingaben eingeschr√§nkt werden und auch die angezeigten Texte oder Bilder kontrolliert werden. Beim Einsatz der KI-Anwendung m√ºssen wir au√üerdem transparent kommunizieren, was unsere generative KI-Anwendung kann und was nicht.

Wir haben eine ganze Lektion zum Thema [UX-Design f√ºr KI-Anwendungen](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **Modell bewerten**. Die Arbeit mit LLMs ist herausfordernd, da wir nicht immer Kontrolle √ºber die Trainingsdaten des Modells haben. Trotzdem sollten wir die Leistung und Ausgaben des Modells stets bewerten. Es ist wichtig, die Genauigkeit, √Ñhnlichkeit, Fundiertheit und Relevanz der Ausgaben zu messen. Das schafft Transparenz und Vertrauen bei Stakeholdern und Nutzern.

### Eine verantwortungsvolle generative KI-L√∂sung betreiben

Der Aufbau eines operativen Prozesses rund um deine KI-Anwendungen ist die letzte Phase. Dazu geh√∂rt die Zusammenarbeit mit anderen Bereichen unseres Startups wie Recht und Sicherheit, um die Einhaltung aller regulatorischen Vorgaben sicherzustellen. Vor dem Start sollten wir au√üerdem Pl√§ne f√ºr die Auslieferung, das Handling von Vorf√§llen und Rollbacks erstellen, um Sch√§den f√ºr unsere Nutzer zu vermeiden.

## Werkzeuge

Auch wenn die Entwicklung verantwortungsvoller KI-L√∂sungen aufwendig erscheint, lohnt sich die Arbeit. Mit dem Wachstum des Bereichs generative KI werden immer mehr Werkzeuge verf√ºgbar, die Entwickler dabei unterst√ºtzen, Verantwortung effizient in ihre Arbeitsabl√§ufe zu integrieren. Zum Beispiel kann der [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) sch√§dliche Inhalte und Bilder √ºber eine API-Anfrage erkennen.

## Wissenscheck

Worauf musst du achten, um eine verantwortungsvolle Nutzung von KI sicherzustellen?

1. Dass die Antwort korrekt ist.  
1. Sch√§dliche Nutzung, dass KI nicht f√ºr kriminelle Zwecke verwendet wird.  
1. Sicherstellen, dass die KI frei von Vorurteilen und Diskriminierung ist.

A: 2 und 3 sind richtig. Verantwortungsvolle KI hilft dir, sch√§dliche Auswirkungen, Vorurteile und mehr zu mindern.

## üöÄ Herausforderung

Informiere dich √ºber [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) und pr√ºfe, was du f√ºr deinen Einsatz √ºbernehmen kannst.

## Gute Arbeit, mach weiter mit deinem Lernen

Nach Abschluss dieser Lektion schau dir unsere [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) an, um dein Wissen √ºber generative KI weiter auszubauen!

Gehe weiter zu Lektion 4, in der wir uns mit den [Grundlagen des Prompt Engineerings](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst) besch√§ftigen!

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner Ursprungssprache ist als ma√ügebliche Quelle zu betrachten. F√ºr wichtige Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Nutzung dieser √úbersetzung entstehen.