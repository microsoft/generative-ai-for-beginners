{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das folgende Notizbuch wurde automatisch von GitHub Copilot Chat generiert und ist nur für die Ersteinrichtung gedacht.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in das Prompt Engineering\n",
    "Prompt Engineering ist der Prozess des Entwerfens und Optimierens von Prompts für Aufgaben der natürlichen Sprachverarbeitung. Dabei geht es darum, die passenden Prompts auszuwählen, ihre Parameter anzupassen und ihre Leistung zu bewerten. Prompt Engineering ist entscheidend, um eine hohe Genauigkeit und Effizienz bei NLP-Modellen zu erreichen. In diesem Abschnitt werden wir die Grundlagen des Prompt Engineerings anhand der OpenAI-Modelle näher betrachten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 1: Tokenisierung\n",
    "Erkunde die Tokenisierung mit tiktoken, einem schnellen Open-Source-Tokenizer von OpenAI.\n",
    "Weitere Beispiele findest du im [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb?WT.mc_id=academic-105485-koreyst).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE:\n",
    "# 1. Run the exercise as is first\n",
    "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Define the prompt you want tokenized\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "# Set the model you want encoding for\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Encode the text - gives you the tokens in integer form\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens);\n",
    "\n",
    "# Decode the integers to see what the text versions look like\n",
    "[encoding.decode_single_token_bytes(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 2: Überprüfe die Einrichtung des OpenAI API-Schlüssels\n",
    "\n",
    "Führe den folgenden Code aus, um zu überprüfen, ob dein OpenAI-Endpunkt korrekt eingerichtet ist. Der Code testet eine einfache Eingabeaufforderung und prüft die Antwort. Die Eingabe `oh say can you see` sollte etwa mit `by the dawn's early light..` vervollständigt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OpenAI SDK was updated on Nov 8, 2023 with new guidance for migration\n",
    "# See: https://github.com/openai/openai-python/discussions/742\n",
    "\n",
    "## Updated\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-05-15\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']\n",
    "\n",
    "## Updated\n",
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]       \n",
    "    response = client.chat.completions.create(   \n",
    "        model=deployment,                                         \n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "## ---------- Call the helper method\n",
    "\n",
    "### 1. Set primary content or prompt text\n",
    "text = f\"\"\"\n",
    "oh say can you see\n",
    "\"\"\"\n",
    "\n",
    "### 2. Use that in the prompt template below\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 3: Erfindungen\n",
    "Untersuche, was passiert, wenn du das LLM bittest, Antworten zu einem Thema zu geben, das möglicherweise gar nicht existiert oder zu Themen, über die es nichts wissen kann, weil sie außerhalb seines vortrainierten Datensatzes liegen (zum Beispiel neuere Themen). Schau dir an, wie sich die Antwort verändert, wenn du einen anderen Prompt oder ein anderes Modell ausprobierst.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Set the text for simple prompt or primary content\n",
    "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
    "## Run the completion \n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the Martian War of 2076.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 4: Anweisungsbasiert\n",
    "Verwende die Variable \"text\", um den Hauptinhalt festzulegen,\n",
    "und die Variable \"prompt\", um eine Anweisung zu diesem Hauptinhalt zu geben.\n",
    "\n",
    "Hier bitten wir das Modell, den Text für eine Zweitklässlerin oder einen Zweitklässler zusammenzufassen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Example\n",
    "# https://platform.openai.com/playground/p/default-summarize\n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize content you are provided with for a second-grade student.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 5: Komplexe Eingabeaufforderung\n",
    "Versuche eine Anfrage, die System-, Benutzer- und Assistenten-Nachrichten enthält\n",
    "Das System legt den Kontext des Assistenten fest\n",
    "Benutzer- und Assistenten-Nachrichten bieten einen mehrstufigen Gesprächskontext\n",
    "\n",
    "Beachte, wie die Persönlichkeit des Assistenten im Systemkontext auf „sarkastisch“ eingestellt ist.\n",
    "Probiere einen anderen Persönlichkeitskontext aus. Oder versuche eine andere Reihe von Eingabe-/Ausgabenachrichten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung: Erkunde dein Bauchgefühl\n",
    "Die obigen Beispiele zeigen dir Muster, mit denen du neue Prompts erstellen kannst (einfach, komplex, Anweisung usw.) – versuche, weitere Übungen zu entwickeln, um einige der anderen Ideen zu erforschen, über die wir gesprochen haben, wie zum Beispiel Beispiele, Hinweise und mehr.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Haftungsausschluss**:  \nDieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ausgangssprache gilt als maßgebliche Quelle. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "9b900544221c8d986e06ee6d25debef7",
   "translation_date": "2025-08-25T13:00:20+00:00",
   "source_file": "04-prompt-engineering-fundamentals/python/aoai-assignment.ipynb",
   "language_code": "de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}