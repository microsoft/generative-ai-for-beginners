# Grundlagen des Prompt Engineerings

[![Grundlagen des Prompt Engineerings](../../../translated_images/de/04-lesson-banner.a2c90deba7fedacd.webp)](https://youtu.be/GElCu2kUlRs?si=qrXsBvXnCW12epb8)

## Einf√ºhrung
Dieses Modul behandelt wesentliche Konzepte und Techniken zur Erstellung effektiver Prompts f√ºr generative KI-Modelle. Die Art und Weise, wie Sie Ihren Prompt an ein LLM schreiben, ist ebenfalls wichtig. Ein sorgf√§ltig gestalteter Prompt kann eine bessere Antwortqualit√§t erzielen. Aber was bedeuten Begriffe wie _Prompt_ und _Prompt Engineering_ genau? Und wie verbessere ich den Prompt-_Input_, den ich an das LLM sende? Diese Fragen versuchen wir im Rahmen dieses Kapitels und des n√§chsten zu beantworten.

_Generative KI_ ist in der Lage, neue Inhalte (z. B. Text, Bilder, Audio, Code usw.) als Antwort auf Benutzeranfragen zu erstellen. Dies wird mit Hilfe von _Large Language Models_ wie der GPT-Serie von OpenAI (‚ÄûGenerative Pre-trained Transformer‚Äú) erreicht, die f√ºr die Verwendung von nat√ºrlicher Sprache und Code trainiert wurden.

Benutzer k√∂nnen jetzt mit diesen Modellen √ºber vertraute Paradigmen wie Chat interagieren, ohne technische Kenntnisse oder Schulungen zu ben√∂tigen. Die Modelle sind _promptbasiert_ ‚Äì Benutzer senden eine Texteingabe (Prompt) und erhalten eine KI-Antwort (Completion) zur√ºck. Sie k√∂nnen dann iterativ ‚Äûmit der KI chatten‚Äú in mehrstufigen Gespr√§chen und ihren Prompt so lange verfeinern, bis die Antwort ihren Erwartungen entspricht.

‚ÄûPrompts‚Äú werden nun zur prim√§ren _Programmierschnittstelle_ f√ºr generative KI-Anwendungen, da sie den Modellen mitteilen, was zu tun ist, und die Qualit√§t der zur√ºckgegebenen Antworten beeinflussen. ‚ÄûPrompt Engineering‚Äú ist ein schnell wachsendes Forschungsfeld, das sich auf die _Gestaltung und Optimierung_ von Prompts konzentriert, um konsistente und qualitativ hochwertige Antworten in gro√üem Ma√üstab zu liefern.

## Lernziele

In dieser Lektion lernen wir, was Prompt Engineering ist, warum es wichtig ist und wie wir effektivere Prompts f√ºr ein gegebenes Modell und Anwendungsziel erstellen k√∂nnen. Wir werden grundlegende Konzepte und Best Practices f√ºr das Prompt Engineering verstehen ‚Äì und lernen eine interaktive Jupyter-Notebook-‚ÄûSandbox‚Äú kennen, in der wir diese Konzepte an realen Beispielen sehen k√∂nnen.

Am Ende dieser Lektion k√∂nnen wir:

1. Erkl√§ren, was Prompt Engineering ist und warum es wichtig ist.
2. Die Bestandteile eines Prompts beschreiben und wie sie verwendet werden.
3. Beste Praktiken und Techniken f√ºr Prompt Engineering erlernen.
4. Erlernte Techniken mit realen Beispielen anwenden, unter Nutzung eines OpenAI-Endpunkts.

## Schl√ºsselbegriffe

Prompt Engineering: Die Praxis, Eingaben so zu gestalten und zu verfeinern, dass KI-Modelle gew√ºnschte Ausgaben liefern.
Tokenisierung: Der Prozess der Umwandlung von Text in kleinere Einheiten, sogenannte Tokens, die ein Modell verstehen und verarbeiten kann.
Instruction-Tuned LLMs: Gro√üe Sprachmodelle (LLMs), die mit spezifischen Anweisungen feinabgestimmt wurden, um ihre Antwortgenauigkeit und Relevanz zu verbessern.

## Lern-Sandbox

Prompt Engineering ist derzeit mehr Kunst als Wissenschaft. Der beste Weg, unsere Intuition daf√ºr zu verbessern, besteht darin, _mehr zu √ºben_ und einen Trial-and-Error-Ansatz zu verfolgen, der fachliches Anwendungswissen mit empfohlenen Techniken und modellspezifischen Optimierungen kombiniert.

Das zur Lektion begleitende Jupyter-Notebook bietet eine _Sandbox_-Umgebung, in der du das Gelernte ausprobieren kannst ‚Äì entweder direkt oder als Teil der Code-Herausforderung am Ende. Um die √úbungen auszuf√ºhren, ben√∂tigst du:

1. **Einen Azure OpenAI API-Schl√ºssel** ‚Äì den Service-Endpunkt f√ºr ein bereitgestelltes LLM.
2. **Eine Python Runtime** ‚Äì in der das Notebook ausgef√ºhrt werden kann.
3. **Lokale Umgebungsvariablen** ‚Äì _f√ºhre jetzt die [SETUP](./../00-course-setup/02-setup-local.md?WT.mc_id=academic-105485-koreyst) Schritte aus, um bereit zu sein_.

Das Notebook enth√§lt _Starter_-√úbungen ‚Äì aber du bist eingeladen, eigene _Markdown_- (Beschreibungen) und _Code_- (Prompt-Anfragen) Abschnitte hinzuzuf√ºgen, um weitere Beispiele oder Ideen zu erproben und deine Intuition f√ºr die Prompt-Gestaltung zu st√§rken.

## Illustrierter Leitfaden

M√∂chtest du vor dem Eintauchen einen √úberblick dar√ºber, was diese Lektion abdeckt? Sieh dir diesen illustrierten Leitfaden an, der dir ein Gef√ºhl f√ºr die Hauptthemen und die wichtigsten Erkenntnisse gibt, √ºber die du jeweils nachdenken kannst. Die Lesson-Roadmap f√ºhrt dich vom Verst√§ndnis der Kernkonzepte und Herausforderungen bis zu deren L√∂sung mit relevanten Prompt-Engineering-Techniken und Best Practices. Beachte, dass sich der Abschnitt ‚ÄûErweiterte Techniken‚Äú in diesem Guide auf Inhalte bezieht, die im _n√§chsten_ Kapitel dieses Curriculums behandelt werden.

![Illustrierter Leitfaden zum Prompt Engineering](../../../translated_images/de/04-prompt-engineering-sketchnote.d5f33336957a1e4f.webp)

## Unser Startup

Kommen wir nun dazu, wie _dieses Thema_ mit unserer Startup-Mission zur [F√∂rderung von KI-Innovationen im Bildungsbereich](https://educationblog.microsoft.com/2023/06/collaborating-to-bring-ai-innovation-to-education?WT.mc_id=academic-105485-koreyst) zusammenh√§ngt. Wir wollen KI-gest√ºtzte Anwendungen f√ºr _personalisiertes Lernen_ entwickeln ‚Äì also denken wir dar√ºber nach, wie verschiedene Nutzer unserer Anwendung ‚ÄûPrompts gestalten‚Äú k√∂nnten:

- **Administratoren** k√∂nnten die KI bitten, _Lehrplandaten zu analysieren, um L√ºcken in der Abdeckung zu identifizieren_. Die KI kann Ergebnisse zusammenfassen oder durch Code visualisieren.
- **Lehrkr√§fte** k√∂nnten die KI bitten, _einen Unterrichtsplan f√ºr eine bestimmte Zielgruppe und ein Thema zu erstellen_. Die KI kann den personalisierten Plan in einem angegebenen Format erstellen.
- **Sch√ºler*innen** k√∂nnten die KI bitten, _sie in einem schwierigen Fach zu unterrichten_. Die KI kann die Lernenden nun mit Lektionen, Hinweisen und Beispielen auf ihrem Niveau anleiten.

Das ist nur die Spitze des Eisbergs. Schau dir [Prompts For Education](https://github.com/microsoft/prompts-for-edu/tree/main?WT.mc_id=academic-105485-koreyst) an ‚Äì eine Open-Source-Prompt-Bibliothek, kuratiert von Bildungsexperten ‚Äì, um ein breiteres Spektrum an M√∂glichkeiten kennenzulernen! _Probiere einige dieser Prompts in der Sandbox oder im OpenAI Playground aus, um zu sehen, was passiert!_

<!--
LESSON TEMPLATE:
This unit should cover core concept #1.
Reinforce the concept with examples and references.

CONCEPT #1:
Prompt Engineering.
Define it and explain why it is needed.
-->

## Was ist Prompt Engineering?

Wir begannen diese Lektion mit der Definition von **Prompt Engineering** als dem Prozess des _Gestaltens und Optimierens_ von Texteingaben (Prompts), um f√ºr ein gegebenes Anwendungsziel und Modell konsistente und qualitativ hochwertige Antworten (Completions) zu liefern. Man kann dies als einen 2-Schritte-Prozess sehen:

- den initialen Prompt f√ºr ein bestimmtes Modell und Ziel _entwerfen_
- den Prompt iterativ _verfeinern_, um die Antwortqualit√§t zu verbessern

Dies ist notwendigerweise ein Trial-and-Error-Prozess, der Nutzerintuition und Aufwand erfordert, um optimale Ergebnisse zu erzielen. Aber warum ist das wichtig? Um diese Frage zu beantworten, m√ºssen wir zun√§chst drei Konzepte verstehen:

- _Tokenisierung_ = wie das Modell den Prompt ‚Äûsieht‚Äú
- _Basis-LLMs_ = wie das Foundation-Modell einen Prompt ‚Äûverarbeitet‚Äú
- _Instruction-Tuned LLMs_ = wie das Modell jetzt ‚ÄûAufgaben‚Äú erkennen kann

### Tokenisierung

Ein LLM betrachtet Prompts als _Sequenz von Tokens_, wobei verschiedene Modelle (oder Versionen desselben Modells) denselben Prompt unterschiedlich tokenisieren k√∂nnen. Da LLMs auf Tokens (und nicht auf Rohtext) trainiert sind, wirkt sich die Art der Tokenisierung direkt auf die Qualit√§t der erzeugten Antwort aus.

Um ein Gef√ºhl daf√ºr zu bekommen, wie Tokenisierung funktioniert, probiere Tools wie den [OpenAI Tokenizer](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst) aus, der unten gezeigt wird. Kopiere deinen Prompt hinein ‚Äì und sieh, wie dieser in Tokens umgewandelt wird, achte darauf, wie Leerzeichenzeichen und Satzzeichen behandelt werden. Beachte, dass dieses Beispiel ein √§lteres LLM (GPT-3) zeigt ‚Äì bei neueren Modellen kann das Ergebnis abweichen.

![Tokenisierung](../../../translated_images/de/04-tokenizer-example.e71f0a0f70356c5c.webp)

### Konzept: Foundation-Modelle

Nachdem ein Prompt tokenisiert wurde, besteht die Hauptfunktion des ["Base LLM"](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) (oder Foundation-Modells) darin, das n√§chste Token in der Sequenz vorherzusagen. Da LLMs auf massiven Textdatens√§tzen trainiert sind, haben sie ein gutes Verst√§ndnis der statistischen Beziehungen zwischen Tokens und k√∂nnen diese Vorhersage mit gewisser Sicherheit treffen. Beachte, dass sie die _Bedeutung_ der W√∂rter im Prompt oder Token nicht verstehen; sie erkennen nur ein Muster, das sie durch die n√§chste Vorhersage ‚Äûvervollst√§ndigen‚Äú k√∂nnen. Sie k√∂nnen diese Vorhersage fortsetzen, bis der Prozess durch Benutzerintervention oder eine vorgegebene Bedingung gestoppt wird.

M√∂chtest du sehen, wie prompt-basierte Vervollst√§ndigungen funktionieren? Gib den oben genannten Prompt im Azure OpenAI Studio [_Chat Playground_](https://oai.azure.com/playground?WT.mc_id=academic-105485-koreyst) mit den Standardeinstellungen ein. Das System ist so konfiguriert, dass Prompts als Informationsanfragen behandelt werden ‚Äì du solltest also eine Completion sehen, die zu diesem Kontext passt.

Aber was, wenn der Benutzer etwas Spezifisches sehen m√∂chte, das bestimmte Kriterien oder eine Aufgabenstellung erf√ºllt? Hier kommen _instruction-tuned_ LLMs ins Spiel.

![Base LLM Chat Completion](../../../translated_images/de/04-playground-chat-base.65b76fcfde0caa67.webp)

### Konzept: Instruction-Tuned LLMs

Ein [Instruction Tuned LLM](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) baut auf dem Foundation-Modell auf und wird mit Beispielen oder Input/Output-Paaren (z. B. mehrstufigen ‚ÄûNachrichten‚Äú) feinabgestimmt, die klare Anweisungen enthalten k√∂nnen ‚Äì und die Antwort der KI versucht, dieser Anweisung zu folgen.

Dies nutzt Techniken wie Reinforcement Learning mit menschlichem Feedback (RLHF), die das Modell darin trainieren, _Anweisungen zu befolgen_ und _aus Feedback zu lernen_, sodass es Antworten produziert, die besser f√ºr praktische Anwendungen geeignet und relevanter f√ºr Benutzerziele sind.

Probieren wir es aus ‚Äì nimm den obigen Prompt und √§ndere nun die _Systemnachricht_, sodass die folgende Anweisung als Kontext bereitgestellt wird:

> _Fasse den bereitgestellten Inhalt f√ºr eine*n Zweitkl√§ssler*in zusammen. Halte das Ergebnis in einem Absatz mit 3-5 Aufz√§hlungspunkten._

Siehst du, wie das Ergebnis nun auf das gew√ºnschte Ziel und Format abgestimmt ist? Eine Lehrkraft kann diese Antwort direkt in ihren Folien f√ºr die Klasse verwenden.

![Instruction Tuned LLM Chat Completion](../../../translated_images/de/04-playground-chat-instructions.b30bbfbdf92f2d05.webp)

## Warum brauchen wir Prompt Engineering?

Nachdem wir nun wissen, wie Prompts von LLMs verarbeitet werden, sprechen wir dar√ºber, _warum_ wir Prompt Engineering brauchen. Die Antwort liegt darin, dass aktuelle LLMs einige Herausforderungen mit sich bringen, die _zuverl√§ssige und konsistente Completions_ ohne Aufwand bei der Prompt-Gestaltung und -Optimierung erschweren. Zum Beispiel:

1. **Modellantworten sind stochastisch.** Derselbe Prompt erzeugt wahrscheinlich unterschiedliche Antworten mit verschiedenen Modellen oder Modellversionen. Und er kann sogar zu unterschiedlichen Zeitpunkten mit _dem gleichen Modell_ verschiedene Ergebnisse liefern. _Prompt Engineering-Techniken k√∂nnen helfen, diese Variationen zu minimieren, indem sie bessere Leitplanken schaffen_.

1. **Modelle k√∂nnen Antworten erfinden.** Modelle werden mit _gro√üen, aber endlichen_ Datens√§tzen vortrainiert, was bedeutet, dass sie keine Kenntnisse √ºber Konzepte au√üerhalb dieses Trainings besitzen. Deshalb k√∂nnen sie Antworten erzeugen, die ungenau, erfunden oder direkt im Widerspruch zu bekannten Fakten stehen. _Prompt Engineering-Techniken helfen Nutzern, solche Erfindungen zu erkennen und zu mindern, z. B. indem die KI nach Quellenangaben oder Begr√ºndungen gefragt wird_.

1. **Modellf√§higkeiten variieren.** Neue Modelle oder Modellgenerationen haben komplexere F√§higkeiten, bringen aber auch eigenst√§ndige Eigenheiten und Kompromisse bei Kosten und Komplexit√§t mit sich. _Prompt Engineering kann helfen, Best Practices und Workflows zu entwickeln, die Unterschiede abstrahieren und sich skalierbar und nahtlos an modellspezifische Anforderungen anpassen_.

Schauen wir uns das im OpenAI- oder Azure OpenAI-Playground an:

- Verwende denselben Prompt mit verschiedenen LLM-Bereitstellungen (z. B. OpenAI, Azure OpenAI, Hugging Face) ‚Äì hast du Variationen gesehen?
- Verwende denselben Prompt wiederholt mit _derselben_ LLM-Bereitstellung (z. B. Azure OpenAI Playground) ‚Äì wie unterschieden sich diese Variationen?

### Beispiel f√ºr Erfindungen

In diesem Kurs verwenden wir den Begriff **‚ÄûErfindung‚Äú** f√ºr das Ph√§nomen, dass LLMs manchmal faktisch falsche Informationen generieren aufgrund von Beschr√§nkungen in ihrem Training oder anderen Einschr√§nkungen. Du hast diesen Sachverhalt vielleicht auch als _‚ÄûHalluzinationen‚Äú_ in popul√§ren Artikeln oder Forschungsarbeiten geh√∂rt. Wir empfehlen jedoch ausdr√ºcklich, den Begriff _‚ÄûErfindung‚Äú_ zu verwenden, um zu vermeiden, dass wir das Verhalten vermenschlichen, indem wir einer maschinell erzeugten Antwort menschliche Eigenschaften zuschreiben. Dies unterst√ºtzt auch die [Verantwortlichen KI-Richtlinien](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-105485-koreyst) aus sprachlicher Sicht und vermeidet Begriffe, die in manchen Kontexten als anst√∂√üig oder nicht inklusiv angesehen werden k√∂nnen.

M√∂chtest du eine Vorstellung davon bekommen, wie Erfindungen entstehen? Denke an einen Prompt, der die KI anweist, Inhalte zu einem nicht existierenden Thema zu generieren (um sicherzustellen, dass es im Trainingsdatensatz nicht vorkommt). Zum Beispiel habe ich diesen Prompt ausprobiert:

> **Prompt:** Erstelle einen Unterrichtsplan zum Marskrieg von 2076.
Eine Websuche zeigte mir, dass es fiktionale Berichte (z. B. Fernsehserien oder B√ºcher) √ºber Marskriege gab ‚Äì aber keine im Jahr 2076. Der gesunde Menschenverstand sagt auch, dass 2076 _in der Zukunft_ liegt und daher keinem realen Ereignis zugeordnet werden kann.

Was passiert also, wenn wir diese Eingabeaufforderung bei verschiedenen LLM-Anbietern ausf√ºhren?

> **Antwort 1**: OpenAI Playground (GPT-35)

![Response 1](../../../translated_images/de/04-fabrication-oai.5818c4e0b2a2678c.webp)

> **Antwort 2**: Azure OpenAI Playground (GPT-35)

![Response 2](../../../translated_images/de/04-fabrication-aoai.b14268e9ecf25caf.webp)

> **Antwort 3**: : Hugging Face Chat Playground (LLama-2)

![Response 3](../../../translated_images/de/04-fabrication-huggingchat.faf82a0a51278956.webp)

Wie erwartet erzeugt jedes Modell (oder jede Modellversion) durch das stochastische Verhalten und Unterschiede in den Modellf√§higkeiten leicht unterschiedliche Antworten. Zum Beispiel richtet sich ein Modell an ein Publikum der 8. Klasse, w√§hrend das andere von einem Sch√ºler der Oberstufe ausgeht. Aber alle drei Modelle generierten Antworten, die einen uninformierten Nutzer davon √ºberzeugen k√∂nnten, dass das Ereignis real sei.

Techniken des Prompt-Engineerings wie _Metaprompting_ und _Temperaturkonfiguration_ k√∂nnen Modell-Erfindungen bis zu einem gewissen Grad reduzieren. Neue Prompt-Engineering-_Architekturen_ integrieren auch nahtlos neue Werkzeuge und Techniken in den Prompt-Ablauf, um einige dieser Effekte zu mildern oder zu verringern.

## Fallstudie: GitHub Copilot

Wir schlie√üen diesen Abschnitt ab, indem wir uns ansehen, wie Prompt-Engineering in realen L√∂sungen verwendet wird, anhand einer Fallstudie: [GitHub Copilot](https://github.com/features/copilot?WT.mc_id=academic-105485-koreyst).

GitHub Copilot ist dein ‚ÄûKI-Paarprogrammierer‚Äú ‚Äì es wandelt Texteingaben in Codeerg√§nzungen um und ist in deine Entwicklungsumgebung (z. B. Visual Studio Code) integriert, um ein nahtloses Nutzererlebnis zu bieten. Wie in den unten stehenden Blog-Serien dokumentiert, basierte die erste Version auf dem OpenAI Codex-Modell ‚Äì wobei die Entwickler schnell erkannten, dass das Modell feinjustiert und bessere Prompt-Engineering-Techniken entwickelt werden m√ºssen, um die Codequalit√§t zu verbessern. Im Juli stellten sie [ein verbessertes KI-Modell vor, das √ºber Codex hinausgeht](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/?WT.mc_id=academic-105485-koreyst) und noch schnellere Vorschl√§ge liefert.

Lies die Beitr√§ge der Reihe nach, um ihre Lernreise nachzuvollziehen.

- **Mai 2023** | [GitHub Copilot versteht Ihren Code immer besser](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/?WT.mc_id=academic-105485-koreyst)
- **Mai 2023** | [Inside GitHub: Arbeit mit den LLMs hinter GitHub Copilot](https://github.blog/2023-05-17-inside-github-working-with-the-llms-behind-github-copilot/?WT.mc_id=academic-105485-koreyst)
- **Jun 2023** | [Wie man bessere Prompts f√ºr GitHub Copilot schreibt](https://github.blog/2023-06-20-how-to-write-better-prompts-for-github-copilot/?WT.mc_id=academic-105485-koreyst)
- **Jul 2023** | [GitHub Copilot geht mit verbessertem KI-Modell √ºber Codex hinaus](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/?WT.mc_id=academic-105485-koreyst)
- **Jul 2023** | [Ein Entwicklerleitfaden f√ºr Prompt Engineering und LLMs](https://github.blog/2023-07-17-prompt-engineering-guide-generative-ai-llms/?WT.mc_id=academic-105485-koreyst)
- **Sep 2023** | [Wie man eine Enterprise-LLM-App baut: Lektionen von GitHub Copilot](https://github.blog/2023-09-06-how-to-build-an-enterprise-llm-application-lessons-from-github-copilot/?WT.mc_id=academic-105485-koreyst)

Du kannst auch ihren [Engineering-Blog](https://github.blog/category/engineering/?WT.mc_id=academic-105485-koreyst) f√ºr weitere Beitr√§ge wie [diesen hier](https://github.blog/2023-09-27-how-i-used-github-copilot-chat-to-build-a-reactjs-gallery-prototype/?WT.mc_id=academic-105485-koreyst) durchsuchen, der zeigt, wie diese Modelle und Techniken f√ºr reale Anwendungen _eingesetzt_ werden.

---

<!--
LESSON TEMPLATE:
This unit should cover core concept #2.
Reinforce the concept with examples and references.

CONCEPT #2:
Prompt Design.
Illustrated with examples.
-->

## Prompt-Erstellung

Wir haben gesehen, warum Prompt-Engineering wichtig ist ‚Äì jetzt wollen wir verstehen, wie Prompts _konstuiert_ werden, um verschiedene Techniken f√ºr ein effektiveres Prompt-Design bewerten zu k√∂nnen.

### Einfacher Prompt

Fangen wir mit dem einfachen Prompt an: eine Texteingabe, die ohne weiteren Kontext an das Modell gesendet wird. Hier ein Beispiel ‚Äì wenn wir die ersten W√∂rter der US-Nationalhymne an die OpenAI [Completion API](https://platform.openai.com/docs/api-reference/completions?WT.mc_id=academic-105485-koreyst) schicken, _erg√§nzt_ sie sofort die n√§chsten Zeilen, was das grundlegende Vorhersageverhalten illustriert.

| Prompt (Eingabe)    | Erg√§nzung (Ausgabe)                                                                                                                           |
| :------------------ | :--------------------------------------------------------------------------------------------------------------------------------------------- |
| Oh say can you see  | Es klingt, als beg√§nnest du mit dem Liedtext von ‚ÄûThe Star-Spangled Banner‚Äú, der Nationalhymne der Vereinigten Staaten. Der vollst√§ndige Text lautet ... |

### Komplexer Prompt

Jetzt f√ºgen wir Kontext und Anweisungen zu diesem einfachen Prompt hinzu. Die [Chat Completion API](https://learn.microsoft.com/azure/ai-services/openai/how-to/chatgpt?WT.mc_id=academic-105485-koreyst) erlaubt es uns, einen komplexen Prompt als Sammlung von _Nachrichten_ zu erstellen mit:

- Eingabe/Ausgabe-Paaren, die die _User_-Eingabe und _Assistant_-Antwort widerspiegeln.
- Systemnachricht, die den Kontext f√ºr das Verhalten oder die Pers√∂nlichkeit des Assistenten festlegt.

Die Anfrage hat nun die unten gezeigte Form, bei der die _Tokenisierung_ relevante Informationen aus dem Kontext und dem Gespr√§ch effektiv erfasst. Das √Ñndern des Systemkontexts kann sich dabei auf die Qualit√§t der Erg√§nzungen genauso stark auswirken wie die vom Nutzer gegebenen Eingaben.

```python
response = openai.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who won the world series in 2020?"},
        {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
        {"role": "user", "content": "Where was it played?"}
    ]
)
```

### Instruktions-Prompt

In den obigen Beispielen war der User-Prompt eine einfache Textanfrage, die als Informationsabfrage interpretiert werden kann. Mit _Instruktions_-Prompts k√∂nnen wir diesen Text verwenden, um eine Aufgabe detaillierter zu spezifizieren und der KI bessere Anweisungen zu geben. Hier ein Beispiel:

| Prompt (Eingabe)                                                                                                                                                                                                                         | Erg√§nzung (Ausgabe)                                                                                                  | Anweisungstyp     |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------- | :---------------- |
| Schreibe eine Beschreibung des B√ºrgerkriegs                                                                                                                                                                                           | _gab einen einfachen Absatz zur√ºck_                                                                                   | Einfach           |
| Schreibe eine Beschreibung des B√ºrgerkriegs. Gib wichtige Daten und Ereignisse an und beschreibe deren Bedeutung                                                                                                                    | _gab einen Absatz mit anschlie√üender Liste wichtiger Ereignisse mit Beschreibungen zur√ºck_                            | Komplex           |
| Schreibe eine Beschreibung des B√ºrgerkriegs in 1 Absatz. Gib 3 Stichpunkte mit Schl√ºsseldaten und deren Bedeutung. Gib 3 weitere Stichpunkte mit wichtigen historischen Pers√∂nlichkeiten und deren Beitrag. Gib die Ausgabe als JSON-Datei zur√ºck | _gibt umfangreichere Details in einem Textfeld zur√ºck, formatiert als JSON, das kopiert und validiert werden kann_    | Komplex. Formatiert|

## Prim√§rer Inhalt

In den obigen Beispielen war der Prompt noch recht offen, sodass das LLM entscheiden konnte, welcher Teil seines vortrainierten Datensatzes relevant ist. Mit dem Designmuster _prim√§rer Inhalt_ wird der Eingabetext in zwei Teile gegliedert:

- eine Anweisung (Aktion)
- relevanter Inhalt (der die Aktion beeinflusst)

Hier ein Beispiel, bei dem die Anweisung lautet, ‚Äûfasse dies in 2 S√§tzen zusammen‚Äú.

| Prompt (Eingabe)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Erg√§nzung (Ausgabe)                                                                                                                                                                                                                                                               |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Jupiter ist der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem. Er ist ein Gasriese mit einer Masse von einem Tausendstel der Sonne, aber zweieinhalbmal so gro√ü wie alle anderen Planeten des Sonnensystems zusammen. Jupiter ist eines der hellsten Objekte, die man mit blo√üem Auge am Nachthimmel sehen kann, und war schon alten Zivilisationen vor der schriftlichen √úberlieferung bekannt. Er ist nach dem r√∂mischen Gott Jupiter benannt.[19] Von der Erde aus betrachtet kann Jupiter hell genug sein, um sichtbare Schatten zu werfen,[20] und ist im Durchschnitt das dritthellste nat√ºrliche Objekt am Nachthimmel nach Mond und Venus. <br/> **Fasse dies in 2 kurzen S√§tzen zusammen** | Jupiter, der f√ºnfte Planet von der Sonne, ist der gr√∂√üte im Sonnensystem und bekannt als eines der hellsten Objekte am Nachthimmel. Er ist nach dem r√∂mischen Gott Jupiter benannt, ein Gasriese mit einer Masse, die zweieinhalbmal so gro√ü ist wie die aller anderen Planeten des Sonnensystems zusammen. |

Das Segment f√ºr den prim√§ren Inhalt kann auf verschiedene Weise genutzt werden, um wirkungsvollere Anweisungen zu geben:

- **Beispiele** ‚Äì Anstatt dem Modell durch eine explizite Anweisung zu sagen, was es tun soll, gibt man ihm Beispiele, was zu tun ist, und l√§sst es das Muster ableiten.
- **Hinweise** ‚Äì Auf die Anweisung folgt ein ‚ÄûHinweis‚Äú, der die Erg√§nzung vorbereitet und das Modell auf relevantere Antworten lenkt.
- **Vorlagen** ‚Äì Das sind wiederholbare ‚ÄûRezepte‚Äú f√ºr Prompts mit Platzhaltern (Variablen), die mit Daten f√ºr spezifische Anwendungsf√§lle angepasst werden k√∂nnen.

Lass uns diese in der Praxis betrachten.

### Verwendung von Beispielen

Dies ist eine Methode, bei der du dem Modell mit dem prim√§ren Inhalt einige Beispiele f√ºr die gew√ºnschte Ausgabe zur Verf√ºgung stellst, und es das Muster f√ºr die erw√ºnschte Ausgabe ableitet. Je nach Anzahl der Beispiele spricht man von Zero-Shot-Prompting, One-Shot-Prompting, Few-Shot-Prompting usw.

Der Prompt besteht nun aus drei Komponenten:

- Einer Aufgabenbeschreibung
- Ein paar Beispielen der gew√ºnschten Ausgabe
- Dem Beginn eines neuen Beispiels (was zur impliziten Aufgabenbeschreibung wird)

| Lernart       | Prompt (Eingabe)                                                                                                                                              | Erg√§nzung (Ausgabe)            |
| :------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------ |
| Zero-shot     | ‚ÄûThe Sun is Shining‚Äú. √úbersetze ins Spanische                                                                                                              | ‚ÄûEl Sol est√° brillando‚Äú.        |
| One-shot      | ‚ÄûThe Sun is Shining‚Äú => ‚ÄûEl Sol est√° brillando‚Äú. <br> ‚ÄûIt's a Cold and Windy Day‚Äú =>                                                                       | ‚ÄûEs un d√≠a fr√≠o y ventoso‚Äú.     |
| Few-shot      | Der Spieler lief die Bases ab => Baseball <br/> Der Spieler spielte ein Ass => Tennis <br/> Der Spieler schlug eine Six => Cricket <br/> Der Spieler machte einen Dunk => | Basketball                     |
|               |                                                                                                                                                             |                                |

Beachte, dass wir beim Zero-Shot-Prompting eine explizite Anweisung (‚Äû√úbersetze ins Spanische‚Äú) geben mussten, die aber beim One-Shot-Prompting-Beispiel implizit wurde. Das Few-Shot-Beispiel zeigt, dass durch zus√§tzliche Beispiele Modelle noch genauere Ableitungen ohne zus√§tzliche Anweisungen treffen k√∂nnen.

### Prompt-Hinweise

Eine weitere Technik mit prim√§rem Inhalt ist es, _Hinweise_ statt Beispiele zu geben. Dabei bekommt das Modell einen Ansto√ü in die richtige Richtung, indem es mit einem Textabschnitt beginnt, der das gew√ºnschte Antwortformat widerspiegelt. Das Modell ‚Äûnimmt den Hinweis auf‚Äú und f√ºhrt die Antwort entsprechend fort.

| Anzahl der Hinweise | Prompt (Eingabe)                                                                                                                                                                                                                                                                                                                                                                                                                     | Erg√§nzung (Ausgabe)                                                                                                                                                                                                                                                                                                                                                 |
| :------------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0                   | Jupiter ist der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem. Er ist ein Gasriese mit einer Masse von einem Tausendstel der Sonne, aber zweieinhalbmal so gro√ü wie alle anderen Planeten des Sonnensystems zusammen. Jupiter ist eines der hellsten Objekte, die man mit blo√üem Auge am Nachthimmel sehen kann, und war schon alten Zivilisationen vor der schriftlichen √úberlieferung bekannt. <br/>**Fasse dies zusammen**                       | Jupiter ist der gr√∂√üte Planet in unserem Sonnensystem und der f√ºnfte von der Sonne aus. Er ist ein Gasriese mit einer Masse von 1/1000 der Sonnenmasse, aber schwerer als alle anderen Planeten zusammen. Alte Zivilisationen kennen Jupiter schon lange, und er ist leicht am Nachthimmel zu sehen.                                                                                                       |
| 1              | Jupiter ist der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem. Es ist ein Gasriese mit einer Masse von einem Tausendstel der Sonne, aber zweieinhalbmal so gro√ü wie alle anderen Planeten im Sonnensystem zusammen. Jupiter ist eines der hellsten Objekte, die mit blo√üem Auge im Nachthimmel sichtbar sind, und ist seit der Vorgeschichte alten Zivilisationen bekannt. <br/>**Fasse dies zusammen** <br/> Was wir gelernt haben, ist, dass Jupiter | der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem ist. Es ist ein Gasriese mit einer Masse von einem Tausendstel der Sonne, aber zweieinhalbmal so gro√ü wie alle anderen Planeten zusammen. Er ist mit blo√üem Auge leicht sichtbar und seit der Antike bekannt.                        |
| 2              | Jupiter ist der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem. Es ist ein Gasriese mit einer Masse von einem Tausendstel der Sonne, aber zweieinhalbmal so gro√ü wie alle anderen Planeten im Sonnensystem zusammen. Jupiter ist eines der hellsten Objekte, die mit blo√üem Auge im Nachthimmel sichtbar sind, und ist seit der Vorgeschichte alten Zivilisationen bekannt. <br/>**Fasse dies zusammen** <br/> Die Top 3 Fakten, die wir gelernt haben:         | 1. Jupiter ist der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem. <br/> 2. Es ist ein Gasriese mit einer Masse von einem Tausendstel der Sonne...<br/> 3. Jupiter ist seit der Antike mit blo√üem Auge sichtbar ...                                                                       |
|                |                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                           |

### Prompt-Vorlagen

Eine Prompt-Vorlage ist ein _vordefiniertes Rezept f√ºr einen Prompt_, das bei Bedarf gespeichert und wiederverwendet werden kann, um konsistentere Benutzererfahrungen im gro√üen Ma√üstab zu erm√∂glichen. In seiner einfachsten Form ist es einfach eine Sammlung von Prompt-Beispielen wie [dieses von OpenAI](https://platform.openai.com/examples?WT.mc_id=academic-105485-koreyst), das sowohl die interaktiven Prompt-Komponenten (Benutzer- und Systemnachrichten) als auch das API-gesteuerte Anfrageformat bereitstellt ‚Äì zur Unterst√ºtzung der Wiederverwendung.

In einer komplexeren Form wie [dieses Beispiel von LangChain](https://python.langchain.com/docs/concepts/prompt_templates/?WT.mc_id=academic-105485-koreyst) enth√§lt es _Platzhalter_, die mit Daten aus verschiedenen Quellen (Benutzereingabe, Systemkontext, externe Datenquellen usw.) dynamisch ersetzt werden k√∂nnen, um einen Prompt zu erzeugen. Dies erm√∂glicht uns, eine Bibliothek von wiederverwendbaren Prompts zu erstellen, die zur programmgesteuerten F√∂rderung konsistenter Benutzererfahrungen im gro√üen Ma√üstab genutzt werden k√∂nnen.

Der tats√§chliche Wert von Vorlagen liegt schlie√ülich in der M√∂glichkeit, _Prompt-Bibliotheken_ f√ºr vertikale Anwendungsbereiche zu erstellen und zu ver√∂ffentlichen ‚Äì bei denen die Prompt-Vorlage nun _optimiert_ ist, um spezifischen Anwendungskontext oder Beispiele widerzuspiegeln, die die Antworten f√ºr die Zielgruppe relevanter und genauer machen. Das [Prompts For Edu](https://github.com/microsoft/prompts-for-edu?WT.mc_id=academic-105485-koreyst) Repository ist ein hervorragendes Beispiel f√ºr diesen Ansatz, das eine Bibliothek von Prompts f√ºr den Bildungsbereich mit Schwerpunkt auf Schl√ºsselzielen wie Unterrichtsplanung, Curriculum-Design, Sch√ºlerbetreuung usw. kuratiert.

## Unterst√ºtzender Inhalt

Wenn wir an die Konstruktion von Prompts denken als eine Anweisung (Aufgabe) und ein Ziel (Prim√§rinhalt), ist _sekund√§rer Inhalt_ wie zus√§tzlicher Kontext, den wir bereitstellen, um die Ausgabe in irgendeiner Weise zu **beeinflussen**. Es k√∂nnten Einstellungsparameter, Formatierungsanweisungen, Themen-Taxonomien usw. sein, die dem Modell helfen, seine Antwort besser an die gew√ºnschten Benutzerziele oder Erwartungen _anzupassen_.

Zum Beispiel: Angenommen, wir haben einen Kurskatalog mit umfangreichen Metadaten (Name, Beschreibung, Niveau, Metadaten-Tags, Dozent usw.) aller verf√ºgbaren Kurse im Curriculum:

- Wir k√∂nnen eine Anweisung definieren, den Kurskatalog f√ºr Herbst 2023 ‚Äûzusammenzufassen‚Äú.
- Den Prim√§rinhalt verwenden wir, um einige Beispiele des gew√ºnschten Outputs bereitzustellen.
- Den sekund√§ren Inhalt setzen wir ein, um die Top 5 ‚ÄûTags‚Äú von Interesse zu identifizieren.

Nun kann das Modell eine Zusammenfassung in dem durch die Beispiele gezeigten Format liefern ‚Äì aber wenn ein Ergebnis mehrere Tags hat, kann es die im sekund√§ren Inhalt identifizierten f√ºnf Tags priorisieren.

---

<!--
LESSON TEMPLATE:
Diese Einheit sollte Kernkonzept #1 abdecken.
Verst√§rken Sie das Konzept mit Beispielen und Verweisen.

KONZEPT #3:
Techniken des Prompt Engineerings.
Was sind einige grundlegende Techniken des Prompt Engineerings?
Veranschaulichen Sie es mit einigen √úbungen.
-->

## Best Practices beim Prompting

Jetzt, da wir wissen, wie Prompts _konstruiert_ werden k√∂nnen, k√∂nnen wir anfangen, dar√ºber nachzudenken, wie man sie _designt_, um bew√§hrte Praktiken widerzuspiegeln. Wir k√∂nnen das in zwei Teile zerlegen ‚Äì die richtige _Einstellung_ zu haben und die richtigen _Techniken_ anzuwenden.

### Prompt Engineering Einstellung

Prompt Engineering ist ein Prozess des Versuch-und-Irrtum, daher behalten Sie drei breite Leitfaktoren im Kopf:

1. **Dom√§nenverst√§ndnis ist wichtig.** Die Genauigkeit und Relevanz der Antwort h√§ngt von der _Dom√§ne_ ab, in der die Anwendung oder der Benutzer t√§tig ist. Wenden Sie Ihre Intuition und Dom√§nenexpertise an, um **Techniken weiter anzupassen**. Definieren Sie z.B. _dom√§nenspezifische Pers√∂nlichkeiten_ in Ihren Systemprompts oder nutzen Sie _dom√§nenspezifische Vorlagen_ in Benutzermeldungen. Stellen Sie sekund√§re Inhalte bereit, die dom√§nenspezifische Kontexte widerspiegeln, oder verwenden Sie _dom√§nenspezifische Hinweise und Beispiele_, um das Modell zu gewohnten Mustern zu f√ºhren.

2. **Modellverst√§ndnis ist wichtig.** Wir wissen, dass Modelle naturgem√§√ü stochastisch sind. Aber Implementierungen des Modells k√∂nnen auch hinsichtlich des Trainingsdatensatzes (vortrainiertes Wissen), der bereitgestellten Funktionen (z.B. API oder SDK) und der Art der optimierten Inhalte (z.B. Code vs. Bilder vs. Text) variieren. Verstehen Sie die St√§rken und Grenzen des genutzten Modells und verwenden Sie dieses Wissen, um _Aufgaben zu priorisieren_ oder _angepasste Vorlagen_ zu erstellen, die auf die F√§higkeiten des Modells optimiert sind.

3. **Iteration & Validierung sind wichtig.** Modelle entwickeln sich schnell weiter, ebenso die Techniken des Prompt Engineerings. Als Dom√§nenexperte haben Sie m√∂glicherweise weitere Kontexte oder Kriterien f√ºr _Ihre_ spezifische Anwendung, die f√ºr die gr√∂√üere Gemeinschaft nicht gelten. Nutzen Sie Tools und Techniken des Prompt Engineerings, um die Konstruktion von Prompts ‚Äûanzuwerfen‚Äú, dann iterieren und validieren Sie die Ergebnisse mit Ihrer eigenen Intuition und Dom√§nenexpertise. Dokumentieren Sie Ihre Erkenntnisse und erstellen Sie eine **Wissensdatenbank** (z.B. Prompt-Bibliotheken), die andere als neue Ausgangsbasis f√ºr schnellere Iterationen verwenden k√∂nnen.

## Best Practices

Sehen wir uns nun g√§ngige Best Practices an, die von [OpenAI](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api?WT.mc_id=academic-105485-koreyst) und [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering#best-practices?WT.mc_id=academic-105485-koreyst) empfohlen werden.

| Was                              | Warum                                                                                                                                                                                                                                               |
| :-------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Bewerten Sie die neuesten Modelle | Neue Modellgenerationen haben wahrscheinlich verbesserte Funktionen und Qualit√§t ‚Äì k√∂nnen aber auch h√∂here Kosten verursachen. Bewerten Sie sie hinsichtlich Einfluss und treffen Sie dann Migrationsentscheidungen.                              |
| Trennen Sie Anweisungen & Kontext | Pr√ºfen Sie, ob Ihr Modell/Anbieter _Trennzeichen_ definiert, um Anweisungen, prim√§ren und sekund√§ren Inhalt besser zu unterscheiden. Dies kann dem Modell helfen, Token genauer zu gewichten.                                                      |
| Seien Sie spezifisch und klar      | Geben Sie mehr Details zum gew√ºnschten Kontext, Ergebnis, L√§nge, Format, Stil usw. Dies verbessert sowohl Qualit√§t als auch Konsistenz der Antworten. Legen Sie Rezepte in wiederverwendbaren Vorlagen fest.                                        |
| Seien Sie beschreibend, nutzen Sie Beispiele | Modelle reagieren oft besser auf einen ‚ÄûShow and Tell‚Äú-Ansatz. Beginnen Sie mit einem `Zero-Shot`-Ansatz, bei dem Sie nur eine Anweisung geben (aber keine Beispiele), und verfeinern Sie dann mit `Few-Shot`, indem Sie einige Beispiele der gew√ºnschten Ausgabe geben. Verwenden Sie Analogien. |
| Nutzen Sie Hinweise, um Antworten anzusto√üen | Lenken Sie das Modell zu einem gew√ºnschten Ergebnis, indem Sie ihm einige Anfangsw√∂rter oder -phrasen geben, die es als Ausgangspunkt f√ºr die Antwort verwenden kann.                                                                              |
| Verst√§rken Sie                      | Manchmal m√ºssen Sie sich gegen√ºber dem Modell wiederholen. Geben Sie Anweisungen vor und nach Ihrem Prim√§rinhalt, verwenden Sie eine Anweisung und einen Hinweis etc. Iterieren und validieren Sie, was am besten funktioniert.                      |
| Reihenfolge ist wichtig             | Die Reihenfolge, in der Sie Informationen dem Modell pr√§sentieren, kann die Ausgabe beeinflussen, auch bei Lernbeispielen, aufgrund von Neuheits-Bias. Probieren Sie verschiedene Optionen aus, um das beste Ergebnis zu finden.                     |
| Geben Sie dem Modell eine ‚ÄûAusweichm√∂glichkeit‚Äú | Geben Sie dem Modell eine _Fallback_-Antwort, die es geben kann, wenn es die Aufgabe aus irgendeinem Grund nicht erf√ºllen kann. Dies verringert die Wahrscheinlichkeit, dass Modelle falsche oder erfundene Antworten erzeugen.                    |
|                                   |                                                                                                                                                                                                                                                   |

Wie bei jeder Best Practice gilt: _Ihre Erfahrungen k√∂nnen je nach Modell, Aufgabe und Dom√§ne variieren_. Nutzen Sie diese als Ausgangspunkt und iterieren Sie, um herauszufinden, was f√ºr Sie am besten funktioniert. Bewerten Sie Ihren Prompt Engineering-Prozess st√§ndig neu, wenn neue Modelle und Werkzeuge verf√ºgbar werden, mit Fokus auf Skalierbarkeit des Prozesses und Qualit√§t der Antworten.

<!--
LESSON TEMPLATE:
Diese Einheit sollte eine Code-Herausforderung bieten, falls zutreffend.

HERAUSFORDERUNG:
Link zu einem Jupyter Notebook mit nur Code-Kommentaren in den Anweisungen (Codeabschnitte sind leer).

L√ñSUNG:
Link zu einer Kopie dieses Notebooks mit ausgef√ºllten und ausgef√ºhrten Prompts, die ein Beispiel zeigen.
-->

## Aufgabe

Herzlichen Gl√ºckwunsch! Sie haben das Ende der Lektion erreicht! Nun ist es Zeit, einige dieser Konzepte und Techniken mit echten Beispielen zu testen!

F√ºr unsere Aufgabe verwenden wir ein Jupyter Notebook mit √úbungen, die Sie interaktiv durchf√ºhren k√∂nnen. Sie k√∂nnen das Notebook auch mit eigenen Markdown- und Codezellen erweitern, um Ideen und Techniken eigenst√§ndig zu erkunden.

### Zum Starten, forken Sie das Repository und dann

- (Empfohlen) Starten Sie GitHub Codespaces
- (Alternativ) Klonen Sie das Repository auf Ihr lokales Ger√§t und verwenden Sie es mit Docker Desktop
- (Alternativ) √ñffnen Sie das Notebook mit Ihrer bevorzugten Notebook-Laufzeitumgebung.

### Als N√§chstes konfigurieren Sie Ihre Umgebungsvariablen

- Kopieren Sie die Datei `.env.copy` im Stammverzeichnis des Repos nach `.env` und f√ºllen Sie die Werte f√ºr `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` und `AZURE_OPENAI_DEPLOYMENT` aus. Gehen Sie dann zur√ºck zum [Learning Sandbox Abschnitt](../../../04-prompt-engineering-fundamentals), um zu lernen, wie das geht.

### Danach √∂ffnen Sie das Jupyter Notebook

- W√§hlen Sie den Laufzeit-Kernel. Wenn Sie Option 1 oder 2 verwenden, w√§hlen Sie einfach den standardm√§√üig vom Dev-Container bereitgestellten Python 3.10.x Kernel aus.

Sie sind bereit, die √úbungen durchzuf√ºhren. Beachten Sie, dass es hier keine _richtigen oder falschen_ Antworten gibt ‚Äì es geht darum, Optionen durch Versuch und Irrtum zu erkunden und Intuition daf√ºr zu entwickeln, was bei einem bestimmten Modell und Anwendungsbereich funktioniert.

_Aus diesem Grund gibt es in dieser Lektion keine Code-L√∂sungsteile. Stattdessen enth√§lt das Notebook Markdown-Zellen mit dem Titel "Meine L√∂sung:", die ein Beispielergebnis zur Referenz zeigen._

 <!--
LESSON TEMPLATE:
Fassen Sie den Abschnitt mit einer Zusammenfassung und Ressourcen f√ºr selbstgesteuertes Lernen zusammen.
-->

## Wissenscheck

Welche der folgenden Prompts folgt einigen vern√ºnftigen Best Practices?

1. Zeige mir ein Bild eines roten Autos
2. Zeige mir ein Bild eines roten Autos der Marke Volvo und Modell XC90, das an einer Klippe mit Sonnenuntergang geparkt ist
3. Zeige mir ein Bild eines roten Autos der Marke Volvo und Modell XC90

A: 2, es ist der beste Prompt, da er Details zum ‚ÄûWas‚Äú liefert und ins Spezifische geht (nicht einfach irgendein Auto, sondern eine bestimmte Marke und Modell) und zudem die Umgebung beschreibt. 3 ist das n√§chstbeste, da es auch viele Beschreibungen enth√§lt.

## üöÄ Herausforderung

Versuchen Sie, die ‚ÄûHinweis‚Äú-Technik mit dem Prompt zu nutzen: Vervollst√§ndige den Satz "Zeige mir ein Bild eines roten Autos der Marke Volvo und ". Womit antwortet es, und wie w√ºrden Sie es verbessern?

## Gro√üartige Arbeit! Fahren Sie mit Ihrem Lernen fort

M√∂chten Sie mehr √ºber verschiedene Konzepte des Prompt Engineerings lernen? Gehen Sie zur [Seite f√ºr weiterf√ºhrendes Lernen](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), um weitere gro√üartige Ressourcen zu diesem Thema zu finden.

Gehen Sie weiter zu Lektion 5, in der wir uns [fortgeschrittene Prompting-Techniken](../05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst) ansehen!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir auf Genauigkeit achten, bitten wir zu beachten, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das urspr√ºngliche Dokument in seiner Originalsprache ist als ma√ügebliche Quelle zu betrachten. F√ºr wichtige Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Nutzung dieser √úbersetzung entstehen.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->