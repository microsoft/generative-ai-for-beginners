<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a45c318dc6ebc2604f35b8b829f93af2",
  "translation_date": "2025-05-19T09:52:03+00:00",
  "source_file": "04-prompt-engineering-fundamentals/README.md",
  "language_code": "de"
}
-->
# Grundlagen der Prompt-Entwicklung

## Einf√ºhrung
Dieses Modul behandelt wesentliche Konzepte und Techniken zur Erstellung effektiver Prompts in generativen KI-Modellen. Auch die Art und Weise, wie Sie Ihren Prompt an ein LLM schreiben, ist wichtig. Ein sorgf√§ltig gestalteter Prompt kann eine bessere Qualit√§t der Antwort erzielen. Aber was genau bedeuten Begriffe wie _Prompt_ und _Prompt Engineering_? Und wie verbessere ich den Prompt _Input_, den ich an das LLM sende? Dies sind die Fragen, die wir in diesem Kapitel und dem n√§chsten zu beantworten versuchen.

_Generative KI_ ist in der Lage, neue Inhalte (z.B. Text, Bilder, Audio, Code etc.) als Antwort auf Benutzeranfragen zu erstellen. Dies erreicht sie durch _Large Language Models_ wie die GPT-Serie ("Generative Pre-trained Transformer") von OpenAI, die f√ºr die Nutzung von nat√ºrlicher Sprache und Code trainiert sind.

Benutzer k√∂nnen jetzt mit diesen Modellen √ºber vertraute Paradigmen wie Chat interagieren, ohne technische Kenntnisse oder Schulungen zu ben√∂tigen. Die Modelle sind _prompt-basiert_ - Benutzer senden einen Texteingabe (Prompt) und erhalten die KI-Antwort (Vervollst√§ndigung) zur√ºck. Sie k√∂nnen dann iterativ "mit der KI chatten" und ihre Prompts verfeinern, bis die Antwort ihren Erwartungen entspricht.

"Prompts" werden nun zur prim√§ren _Programmierschnittstelle_ f√ºr generative KI-Anwendungen, die den Modellen sagen, was zu tun ist, und die Qualit√§t der zur√ºckgegebenen Antworten beeinflussen. "Prompt Engineering" ist ein schnell wachsendes Forschungsfeld, das sich auf das _Design und die Optimierung_ von Prompts konzentriert, um konsistente und qualitativ hochwertige Antworten in gro√üem Ma√üstab zu liefern.

## Lernziele

In dieser Lektion lernen wir, was Prompt Engineering ist, warum es wichtig ist und wie wir effektivere Prompts f√ºr ein gegebenes Modell und Anwendungsziel erstellen k√∂nnen. Wir verstehen grundlegende Konzepte und Best Practices f√ºr Prompt Engineering - und lernen eine interaktive Jupyter Notebooks "Sandbox"-Umgebung kennen, in der wir diese Konzepte an realen Beispielen anwenden k√∂nnen.

Am Ende dieser Lektion werden wir in der Lage sein:

1. Zu erkl√§ren, was Prompt Engineering ist und warum es wichtig ist.
2. Die Komponenten eines Prompts zu beschreiben und wie sie verwendet werden.
3. Best Practices und Techniken f√ºr Prompt Engineering zu erlernen.
4. Gelernte Techniken an realen Beispielen anzuwenden, unter Verwendung eines OpenAI-Endpunkts.

## Schl√ºsselbegriffe

Prompt Engineering: Die Praxis des Entwerfens und Verfeinerns von Eingaben, um KI-Modelle dazu zu f√ºhren, gew√ºnschte Ausgaben zu erzeugen.
Tokenisierung: Der Prozess der Umwandlung von Text in kleinere Einheiten, sogenannte Tokens, die ein Modell verstehen und verarbeiten kann.
Instruction-Tuned LLMs: Gro√üe Sprachmodelle (LLMs), die mit spezifischen Anweisungen feinabgestimmt wurden, um ihre Antwortgenauigkeit und Relevanz zu verbessern.

## Lern-Sandbox

Prompt Engineering ist derzeit mehr Kunst als Wissenschaft. Der beste Weg, unser Gesp√ºr daf√ºr zu verbessern, ist _mehr zu √ºben_ und einen Trial-and-Error-Ansatz zu verfolgen, der Fachwissen aus dem Anwendungsbereich mit empfohlenen Techniken und modellspezifischen Optimierungen kombiniert.

Das Jupyter Notebook, das diese Lektion begleitet, bietet eine _Sandbox_-Umgebung, in der Sie ausprobieren k√∂nnen, was Sie lernen - entweder w√§hrend der Lektion oder als Teil der Code-Herausforderung am Ende. Um die √úbungen auszuf√ºhren, ben√∂tigen Sie:

1. **Einen Azure OpenAI API-Schl√ºssel** - den Dienstendpunkt f√ºr ein bereitgestelltes LLM.
2. **Eine Python-Laufzeitumgebung** - in der das Notebook ausgef√ºhrt werden kann.
3. **Lokale Umgebungsvariablen** - _schlie√üen Sie jetzt die [SETUP](./../00-course-setup/SETUP.md?WT.mc_id=academic-105485-koreyst) Schritte ab, um bereit zu sein_.

Das Notebook enth√§lt _Starter_-√úbungen - aber Sie werden ermutigt, Ihre eigenen _Markdown_- (Beschreibung) und _Code_- (Prompt-Anfragen) Abschnitte hinzuzuf√ºgen, um mehr Beispiele oder Ideen auszuprobieren - und Ihr Gesp√ºr f√ºr das Prompt-Design zu entwickeln.

## Illustrierte Anleitung

M√∂chten Sie das gro√üe Ganze dessen, was diese Lektion abdeckt, sehen, bevor Sie eintauchen? Schauen Sie sich diese illustrierte Anleitung an, die Ihnen einen Eindruck von den Hauptthemen vermittelt und die wichtigsten Erkenntnisse, √ºber die Sie nachdenken sollten. Der Lektionfahrplan f√ºhrt Sie von der Verst√§ndnis der grundlegenden Konzepte und Herausforderungen zur Bew√§ltigung dieser mit relevanten Prompt-Engineering-Techniken und Best Practices. Beachten Sie, dass sich der Abschnitt "Erweiterte Techniken" in dieser Anleitung auf Inhalte bezieht, die im _n√§chsten_ Kapitel dieses Lehrplans behandelt werden.

## Unser Startup

Nun, lassen Sie uns dar√ºber sprechen, wie _dieses Thema_ mit unserer Startup-Mission zusammenh√§ngt, [KI-Innovationen in die Bildung zu bringen](https://educationblog.microsoft.com/2023/06/collaborating-to-bring-ai-innovation-to-education?WT.mc_id=academic-105485-koreyst). Wir wollen KI-gest√ºtzte Anwendungen f√ºr _personalisiertes Lernen_ entwickeln - also denken wir dar√ºber nach, wie verschiedene Benutzer unserer Anwendung "Prompts entwerfen" k√∂nnten:

- **Administratoren** k√∂nnten die KI bitten, _Lehrplandaten zu analysieren, um L√ºcken in der Abdeckung zu identifizieren_. Die KI kann Ergebnisse zusammenfassen oder sie mit Code visualisieren.
- **Lehrer** k√∂nnten die KI bitten, _einen Unterrichtsplan f√ºr eine Zielgruppe und ein Thema zu erstellen_. Die KI kann den personalisierten Plan in einem bestimmten Format erstellen.
- **Sch√ºler** k√∂nnten die KI bitten, _sie in einem schwierigen Fach zu unterrichten_. Die KI kann jetzt Sch√ºler mit Lektionen, Hinweisen & Beispielen unterst√ºtzen, die auf ihrem Niveau zugeschnitten sind.

Das ist nur die Spitze des Eisbergs. Schauen Sie sich [Prompts For Education](https://github.com/microsoft/prompts-for-edu/tree/main?WT.mc_id=academic-105485-koreyst) an - eine Open-Source-Prompt-Bibliothek, die von Bildungsexperten kuratiert wurde - um ein breiteres Gef√ºhl f√ºr die M√∂glichkeiten zu bekommen! _Versuchen Sie, einige dieser Prompts in der Sandbox auszuf√ºhren oder den OpenAI Playground zu verwenden, um zu sehen, was passiert!_

## Was ist Prompt Engineering?

Wir haben diese Lektion begonnen, indem wir **Prompt Engineering** als den Prozess des _Entwerfens und Optimierens_ von Texteingaben (Prompts) definiert haben, um konsistente und qualitativ hochwertige Antworten (Vervollst√§ndigungen) f√ºr ein gegebenes Anwendungsziel und Modell zu liefern. Wir k√∂nnen dies als einen zweistufigen Prozess betrachten:

- _Entwerfen_ des urspr√ºnglichen Prompts f√ºr ein gegebenes Modell und Ziel
- _Verfeinern_ des Prompts iterativ, um die Qualit√§t der Antwort zu verbessern

Dies ist notwendigerweise ein Trial-and-Error-Prozess, der Benutzerintelligenz und Aufwand erfordert, um optimale Ergebnisse zu erzielen. Warum ist das wichtig? Um diese Frage zu beantworten, m√ºssen wir zun√§chst drei Konzepte verstehen:

- _Tokenisierung_ = wie das Modell den Prompt "sieht"
- _Basis-LLMs_ = wie das Grundmodell einen Prompt "verarbeitet"
- _Instruction-Tuned LLMs_ = wie das Modell jetzt "Aufgaben" sehen kann

### Tokenisierung

Ein LLM sieht Prompts als eine _Sequenz von Tokens_, bei der verschiedene Modelle (oder Versionen eines Modells) denselben Prompt auf unterschiedliche Weise tokenisieren k√∂nnen. Da LLMs auf Tokens trainiert werden (und nicht auf Rohtext), hat die Art und Weise, wie Prompts tokenisiert werden, einen direkten Einfluss auf die Qualit√§t der generierten Antwort.

Um ein Gesp√ºr daf√ºr zu bekommen, wie Tokenisierung funktioniert, probieren Sie Tools wie den [OpenAI Tokenizer](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst) aus, der unten gezeigt wird. Kopieren Sie Ihren Prompt hinein - und sehen Sie, wie dieser in Tokens umgewandelt wird, wobei Sie darauf achten, wie Leerzeichen und Satzzeichen behandelt werden. Beachten Sie, dass dieses Beispiel ein √§lteres LLM (GPT-3) zeigt - das Ausprobieren mit einem neueren Modell kann ein anderes Ergebnis liefern.

### Konzept: Basis-Modelle

Sobald ein Prompt tokenisiert ist, besteht die Hauptfunktion des ["Basis-LLM"](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) (oder Grundmodell) darin, das Token in dieser Sequenz vorherzusagen. Da LLMs auf riesigen Textdatens√§tzen trainiert sind, haben sie ein gutes Gesp√ºr f√ºr die statistischen Beziehungen zwischen Tokens und k√∂nnen diese Vorhersage mit einiger Zuversicht treffen. Beachten Sie, dass sie nicht die _Bedeutung_ der W√∂rter im Prompt oder Token verstehen; sie sehen nur ein Muster, das sie mit ihrer n√§chsten Vorhersage "vervollst√§ndigen" k√∂nnen. Sie k√∂nnen die Sequenz weiter vorhersagen, bis sie durch Benutzereingriff oder eine vorab festgelegte Bedingung beendet wird.

M√∂chten Sie sehen, wie promptbasierte Vervollst√§ndigung funktioniert? Geben Sie den obigen Prompt in das Azure OpenAI Studio [_Chat Playground_](https://oai.azure.com/playground?WT.mc_id=academic-105485-koreyst) mit den Standardeinstellungen ein. Das System ist so konfiguriert, dass es Prompts als Informationsanfragen behandelt - Sie sollten also eine Vervollst√§ndigung sehen, die diesen Kontext erf√ºllt.

Aber was ist, wenn der Benutzer etwas Spezifisches sehen m√∂chte, das einige Kriterien oder Aufgaben erf√ºllt? Hier kommen _instruction-tuned_ LLMs ins Spiel.

### Konzept: Instruction Tuned LLMs

Ein [Instruction Tuned LLM](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) beginnt mit dem Grundmodell und wird mit Beispielen oder Eingabe-/Ausgabepaaren (z.B. mehrstufige "Nachrichten") feinabgestimmt, die klare Anweisungen enthalten k√∂nnen - und die Antwort der KI versucht, dieser Anweisung zu folgen.

Dies verwendet Techniken wie Reinforcement Learning mit menschlichem Feedback (RLHF), die das Modell trainieren k√∂nnen, _Anweisungen zu folgen_ und _aus Feedback zu lernen_, sodass es Antworten liefert, die besser f√ºr praktische Anwendungen geeignet und relevanter f√ºr Benutzerziele sind.

Probieren wir es aus - rufen Sie den obigen Prompt erneut auf, aber √§ndern Sie jetzt die _Systemnachricht_, um die folgende Anweisung als Kontext bereitzustellen:

> _Fassen Sie den Inhalt, den Sie erhalten, f√ºr einen Zweitkl√§ssler zusammen. Halten Sie das Ergebnis in einem Absatz mit 3-5 Aufz√§hlungspunkten._

Sehen Sie, wie das Ergebnis jetzt auf das gew√ºnschte Ziel und Format abgestimmt ist? Ein Lehrer kann diese Antwort jetzt direkt in seinen Folien f√ºr diese Klasse verwenden.

## Warum brauchen wir Prompt Engineering?

Da wir nun wissen, wie Prompts von LLMs verarbeitet werden, sprechen wir dar√ºber, _warum_ wir Prompt Engineering ben√∂tigen. Die Antwort liegt darin, dass aktuelle LLMs eine Reihe von Herausforderungen darstellen, die es schwieriger machen, _zuverl√§ssige und konsistente Vervollst√§ndigungen_ zu erreichen, ohne Aufwand in die Erstellung und Optimierung von Prompts zu stecken. Zum Beispiel:

1. **Modellantworten sind stochastisch.** Der _gleiche Prompt_ wird wahrscheinlich unterschiedliche Antworten mit verschiedenen Modellen oder Modellversionen erzeugen. Und er kann sogar unterschiedliche Ergebnisse mit demselben Modell zu unterschiedlichen Zeiten erzeugen. _Prompt Engineering-Techniken k√∂nnen uns helfen, diese Variationen zu minimieren, indem sie bessere Leitplanken bieten_.

1. **Modelle k√∂nnen Antworten erfinden.** Modelle sind mit _gro√üen, aber endlichen_ Datens√§tzen vortrainiert, was bedeutet, dass sie kein Wissen √ºber Konzepte au√üerhalb dieses Trainingsumfangs haben. Infolgedessen k√∂nnen sie Vervollst√§ndigungen erzeugen, die ungenau, erfunden oder direkt im Widerspruch zu bekannten Fakten stehen. _Prompt Engineering-Techniken helfen Benutzern, solche Erfindungen zu identifizieren und zu mindern, z.B. indem sie die KI um Zitate oder Begr√ºndungen bitten_.

1. **Modellf√§higkeiten variieren.** Neuere Modelle oder Modellgenerationen haben reichere F√§higkeiten, bringen aber auch einzigartige Eigenheiten und Kompromisse in Bezug auf Kosten und Komplexit√§t mit sich. _Prompt Engineering kann uns helfen, Best Practices und Workflows zu entwickeln, die Unterschiede abstrahieren und sich nahtlos an modellspezifische Anforderungen anpassen_.

Lassen Sie uns dies in Aktion im OpenAI oder Azure OpenAI Playground sehen:

- Verwenden Sie denselben Prompt mit verschiedenen LLM-Bereitstellungen (z.B. OpenAI, Azure OpenAI, Hugging Face) - haben Sie die Variationen gesehen?
- Verwenden Sie denselben Prompt wiederholt mit derselben LLM-Bereitstellung (z.B. Azure OpenAI Playground) - wie unterschieden sich diese Variationen?

### Beispiel f√ºr Erfindungen

In diesem Kurs verwenden wir den Begriff **"Erfindung"**, um das Ph√§nomen zu beschreiben, bei dem LLMs manchmal faktisch falsche Informationen aufgrund von Einschr√§nkungen in ihrem Training oder anderen Beschr√§nkungen generieren. M√∂glicherweise haben Sie dies auch als _"Halluzinationen"_ in popul√§ren Artikeln oder Forschungspapieren geh√∂rt. Wir empfehlen jedoch dringend, den Begriff _"Erfindung"_ zu verwenden, um das Verhalten nicht versehentlich zu vermenschlichen, indem wir eine menschliche Eigenschaft einem maschinengesteuerten Ergebnis zuschreiben. Dies verst√§rkt auch die [Richtlinien f√ºr verantwortungsvolle KI](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-105485-koreyst) aus einer terminologischen Perspektive, indem Begriffe entfernt werden, die in einigen Kontexten als anst√∂√üig oder nicht inklusiv angesehen werden k√∂nnten.

M√∂chten Sie ein Gef√ºhl daf√ºr bekommen, wie Erfindungen funktionieren? Denken Sie an einen Prompt, der die KI anweist, Inhalte f√ºr ein nicht existierendes Thema zu generieren (um sicherzustellen, dass es nicht im Trainingsdatensatz zu finden ist). Zum Beispiel habe ich diesen Prompt ausprobiert:

> **Prompt:** Erstellen Sie einen Unterrichtsplan √ºber den Marskrieg von 2076.

Eine Websuche zeigte mir, dass es fiktionale Berichte (z.B. Fernsehserien oder B√ºcher) √ºber Marskriege gab - aber keinen im Jahr 2076. Der gesunde Menschenverstand sagt uns auch, dass 2076 _in der Zukunft_ liegt und daher nicht mit einem realen Ereignis in Verbindung gebracht werden kann.

Was passiert also, wenn wir diesen Prompt mit verschiedenen LLM-Anbietern ausf√ºhren?

> **Antwort 1**: OpenAI Playground (GPT-35)

> **Antwort 2**: Azure OpenAI Playground (GPT-35)

> **Antwort 3**: : Hugging Face Chat Playground (LLama-2)

Wie erwartet, erzeugt jedes Modell (oder Modellversion) aufgrund des stochastischen Verhaltens und der Variationen der Modellf√§higkeiten leicht unterschiedliche Antworten. Zum Beispiel richtet sich ein Modell an ein Publikum der 8. Klasse, w√§hrend das andere von einem Sch√ºler der High School ausgeht. Aber alle drei Modelle erzeugten Antworten, die einen uninformierten Benutzer davon √ºberzeugen k√∂nnten, dass das Ereignis real war.

Prompt Engineering-Techniken wie _Metaprompting_ und _Temperaturkonfiguration_ k√∂nnen Modell-Erfindungen bis zu einem gewissen Grad reduzieren. Neue Prompt Engineering-_Architekturen_ integrieren auch nahtlos neue Tools und Techniken in den Prompt-Flow, um einige dieser Effekte zu mindern oder zu reduzieren.

## Fallstudie: GitHub Copilot

Lassen Sie uns diesen Abschnitt abschlie√üen, indem wir ein Gef√ºhl daf√ºr bekommen, wie Prompt Engineering in realen L√∂sungen verwendet wird, indem wir eine Fallstudie betrachten: [GitHub Copilot](https://github.com/features/copilot?WT.mc_id=academic-105485-koreyst).

GitHub Copilot ist Ihr "KI-Paar-Programmierer" - es wandelt Text-Prompts in Code-Vervollst√§ndigungen um und ist in Ihre Entwicklungsumgebung (z.B. Visual Studio Code) f√ºr ein nahtloses Benutzererlebnis integriert. Wie in der unten stehenden Blog-Serie dokumentiert, basierte die fr√ºheste Version auf dem OpenAI Codex-Modell - wobei Ingenieure schnell die Notwendigkeit erkannten, das Modell zu verfeinern und bessere Prompt Engineering-Techniken zu entwickeln, um die Codequalit√§t zu verbessern. Im Juli [deb√ºtierten sie ein verbessertes KI-Modell, das √ºber Codex hinausgeht](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/?WT.mc_id=academic-105485-koreyst) f√ºr noch schnellere Vorschl√§ge.

Lesen Sie die Beitr√§ge der Reihe nach, um ihre Lernreise zu verfolgen.

- **Mai 2023** | [GitHub Copilot wird besser darin, Ihren Code zu verstehen](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/?WT.mc_id=academic-105485-koreyst)
- **Mai 2023** | [Inside GitHub: Arbeiten mit den LLMs hinter GitHub Copilot](https://github.blog/2023-05-17-inside-github-working-with-the-llms-behind-github-copilot/?WT.mc_id=academic-105485-koreyst).
- **Juni 2023** | [Wie man bessere Prompts f√ºr GitHub Copilot schreibt](https://github.blog/2023-06-20-how-to-write-better-prompts-for-github-copilot/?WT.mc_id=academic-105485-koreyst).
- **Juli 2023** | [.. GitHub Copilot geht √ºber Codex hinaus mit verbessertem KI-Modell](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/?WT.mc_id=academic-105485-koreyst)
- **Juli 2023** | [Ein Entwicklerleitfaden f√ºr Prompt Engineering und LLMs](https://github.blog/2023-07-17-prompt-engineering-guide-generative-ai-llms
Schlie√ülich liegt der wahre Wert von Vorlagen in der F√§higkeit, _Prompt-Bibliotheken_ f√ºr vertikale Anwendungsdom√§nen zu erstellen und zu ver√∂ffentlichen - wo die Prompt-Vorlage nun _optimiert_ wird, um anwendungsspezifischen Kontext oder Beispiele widerzuspiegeln, die die Antworten f√ºr die Zielnutzergruppe relevanter und genauer machen. Das [Prompts For Edu](https://github.com/microsoft/prompts-for-edu?WT.mc_id=academic-105485-koreyst) Repository ist ein gro√üartiges Beispiel f√ºr diesen Ansatz und kuratiert eine Bibliothek von Prompts f√ºr den Bildungsbereich mit Schwerpunkt auf Schl√ºsselzielen wie Unterrichtsplanung, Lehrplangestaltung, Sch√ºlerbetreuung usw.

## Unterst√ºtzende Inhalte

Wenn wir die Erstellung von Prompts als Anweisung (Aufgabe) und Ziel (Hauptinhalt) betrachten, dann ist _sekund√§rer Inhalt_ wie zus√§tzlicher Kontext, den wir bereitstellen, um **die Ausgabe in gewisser Weise zu beeinflussen**. Es k√∂nnten Parameteranpassungen, Formatierungsanweisungen, Themen-Taxonomien usw. sein, die dem Modell helfen k√∂nnen, seine Antwort an die gew√ºnschten Nutzerziele oder Erwartungen anzupassen.

Beispiel: Gegeben sei ein Kurskatalog mit umfangreichen Metadaten (Name, Beschreibung, Niveau, Metadaten-Tags, Dozent usw.) zu allen verf√ºgbaren Kursen im Lehrplan:

- Wir k√∂nnen eine Anweisung definieren, um "den Kurskatalog f√ºr Herbst 2023 zusammenzufassen".
- Wir k√∂nnen den Hauptinhalt nutzen, um einige Beispiele des gew√ºnschten Outputs bereitzustellen.
- Wir k√∂nnen den sekund√§ren Inhalt verwenden, um die Top 5 "Tags" von Interesse zu identifizieren.

Nun kann das Modell eine Zusammenfassung im Format der gezeigten Beispiele liefern - aber wenn ein Ergebnis mehrere Tags hat, kann es die 5 im sekund√§ren Inhalt identifizierten Tags priorisieren.

---

<!--
UNTERRICHTSVORLAGE:
Diese Einheit sollte das Kernkonzept #1 abdecken.
Verst√§rken Sie das Konzept mit Beispielen und Referenzen.

KONZEPT #3:
Techniken des Prompt-Engineerings.
Was sind einige grundlegende Techniken f√ºr das Prompt-Engineering?
Veranschaulichen Sie es mit einigen √úbungen.
-->

## Best Practices f√ºr Prompts

Jetzt, da wir wissen, wie Prompts _konstruiert_ werden k√∂nnen, k√∂nnen wir dar√ºber nachdenken, wie wir sie _gestalten_ k√∂nnen, um Best Practices widerzuspiegeln. Wir k√∂nnen dies in zwei Teile unterteilen - die richtige _Einstellung_ und die Anwendung der richtigen _Techniken_.

### Einstellung zum Prompt-Engineering

Prompt-Engineering ist ein Prozess des Ausprobierens, daher sollten Sie drei breite Leitfaktoren im Auge behalten:

1. **Dom√§nenverst√§ndnis z√§hlt.** Die Genauigkeit und Relevanz der Antworten h√§ngt von der _Dom√§ne_ ab, in der die Anwendung oder der Benutzer arbeitet. Nutzen Sie Ihre Intuition und Fachkenntnisse, um **Techniken weiter anzupassen**. Definieren Sie beispielsweise _dom√§nenspezifische Pers√∂nlichkeiten_ in Ihren System-Prompts oder verwenden Sie _dom√§nenspezifische Vorlagen_ in Ihren Nutzer-Prompts. Stellen Sie sekund√§ren Inhalt bereit, der dom√§nenspezifische Kontexte widerspiegelt, oder verwenden Sie _dom√§nenspezifische Hinweise und Beispiele_, um das Modell zu vertrauten Nutzungsmustern zu f√ºhren.

2. **Modellverst√§ndnis z√§hlt.** Wir wissen, dass Modelle von Natur aus stochastisch sind. Aber Modellimplementierungen k√∂nnen auch hinsichtlich des verwendeten Trainingsdatensatzes (vortrainiertes Wissen), der bereitgestellten F√§higkeiten (z.B. √ºber API oder SDK) und der Art der Inhalte, f√ºr die sie optimiert sind (z.B. Code vs. Bilder vs. Text), variieren. Verstehen Sie die St√§rken und Einschr√§nkungen des von Ihnen verwendeten Modells und nutzen Sie dieses Wissen, um _Aufgaben zu priorisieren_ oder _angepasste Vorlagen zu erstellen_, die f√ºr die F√§higkeiten des Modells optimiert sind.

3. **Iteration & Validierung z√§hlen.** Modelle entwickeln sich schnell weiter, ebenso wie die Techniken des Prompt-Engineerings. Als Fachexperte k√∂nnen Sie andere Kontexte oder Kriterien f√ºr _Ihre_ spezifische Anwendung haben, die m√∂glicherweise nicht auf die breitere Gemeinschaft zutreffen. Verwenden Sie Tools und Techniken des Prompt-Engineerings, um den Prompt-Aufbau zu "beschleunigen", und iterieren und validieren Sie die Ergebnisse dann mit Ihrer eigenen Intuition und Fachkenntnis. Zeichnen Sie Ihre Erkenntnisse auf und erstellen Sie eine **Wissensbasis** (z.B. Prompt-Bibliotheken), die als neue Grundlage f√ºr andere verwendet werden kann, um in Zukunft schnellere Iterationen zu erm√∂glichen.

## Best Practices

Schauen wir uns nun allgemeine Best Practices an, die von [OpenAI](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api?WT.mc_id=academic-105485-koreyst) und [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering#best-practices?WT.mc_id=academic-105485-koreyst) Praktikern empfohlen werden.

| Was                               | Warum                                                                                                                                                                                                                                               |
| :-------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Bewerten Sie die neuesten Modelle. | Neue Modellgenerationen haben wahrscheinlich verbesserte Funktionen und Qualit√§t - k√∂nnen aber auch h√∂here Kosten verursachen. Bewerten Sie sie auf ihre Auswirkungen hin und treffen Sie dann Migrationsentscheidungen.                             |
| Trennen Sie Anweisungen & Kontext | √úberpr√ºfen Sie, ob Ihr Modell/Anbieter _Trennzeichen_ definiert, um Anweisungen, Haupt- und Sekund√§rinhalte klarer zu unterscheiden. Dies kann den Modellen helfen, Tokens genauer zu gewichten.                                                     |
| Seien Sie spezifisch und klar     | Geben Sie mehr Details √ºber den gew√ºnschten Kontext, das Ergebnis, die L√§nge, das Format, den Stil usw. an. Dies wird sowohl die Qualit√§t als auch die Konsistenz der Antworten verbessern. Erfassen Sie Rezepte in wiederverwendbaren Vorlagen.      |
| Seien Sie beschreibend, verwenden Sie Beispiele | Modelle reagieren m√∂glicherweise besser auf einen "zeigen und erz√§hlen" Ansatz. Beginnen Sie mit einem `zero-shot` approach where you give it an instruction (but no examples) then try `few-shot` as a refinement, providing a few examples of the desired output. Use analogies. |
| Use cues to jumpstart completions | Nudge it towards a desired outcome by giving it some leading words or phrases that it can use as a starting point for the response.                                                                                                               |
| Double Down                       | Sometimes you may need to repeat yourself to the model. Give instructions before and after your primary content, use an instruction and a cue, etc. Iterate & validate to see what works.                                                         |
| Order Matters                     | The order in which you present information to the model may impact the output, even in the learning examples, thanks to recency bias. Try different options to see what works best.                                                               |
| Give the model an ‚Äúout‚Äù           | Give the model a _fallback_ completion response it can provide if it cannot complete the task for any reason. This can reduce chances of models generating false or fabricated responses.                                                         |
|                                   |                                                                                                                                                                                                                                                   |

As with any best practice, remember that _your mileage may vary_ based on the model, the task and the domain. Use these as a starting point, and iterate to find what works best for you. Constantly re-evaluate your prompt engineering process as new models and tools become available, with a focus on process scalability and response quality.

<!--
LESSON TEMPLATE:
This unit should provide a code challenge if applicable

CHALLENGE:
Link to a Jupyter Notebook with only the code comments in the instructions (code sections are empty).

SOLUTION:
Link to a copy of that Notebook with the prompts filled in and run, showing what one example could be.
-->

## Assignment

Congratulations! You made it to the end of the lesson! It's time to put some of those concepts and techniques to the test with real examples!

For our assignment, we'll be using a Jupyter Notebook with exercises you can complete interactively. You can also extend the Notebook with your own Markdown and Code cells to explore ideas and techniques on your own.

### To get started, fork the repo, then

- (Recommended) Launch GitHub Codespaces
- (Alternatively) Clone the repo to your local device and use it with Docker Desktop
- (Alternatively) Open the Notebook with your preferred Notebook runtime environment.

### Next, configure your environment variables

- Copy the `.env.copy` file in repo root to `.env` and fill in the `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_DEPLOYMENT` Werte. Kehren Sie zum Abschnitt [Learning Sandbox](../../../04-prompt-engineering-fundamentals/04-prompt-engineering-fundamentals) zur√ºck, um zu erfahren, wie.

### √ñffnen Sie als n√§chstes das Jupyter Notebook

- W√§hlen Sie den Laufzeitkernel aus. Wenn Sie die Optionen 1 oder 2 verwenden, w√§hlen Sie einfach den Standard-Python 3.10.x-Kernel aus, der vom Dev-Container bereitgestellt wird.

Sie sind bereit, die √úbungen durchzuf√ºhren. Beachten Sie, dass es hier keine _richtigen und falschen_ Antworten gibt - es geht nur darum, Optionen durch Ausprobieren zu erkunden und ein Gef√ºhl daf√ºr zu entwickeln, was f√ºr ein bestimmtes Modell und eine bestimmte Anwendungsdom√§ne funktioniert.

_Aus diesem Grund gibt es in dieser Lektion keine Code-L√∂sungssegmente. Stattdessen enth√§lt das Notebook Markdown-Zellen mit dem Titel "Meine L√∂sung:", die ein Beispielergebnis als Referenz zeigen._

<!--
UNTERRICHTSVORLAGE:
Schlie√üen Sie den Abschnitt mit einer Zusammenfassung und Ressourcen f√ºr selbstgesteuertes Lernen ab.
-->

## Wissens√ºberpr√ºfung

Welche der folgenden Optionen ist ein gutes Prompt, das einigen vern√ºnftigen Best Practices folgt?

1. Zeige mir ein Bild eines roten Autos
2. Zeige mir ein Bild eines roten Autos der Marke Volvo und Modell XC90, das an einer Klippe mit untergehender Sonne geparkt ist
3. Zeige mir ein Bild eines roten Autos der Marke Volvo und Modell XC90

A: 2, es ist das beste Prompt, da es Details zu "was" liefert und spezifisch wird (nicht nur ein beliebiges Auto, sondern eine bestimmte Marke und ein bestimmtes Modell) und es beschreibt auch die Gesamtszenerie. 3 ist das n√§chstbeste, da es ebenfalls viele Beschreibungen enth√§lt.

## üöÄ Herausforderung

Sehen Sie, ob Sie die "Hinweis"-Technik mit dem Prompt nutzen k√∂nnen: Vervollst√§ndigen Sie den Satz "Zeige mir ein Bild eines roten Autos der Marke Volvo und ". Was antwortet es, und wie w√ºrden Sie es verbessern?

## Gro√üartige Arbeit! Setzen Sie Ihr Lernen fort

M√∂chten Sie mehr √ºber verschiedene Konzepte des Prompt-Engineerings erfahren? Besuchen Sie die [Seite f√ºr kontinuierliches Lernen](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), um weitere gro√üartige Ressourcen zu diesem Thema zu finden.

Gehen Sie zu Lektion 5, in der wir uns [fortgeschrittene Prompting-Techniken](../05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst) ansehen werden!

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, weisen wir darauf hin, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir haften nicht f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Verwendung dieser √úbersetzung ergeben.