<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8b3cb38518cf4fe7714d2f5e74dfa3eb",
  "translation_date": "2025-10-03T08:11:56+00:00",
  "source_file": "04-prompt-engineering-fundamentals/README.md",
  "language_code": "de"
}
-->
# Grundlagen des Prompt Engineering

[![Grundlagen des Prompt Engineering](../../../translated_images/04-lesson-banner.a2c90deba7fedacda69f35b41636a8951ec91c2e33f5420b1254534ac85bc18e.de.png)](https://aka.ms/gen-ai-lesson4-gh?WT.mc_id=academic-105485-koreyst)

## Einf√ºhrung
Dieses Modul behandelt grundlegende Konzepte und Techniken zur Erstellung effektiver Prompts f√ºr generative KI-Modelle. Die Art und Weise, wie Sie Ihren Prompt an ein LLM schreiben, ist entscheidend. Ein sorgf√§ltig gestalteter Prompt kann die Qualit√§t der Antwort erheblich verbessern. Aber was genau bedeuten Begriffe wie _Prompt_ und _Prompt Engineering_? Und wie kann ich den _Eingabe_-Prompt, den ich an das LLM sende, verbessern? Diese Fragen werden wir in diesem Kapitel und im n√§chsten beantworten.

_Generative KI_ ist in der Lage, neue Inhalte (z. B. Texte, Bilder, Audio, Code usw.) als Antwort auf Benutzeranfragen zu erstellen. Dies wird durch _Large Language Models_ wie die GPT-Serie ("Generative Pre-trained Transformer") von OpenAI erm√∂glicht, die f√ºr die Verarbeitung nat√ºrlicher Sprache und Code trainiert wurden.

Benutzer k√∂nnen jetzt mit diesen Modellen √ºber vertraute Paradigmen wie Chat interagieren, ohne technische Vorkenntnisse oder Schulungen zu ben√∂tigen. Die Modelle sind _promptbasiert_ ‚Äì Benutzer senden eine Texteingabe (Prompt) und erhalten eine KI-Antwort (Completion). Sie k√∂nnen dann iterativ mit der KI "chatten", in mehrstufigen Gespr√§chen ihren Prompt verfeinern, bis die Antwort ihren Erwartungen entspricht.

"Prompts" werden somit zur prim√§ren _Programmierschnittstelle_ f√ºr generative KI-Anwendungen, die den Modellen mitteilen, was zu tun ist, und die Qualit√§t der zur√ºckgegebenen Antworten beeinflussen. "Prompt Engineering" ist ein schnell wachsendes Forschungsfeld, das sich auf die _Gestaltung und Optimierung_ von Prompts konzentriert, um konsistente und qualitativ hochwertige Antworten in gro√üem Ma√üstab zu liefern.

## Lernziele

In dieser Lektion lernen wir, was Prompt Engineering ist, warum es wichtig ist und wie wir effektivere Prompts f√ºr ein bestimmtes Modell und ein bestimmtes Anwendungsziel erstellen k√∂nnen. Wir werden grundlegende Konzepte und Best Practices f√ºr das Prompt Engineering verstehen ‚Äì und erfahren von einer interaktiven Jupyter-Notebook-Umgebung, in der wir diese Konzepte anhand realer Beispiele anwenden k√∂nnen.

Am Ende dieser Lektion werden wir in der Lage sein:

1. Zu erkl√§ren, was Prompt Engineering ist und warum es wichtig ist.
2. Die Bestandteile eines Prompts zu beschreiben und wie sie verwendet werden.
3. Best Practices und Techniken f√ºr das Prompt Engineering zu erlernen.
4. Gelernte Techniken auf reale Beispiele anzuwenden, unter Verwendung eines OpenAI-Endpunkts.

## Schl√ºsselbegriffe

Prompt Engineering: Die Praxis, Eingaben so zu gestalten und zu verfeinern, dass KI-Modelle gew√ºnschte Ausgaben erzeugen.
Tokenisierung: Der Prozess, Text in kleinere Einheiten, sogenannte Tokens, umzuwandeln, die ein Modell verstehen und verarbeiten kann.
Instruction-Tuned LLMs: Gro√üe Sprachmodelle (LLMs), die mit spezifischen Anweisungen feinabgestimmt wurden, um die Genauigkeit und Relevanz ihrer Antworten zu verbessern.

## Lernumgebung

Prompt Engineering ist derzeit mehr Kunst als Wissenschaft. Der beste Weg, unser Gesp√ºr daf√ºr zu verbessern, ist _mehr √úbung_ und ein Trial-and-Error-Ansatz, der Fachwissen aus dem Anwendungsbereich mit empfohlenen Techniken und modellbezogenen Optimierungen kombiniert.

Das Jupyter-Notebook, das diese Lektion begleitet, bietet eine _Sandbox_-Umgebung, in der Sie das Gelernte ausprobieren k√∂nnen ‚Äì entweder w√§hrend der Lektion oder im Rahmen der Code-Challenge am Ende. Um die √úbungen auszuf√ºhren, ben√∂tigen Sie:

1. **Einen Azure OpenAI API-Schl√ºssel** ‚Äì den Service-Endpunkt f√ºr ein bereitgestelltes LLM.
2. **Eine Python-Laufzeitumgebung** ‚Äì in der das Notebook ausgef√ºhrt werden kann.
3. **Lokale Umgebungsvariablen** ‚Äì _f√ºhren Sie die [EINRICHTUNG](./../00-course-setup/02-setup-local.md?WT.mc_id=academic-105485-koreyst) jetzt durch, um sich vorzubereiten_.

Das Notebook enth√§lt _Starter_-√úbungen ‚Äì Sie werden jedoch ermutigt, Ihre eigenen _Markdown_- (Beschreibung) und _Code_- (Prompt-Anfragen) Abschnitte hinzuzuf√ºgen, um weitere Beispiele oder Ideen auszuprobieren ‚Äì und Ihr Gesp√ºr f√ºr die Gestaltung von Prompts zu entwickeln.

## Illustrierter Leitfaden

M√∂chten Sie sich einen √úberblick √ºber die Inhalte dieser Lektion verschaffen, bevor Sie eintauchen? Schauen Sie sich diesen illustrierten Leitfaden an, der Ihnen die Hauptthemen und die wichtigsten Erkenntnisse vermittelt, die Sie in jedem Abschnitt ber√ºcksichtigen sollten. Der Lehrplan f√ºhrt Sie von der Einf√ºhrung in die grundlegenden Konzepte und Herausforderungen bis hin zu deren Bew√§ltigung mit relevanten Techniken und Best Practices des Prompt Engineering. Beachten Sie, dass der Abschnitt "Fortgeschrittene Techniken" in diesem Leitfaden Inhalte behandelt, die im _n√§chsten_ Kapitel dieses Lehrplans behandelt werden.

![Illustrierter Leitfaden zum Prompt Engineering](../../../translated_images/04-prompt-engineering-sketchnote.d5f33336957a1e4f623b826195c2146ef4cc49974b72fa373de6929b474e8b70.de.png)

## Unser Startup

Nun wollen wir dar√ºber sprechen, wie _dieses Thema_ mit unserer Startup-Mission zusammenh√§ngt, [KI-Innovationen in die Bildung zu bringen](https://educationblog.microsoft.com/2023/06/collaborating-to-bring-ai-innovation-to-education?WT.mc_id=academic-105485-koreyst). Wir m√∂chten KI-gest√ºtzte Anwendungen f√ºr _personalisiertes Lernen_ entwickeln ‚Äì denken wir also dar√ºber nach, wie verschiedene Benutzer unserer Anwendung "Prompts" gestalten k√∂nnten:

- **Administratoren** k√∂nnten die KI bitten, _Lehrplandaten zu analysieren, um L√ºcken in der Abdeckung zu identifizieren_. Die KI kann Ergebnisse zusammenfassen oder sie mit Code visualisieren.
- **Lehrkr√§fte** k√∂nnten die KI bitten, _einen Unterrichtsplan f√ºr eine Zielgruppe und ein Thema zu erstellen_. Die KI kann den personalisierten Plan in einem bestimmten Format erstellen.
- **Sch√ºler** k√∂nnten die KI bitten, _sie in einem schwierigen Fach zu unterrichten_. Die KI kann Sch√ºler nun mit Lektionen, Hinweisen und Beispielen unterst√ºtzen, die auf ihr Niveau zugeschnitten sind.

Das ist nur die Spitze des Eisbergs. Schauen Sie sich [Prompts For Education](https://github.com/microsoft/prompts-for-edu/tree/main?WT.mc_id=academic-105485-koreyst) an ‚Äì eine Open-Source-Bibliothek mit Prompts, die von Bildungsexperten kuratiert wurde ‚Äì um ein breiteres Verst√§ndnis der M√∂glichkeiten zu bekommen! _Probieren Sie einige dieser Prompts in der Sandbox oder im OpenAI Playground aus, um zu sehen, was passiert!_

<!--
LEKTIONSVORLAGE:
Diese Einheit sollte das Kernkonzept #1 abdecken.
Verst√§rken Sie das Konzept mit Beispielen und Referenzen.

KONZEPT #1:
Prompt Engineering.
Definieren Sie es und erkl√§ren Sie, warum es ben√∂tigt wird.
-->

## Was ist Prompt Engineering?

Wir haben diese Lektion begonnen, indem wir **Prompt Engineering** als den Prozess definiert haben, _Texteingaben (Prompts) zu gestalten und zu optimieren_, um konsistente und qualitativ hochwertige Antworten (Completions) f√ºr ein bestimmtes Anwendungsziel und Modell zu liefern. Wir k√∂nnen dies als einen zweistufigen Prozess betrachten:

- _Gestaltung_ des initialen Prompts f√ºr ein bestimmtes Modell und Ziel
- _Verfeinerung_ des Prompts in Iterationen, um die Qualit√§t der Antwort zu verbessern

Dies ist notwendigerweise ein Trial-and-Error-Prozess, der Benutzerintuition und Aufwand erfordert, um optimale Ergebnisse zu erzielen. Warum ist das wichtig? Um diese Frage zu beantworten, m√ºssen wir zun√§chst drei Konzepte verstehen:

- _Tokenisierung_ = wie das Modell den Prompt "sieht"
- _Basis-LLMs_ = wie das Grundmodell einen Prompt "verarbeitet"
- _Instruction-Tuned LLMs_ = wie das Modell jetzt "Aufgaben" sehen kann

### Tokenisierung

Ein LLM sieht Prompts als eine _Sequenz von Tokens_, wobei verschiedene Modelle (oder Versionen eines Modells) denselben Prompt unterschiedlich tokenisieren k√∂nnen. Da LLMs auf Tokens (und nicht auf Rohtext) trainiert werden, hat die Art und Weise, wie Prompts tokenisiert werden, direkten Einfluss auf die Qualit√§t der generierten Antwort.

Um ein Gesp√ºr daf√ºr zu bekommen, wie Tokenisierung funktioniert, probieren Sie Tools wie den [OpenAI Tokenizer](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst) aus, der unten gezeigt wird. Kopieren Sie Ihren Prompt hinein ‚Äì und sehen Sie, wie dieser in Tokens umgewandelt wird, wobei Sie darauf achten, wie Leerzeichen und Satzzeichen behandelt werden. Beachten Sie, dass dieses Beispiel ein √§lteres LLM (GPT-3) zeigt ‚Äì das Ausprobieren mit einem neueren Modell k√∂nnte ein anderes Ergebnis liefern.

![Tokenisierung](../../../translated_images/04-tokenizer-example.e71f0a0f70356c5c7d80b21e8753a28c18a7f6d4aaa1c4b08e65d17625e85642.de.png)

### Konzept: Grundmodelle

Sobald ein Prompt tokenisiert ist, besteht die Hauptfunktion des ["Basis-LLM"](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) (oder Grundmodells) darin, das n√§chste Token in dieser Sequenz vorherzusagen. Da LLMs auf riesigen Textdatens√§tzen trainiert werden, haben sie ein gutes Verst√§ndnis f√ºr die statistischen Beziehungen zwischen Tokens und k√∂nnen diese Vorhersage mit einer gewissen Sicherheit treffen. Beachten Sie, dass sie die _Bedeutung_ der W√∂rter im Prompt oder Token nicht verstehen; sie sehen lediglich ein Muster, das sie mit ihrer n√§chsten Vorhersage "vervollst√§ndigen" k√∂nnen. Sie k√∂nnen die Sequenz weiter vorhersagen, bis sie durch Benutzereingriff oder eine vorab festgelegte Bedingung beendet wird.

M√∂chten Sie sehen, wie promptbasierte Completion funktioniert? Geben Sie den obigen Prompt in das Azure OpenAI Studio [_Chat Playground_](https://oai.azure.com/playground?WT.mc_id=academic-105485-koreyst) mit den Standardeinstellungen ein. Das System ist so konfiguriert, dass Prompts als Informationsanfragen behandelt werden ‚Äì Sie sollten also eine Completion sehen, die diesen Kontext erf√ºllt.

Aber was, wenn der Benutzer etwas Spezifisches sehen m√∂chte, das bestimmte Kriterien oder ein Aufgaben-Ziel erf√ºllt? Hier kommen _instruction-tuned_ LLMs ins Spiel.

![Basis-LLM Chat Completion](../../../translated_images/04-playground-chat-base.65b76fcfde0caa6738e41d20f1a6123f9078219e6f91a88ee5ea8014f0469bdf.de.png)

### Konzept: Instruction-Tuned LLMs

Ein [Instruction-Tuned LLM](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) beginnt mit dem Grundmodell und wird mit Beispielen oder Eingabe-/Ausgabe-Paaren (z. B. mehrstufigen "Nachrichten") feinabgestimmt, die klare Anweisungen enthalten k√∂nnen ‚Äì und die Antwort der KI versucht, diese Anweisung zu befolgen.

Dies nutzt Techniken wie Reinforcement Learning mit menschlichem Feedback (RLHF), die das Modell trainieren k√∂nnen, _Anweisungen zu folgen_ und _aus Feedback zu lernen_, sodass es Antworten liefert, die besser f√ºr praktische Anwendungen geeignet und relevanter f√ºr Benutzerziele sind.

Probieren wir es aus ‚Äì gehen Sie zur√ºck zum obigen Prompt, √§ndern Sie jedoch jetzt die _Systemnachricht_, um die folgende Anweisung als Kontext bereitzustellen:

> _Fassen Sie Inhalte, die Ihnen bereitgestellt werden, f√ºr einen Zweitkl√§ssler zusammen. Halten Sie das Ergebnis in einem Absatz mit 3-5 Aufz√§hlungspunkten._

Sehen Sie, wie das Ergebnis jetzt auf das gew√ºnschte Ziel und Format abgestimmt ist? Ein Lehrer kann diese Antwort nun direkt in seinen Folien f√ºr diese Klasse verwenden.

![Instruction-Tuned LLM Chat Completion](../../../translated_images/04-playground-chat-instructions.b30bbfbdf92f2d051639c9bc23f74a0e2482f8dc7f0dafc6cc6fda81b2b00534.de.png)

## Warum brauchen wir Prompt Engineering?

Jetzt, da wir wissen, wie Prompts von LLMs verarbeitet werden, sprechen wir dar√ºber, _warum_ wir Prompt Engineering ben√∂tigen. Die Antwort liegt in der Tatsache, dass aktuelle LLMs eine Reihe von Herausforderungen darstellen, die es schwieriger machen, _zuverl√§ssige und konsistente Completions_ zu erzielen, ohne Aufwand in die Gestaltung und Optimierung von Prompts zu investieren. Zum Beispiel:

1. **Modellantworten sind stochastisch.** Der _gleiche Prompt_ wird wahrscheinlich unterschiedliche Antworten mit verschiedenen Modellen oder Modellversionen erzeugen. Und er kann sogar mit demselben Modell zu unterschiedlichen Zeiten unterschiedliche Ergebnisse liefern. _Prompt Engineering-Techniken k√∂nnen uns helfen, diese Variationen zu minimieren, indem sie bessere Leitplanken bieten_.

1. **Modelle k√∂nnen Antworten erfinden.** Modelle sind mit _gro√üen, aber endlichen_ Datens√§tzen vortrainiert, was bedeutet, dass sie kein Wissen √ºber Konzepte au√üerhalb dieses Trainingsumfangs haben. Infolgedessen k√∂nnen sie Completions erzeugen, die ungenau, erfunden oder direkt widerspr√ºchlich zu bekannten Fakten sind. _Prompt Engineering-Techniken helfen Benutzern, solche Erfindungen zu identifizieren und zu mindern, z. B. indem sie die KI um Zitate oder Begr√ºndungen bitten_.

1. **Modellf√§higkeiten variieren.** Neuere Modelle oder Modellgenerationen haben umfangreichere F√§higkeiten, bringen aber auch einzigartige Eigenheiten und Kompromisse in Bezug auf Kosten und Komplexit√§t mit sich. _Prompt Engineering kann uns helfen, Best Practices und Workflows zu entwickeln, die Unterschiede abstrahieren und sich nahtlos an modellbezogene Anforderungen anpassen_.

Sehen wir uns dies in Aktion im OpenAI- oder Azure OpenAI-Playground an:

- Verwenden Sie denselben Prompt mit verschiedenen LLM-Bereitstellungen (z. B. OpenAI, Azure OpenAI, Hugging Face) ‚Äì haben Sie die Variationen bemerkt?
- Verwenden Sie denselben Prompt wiederholt mit derselben LLM-Bereitstellung (z. B. Azure OpenAI-Playground) ‚Äì wie unterschieden sich diese Variationen?

### Beispiel f√ºr Erfindungen

In diesem Kurs verwenden wir den Begriff **"Erfindung"**, um das Ph√§nomen zu beschreiben, bei dem LLMs manchmal faktisch falsche Informationen generieren, aufgrund von Einschr√§nkungen in ihrem Training oder anderen Beschr√§nkungen. Sie haben dies m√∂glicherweise auch als _"Halluzinationen"_ in popul√§ren Artikeln oder Forschungspapieren geh√∂rt. Wir empfehlen jedoch dringend, den Begriff _"Erfindung"_ zu verwenden, um das Verhalten nicht zu vermenschlichen, indem wir ihm eine menschliche Eigenschaft zuschreiben. Dies st√§rkt auch die [Richtlinien f√ºr verantwortungsvolle KI](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-105485-koreyst) aus terminologischer Sicht, indem Begriffe entfernt werden, die in einigen Kontexten als beleidigend oder nicht inklusiv angesehen werden k√∂nnten.

M√∂chten Sie ein Gef√ºhl daf√ºr bekommen, wie Erfindungen funktionieren? Denken Sie an einen Prompt, der die KI anweist, Inhalte zu einem nicht existierenden Thema zu generieren (um sicherzustellen, dass es nicht im Trainingsdatensatz enthalten ist). Zum Beispiel habe ich diesen Prompt ausprobiert:

> **Prompt:** Erstellen Sie einen Unterrichtsplan √ºber den Marskrieg von 2076.
Eine Websuche zeigte, dass es fiktive Darstellungen (z. B. Fernsehserien oder B√ºcher) √ºber Kriege auf dem Mars gab ‚Äì jedoch keine im Jahr 2076. Der gesunde Menschenverstand sagt uns au√üerdem, dass 2076 _in der Zukunft_ liegt und daher nicht mit einem realen Ereignis in Verbindung gebracht werden kann.

Was passiert also, wenn wir diesen Prompt mit verschiedenen LLM-Anbietern testen?

> **Antwort 1**: OpenAI Playground (GPT-35)

![Antwort 1](../../../translated_images/04-fabrication-oai.5818c4e0b2a2678c40e0793bf873ef4a425350dd0063a183fb8ae02cae63aa0c.de.png)

> **Antwort 2**: Azure OpenAI Playground (GPT-35)

![Antwort 2](../../../translated_images/04-fabrication-aoai.b14268e9ecf25caf613b7d424c16e2a0dc5b578f8f960c0c04d4fb3a68e6cf61.de.png)

> **Antwort 3**: Hugging Face Chat Playground (LLama-2)

![Antwort 3](../../../translated_images/04-fabrication-huggingchat.faf82a0a512789565e410568bce1ac911075b943dec59b1ef4080b61723b5bf4.de.png)

Wie erwartet, erzeugt jedes Modell (oder jede Modellversion) leicht unterschiedliche Antworten, was auf stochastisches Verhalten und Unterschiede in den Modellf√§higkeiten zur√ºckzuf√ºhren ist. Beispielsweise richtet sich ein Modell an ein Publikum der 8. Klasse, w√§hrend ein anderes von Sch√ºlern der Oberstufe ausgeht. Aber alle drei Modelle generierten Antworten, die einen uninformierten Nutzer davon √ºberzeugen k√∂nnten, dass das Ereignis real ist.

Techniken des Prompt-Engineerings wie _Metaprompting_ und _Temperaturkonfiguration_ k√∂nnen Modellf√§lschungen bis zu einem gewissen Grad reduzieren. Neue Architekturen des Prompt-Engineerings integrieren auch nahtlos neue Werkzeuge und Techniken in den Prompt-Fluss, um einige dieser Effekte zu mindern oder zu reduzieren.

## Fallstudie: GitHub Copilot

Lassen Sie uns diesen Abschnitt abschlie√üen, indem wir uns ansehen, wie Prompt-Engineering in realen L√∂sungen eingesetzt wird, und zwar anhand einer Fallstudie: [GitHub Copilot](https://github.com/features/copilot?WT.mc_id=academic-105485-koreyst).

GitHub Copilot ist Ihr "KI-Paarprogrammierer" ‚Äì er wandelt Text-Prompts in Code-Vervollst√§ndigungen um und ist in Ihre Entwicklungsumgebung (z. B. Visual Studio Code) integriert, um ein nahtloses Benutzererlebnis zu bieten. Wie in der unten stehenden Blog-Serie dokumentiert, basierte die fr√ºheste Version auf dem OpenAI Codex-Modell ‚Äì wobei die Ingenieure schnell erkannten, dass das Modell feinabgestimmt und bessere Prompt-Engineering-Techniken entwickelt werden mussten, um die Codequalit√§t zu verbessern. Im Juli [stellten sie ein verbessertes KI-Modell vor, das √ºber Codex hinausgeht](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/?WT.mc_id=academic-105485-koreyst) und noch schnellere Vorschl√§ge liefert.

Lesen Sie die Beitr√§ge in der Reihenfolge, um ihre Lernreise zu verfolgen.

- **Mai 2023** | [GitHub Copilot wird besser darin, Ihren Code zu verstehen](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/?WT.mc_id=academic-105485-koreyst)
- **Mai 2023** | [Inside GitHub: Zusammenarbeit mit den LLMs hinter GitHub Copilot](https://github.blog/2023-05-17-inside-github-working-with-the-llms-behind-github-copilot/?WT.mc_id=academic-105485-koreyst).
- **Juni 2023** | [Wie man bessere Prompts f√ºr GitHub Copilot schreibt](https://github.blog/2023-06-20-how-to-write-better-prompts-for-github-copilot/?WT.mc_id=academic-105485-koreyst).
- **Juli 2023** | [.. GitHub Copilot geht √ºber Codex hinaus mit verbessertem KI-Modell](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/?WT.mc_id=academic-105485-koreyst)
- **Juli 2023** | [Ein Leitfaden f√ºr Entwickler zu Prompt-Engineering und LLMs](https://github.blog/2023-07-17-prompt-engineering-guide-generative-ai-llms/?WT.mc_id=academic-105485-koreyst)
- **September 2023** | [Wie man eine Unternehmens-LLM-App erstellt: Lektionen von GitHub Copilot](https://github.blog/2023-09-06-how-to-build-an-enterprise-llm-application-lessons-from-github-copilot/?WT.mc_id=academic-105485-koreyst)

Sie k√∂nnen auch ihren [Engineering-Blog](https://github.blog/category/engineering/?WT.mc_id=academic-105485-koreyst) f√ºr weitere Beitr√§ge wie [diesen hier](https://github.blog/2023-09-27-how-i-used-github-copilot-chat-to-build-a-reactjs-gallery-prototype/?WT.mc_id=academic-105485-koreyst) durchsuchen, der zeigt, wie diese Modelle und Techniken _angewendet_ werden, um reale Anwendungen voranzutreiben.

---

## Prompt-Konstruktion

Wir haben gesehen, warum Prompt-Engineering wichtig ist ‚Äì jetzt wollen wir verstehen, wie Prompts _konstruiert_ werden, damit wir verschiedene Techniken f√ºr ein effektiveres Prompt-Design bewerten k√∂nnen.

### Basis-Prompt

Beginnen wir mit dem Basis-Prompt: einer Texteingabe, die ohne weiteren Kontext an das Modell gesendet wird. Hier ist ein Beispiel ‚Äì wenn wir die ersten W√∂rter der US-amerikanischen Nationalhymne an die OpenAI [Completion API](https://platform.openai.com/docs/api-reference/completions?WT.mc_id=academic-105485-koreyst) senden, vervollst√§ndigt sie die Antwort sofort mit den n√§chsten Zeilen, was das grundlegende Vorhersageverhalten veranschaulicht.

| Prompt (Eingabe)     | Completion (Ausgabe)                                                                                                                        |
| :------------------- | :----------------------------------------------------------------------------------------------------------------------------------------- |
| Oh say can you see   | Es klingt, als w√ºrden Sie die ersten Zeilen von "The Star-Spangled Banner", der Nationalhymne der Vereinigten Staaten, zitieren. Der vollst√§ndige Text lautet ... |

### Komplexer Prompt

Nun f√ºgen wir Kontext und Anweisungen zu diesem Basis-Prompt hinzu. Die [Chat Completion API](https://learn.microsoft.com/azure/ai-services/openai/how-to/chatgpt?WT.mc_id=academic-105485-koreyst) erm√∂glicht es uns, einen komplexen Prompt als Sammlung von _Nachrichten_ zu erstellen mit:

- Eingabe-/Ausgabe-Paaren, die die _Benutzereingabe_ und die _Antwort des Assistenten_ widerspiegeln.
- Systemnachrichten, die den Kontext f√ºr das Verhalten oder die Pers√∂nlichkeit des Assistenten festlegen.

Die Anfrage hat nun die folgende Form, wobei die _Tokenisierung_ relevante Informationen aus Kontext und Konversation effektiv erfasst. Das √Ñndern des Systemkontexts kann genauso wirkungsvoll f√ºr die Qualit√§t der Vervollst√§ndigungen sein wie die bereitgestellten Benutzereingaben.

```python
response = openai.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who won the world series in 2020?"},
        {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
        {"role": "user", "content": "Where was it played?"}
    ]
)
```


### Anweisungs-Prompt

In den obigen Beispielen war der Benutzer-Prompt eine einfache Textanfrage, die als Informationsanfrage interpretiert werden kann. Mit _Anweisungs_-Prompts k√∂nnen wir diesen Text verwenden, um eine Aufgabe detaillierter zu spezifizieren und der KI eine bessere Orientierung zu geben. Hier ist ein Beispiel:

| Prompt (Eingabe)                                                                                                                                                                                                                         | Completion (Ausgabe)                                                                                                        | Anweisungstyp       |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------- | :------------------ |
| Schreibe eine Beschreibung des B√ºrgerkriegs                                                                                                                                                                                             | _gab einen einfachen Absatz zur√ºck_                                                                                         | Einfach             |
| Schreibe eine Beschreibung des B√ºrgerkriegs. Nenne wichtige Daten und Ereignisse und beschreibe deren Bedeutung                                                                                                                         | _gab einen Absatz zur√ºck, gefolgt von einer Liste wichtiger Ereignisdaten mit Beschreibungen_                               | Komplex             |
| Schreibe eine Beschreibung des B√ºrgerkriegs in einem Absatz. Nenne 3 Stichpunkte mit wichtigen Daten und deren Bedeutung. Nenne 3 weitere Stichpunkte mit wichtigen historischen Pers√∂nlichkeiten und deren Beitr√§gen. Gib die Ausgabe als JSON-Datei zur√ºck. | _gab umfangreichere Details in einer Textbox zur√ºck, formatiert als JSON, das kopiert und in eine Datei eingef√ºgt werden kann_ | Komplex. Formatiert |

## Prim√§rinhalt

In den obigen Beispielen war der Prompt immer noch ziemlich offen, sodass das LLM entscheiden konnte, welcher Teil seines vortrainierten Datensatzes relevant war. Mit dem Designmuster des _Prim√§rinhalts_ wird der Eingabetext in zwei Teile unterteilt:

- eine Anweisung (Aktion)
- relevanter Inhalt (der die Aktion beeinflusst)

Hier ist ein Beispiel, bei dem die Anweisung lautet: "Fasse dies in 2 S√§tzen zusammen".

| Prompt (Eingabe)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Completion (Ausgabe)                                                                                                                                                                                                                                                                             |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Jupiter ist der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem. Es ist ein Gasriese mit einer Masse, die ein Tausendstel der Sonne betr√§gt, aber zweieinhalb Mal so gro√ü ist wie die aller anderen Planeten im Sonnensystem zusammen. Jupiter ist eines der hellsten Objekte, die mit blo√üem Auge am Nachthimmel sichtbar sind, und ist seit der Antike bekannt. Er ist nach dem r√∂mischen Gott Jupiter benannt. Wenn man ihn von der Erde aus betrachtet, kann Jupiter so hell sein, dass sein reflektiertes Licht sichtbare Schatten werfen kann und im Durchschnitt das dritthellste nat√ºrliche Objekt am Nachthimmel nach dem Mond und der Venus ist. <br/> **Fasse dies in 2 kurzen S√§tzen zusammen** | Jupiter, der f√ºnfte Planet von der Sonne, ist der gr√∂√üte im Sonnensystem und bekannt daf√ºr, eines der hellsten Objekte am Nachthimmel zu sein. Benannt nach dem r√∂mischen Gott Jupiter, ist er ein Gasriese, dessen Masse zweieinhalb Mal so gro√ü ist wie die aller anderen Planeten im Sonnensystem zusammen. |

Das Segment des Prim√§rinhalts kann auf verschiedene Weise verwendet werden, um effektivere Anweisungen zu geben:

- **Beispiele** ‚Äì anstatt dem Modell explizit zu sagen, was es tun soll, geben Sie ihm Beispiele daf√ºr, was es tun soll, und lassen Sie es das Muster ableiten.
- **Hinweise** ‚Äì folgen Sie der Anweisung mit einem "Hinweis", der die Vervollst√§ndigung anregt und das Modell zu relevanteren Antworten f√ºhrt.
- **Vorlagen** ‚Äì dies sind wiederholbare "Rezepte" f√ºr Prompts mit Platzhaltern (Variablen), die mit Daten f√ºr spezifische Anwendungsf√§lle angepasst werden k√∂nnen.

Lassen Sie uns diese in Aktion sehen.

### Verwendung von Beispielen

Dies ist ein Ansatz, bei dem Sie den Prim√§rinhalt verwenden, um dem Modell einige Beispiele f√ºr die gew√ºnschte Ausgabe f√ºr eine gegebene Anweisung zu "f√ºttern" und es das Muster f√ºr die gew√ºnschte Ausgabe ableiten lassen. Je nach Anzahl der bereitgestellten Beispiele k√∂nnen wir Zero-Shot-Prompting, One-Shot-Prompting, Few-Shot-Prompting usw. haben.

Der Prompt besteht nun aus drei Komponenten:

- Einer Aufgabenbeschreibung
- Einigen Beispielen der gew√ºnschten Ausgabe
- Dem Beginn eines neuen Beispiels (das zu einer impliziten Aufgabenbeschreibung wird)

| Lerntyp       | Prompt (Eingabe)                                                                                                                                        | Completion (Ausgabe)         |
| :------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------- |
| Zero-Shot     | "Die Sonne scheint". √úbersetze ins Spanische                                                                                                            | "El Sol est√° brillando".    |
| One-Shot      | "Die Sonne scheint" => ""El Sol est√° brillando". <br> "Es ist ein kalter und windiger Tag" =>                                                           | "Es un d√≠a fr√≠o y ventoso". |
| Few-Shot      | Der Spieler lief die Bases ab => Baseball <br/> Der Spieler schlug ein Ass => Tennis <br/> Der Spieler schlug eine Sechs => Cricket <br/> Der Spieler machte einen Slam-Dunk => | Basketball                  |
|               |                                                                                                                                                       |                             |

Beachten Sie, wie wir im Zero-Shot-Prompting eine explizite Anweisung ("√úbersetze ins Spanische") geben mussten, die im One-Shot-Prompting abgeleitet wird. Das Few-Shot-Beispiel zeigt, wie das Hinzuf√ºgen weiterer Beispiele es den Modellen erm√∂glicht, genauere Ableitungen ohne zus√§tzliche Anweisungen zu treffen.

### Prompt-Hinweise

Eine weitere Technik zur Verwendung von Prim√§rinhalt ist das Bereitstellen von _Hinweisen_ anstelle von Beispielen. In diesem Fall geben wir dem Modell einen Schubs in die richtige Richtung, indem wir es mit einem Ausschnitt starten, der das gew√ºnschte Antwortformat widerspiegelt. Das Modell "nimmt den Hinweis" und setzt in diesem Stil fort.

| Anzahl der Hinweise | Prompt (Eingabe)                                                                                                                                                                                                                                                                                                                                                                                                                                               | Completion (Ausgabe)                                                                                                                                                                                                                                                                                       |
| :------------------ | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0                   | Jupiter ist der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem. Es ist ein Gasriese mit einer Masse, die ein Tausendstel der Sonne betr√§gt, aber zweieinhalb Mal so gro√ü ist wie die aller anderen Planeten im Sonnensystem zusammen. Jupiter ist eines der hellsten Objekte, die mit blo√üem Auge am Nachthimmel sichtbar sind, und ist seit der Antike bekannt. <br/>**Fasse dies zusammen**                                       | Jupiter ist der gr√∂√üte Planet in unserem Sonnensystem und der f√ºnfte von der Sonne. Es ist ein Gasriese mit einer Masse, die 1/1000 der Sonne betr√§gt, aber schwerer ist als alle anderen Planeten zusammen. Antike Zivilisationen kennen Jupiter seit langem, und er ist leicht am Nachthimmel sichtbar. |
| 1              | Jupiter ist der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem. Es ist ein Gasriese mit einer Masse, die nur ein Tausendstel der Sonne betr√§gt, aber zweieinhalb Mal so gro√ü ist wie die aller anderen Planeten im Sonnensystem zusammen. Jupiter ist eines der hellsten Objekte, die mit blo√üem Auge am Nachthimmel sichtbar sind, und war bereits vor der aufgezeichneten Geschichte den antiken Zivilisationen bekannt. <br/>**Zusammenfassung** <br/> Was wir gelernt haben, ist, dass Jupiter | der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem ist. Es ist ein Gasriese mit einer Masse, die nur ein Tausendstel der Sonne betr√§gt, aber zweieinhalb Mal so gro√ü ist wie die aller anderen Planeten zusammen. Es ist leicht mit blo√üem Auge sichtbar und seit der Antike bekannt.                        |
| 2              | Jupiter ist der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem. Es ist ein Gasriese mit einer Masse, die nur ein Tausendstel der Sonne betr√§gt, aber zweieinhalb Mal so gro√ü ist wie die aller anderen Planeten im Sonnensystem zusammen. Jupiter ist eines der hellsten Objekte, die mit blo√üem Auge am Nachthimmel sichtbar sind, und war bereits vor der aufgezeichneten Geschichte den antiken Zivilisationen bekannt. <br/>**Zusammenfassung** <br/> Die Top 3 Fakten, die wir gelernt haben:         | 1. Jupiter ist der f√ºnfte Planet von der Sonne und der gr√∂√üte im Sonnensystem. <br/> 2. Es ist ein Gasriese mit einer Masse, die nur ein Tausendstel der Sonne betr√§gt...<br/> 3. Jupiter ist seit der Antike mit blo√üem Auge sichtbar ...                                                                       |
|                |                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                           |

### Prompt-Vorlagen

Eine Prompt-Vorlage ist ein _vordefiniertes Rezept f√ºr einen Prompt_, das gespeichert und bei Bedarf wiederverwendet werden kann, um konsistentere Benutzererfahrungen in gro√üem Ma√üstab zu erm√∂glichen. In seiner einfachsten Form ist es einfach eine Sammlung von Prompt-Beispielen wie [dieses von OpenAI](https://platform.openai.com/examples?WT.mc_id=academic-105485-koreyst), das sowohl die interaktiven Prompt-Komponenten (Benutzer- und Systemnachrichten) als auch das API-gesteuerte Anfrageformat bereitstellt ‚Äì zur Unterst√ºtzung der Wiederverwendung.

In seiner komplexeren Form wie [dieses Beispiel von LangChain](https://python.langchain.com/docs/concepts/prompt_templates/?WT.mc_id=academic-105485-koreyst) enth√§lt es _Platzhalter_, die durch Daten aus verschiedenen Quellen (Benutzereingaben, Systemkontext, externe Datenquellen usw.) ersetzt werden k√∂nnen, um einen Prompt dynamisch zu generieren. Dies erm√∂glicht es uns, eine Bibliothek wiederverwendbarer Prompts zu erstellen, die verwendet werden k√∂nnen, um konsistente Benutzererfahrungen **programmatisch** in gro√üem Ma√üstab zu erm√∂glichen.

Der eigentliche Wert von Vorlagen liegt schlie√ülich in der F√§higkeit, _Prompt-Bibliotheken_ f√ºr vertikale Anwendungsbereiche zu erstellen und zu ver√∂ffentlichen ‚Äì wobei die Prompt-Vorlage jetzt _optimiert_ ist, um anwendungsspezifischen Kontext oder Beispiele widerzuspiegeln, die die Antworten f√ºr die Zielgruppe relevanter und genauer machen. Das [Prompts For Edu](https://github.com/microsoft/prompts-for-edu?WT.mc_id=academic-105485-koreyst)-Repository ist ein gro√üartiges Beispiel f√ºr diesen Ansatz, da es eine Bibliothek von Prompts f√ºr den Bildungsbereich kuratiert, mit Schwerpunkt auf Schl√ºsselzielen wie Unterrichtsplanung, Lehrplangestaltung, Sch√ºlerbetreuung usw.

## Unterst√ºtzende Inhalte

Wenn wir die Konstruktion von Prompts als eine Aufgabe mit einer Anweisung (Aufgabe) und einem Ziel (prim√§rer Inhalt) betrachten, dann ist _sekund√§rer Inhalt_ wie zus√§tzlicher Kontext, den wir bereitstellen, um **die Ausgabe in irgendeiner Weise zu beeinflussen**. Es k√∂nnten Abstimmungsparameter, Formatierungsanweisungen, Themenklassifikationen usw. sein, die dem Modell helfen k√∂nnen, seine Antwort an die gew√ºnschten Benutzerziele oder Erwartungen anzupassen.

Beispiel: Angenommen, wir haben einen Kurskatalog mit umfangreichen Metadaten (Name, Beschreibung, Niveau, Metadaten-Tags, Dozent usw.) zu allen verf√ºgbaren Kursen im Lehrplan:

- Wir k√∂nnen eine Anweisung definieren, um "den Kurskatalog f√ºr Herbst 2023 zusammenzufassen".
- Wir k√∂nnen den prim√§ren Inhalt verwenden, um einige Beispiele f√ºr die gew√ºnschte Ausgabe bereitzustellen.
- Wir k√∂nnen den sekund√§ren Inhalt verwenden, um die Top 5 "Tags" von Interesse zu identifizieren.

Nun kann das Modell eine Zusammenfassung im Format der gezeigten Beispiele bereitstellen ‚Äì aber wenn ein Ergebnis mehrere Tags hat, kann es die 5 im sekund√§ren Inhalt identifizierten Tags priorisieren.

---

<!--
LEKTIONSVORLAGE:
Diese Einheit sollte das Kernkonzept #1 abdecken.
Verst√§rken Sie das Konzept mit Beispielen und Referenzen.

KONZEPT #3:
Techniken des Prompt-Engineerings.
Was sind einige grundlegende Techniken f√ºr das Prompt-Engineering?
Veranschaulichen Sie es mit einigen √úbungen.
-->

## Best Practices f√ºr Prompts

Jetzt, da wir wissen, wie Prompts _konstruiert_ werden k√∂nnen, k√∂nnen wir dar√ºber nachdenken, wie man sie _gestaltet_, um Best Practices widerzuspiegeln. Wir k√∂nnen dies in zwei Teile unterteilen ‚Äì die richtige _Einstellung_ haben und die richtigen _Techniken_ anwenden.

### Einstellung f√ºr Prompt-Engineering

Prompt-Engineering ist ein Trial-and-Error-Prozess, daher sollten Sie drei allgemeine Leitfaktoren im Auge behalten:

1. **Dom√§nenverst√§ndnis ist entscheidend.** Die Genauigkeit und Relevanz der Antwort h√§ngt von der _Dom√§ne_ ab, in der die Anwendung oder der Benutzer operiert. Nutzen Sie Ihre Intuition und Fachkenntnisse, um **Techniken weiter anzupassen**. Definieren Sie beispielsweise _dom√§nenspezifische Pers√∂nlichkeiten_ in Ihren System-Prompts oder verwenden Sie _dom√§nenspezifische Vorlagen_ in Ihren Benutzer-Prompts. Stellen Sie sekund√§re Inhalte bereit, die dom√§nenspezifische Kontexte widerspiegeln, oder verwenden Sie _dom√§nenspezifische Hinweise und Beispiele_, um das Modell zu vertrauten Nutzungsmustern zu f√ºhren.

2. **Modellverst√§ndnis ist entscheidend.** Wir wissen, dass Modelle von Natur aus stochastisch sind. Aber Modellimplementierungen k√∂nnen auch in Bezug auf den Trainingsdatensatz, den sie verwenden (vortrainiertes Wissen), die F√§higkeiten, die sie bieten (z. B. √ºber API oder SDK), und die Art des Inhalts, f√ºr den sie optimiert sind (z. B. Code vs. Bilder vs. Text), variieren. Verstehen Sie die St√§rken und Schw√§chen des Modells, das Sie verwenden, und nutzen Sie dieses Wissen, um _Aufgaben zu priorisieren_ oder _angepasste Vorlagen_ zu erstellen, die f√ºr die F√§higkeiten des Modells optimiert sind.

3. **Iteration und Validierung sind entscheidend.** Modelle entwickeln sich schnell weiter, ebenso wie die Techniken f√ºr das Prompt-Engineering. Als Dom√§nenexperte haben Sie m√∂glicherweise anderen Kontext oder Kriterien f√ºr _Ihre_ spezifische Anwendung, die m√∂glicherweise nicht auf die breitere Community zutreffen. Verwenden Sie Tools und Techniken des Prompt-Engineerings, um den Prompt-Aufbau zu "beschleunigen", und iterieren und validieren Sie die Ergebnisse dann mit Ihrer eigenen Intuition und Fachkenntnis. Dokumentieren Sie Ihre Erkenntnisse und erstellen Sie eine **Wissensdatenbank** (z. B. Prompt-Bibliotheken), die von anderen als neue Basis f√ºr schnellere Iterationen in der Zukunft verwendet werden kann.

## Best Practices

Schauen wir uns nun g√§ngige Best Practices an, die von [OpenAI](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api?WT.mc_id=academic-105485-koreyst) und [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering#best-practices?WT.mc_id=academic-105485-koreyst)-Praktikern empfohlen werden.

| Was                               | Warum                                                                                                                                                                                                                                               |
| :-------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Evaluieren Sie die neuesten Modelle.       | Neue Modellgenerationen haben wahrscheinlich verbesserte Funktionen und Qualit√§t ‚Äì k√∂nnen jedoch auch h√∂here Kosten verursachen. Bewerten Sie sie hinsichtlich ihrer Auswirkungen und treffen Sie dann Migrationsentscheidungen.                                                                                |
| Trennen Sie Anweisungen und Kontext   | √úberpr√ºfen Sie, ob Ihr Modell/Anbieter _Trennzeichen_ definiert, um Anweisungen, prim√§re und sekund√§re Inhalte klarer zu unterscheiden. Dies kann Modellen helfen, Token genauer zu gewichten.                                                         |
| Seien Sie spezifisch und klar             | Geben Sie mehr Details √ºber den gew√ºnschten Kontext, das Ergebnis, die L√§nge, das Format, den Stil usw. an. Dies verbessert sowohl die Qualit√§t als auch die Konsistenz der Antworten. Erfassen Sie Rezepte in wiederverwendbaren Vorlagen.                                                          |
| Seien Sie beschreibend, verwenden Sie Beispiele      | Modelle reagieren m√∂glicherweise besser auf einen "zeigen und erz√§hlen"-Ansatz. Beginnen Sie mit einem `Zero-Shot`-Ansatz, bei dem Sie ihm eine Anweisung geben (aber keine Beispiele), und versuchen Sie dann `Few-Shot` als Verfeinerung, indem Sie einige Beispiele f√ºr die gew√ºnschte Ausgabe bereitstellen. Verwenden Sie Analogien. |
| Verwenden Sie Hinweise, um Abschl√ºsse zu starten | Sto√üen Sie es in Richtung eines gew√ºnschten Ergebnisses, indem Sie ihm einige f√ºhrende W√∂rter oder Phrasen geben, die es als Ausgangspunkt f√ºr die Antwort verwenden kann.                                                                                                               |
| Wiederholen Sie sich                       | Manchmal m√ºssen Sie sich dem Modell gegen√ºber wiederholen. Geben Sie Anweisungen vor und nach Ihrem prim√§ren Inhalt, verwenden Sie eine Anweisung und einen Hinweis usw. Iterieren und validieren Sie, um zu sehen, was funktioniert.                                                         |
| Reihenfolge ist wichtig                     | Die Reihenfolge, in der Sie Informationen dem Modell pr√§sentieren, kann die Ausgabe beeinflussen, selbst in den Lernbeispielen, dank des Recency-Bias. Probieren Sie verschiedene Optionen aus, um zu sehen, was am besten funktioniert.                                                               |
| Geben Sie dem Modell eine ‚ÄûAusweichm√∂glichkeit‚Äú           | Geben Sie dem Modell eine _Fallback_-Antwort, die es bereitstellen kann, wenn es die Aufgabe aus irgendeinem Grund nicht abschlie√üen kann. Dies kann die Wahrscheinlichkeit verringern, dass Modelle falsche oder erfundene Antworten generieren.                                                         |
|                                   |                                                                                                                                                                                                                                                   |

Wie bei jeder Best Practice gilt: _Ihre Ergebnisse k√∂nnen variieren_, je nach Modell, Aufgabe und Dom√§ne. Verwenden Sie diese als Ausgangspunkt und iterieren Sie, um herauszufinden, was f√ºr Sie am besten funktioniert. Bewerten Sie Ihren Prompt-Engineering-Prozess st√§ndig neu, wenn neue Modelle und Tools verf√ºgbar werden, mit Fokus auf Prozessskalierbarkeit und Antwortqualit√§t.

<!--
LEKTIONSVORLAGE:
Diese Einheit sollte eine Code-Herausforderung bereitstellen, falls zutreffend.

HERAUSFORDERUNG:
Link zu einem Jupyter Notebook mit nur den Code-Kommentaren in den Anweisungen (Code-Abschnitte sind leer).

L√ñSUNG:
Link zu einer Kopie dieses Notebooks mit den ausgef√ºllten und ausgef√ºhrten Prompts, die ein Beispiel zeigen.
-->

## Aufgabe

Herzlichen Gl√ºckwunsch! Sie haben das Ende der Lektion erreicht! Es ist Zeit, einige dieser Konzepte und Techniken mit echten Beispielen zu testen!

F√ºr unsere Aufgabe verwenden wir ein Jupyter Notebook mit √úbungen, die Sie interaktiv durchf√ºhren k√∂nnen. Sie k√∂nnen das Notebook auch mit Ihren eigenen Markdown- und Code-Zellen erweitern, um Ideen und Techniken selbst zu erkunden.

### Um loszulegen, forken Sie das Repository, dann

- (Empfohlen) Starten Sie GitHub Codespaces
- (Alternativ) Klonen Sie das Repository auf Ihr lokales Ger√§t und verwenden Sie es mit Docker Desktop
- (Alternativ) √ñffnen Sie das Notebook mit Ihrer bevorzugten Notebook-Laufzeitumgebung.

### Als N√§chstes konfigurieren Sie Ihre Umgebungsvariablen

- Kopieren Sie die Datei `.env.copy` im Repository-Stamm nach `.env` und f√ºllen Sie die Werte f√ºr `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` und `AZURE_OPENAI_DEPLOYMENT` aus. Kehren Sie zum Abschnitt [Learning Sandbox](../../../04-prompt-engineering-fundamentals/04-prompt-engineering-fundamentals) zur√ºck, um zu erfahren, wie.

### Als N√§chstes √∂ffnen Sie das Jupyter Notebook

- W√§hlen Sie den Laufzeit-Kernel aus. Wenn Sie Option 1 oder 2 verwenden, w√§hlen Sie einfach den Standard-Python 3.10.x-Kernel aus, der vom Entwicklungscontainer bereitgestellt wird.

Sie sind bereit, die √úbungen auszuf√ºhren. Beachten Sie, dass es hier keine _richtigen und falschen_ Antworten gibt ‚Äì es geht nur darum, Optionen durch Trial-and-Error zu erkunden und ein Gef√ºhl daf√ºr zu entwickeln, was f√ºr ein bestimmtes Modell und Anwendungsbereich funktioniert.

_Aus diesem Grund gibt es in dieser Lektion keine Code-L√∂sungssegmente. Stattdessen enth√§lt das Notebook Markdown-Zellen mit dem Titel "Meine L√∂sung:", die ein Beispielergebnis zur Orientierung zeigen._

 <!--
LEKTIONSVORLAGE:
Schlie√üen Sie den Abschnitt mit einer Zusammenfassung und Ressourcen f√ºr selbstgesteuertes Lernen ab.
-->

## Wissens√ºberpr√ºfung

Welcher der folgenden Prompts ist ein guter Prompt, der einigen vern√ºnftigen Best Practices folgt?

1. Zeige mir ein Bild von einem roten Auto
2. Zeige mir ein Bild von einem roten Auto der Marke Volvo und Modell XC90, geparkt an einer Klippe mit Sonnenuntergang
3. Zeige mir ein Bild von einem roten Auto der Marke Volvo und Modell XC90

A: 2, es ist der beste Prompt, da er Details dar√ºber enth√§lt, "was" gezeigt werden soll, und ins Detail geht (nicht irgendein Auto, sondern eine bestimmte Marke und ein bestimmtes Modell). Au√üerdem beschreibt er die gesamte Umgebung. 3 ist der n√§chstbeste, da er ebenfalls viele Beschreibungen enth√§lt.

## üöÄ Herausforderung

Versuchen Sie, die Technik des "Hinweises" mit dem Prompt zu nutzen: Vervollst√§ndigen Sie den Satz "Zeige mir ein Bild von einem roten Auto der Marke Volvo und ". Was antwortet es, und wie w√ºrden Sie es verbessern?

## Gro√üartige Arbeit! Setzen Sie Ihr Lernen fort

M√∂chten Sie mehr √ºber verschiedene Konzepte des Prompt-Engineerings erfahren? Besuchen Sie die [Seite f√ºr weiterf√ºhrendes Lernen](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), um weitere gro√üartige Ressourcen zu diesem Thema zu finden.

Gehen Sie zu Lektion 5, wo wir uns [fortgeschrittene Prompt-Techniken](../05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst) ansehen!

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.