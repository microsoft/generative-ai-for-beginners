<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a45c318dc6ebc2604f35b8b829f93af2",
  "translation_date": "2025-05-19T15:42:48+00:00",
  "source_file": "04-prompt-engineering-fundamentals/README.md",
  "language_code": "da"
}
-->
# Grundl√¶ggende om Prompt Engineering

## Introduktion
Dette modul d√¶kker vigtige begreber og teknikker til at skabe effektive prompts i generative AI-modeller. M√•den, du skriver din prompt til en LLM, betyder ogs√• noget. En omhyggeligt udformet prompt kan opn√• en bedre kvalitet af respons. Men hvad betyder begreber som _prompt_ og _prompt engineering_ egentlig? Og hvordan forbedrer jeg prompt _input_, som jeg sender til LLM? Disse er de sp√∏rgsm√•l, vi vil fors√∏ge at besvare i dette kapitel og det n√¶ste.

_Generativ AI_ er i stand til at skabe nyt indhold (f.eks. tekst, billeder, lyd, kode osv.) som svar p√• brugerforesp√∏rgsler. Den opn√•r dette ved hj√¶lp af _Large Language Models_ som OpenAI's GPT ("Generative Pre-trained Transformer") serie, der er tr√¶net til at bruge naturligt sprog og kode.

Brugere kan nu interagere med disse modeller ved hj√¶lp af velkendte paradigmer som chat, uden at have brug for teknisk ekspertise eller tr√¶ning. Modellerne er _prompt-baserede_ - brugere sender en tekstinput (prompt) og f√•r AI-svar tilbage (completion). De kan derefter "chatte med AI'en" iterativt i samtaler med flere omgange og finjustere deres prompt, indtil responsen matcher deres forventninger.

"Prompts" bliver nu det prim√¶re _programmeringsinterface_ for generative AI-apps, der fort√¶ller modellerne, hvad de skal g√∏re, og p√•virker kvaliteten af de returnerede svar. "Prompt Engineering" er et hurtigt voksende studieomr√•de, der fokuserer p√• _design og optimering_ af prompts for at levere konsistente og kvalitetsrige svar i stor skala.

## L√¶ringsm√•l

I denne lektion l√¶rer vi, hvad Prompt Engineering er, hvorfor det er vigtigt, og hvordan vi kan udforme mere effektive prompts for en given model og applikationsm√•l. Vi vil forst√• kernekoncepter og bedste praksis for prompt engineering - og l√¶re om et interaktivt Jupyter Notebooks "sandbox"-milj√∏, hvor vi kan se disse koncepter anvendt p√• virkelige eksempler.

Ved slutningen af denne lektion vil vi v√¶re i stand til at:

1. Forklare, hvad prompt engineering er, og hvorfor det er vigtigt.
2. Beskrive komponenterne i en prompt og hvordan de bruges.
3. L√¶re bedste praksis og teknikker for prompt engineering.
4. Anvende l√¶rte teknikker p√• virkelige eksempler ved hj√¶lp af en OpenAI-endpoint.

## N√∏glebegreber

Prompt Engineering: Praktikken med at designe og finjustere inputs for at guide AI-modeller mod at producere √∏nskede outputs.
Tokenization: Processen med at konvertere tekst til mindre enheder, kaldet tokens, som en model kan forst√• og behandle.
Instruction-Tuned LLMs: Store sprogmodeller (LLMs), der er blevet finjusteret med specifikke instruktioner for at forbedre deres responsn√∏jagtighed og relevans.

## L√¶ringssandbox

Prompt engineering er i √∏jeblikket mere kunst end videnskab. Den bedste m√•de at forbedre vores intuition for det er at _√∏ve mere_ og adoptere en trial-and-error tilgang, der kombinerer applikationsdom√¶neekspertise med anbefalede teknikker og model-specifikke optimeringer.

Den Jupyter Notebook, der ledsager denne lektion, giver et _sandbox_-milj√∏, hvor du kan pr√∏ve det, du l√¶rer - som du g√•r eller som en del af kodeudfordringen i slutningen. For at udf√∏re √∏velserne skal du bruge:

1. **En Azure OpenAI API-n√∏gle** - serviceendpointet for en implementeret LLM.
2. **En Python Runtime** - hvor Notebooks kan udf√∏res.
3. **Lokale milj√∏variabler** - _fuldf√∏r [SETUP](./../00-course-setup/SETUP.md?WT.mc_id=academic-105485-koreyst) trin nu for at blive klar_.

Notebogen kommer med _start_-√∏velser - men du opfordres til at tilf√∏je dine egne _Markdown_ (beskrivelse) og _Code_ (prompt foresp√∏rgsler) sektioner for at pr√∏ve flere eksempler eller ideer - og bygge din intuition for prompt design.

## Illustreret guide

Vil du have det store billede af, hvad denne lektion d√¶kker, f√∏r du dykker ind? Se denne illustrerede guide, der giver dig en fornemmelse af de vigtigste emner, der d√¶kkes, og de vigtigste takeaways for dig at t√¶nke over i hver enkelt. Lektionens roadmap f√∏rer dig fra at forst√• kernekoncepterne og udfordringerne til at adressere dem med relevante prompt engineering teknikker og bedste praksis. Bem√¶rk, at sektionen "Avancerede teknikker" i denne guide henviser til indhold, der d√¶kkes i det _n√¶ste_ kapitel af dette pensum.

## Vores Startup

Lad os nu tale om, hvordan _dette emne_ relaterer til vores startup-mission om at [bringe AI-innovation til uddannelse](https://educationblog.microsoft.com/2023/06/collaborating-to-bring-ai-innovation-to-education?WT.mc_id=academic-105485-koreyst). Vi √∏nsker at bygge AI-drevne applikationer til _personlig l√¶ring_ - s√• lad os t√¶nke over, hvordan forskellige brugere af vores applikation kan "designe" prompts:

- **Administratorer** kan bede AI om at _analysere pensumdata for at identificere huller i d√¶kningen_. AI kan opsummere resultater eller visualisere dem med kode.
- **Undervisere** kan bede AI om at _generere en lektionsplan for en m√•lgruppe og emne_. AI kan bygge den personlige plan i et specificeret format.
- **Studerende** kan bede AI om at _tutore dem i et vanskeligt emne_. AI kan nu guide studerende med lektioner, hints og eksempler tilpasset deres niveau.

Det er kun toppen af isbjerget. Se [Prompts For Education](https://github.com/microsoft/prompts-for-edu/tree/main?WT.mc_id=academic-105485-koreyst) - et open-source prompts bibliotek kurateret af uddannelseseksperter - for at f√• en bredere fornemmelse af mulighederne! _Pr√∏v at k√∏re nogle af disse prompts i sandboxen eller ved hj√¶lp af OpenAI Playground for at se, hvad der sker!_

## Hvad er Prompt Engineering?

Vi startede denne lektion med at definere **Prompt Engineering** som processen med _design og optimering_ af tekstinputs (prompts) for at levere konsistente og kvalitetsrige svar (completions) for en given applikationsm√•l og model. Vi kan t√¶nke p√• dette som en 2-trins proces:

- _designe_ den indledende prompt for en given model og m√•l
- _finjustere_ prompten iterativt for at forbedre kvaliteten af responsen

Dette er n√∏dvendigvis en trial-and-error proces, der kr√¶ver brugerintuition og indsats for at opn√• optimale resultater. S√• hvorfor er det vigtigt? For at besvare det sp√∏rgsm√•l skal vi f√∏rst forst√• tre begreber:

- _Tokenization_ = hvordan modellen "ser" prompten
- _Base LLMs_ = hvordan grundmodellen "behandler" en prompt
- _Instruction-Tuned LLMs_ = hvordan modellen nu kan se "opgaver"

### Tokenization

En LLM ser prompts som en _sekvens af tokens_, hvor forskellige modeller (eller versioner af en model) kan tokenisere den samme prompt p√• forskellige m√•der. Da LLM'er er tr√¶net p√• tokens (og ikke p√• r√• tekst), har m√•den, hvorp√• prompts bliver tokeniseret, en direkte indflydelse p√• kvaliteten af det genererede svar.

For at f√• en intuition for, hvordan tokenization fungerer, pr√∏v v√¶rkt√∏jer som [OpenAI Tokenizer](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst) vist nedenfor. Kopier din prompt ind - og se, hvordan den bliver konverteret til tokens, og l√¶g m√¶rke til, hvordan mellemrumstegn og tegns√¶tning h√•ndteres. Bem√¶rk, at dette eksempel viser en √¶ldre LLM (GPT-3) - s√• at pr√∏ve dette med en nyere model kan give et andet resultat.

### Koncept: Foundation Models

N√•r en prompt er tokeniseret, er den prim√¶re funktion af ["Base LLM"](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) (eller Foundation model) at forudsige tokenet i den sekvens. Da LLM'er er tr√¶net p√• massive tekstdatas√¶t, har de en god fornemmelse af de statistiske relationer mellem tokens og kan foretage den forudsigelse med en vis sikkerhed. Bem√¶rk, at de ikke forst√•r _betydningen_ af ordene i prompten eller tokenet; de ser blot et m√∏nster, de kan "fuldf√∏re" med deres n√¶ste forudsigelse. De kan forts√¶tte med at forudsige sekvensen, indtil den afsluttes af brugerintervention eller en forudbestemt betingelse.

Vil du se, hvordan prompt-baseret completion fungerer? Indtast ovenst√•ende prompt i Azure OpenAI Studio [_Chat Playground_](https://oai.azure.com/playground?WT.mc_id=academic-105485-koreyst) med standardindstillingerne. Systemet er konfigureret til at behandle prompts som anmodninger om information - s√• du b√∏r se en completion, der tilfredsstiller denne kontekst.

Men hvad hvis brugeren √∏nskede at se noget specifikt, der opfyldte nogle kriterier eller opgavem√•l? Det er her, _instruction-tuned_ LLMs kommer ind i billedet.

### Koncept: Instruction Tuned LLMs

En [Instruction Tuned LLM](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) starter med grundmodellen og finjusterer den med eksempler eller input/output-par (f.eks. samtaler med flere omgange), der kan indeholde klare instruktioner - og responsen fra AI fors√∏ger at f√∏lge den instruktion.

Dette bruger teknikker som Reinforcement Learning med Human Feedback (RLHF), der kan tr√¶ne modellen til _at f√∏lge instruktioner_ og _l√¶re af feedback_, s√• den producerer svar, der er bedre egnet til praktiske applikationer og mere relevante for brugerens m√•l.

Lad os pr√∏ve det - genbes√∏g ovenst√•ende prompt, men √¶ndr nu _systemmeddelelsen_ for at give f√∏lgende instruktion som kontekst:

> _Opsummer indholdet, du f√•r, for en elev i anden klasse. Hold resultatet til et afsnit med 3-5 punktlister._

Se, hvordan resultatet nu er tilpasset til at afspejle det √∏nskede m√•l og format? En underviser kan nu direkte bruge dette svar i deres slides til den klasse.

## Hvorfor har vi brug for Prompt Engineering?

Nu hvor vi ved, hvordan prompts behandles af LLM'er, lad os tale om _hvorfor_ vi har brug for prompt engineering. Svaret ligger i det faktum, at nuv√¶rende LLM'er udg√∏r en r√¶kke udfordringer, der g√∏r _p√•lidelige og konsistente completions_ mere udfordrende at opn√• uden at l√¶gge indsats i promptkonstruktion og optimering. For eksempel:

1. **Modelresponser er stokastiske.** Den _samme prompt_ vil sandsynligvis producere forskellige svar med forskellige modeller eller modelversioner. Og det kan endda producere forskellige resultater med den _samme model_ p√• forskellige tidspunkter. _Prompt engineering teknikker kan hj√¶lpe os med at minimere disse variationer ved at give bedre rammer_.

2. **Modeller kan fabrikere svar.** Modeller er forudtr√¶net med _store men begr√¶nsede_ datas√¶t, hvilket betyder, at de mangler viden om koncepter uden for den tr√¶ningsomfang. Som et resultat kan de producere completions, der er un√∏jagtige, imagin√¶re eller direkte modstridende med kendte fakta. _Prompt engineering teknikker hj√¶lper brugere med at identificere og afb√∏de s√•danne fabrikationer, f.eks. ved at bede AI om citater eller r√¶sonnering_.

3. **Modelkapaciteter vil variere.** Nyere modeller eller modelgenerationer vil have rigere kapaciteter, men ogs√• bringe unikke quirks og kompromiser i omkostninger og kompleksitet. _Prompt engineering kan hj√¶lpe os med at udvikle bedste praksis og workflows, der abstraherer forskelle og tilpasser sig model-specifikke krav p√• skalerbare, problemfri m√•der_.

Lad os se dette i aktion i OpenAI eller Azure OpenAI Playground:

- Brug den samme prompt med forskellige LLM-implementeringer (f.eks., OpenAI, Azure OpenAI, Hugging Face) - s√• du variationerne?
- Brug den samme prompt gentagne gange med den _samme_ LLM-implementering (f.eks., Azure OpenAI Playground) - hvordan adskilte disse variationer sig?

### Eksempel p√• fabrikationer

I dette kursus bruger vi termen **"fabrikation"** til at referere til f√¶nomenet, hvor LLM'er nogle gange genererer faktuelt forkert information p√• grund af begr√¶nsninger i deres tr√¶ning eller andre begr√¶nsninger. Du har m√•ske ogs√• h√∏rt dette omtalt som _"hallucinationer"_ i popul√¶re artikler eller forskningspapirer. Vi anbefaler dog st√¶rkt at bruge _"fabrikation"_ som termen, s√• vi ikke utilsigtet antropomorfiserer adf√¶rden ved at tilskrive en menneskelignende egenskab til et maskinedrevet resultat. Dette styrker ogs√• [Ansvarlige AI-retningslinjer](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-105485-koreyst) fra et terminologisk perspektiv, ved at fjerne termer, der ogs√• kan betragtes som st√∏dende eller ikke-inkluderende i nogle sammenh√¶nge.

Vil du f√• en fornemmelse af, hvordan fabrikationer fungerer? T√¶nk p√• en prompt, der instruerer AI til at generere indhold for et ikke-eksisterende emne (for at sikre, at det ikke findes i tr√¶ningsdatas√¶ttet). For eksempel - jeg pr√∏vede denne prompt:

> **Prompt:** generer en lektionsplan om den Marskrig i 2076.

En webs√∏gnings viste mig, at der var fiktive beretninger (f.eks., tv-serier eller b√∏ger) om Marskrige - men ingen i 2076. Sund fornuft fort√¶ller os ogs√•, at 2076 er _i fremtiden_ og derfor ikke kan v√¶re forbundet med en virkelig begivenhed.

S√• hvad sker der, n√•r vi k√∏rer denne prompt med forskellige LLM-udbydere?

Som forventet producerer hver model (eller modelversion) lidt forskellige svar takket v√¶re stokastisk adf√¶rd og modelkapacitetsvariationer. For eksempel, en model m√•lretter en 8. klasse publikum, mens den anden antager en gymnasieelev. Men alle tre modeller genererede svar, der kunne overbevise en uinformeret bruger om, at begivenheden var reel.

Prompt engineering teknikker som _metaprompting_ og _temperature configuration_ kan reducere modelfabrikeringer til en vis grad. Nye prompt engineering _arkitekturer_ inkorporerer ogs√• nye v√¶rkt√∏jer og teknikker problemfrit i promptflowet for at afb√∏de eller reducere nogle af disse effekter.

## Case Study: GitHub Copilot

Lad os afslutte denne sektion ved at f√• en fornemmelse af, hvordan prompt engineering bruges i l√∏sninger i den virkelige verden ved at se p√• en Case Study: [GitHub Copilot](https://github.com/features/copilot?WT.mc_id=academic-105485-koreyst).

GitHub Copilot er din "AI Pair Programmer" - det konverterer tekstprompts til kode completions og er integreret i dit udviklingsmilj√∏ (f.eks., Visual Studio Code) for en problemfri brugeroplevelse. Som dokumenteret i serien af blogs nedenfor, var den tidligste version baseret p√• OpenAI Codex-modellen - med ingeni√∏rer, der hurtigt inds√• behovet for at finjustere modellen og udvikle bedre prompt engineering teknikker for at forbedre kodekvaliteten. I juli [debuterede de en forbedret AI-model, der g√•r ud over Codex](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/?WT.mc_id=academic-105485-koreyst) for endnu hurtigere forslag.

L√¶s indl√¶ggene i r√¶kkef√∏lge for at f√∏lge deres l√¶ringsrejse.

Du kan ogs√• gennemse deres [Engineering blog](https://github.blog/category/engineering/?WT.mc_id=academic-105485-koreyst) for flere indl√¶g som [dette](https://github.blog/2023-09-27-how-i-used-github-copilot-chat-to-build-a-reactjs-gallery-prototype/?WT.mc_id=academic-105485-koreyst), der viser, hvordan disse modeller og teknikker er _anvendt_ til at drive virkelige applikationer.

## Promptkonstruktion

Vi har set, hvorfor prompt engineering er vigtigt - nu lad os forst√•, hvordan prompts er _konstrueret_, s√• vi kan evaluere forskellige teknikker til mere effektiv prompt design.

### Grundl√¶ggende Prompt

Lad os starte med den grundl√¶ggende prompt: en tekstinput sendt til modellen uden anden kontekst. Her er et eksempel - n√•r vi sender de f√∏rste ord af den amerikanske nationalsang til OpenAI [Completion API](https://platform.openai.com/docs/api-reference/completions?WT.mc_id=academic-105485-koreyst) fuldender det straks responsen med de n√¶ste linjer, hvilket illustrerer den grundl√¶ggende forudsigelsesadf√¶rd.

### Kompleks Prompt

Nu lad os tilf√∏je kontekst og instruktioner til den grund
Den virkelige v√¶rdi af skabeloner ligger i evnen til at skabe og udgive _promptbiblioteker_ for vertikale applikationsdom√¶ner - hvor promptskabelonen nu er _optimeret_ til at afspejle applikationsspecifik kontekst eller eksempler, der g√∏r svarene mere relevante og pr√¶cise for den m√•lrettede brugergruppe. Repositoriet [Prompts For Edu](https://github.com/microsoft/prompts-for-edu?WT.mc_id=academic-105485-koreyst) er et godt eksempel p√• denne tilgang, der samler et bibliotek af prompts til uddannelsesdom√¶net med fokus p√• n√∏glem√•l som lektionsplanl√¶gning, l√¶seplanl√¶gning, elevvejledning osv.

## Underst√∏ttende indhold

Hvis vi t√¶nker p√• promptkonstruktion som at have en instruktion (opgave) og et m√•l (prim√¶rt indhold), s√• er _sekund√¶rt indhold_ som ekstra kontekst, vi giver for at **p√•virke output p√• en eller anden m√•de**. Det kunne v√¶re indstillingsparametre, formateringsinstruktioner, emnetaksonomier osv., der kan hj√¶lpe modellen med at _tilpasse_ sit svar til at passe til de √∏nskede brugerform√•l eller forventninger.

For eksempel: Givet en kursuskatalog med omfattende metadata (navn, beskrivelse, niveau, metadatatags, instrukt√∏r osv.) p√• alle tilg√¶ngelige kurser i pensum:

- vi kan definere en instruktion til "at opsummere kursuskataloget for efter√•ret 2023"
- vi kan bruge det prim√¶re indhold til at give nogle eksempler p√• det √∏nskede output
- vi kan bruge det sekund√¶re indhold til at identificere de 5 vigtigste "tags" af interesse.

Nu kan modellen levere en opsummering i det format, der vises af de f√• eksempler - men hvis et resultat har flere tags, kan den prioritere de 5 tags identificeret i det sekund√¶re indhold.

---

<!--
LESSON TEMPLATE:
Denne enhed skal d√¶kke kernekoncept #1.
Styrk konceptet med eksempler og referencer.

CONCEPT #3:
Prompt Engineering Techniques.
Hvad er nogle grundl√¶ggende teknikker til prompt engineering?
Illustrer det med nogle √∏velser.
-->

## Bedste praksis for prompting

Nu hvor vi ved, hvordan prompts kan _konstrueres_, kan vi begynde at t√¶nke over, hvordan vi _designer_ dem til at afspejle bedste praksis. Vi kan t√¶nke p√• dette i to dele - at have den rette _tankegang_ og anvende de rigtige _teknikker_.

### Prompt Engineering Tankegang

Prompt Engineering er en trial-and-error proces, s√• husk tre brede vejledende faktorer:

1. **Dom√¶neforst√•else betyder noget.** Responsens n√∏jagtighed og relevans er en funktion af det _dom√¶ne_, hvor applikationen eller brugeren opererer. Brug din intuition og dom√¶neekspertise til at **tilpasse teknikker** yderligere. For eksempel, defin√©r _dom√¶nespecifikke personligheder_ i dine systemprompts, eller brug _dom√¶nespecifikke skabeloner_ i dine brugerprompts. Giv sekund√¶rt indhold, der afspejler dom√¶nespecifikke kontekster, eller brug _dom√¶nespecifikke signaler og eksempler_ til at guide modellen mod velkendte brugsm√∏nstre.

2. **Model forst√•else betyder noget.** Vi ved, at modeller er stokastiske af natur. Men modelimplementeringer kan ogs√• variere med hensyn til det tr√¶ningsdatas√¶t, de bruger (forudindl√¶rt viden), de kapabiliteter, de tilbyder (f.eks. via API eller SDK) og den type indhold, de er optimeret til (f.eks. kode vs. billeder vs. tekst). Forst√• styrkerne og begr√¶nsningerne af den model, du bruger, og brug den viden til at _prioritere opgaver_ eller bygge _tilpassede skabeloner_, der er optimeret til modellens kapabiliteter.

3. **Iteration & validering betyder noget.** Modeller udvikler sig hurtigt, og det g√∏r teknikkerne til prompt engineering ogs√•. Som dom√¶neekspert kan du have anden kontekst eller kriterier for _din_ specifikke applikation, der m√•ske ikke g√¶lder for det bredere f√¶llesskab. Brug v√¶rkt√∏jer og teknikker til prompt engineering til at "springe i gang" med promptkonstruktion, og iter√©r og valider resultaterne ved hj√¶lp af din egen intuition og dom√¶neekspertise. Optag dine indsigter og skab en **vidensbase** (f.eks. promptbiblioteker), der kan bruges som en ny baseline af andre, for hurtigere iterationer i fremtiden.

## Bedste praksis

Lad os nu se p√• almindelige bedste praksis, der anbefales af [OpenAI](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api?WT.mc_id=academic-105485-koreyst) og [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering#best-practices?WT.mc_id=academic-105485-koreyst) praktikere.

| Hvad                              | Hvorfor                                                                                                                                                                                                                                               |
| :-------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Evalu√©r de nyeste modeller.       | Nye modelgenerationer har sandsynligvis forbedrede funktioner og kvalitet - men kan ogs√• medf√∏re h√∏jere omkostninger. Evalu√©r dem for indvirkning, og tag derefter migrationsbeslutninger.                                                                                |
| Adskil instruktioner & kontekst   | Tjek om din model/udbyder definerer _afgr√¶nsere_ for at skelne instruktioner, prim√¶rt og sekund√¶rt indhold mere klart. Dette kan hj√¶lpe modeller med at tildele v√¶gte mere pr√¶cist til tokens.                                                         |
| V√¶r specifik og klar             | Giv flere detaljer om den √∏nskede kontekst, resultat, l√¶ngde, format, stil osv. Dette vil forbedre b√•de kvaliteten og konsistensen af svarene. Fang opskrifter i genanvendelige skabeloner.                                                          |
| V√¶r beskrivende, brug eksempler      | Modeller kan reagere bedre p√• en "show and tell" tilgang. Start med en `zero-shot` approach where you give it an instruction (but no examples) then try `few-shot` as a refinement, providing a few examples of the desired output. Use analogies. |
| Use cues to jumpstart completions | Nudge it towards a desired outcome by giving it some leading words or phrases that it can use as a starting point for the response.                                                                                                               |
| Double Down                       | Sometimes you may need to repeat yourself to the model. Give instructions before and after your primary content, use an instruction and a cue, etc. Iterate & validate to see what works.                                                         |
| Order Matters                     | The order in which you present information to the model may impact the output, even in the learning examples, thanks to recency bias. Try different options to see what works best.                                                               |
| Give the model an ‚Äúout‚Äù           | Give the model a _fallback_ completion response it can provide if it cannot complete the task for any reason. This can reduce chances of models generating false or fabricated responses.                                                         |
|                                   |                                                                                                                                                                                                                                                   |

As with any best practice, remember that _your mileage may vary_ based on the model, the task and the domain. Use these as a starting point, and iterate to find what works best for you. Constantly re-evaluate your prompt engineering process as new models and tools become available, with a focus on process scalability and response quality.

<!--
LESSON TEMPLATE:
This unit should provide a code challenge if applicable

CHALLENGE:
Link to a Jupyter Notebook with only the code comments in the instructions (code sections are empty).

SOLUTION:
Link to a copy of that Notebook with the prompts filled in and run, showing what one example could be.
-->

## Assignment

Congratulations! You made it to the end of the lesson! It's time to put some of those concepts and techniques to the test with real examples!

For our assignment, we'll be using a Jupyter Notebook with exercises you can complete interactively. You can also extend the Notebook with your own Markdown and Code cells to explore ideas and techniques on your own.

### To get started, fork the repo, then

- (Recommended) Launch GitHub Codespaces
- (Alternatively) Clone the repo to your local device and use it with Docker Desktop
- (Alternatively) Open the Notebook with your preferred Notebook runtime environment.

### Next, configure your environment variables

- Copy the `.env.copy` file in repo root to `.env` and fill in the `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_DEPLOYMENT` v√¶rdier. Kom tilbage til [Learning Sandbox sektion](../../../04-prompt-engineering-fundamentals/04-prompt-engineering-fundamentals) for at l√¶re hvordan.

### N√¶ste, √•bn Jupyter Notebook

- V√¶lg runtime kernel. Hvis du bruger mulighed 1 eller 2, skal du blot v√¶lge den standard Python 3.10.x kernel, der leveres af udviklingscontaineren.

Du er klar til at k√∏re √∏velserne. Bem√¶rk, at der ikke er _rigtige og forkerte_ svar her - bare udforskning af muligheder gennem trial-and-error og opbygning af intuition for hvad der virker for en given model og applikationsdom√¶ne.

_Af denne grund er der ingen Code Solution segmenter i denne lektion. I stedet vil Notebook have Markdown celler med titlen "My Solution:" der viser et eksempel output til reference._

 <!--
LESSON TEMPLATE:
Afslut sektionen med en opsummering og ressourcer til selvstyret l√¶ring.
-->

## Videns tjek

Hvilken af f√∏lgende er en god prompt efter nogle rimelige bedste praksis?

1. Vis mig et billede af en r√∏d bil
2. Vis mig et billede af en r√∏d bil af m√¶rket Volvo og model XC90 parkeret ved en klippe med solen, der g√•r ned
3. Vis mig et billede af en r√∏d bil af m√¶rket Volvo og model XC90

A: 2, det er den bedste prompt, da den giver detaljer om "hvad" og g√•r ind i specifikationer (ikke bare en hvilken som helst bil, men en bestemt m√¶rke og model) og beskriver ogs√• den overordnede indstilling. 3 er n√¶stbedst, da den ogs√• indeholder en masse beskrivelse.

## üöÄ Udfordring

Se om du kan udnytte "cue" teknikken med prompten: Fuldf√∏r s√¶tningen "Vis mig et billede af en r√∏d bil af m√¶rket Volvo og ". Hvad svarer den med, og hvordan ville du forbedre det?

## Godt arbejde! Forts√¶t din l√¶ring

Vil du l√¶re mere om forskellige Prompt Engineering koncepter? G√• til [fortsat l√¶ringsside](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at finde andre gode ressourcer om dette emne.

G√• videre til Lektion 5, hvor vi vil se p√• [avancerede prompting teknikker](../05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst)!

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller fejltolkninger, der m√•tte opst√• ved brug af denne overs√¶ttelse.