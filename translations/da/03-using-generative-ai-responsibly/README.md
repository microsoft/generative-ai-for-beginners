<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7f8f4c11f8c1cb6e1794442dead414ea",
  "translation_date": "2025-07-09T08:57:35+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "da"
}
-->
# Brug Generativ AI Ansvarligt

[![Using Generative AI Responsibly](../../../translated_images/03-lesson-banner.1ed56067a452d97709d51f6cc8b6953918b2287132f4909ade2008c936cd4af9.da.png)](https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst)

> _Klik p√• billedet ovenfor for at se videoen til denne lektion_

Det er nemt at blive fascineret af AI og is√¶r generativ AI, men du skal overveje, hvordan du bruger det ansvarligt. Du skal t√¶nke over, hvordan du sikrer, at output er retf√¶rdigt, ikke-skadeligt og mere. Dette kapitel har til form√•l at give dig den n√∏dvendige kontekst, hvad du skal overveje, og hvordan du kan tage aktive skridt for at forbedre din AI-brug.

## Introduktion

Denne lektion vil d√¶kke:

- Hvorfor du b√∏r prioritere Ansvarlig AI, n√•r du bygger Generative AI-applikationer.
- De grundl√¶ggende principper for Ansvarlig AI, og hvordan de relaterer til Generativ AI.
- Hvordan du oms√¶tter disse principper for Ansvarlig AI til praksis gennem strategi og v√¶rkt√∏jer.

## L√¶ringsm√•l

Efter at have gennemf√∏rt denne lektion vil du vide:

- Hvor vigtigt Ansvarlig AI er, n√•r du bygger Generative AI-applikationer.
- Hvorn√•r du skal t√¶nke p√• og anvende de grundl√¶ggende principper for Ansvarlig AI i dit arbejde med Generativ AI.
- Hvilke v√¶rkt√∏jer og strategier der er tilg√¶ngelige for dig til at oms√¶tte konceptet Ansvarlig AI til praksis.

## Principper for Ansvarlig AI

Sp√¶ndingen omkring Generativ AI har aldrig v√¶ret st√∏rre. Denne begejstring har tiltrukket mange nye udviklere, opm√¶rksomhed og finansiering til omr√•det. Selvom det er meget positivt for alle, der √∏nsker at bygge produkter og virksomheder med Generativ AI, er det ogs√• vigtigt, at vi g√•r frem p√• en ansvarlig m√•de.

I l√∏bet af dette kursus fokuserer vi p√• at bygge vores startup og vores AI-uddannelsesprodukt. Vi vil bruge principperne for Ansvarlig AI: Retf√¶rdighed, Inklusion, P√•lidelighed/Sikkerhed, Sikkerhed & Privatliv, Gennemsigtighed og Ansvarlighed. Med disse principper vil vi unders√∏ge, hvordan de relaterer til vores brug af Generativ AI i vores produkter.

## Hvorfor b√∏r du prioritere Ansvarlig AI

N√•r du bygger et produkt, f√∏rer en menneskecentreret tilgang, hvor du har brugerens bedste interesse for √∏je, til de bedste resultater.

Det unikke ved Generativ AI er dens evne til at skabe nyttige svar, information, vejledning og indhold til brugerne. Det kan g√∏res uden mange manuelle trin, hvilket kan f√∏re til meget imponerende resultater. Uden ordentlig planl√¶gning og strategier kan det desv√¶rre ogs√• f√∏re til skadelige resultater for dine brugere, dit produkt og samfundet som helhed.

Lad os se p√• nogle (men ikke alle) af disse potentielt skadelige resultater:

### Hallucinationer

Hallucinationer er et udtryk, der bruges til at beskrive, n√•r en LLM producerer indhold, som enten er fuldst√¶ndig meningsl√∏st eller noget, vi ved er faktuelt forkert baseret p√• andre informationskilder.

Lad os tage et eksempel, hvor vi bygger en funktion til vores startup, der tillader studerende at stille historiske sp√∏rgsm√•l til en model. En studerende sp√∏rger: `Who was the sole survivor of Titanic?`

Modellen giver et svar som det nedenfor:

![Prompt saying "Who was the sole survivor of the Titanic"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp)

> _(Kilde: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

Dette er et meget selvsikkert og grundigt svar. Desv√¶rre er det forkert. Selv med en minimal m√¶ngde research ville man opdage, at der var mere end √©n overlevende fra Titanic-katastrofen. For en studerende, der lige er begyndt at unders√∏ge emnet, kan dette svar v√¶re overbevisende nok til ikke at blive stillet sp√∏rgsm√•l ved og blive betragtet som fakta. Konsekvenserne kan f√∏re til, at AI-systemet bliver up√•lideligt og negativt p√•virker vores startups omd√∏mme.

Med hver iteration af en given LLM har vi set forbedringer i at minimere hallucinationer. Selv med denne forbedring skal vi som applikationsudviklere og brugere stadig v√¶re opm√¶rksomme p√• disse begr√¶nsninger.

### Skadeligt indhold

Vi har tidligere d√¶kket, n√•r en LLM producerer forkerte eller meningsl√∏se svar. En anden risiko, vi skal v√¶re opm√¶rksomme p√•, er, n√•r en model svarer med skadeligt indhold.

Skadeligt indhold kan defineres som:

- At give instruktioner eller opfordre til selvskade eller skade mod bestemte grupper.
- Hadsk eller nedv√¶rdigende indhold.
- At vejlede planl√¶gning af enhver form for angreb eller voldelige handlinger.
- At give instruktioner om, hvordan man finder ulovligt indhold eller beg√•r ulovlige handlinger.
- At vise seksuelt eksplicit indhold.

For vores startup vil vi sikre, at vi har de rette v√¶rkt√∏jer og strategier p√• plads for at forhindre, at denne type indhold bliver set af studerende.

### Manglende retf√¶rdighed

Retf√¶rdighed defineres som "at sikre, at et AI-system er fri for bias og diskrimination, og at det behandler alle retf√¶rdigt og lige." I verden af Generativ AI √∏nsker vi at sikre, at ekskluderende verdenssyn over for marginaliserede grupper ikke forst√¶rkes af modellens output.

Denne type output er ikke kun √∏del√¶ggende for at skabe positive produktoplevelser for vores brugere, men de for√•rsager ogs√• yderligere samfundsm√¶ssig skade. Som applikationsudviklere b√∏r vi altid have en bred og mangfoldig brugerbase i tankerne, n√•r vi bygger l√∏sninger med Generativ AI.

## Hvordan man bruger Generativ AI ansvarligt

Nu hvor vi har identificeret vigtigheden af Ansvarlig Generativ AI, lad os se p√• 4 trin, vi kan tage for at bygge vores AI-l√∏sninger ansvarligt:

![Mitigate Cycle](../../../translated_images/mitigate-cycle.babcd5a5658e1775d5f2cb47f2ff305cca090400a72d98d0f9e57e9db5637c72.da.png)

### M√•l potentielle skader

I softwaretest tester vi forventede handlinger fra en bruger p√• en applikation. P√• samme m√•de er det en god m√•de at m√•le potentielle skader p√• at teste et varieret s√¶t af prompts, som brugerne sandsynligvis vil anvende.

Da vores startup bygger et uddannelsesprodukt, vil det v√¶re godt at forberede en liste over uddannelsesrelaterede prompts. Det kan v√¶re til at d√¶kke et bestemt fag, historiske fakta og prompts om studieliv.

### Begr√¶ns potentielle skader

Det er nu tid til at finde m√•der, hvorp√• vi kan forhindre eller begr√¶nse den potentielle skade, som modellen og dens svar kan for√•rsage. Vi kan se p√• dette i 4 forskellige lag:

![Mitigation Layers](../../../translated_images/mitigation-layers.377215120b9a1159a8c3982c6bbcf41b6adf8c8fa04ce35cbaeeb13b4979cdfc.da.png)

- **Model**. V√¶lg den rette model til den rette brugssag. St√∏rre og mere komplekse modeller som GPT-4 kan udg√∏re en st√∏rre risiko for skadeligt indhold, n√•r de anvendes til mindre og mere specifikke brugssituationer. Brug af tr√¶ningsdata til finjustering reducerer ogs√• risikoen for skadeligt indhold.

- **Sikkerhedssystem**. Et sikkerhedssystem er et s√¶t v√¶rkt√∏jer og konfigurationer p√• platformen, der servicerer modellen, og som hj√¶lper med at begr√¶nse skader. Et eksempel er indholdsfiltreringssystemet p√• Azure OpenAI-tjenesten. Systemer b√∏r ogs√• kunne opdage jailbreak-angreb og u√∏nsket aktivitet som foresp√∏rgsler fra bots.

- **Metaprompt**. Metaprompts og grounding er m√•der, hvorp√• vi kan styre eller begr√¶nse modellen baseret p√• bestemte adf√¶rdsm√∏nstre og information. Det kan v√¶re ved at bruge systeminput til at definere visse gr√¶nser for modellen. Derudover kan det sikre, at output er mere relevante for systemets omfang eller dom√¶ne.

Det kan ogs√• v√¶re ved at bruge teknikker som Retrieval Augmented Generation (RAG), s√• modellen kun henter information fra et udvalg af betroede kilder. Der er en lektion senere i dette kursus om [at bygge s√∏geapplikationer](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **Brugeroplevelse**. Det sidste lag er, hvor brugeren interagerer direkte med modellen gennem vores applikationsgr√¶nseflade p√• en eller anden m√•de. Her kan vi designe UI/UX til at begr√¶nse brugeren i, hvilke typer input de kan sende til modellen, samt hvilken tekst eller billeder der vises for brugeren. N√•r vi implementerer AI-applikationen, skal vi ogs√• v√¶re gennemsigtige omkring, hvad vores Generative AI-applikation kan og ikke kan.

Vi har en hel lektion dedikeret til [Design af UX for AI-applikationer](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **Evaluer modellen**. Arbejde med LLM‚Äôer kan v√¶re udfordrende, fordi vi ikke altid har kontrol over de data, modellen er tr√¶net p√•. Ikke desto mindre b√∏r vi altid evaluere modellens ydeevne og output. Det er stadig vigtigt at m√•le modellens n√∏jagtighed, lighed, forankring og relevans af output. Dette hj√¶lper med at skabe gennemsigtighed og tillid hos interessenter og brugere.

### Drift af en ansvarlig Generativ AI-l√∏sning

At opbygge en operationel praksis omkring dine AI-applikationer er det sidste trin. Det inkluderer samarbejde med andre dele af vores startup som Juridisk og Sikkerhed for at sikre, at vi overholder alle lovgivningsm√¶ssige krav. F√∏r lancering √∏nsker vi ogs√• at lave planer for levering, h√•ndtering af h√¶ndelser og rollback for at forhindre, at skader p√• vores brugere vokser.

## V√¶rkt√∏jer

Selvom arbejdet med at udvikle Ansvarlige AI-l√∏sninger kan virke omfattende, er det indsatsen v√¶rd. Efterh√•nden som omr√•det for Generativ AI vokser, vil flere v√¶rkt√∏jer, der hj√¶lper udviklere med effektivt at integrere ansvarlighed i deres arbejdsgange, modnes. For eksempel kan [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) hj√¶lpe med at opdage skadeligt indhold og billeder via en API-foresp√∏rgsel.

## Videnstjek

Hvilke ting skal du v√¶re opm√¶rksom p√• for at sikre ansvarlig AI-brug?

1. At svaret er korrekt.  
1. Skadelig brug, at AI ikke bruges til kriminelle form√•l.  
1. At sikre, at AI er fri for bias og diskrimination.

A: 2 og 3 er korrekte. Ansvarlig AI hj√¶lper dig med at overveje, hvordan du kan begr√¶nse skadelige effekter, bias og mere.

## üöÄ Udfordring

L√¶s op p√• [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) og se, hvad du kan tage i brug til din egen anvendelse.

## Godt arbejde, forts√¶t din l√¶ring

Efter at have gennemf√∏rt denne lektion, kan du tjekke vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts√¶tte med at styrke din viden om Generativ AI!

G√• videre til Lektion 4, hvor vi ser p√• [Grundl√¶ggende Prompt Engineering](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, bedes du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det oprindelige dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os intet ansvar for misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.