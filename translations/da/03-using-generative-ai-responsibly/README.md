<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4d57fad773cbeb69c5dd62e65c34200d",
  "translation_date": "2025-10-17T19:10:06+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "da"
}
-->
# Brug af Generativ AI Ansvarligt

[![Brug af Generativ AI Ansvarligt](../../../translated_images/03-lesson-banner.1ed56067a452d97709d51f6cc8b6953918b2287132f4909ade2008c936cd4af9.da.png)](https://youtu.be/YOp-e1GjZdA?si=7Wv4wu3x44L1DCVj)

> _Klik p친 billedet ovenfor for at se videoen til denne lektion_

Det er nemt at blive fascineret af AI og is칝r generativ AI, men det er vigtigt at overveje, hvordan man bruger det ansvarligt. Du skal t칝nke over, hvordan du sikrer, at output er retf칝rdigt, ikke skadeligt og mere. Dette kapitel har til form친l at give dig den n칝vnte kontekst, hvad du skal overveje, og hvordan du kan tage aktive skridt for at forbedre din brug af AI.

## Introduktion

Denne lektion vil d칝kke:

- Hvorfor du b칮r prioritere Ansvarlig AI, n친r du bygger applikationer med Generativ AI.
- Kerneprincipperne for Ansvarlig AI og hvordan de relaterer sig til Generativ AI.
- Hvordan du kan oms칝tte disse principper for Ansvarlig AI til praksis gennem strategier og v칝rkt칮jer.

## L칝ringsm친l

Efter at have gennemf칮rt denne lektion vil du vide:

- Vigtigheden af Ansvarlig AI, n친r du bygger applikationer med Generativ AI.
- Hvorn친r du skal t칝nke p친 og anvende kerneprincipperne for Ansvarlig AI, n친r du bygger applikationer med Generativ AI.
- Hvilke v칝rkt칮jer og strategier der er tilg칝ngelige for dig til at oms칝tte konceptet Ansvarlig AI til praksis.

## Principper for Ansvarlig AI

Interessen for Generativ AI har aldrig v칝ret st칮rre. Denne interesse har bragt mange nye udviklere, opm칝rksomhed og finansiering til dette omr친de. Selvom dette er meget positivt for alle, der 칮nsker at bygge produkter og virksomheder med Generativ AI, er det ogs친 vigtigt, at vi g친r frem med ansvarlighed.

I hele dette kursus fokuserer vi p친 at bygge vores startup og vores AI-uddannelsesprodukt. Vi vil bruge principperne for Ansvarlig AI: Retf칝rdighed, Inklusion, P친lidelighed/Sikkerhed, Sikkerhed & Privatliv, Transparens og Ansvarlighed. Med disse principper vil vi unders칮ge, hvordan de relaterer sig til vores brug af Generativ AI i vores produkter.

## Hvorfor skal du prioritere Ansvarlig AI

N친r du bygger et produkt, f칮rer en menneskecentreret tilgang, hvor du har brugerens bedste interesse i tankerne, til de bedste resultater.

Det unikke ved Generativ AI er dens evne til at skabe nyttige svar, information, vejledning og indhold til brugerne. Dette kan g칮res uden mange manuelle trin, hvilket kan f칮re til meget imponerende resultater. Uden ordentlig planl칝gning og strategier kan det desv칝rre ogs친 f칮re til nogle skadelige resultater for dine brugere, dit produkt og samfundet som helhed.

Lad os se p친 nogle (men ikke alle) af disse potentielt skadelige resultater:

### Hallucinationer

Hallucinationer er et udtryk, der bruges til at beskrive, n친r en LLM producerer indhold, der enten er fuldst칝ndig meningsl칮st eller noget, vi ved er faktuelt forkert baseret p친 andre informationskilder.

Lad os for eksempel sige, at vi bygger en funktion til vores startup, der giver studerende mulighed for at stille historiske sp칮rgsm친l til en model. En studerende sp칮rger: `Hvem var den eneste overlevende fra Titanic?`

Modellen producerer et svar som det nedenfor:

![Prompt der siger "Hvem var den eneste overlevende fra Titanic"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp)

> _(Kilde: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

Dette er et meget selvsikkert og grundigt svar. Desv칝rre er det forkert. Selv med en minimal m칝ngde research ville man opdage, at der var mere end 칠n overlevende fra Titanic-katastrofen. For en studerende, der lige er begyndt at unders칮ge dette emne, kan dette svar v칝re overbevisende nok til ikke at blive stillet sp칮rgsm친lstegn ved og behandlet som fakta. Konsekvenserne af dette kan f칮re til, at AI-systemet bliver up친lideligt og negativt p친virker vores startups omd칮mme.

Med hver iteration af en given LLM har vi set forbedringer i ydeevnen omkring minimering af hallucinationer. Selv med denne forbedring skal vi som applikationsbyggere og brugere stadig v칝re opm칝rksomme p친 disse begr칝nsninger.

### Skadeligt indhold

Vi d칝kkede i det tidligere afsnit, n친r en LLM producerer forkerte eller meningsl칮se svar. En anden risiko, vi skal v칝re opm칝rksomme p친, er, n친r en model svarer med skadeligt indhold.

Skadeligt indhold kan defineres som:

- At give instruktioner eller opmuntre til selvskade eller skade p친 visse grupper.
- Hadefuldt eller nedv칝rdigende indhold.
- At vejlede planl칝gning af enhver form for angreb eller voldelige handlinger.
- At give instruktioner om, hvordan man finder ulovligt indhold eller beg친r ulovlige handlinger.
- At vise seksuelt eksplicit indhold.

For vores startup vil vi sikre, at vi har de rette v칝rkt칮jer og strategier p친 plads for at forhindre, at denne type indhold bliver vist til studerende.

### Manglende retf칝rdighed

Retf칝rdighed defineres som "at sikre, at et AI-system er fri for bias og diskrimination, og at det behandler alle retf칝rdigt og lige." I verden af Generativ AI 칮nsker vi at sikre, at ekskluderende verdenssyn for marginaliserede grupper ikke forst칝rkes af modellens output.

Denne type output er ikke kun destruktiv for at bygge positive produktoplevelser for vores brugere, men de for친rsager ogs친 yderligere skade p친 samfundet. Som applikationsbyggere b칮r vi altid have en bred og mangfoldig brugerbase i tankerne, n친r vi bygger l칮sninger med Generativ AI.

## Hvordan man bruger Generativ AI Ansvarligt

Nu hvor vi har identificeret vigtigheden af Ansvarlig Generativ AI, lad os se p친 4 trin, vi kan tage for at bygge vores AI-l칮sninger ansvarligt:

![Mitigate Cycle](../../../translated_images/mitigate-cycle.babcd5a5658e1775d5f2cb47f2ff305cca090400a72d98d0f9e57e9db5637c72.da.png)

### M친l potentielle skader

I softwaretest tester vi de forventede handlinger fra en bruger p친 en applikation. P친 samme m친de er det en god id칠 at teste et mangfoldigt s칝t af prompts, som brugerne sandsynligvis vil bruge, for at m친le potentielle skader.

Da vores startup bygger et uddannelsesprodukt, ville det v칝re godt at forberede en liste over uddannelsesrelaterede prompts. Dette kunne d칝kke et bestemt emne, historiske fakta og prompts om studieliv.

### Begr칝ns potentielle skader

Det er nu tid til at finde m친der, hvor vi kan forhindre eller begr칝nse den potentielle skade for친rsaget af modellen og dens svar. Vi kan se p친 dette i 4 forskellige lag:

![Mitigation Layers](../../../translated_images/mitigation-layers.377215120b9a1159a8c3982c6bbcf41b6adf8c8fa04ce35cbaeeb13b4979cdfc.da.png)

- **Model**. V칝lg den rigtige model til den rigtige brugssag. St칮rre og mere komplekse modeller som GPT-4 kan medf칮re st칮rre risiko for skadeligt indhold, n친r de anvendes til mindre og mere specifikke brugssager. Brug af dine tr칝ningsdata til finjustering reducerer ogs친 risikoen for skadeligt indhold.

- **Sikkerhedssystem**. Et sikkerhedssystem er et s칝t v칝rkt칮jer og konfigurationer p친 platformen, der serverer modellen, som hj칝lper med at begr칝nse skade. Et eksempel p친 dette er indholdsfiltreringssystemet p친 Azure OpenAI-tjenesten. Systemer b칮r ogs친 opdage jailbreak-angreb og u칮nsket aktivitet som foresp칮rgsler fra bots.

- **Metaprompt**. Metaprompts og grounding er m친der, hvorp친 vi kan dirigere eller begr칝nse modellen baseret p친 visse adf칝rdsm칮nstre og information. Dette kunne v칝re brug af systeminput til at definere visse gr칝nser for modellen. Derudover kan det give output, der er mere relevante for systemets omfang eller dom칝ne.

Det kan ogs친 v칝re brug af teknikker som Retrieval Augmented Generation (RAG) for at f친 modellen til kun at hente information fra et udvalg af betroede kilder. Der er en lektion senere i dette kursus om [at bygge s칮geapplikationer](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Brugeroplevelse**. Det sidste lag er, hvor brugeren interagerer direkte med modellen gennem vores applikations interface p친 en eller anden m친de. P친 denne m친de kan vi designe UI/UX til at begr칝nse brugeren i de typer input, de kan sende til modellen, samt tekst eller billeder, der vises til brugeren. N친r vi implementerer AI-applikationen, skal vi ogs친 v칝re transparente omkring, hvad vores Generative AI-applikation kan og ikke kan g칮re.

Vi har en hel lektion dedikeret til [Design af UX for AI-applikationer](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Evalu칠r modellen**. Arbejde med LLM'er kan v칝re udfordrende, fordi vi ikke altid har kontrol over de data, modellen blev tr칝net p친. Uanset hvad, b칮r vi altid evaluere modellens ydeevne og output. Det er stadig vigtigt at m친le modellens n칮jagtighed, lighed, grounding og relevans af output. Dette hj칝lper med at give transparens og tillid til interessenter og brugere.

### Drift af en Ansvarlig Generativ AI-l칮sning

At bygge en operationel praksis omkring dine AI-applikationer er det sidste trin. Dette inkluderer samarbejde med andre dele af vores startup som Jura og Sikkerhed for at sikre, at vi overholder alle lovgivningsm칝ssige politikker. F칮r lancering vil vi ogs친 bygge planer omkring levering, h친ndtering af h칝ndelser og rollback for at forhindre enhver skade p친 vores brugere i at vokse.

## V칝rkt칮jer

Selvom arbejdet med at udvikle Ansvarlige AI-l칮sninger kan virke som meget, er det arbejde, der er v칝rd at g칮re. Efterh친nden som omr친det for Generativ AI vokser, vil flere v칝rkt칮jer, der hj칝lper udviklere med effektivt at integrere ansvarlighed i deres arbejdsgange, modnes. For eksempel kan [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) hj칝lpe med at opdage skadeligt indhold og billeder via en API-foresp칮rgsel.

## Videnscheck

Hvad er nogle ting, du skal tage hensyn til for at sikre ansvarlig AI-brug?

1. At svaret er korrekt.
1. Skadelig brug, at AI ikke bruges til kriminelle form친l.
1. At sikre, at AI er fri for bias og diskrimination.

A: 2 og 3 er korrekte. Ansvarlig AI hj칝lper dig med at overveje, hvordan du kan begr칝nse skadelige effekter og bias og mere.

## 游 Udfordring

L칝s om [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) og se, hvad du kan adoptere til din brug.

## Godt arbejde, forts칝t din l칝ring

Efter at have gennemf칮rt denne lektion, kan du tjekke vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts칝tte med at opbygge din viden om Generativ AI!

G친 videre til Lektion 4, hvor vi vil se p친 [Grundl칝ggende om Prompt Engineering](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 n칮jagtighed, skal det bem칝rkes, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller fejltolkninger, der opst친r som f칮lge af brugen af denne overs칝ttelse.