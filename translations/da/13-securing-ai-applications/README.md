<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:29:30+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "da"
}
-->
# Sikring af dine generative AI-applikationer

[![Sikring af dine generative AI-applikationer](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.da.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Introduktion

Denne lektion vil d√¶kke:

- Sikkerhed i forbindelse med AI-systemer.
- Almindelige risici og trusler mod AI-systemer.
- Metoder og overvejelser til sikring af AI-systemer.

## L√¶ringsm√•l

Efter at have gennemf√∏rt denne lektion vil du have forst√•else for:

- Trusler og risici mod AI-systemer.
- Almindelige metoder og praksisser til sikring af AI-systemer.
- Hvordan implementering af sikkerhedstest kan forhindre uventede resultater og tab af brugerens tillid.

## Hvad betyder sikkerhed i forbindelse med generativ AI?

Efterh√•nden som kunstig intelligens (AI) og maskinl√¶ring (ML) i stigende grad pr√¶ger vores liv, er det afg√∏rende ikke kun at beskytte kundedata, men ogs√• AI-systemerne selv. AI/ML anvendes i stigende grad til at underst√∏tte beslutningsprocesser med h√∏j v√¶rdi i brancher, hvor forkerte beslutninger kan f√• alvorlige konsekvenser.

Her er nogle n√∏glepunkter at overveje:

- **AI/ML‚Äôs indflydelse**: AI/ML har stor betydning for dagligdagen, og derfor er det blevet essentielt at beskytte dem.
- **Sikkerhedsudfordringer**: Den indflydelse, AI/ML har, kr√¶ver s√¶rlig opm√¶rksomhed for at beskytte AI-baserede produkter mod avancerede angreb, hvad enten det er fra trolls eller organiserede grupper.
- **Strategiske udfordringer**: Teknologibranchen skal proaktivt h√•ndtere strategiske udfordringer for at sikre langsigtet kundesikkerhed og datasikkerhed.

Derudover er maskinl√¶ringsmodeller i h√∏j grad ude af stand til at skelne mellem ondsindet input og harml√∏s anomal data. En stor del af tr√¶ningsdata stammer fra ukuraterede, umodererede offentlige datas√¶t, som er √•bne for bidrag fra tredjepart. Angribere beh√∏ver ikke at kompromittere datas√¶t, n√•r de frit kan bidrage til dem. Over tid bliver lavtillids ondsindet data til h√∏jtillids betroet data, hvis datastrukturen/formateringen forbliver korrekt.

Derfor er det afg√∏rende at sikre integriteten og beskyttelsen af de datalagre, som dine modeller bruger til at tr√¶ffe beslutninger.

## Forst√•else af trusler og risici ved AI

Inden for AI og relaterede systemer er dataforgiftning i dag den mest betydningsfulde sikkerhedstrussel. Dataforgiftning opst√•r, n√•r nogen bevidst √¶ndrer den information, der bruges til at tr√¶ne en AI, hvilket f√•r den til at beg√• fejl. Dette skyldes manglen p√• standardiserede metoder til detektion og afb√∏dning samt vores afh√¶ngighed af utrov√¶rdige eller ukuraterede offentlige datas√¶t til tr√¶ning. For at opretholde dataintegritet og forhindre en fejlbeh√¶ftet tr√¶ningsproces er det vigtigt at spore dataenes oprindelse og forl√∏b. Ellers g√¶lder det gamle ordsprog ‚Äúgarbage in, garbage out‚Äù, hvilket f√∏rer til kompromitteret modelpr√¶station.

Her er eksempler p√•, hvordan dataforgiftning kan p√•virke dine modeller:

1. **Label Flipping**: I en bin√¶r klassifikationsopgave vender en modstander bevidst etiketterne p√• en lille delm√¶ngde af tr√¶ningsdata. For eksempel m√¶rkes harml√∏se pr√∏ver som ondsindede, hvilket f√•r modellen til at l√¶re forkerte sammenh√¶nge.\
   **Eksempel**: Et spamfilter, der fejlagtigt klassificerer legitime e-mails som spam p√• grund af manipulerede etiketter.
2. **Feature Poisoning**: En angriber √¶ndrer subtilt tr√¶k i tr√¶ningsdataene for at indf√∏re bias eller vildlede modellen.\
   **Eksempel**: Tilf√∏jelse af irrelevante n√∏gleord til produktbeskrivelser for at manipulere anbefalingssystemer.
3. **Data Injection**: Indspr√∏jtning af ondsindet data i tr√¶ningss√¶ttet for at p√•virke modellens adf√¶rd.\
   **Eksempel**: Indf√∏relse af falske brugeranmeldelser for at sk√¶vvride sentimentanalyse.
4. **Backdoor Attacks**: En modstander inds√¶tter et skjult m√∏nster (bagd√∏r) i tr√¶ningsdataene. Modellen l√¶rer at genkende dette m√∏nster og opf√∏rer sig ondsindet, n√•r det aktiveres.\
   **Eksempel**: Et ansigtsgenkendelsessystem tr√¶net med bagd√∏rsbilleder, der fejlagtigt identificerer en bestemt person.

MITRE Corporation har skabt [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en vidensbase over taktikker og teknikker, som modstandere bruger i virkelige angreb p√• AI-systemer.

> Der findes et stigende antal s√•rbarheder i AI-aktiverede systemer, da integrationen af AI √∏ger angrebsoverfladen i eksisterende systemer ud over traditionelle cyberangreb. Vi udviklede ATLAS for at √∏ge bevidstheden om disse unikke og udviklende s√•rbarheder, efterh√•nden som det globale samfund i stigende grad integrerer AI i forskellige systemer. ATLAS er modelleret efter MITRE ATT&CK¬Æ-rammev√¶rket, og dets taktikker, teknikker og procedurer (TTP‚Äôer) supplerer dem i ATT&CK.

Ligesom MITRE ATT&CK¬Æ-rammev√¶rket, der er udbredt i traditionel cybersikkerhed til planl√¶gning af avancerede trusselsimuleringer, tilbyder ATLAS et let s√∏gbart s√¶t TTP‚Äôer, som kan hj√¶lpe med bedre at forst√• og forberede sig p√• at forsvare mod nye angreb.

Derudover har Open Web Application Security Project (OWASP) lavet en "[Top 10 liste](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" over de mest kritiske s√•rbarheder i applikationer, der bruger LLM‚Äôer. Listen fremh√¶ver risici ved trusler som den f√∏rn√¶vnte dataforgiftning samt andre som:

- **Prompt Injection**: En teknik, hvor angribere manipulerer en Large Language Model (LLM) gennem n√∏je udformede input, hvilket f√•r den til at opf√∏re sig uden for sin tilsigtede adf√¶rd.
- **Supply Chain Vulnerabilities**: De komponenter og software, der udg√∏r applikationerne brugt af en LLM, s√•som Python-moduler eller eksterne datas√¶t, kan selv blive kompromitteret, hvilket f√∏rer til uventede resultater, indf√∏rte bias og endda s√•rbarheder i den underliggende infrastruktur.
- **Overafh√¶ngighed**: LLM‚Äôer er fejlbarlige og har vist sig at hallucinere, hvilket giver un√∏jagtige eller usikre resultater. I flere dokumenterede tilf√¶lde har folk taget resultaterne for p√•lydende, hvilket har f√∏rt til utilsigtede negative konsekvenser i den virkelige verden.

Microsoft Cloud Advocate Rod Trent har skrevet en gratis e-bog, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), som g√•r i dybden med disse og andre nye AI-trusler og giver omfattende vejledning i, hvordan man bedst h√•ndterer disse scenarier.

## Sikkerhedstest af AI-systemer og LLM‚Äôer

Kunstig intelligens (AI) forvandler forskellige dom√¶ner og industrier og tilbyder nye muligheder og fordele for samfundet. Men AI medf√∏rer ogs√• betydelige udfordringer og risici, s√•som databeskyttelse, bias, manglende forklarbarhed og potentiel misbrug. Derfor er det afg√∏rende at sikre, at AI-systemer er sikre og ansvarlige, hvilket betyder, at de overholder etiske og juridiske standarder og kan have brugernes og interessenternes tillid.

Sikkerhedstest er processen med at evaluere sikkerheden af et AI-system eller en LLM ved at identificere og udnytte deres s√•rbarheder. Dette kan udf√∏res af udviklere, brugere eller tredjepartsrevisorer, afh√¶ngigt af form√•let og omfanget af testen. Nogle af de mest almindelige metoder til sikkerhedstest af AI-systemer og LLM‚Äôer er:

- **Datasanitering**: Processen med at fjerne eller anonymisere f√∏lsomme eller private oplysninger fra tr√¶ningsdata eller input til et AI-system eller en LLM. Datasanitering kan hj√¶lpe med at forhindre datal√¶kage og ondsindet manipulation ved at reducere eksponeringen af fortrolige eller personlige data.
- **Adversarial testing**: Processen med at generere og anvende adversarielle eksempler p√• input eller output fra et AI-system eller en LLM for at evaluere dets robusthed og modstandsdygtighed over for adversarielle angreb. Adversarial testing kan hj√¶lpe med at identificere og afb√∏de s√•rbarheder og svagheder, som angribere kan udnytte.
- **Modelverifikation**: Processen med at verificere korrektheden og fuldst√¶ndigheden af modelparametre eller arkitektur i et AI-system eller en LLM. Modelverifikation kan hj√¶lpe med at opdage og forhindre modeltyveri ved at sikre, at modellen er beskyttet og autentificeret.
- **Outputvalidering**: Processen med at validere kvaliteten og p√•lideligheden af output fra et AI-system eller en LLM. Outputvalidering kan hj√¶lpe med at opdage og rette ondsindet manipulation ved at sikre, at output er konsistent og korrekt.

OpenAI, en f√∏rende akt√∏r inden for AI-systemer, har etableret en r√¶kke _sikkerhedsvurderinger_ som en del af deres red teaming-netv√¶rksinitiativ, med det form√•l at teste output fra AI-systemer og bidrage til AI-sikkerhed.

> Vurderinger kan variere fra simple Q&A-tests til mere komplekse simulationer. Som konkrete eksempler er her pr√∏vevurderinger udviklet af OpenAI til at evaluere AI-adf√¶rd fra flere vinkler:

#### Overbevisning

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system narre et andet AI-system til at sige et hemmeligt ord?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system overbevise et andet AI-system om at donere penge?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system p√•virke et andet AI-systems st√∏tte til et politisk forslag?

#### Steganografi (skjult besked)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system sende hemmelige beskeder uden at blive opdaget af et andet AI-system?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system komprimere og dekomprimere beskeder for at skjule hemmelige beskeder?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system koordinere med et andet AI-system uden direkte kommunikation?

### AI-sikkerhed

Det er afg√∏rende, at vi beskytter AI-systemer mod ondsindede angreb, misbrug eller utilsigtede konsekvenser. Dette inkluderer at tage skridt til at sikre AI-systemernes sikkerhed, p√•lidelighed og trov√¶rdighed, s√•som:

- Sikring af de data og algoritmer, der bruges til at tr√¶ne og k√∏re AI-modeller
- Forebyggelse af uautoriseret adgang, manipulation eller sabotage af AI-systemer
- Opdagelse og afb√∏dning af bias, diskrimination eller etiske problemer i AI-systemer
- Sikring af ansvarlighed, gennemsigtighed og forklarbarhed af AI-beslutninger og handlinger
- Tilpasning af AI-systemers m√•l og v√¶rdier med menneskers og samfundets

AI-sikkerhed er vigtig for at sikre integriteten, tilg√¶ngeligheden og fortroligheden af AI-systemer og data. Nogle af udfordringerne og mulighederne inden for AI-sikkerhed er:

- Mulighed: At integrere AI i cybersikkerhedsstrategier, da det kan spille en afg√∏rende rolle i at identificere trusler og forbedre responstider. AI kan hj√¶lpe med at automatisere og styrke opdagelse og afb√∏dning af cyberangreb som phishing, malware eller ransomware.
- Udfordring: AI kan ogs√• bruges af modstandere til at udf√∏re avancerede angreb, s√•som at generere falsk eller vildledende indhold, udgive sig for at v√¶re brugere eller udnytte s√•rbarheder i AI-systemer. Derfor har AI-udviklere et s√¶rligt ansvar for at designe systemer, der er robuste og modstandsdygtige over for misbrug.

### Databeskyttelse

LLM‚Äôer kan udg√∏re risici for privatliv og sikkerhed for de data, de bruger. For eksempel kan LLM‚Äôer potentielt huske og l√¶kke f√∏lsomme oplysninger fra deres tr√¶ningsdata, s√•som personnavne, adresser, adgangskoder eller kreditkortnumre. De kan ogs√• manipuleres eller angribes af ondsindede akt√∏rer, der √∏nsker at udnytte deres s√•rbarheder eller bias. Derfor er det vigtigt at v√¶re opm√¶rksom p√• disse risici og tage passende forholdsregler for at beskytte de data, der bruges med LLM‚Äôer. Her er nogle skridt, du kan tage for at beskytte data, der bruges med LLM‚Äôer:

- **Begr√¶ns m√¶ngden og typen af data, der deles med LLM‚Äôer**: Del kun de data, der er n√∏dvendige og relevante for de tilsigtede form√•l, og undg√• at dele f√∏lsomme, fortrolige eller personlige oplysninger. Brugere b√∏r ogs√• anonymisere eller kryptere de data, de deler med LLM‚Äôer, for eksempel ved at fjerne eller maskere identificerende oplysninger eller bruge sikre kommunikationskanaler.
- **Verificer data, som LLM‚Äôer genererer**: Tjek altid n√∏jagtigheden og kvaliteten af output genereret af LLM‚Äôer for at sikre, at de ikke indeholder u√∏nsket eller upassende information.
- **Rapport√©r og alarmer om databrud eller h√¶ndelser**: V√¶r opm√¶rksom p√• mist√¶nkelig eller unormal adf√¶rd fra LLM‚Äôer, s√•som generering af tekster, der er irrelevante, un√∏jagtige, st√∏dende eller skadelige. Dette kan v√¶re tegn p√• et databrud eller en sikkerhedsh√¶ndelse.

Datasikkerhed, styring og overholdelse er afg√∏rende for enhver organisation, der √∏nsker at udnytte data og AI i et multi-cloud-milj√∏. At sikre og styre alle dine data er en kompleks og mangesidet opgave. Du skal sikre og styre forskellige typer data (strukturerede, ustrukturerede og data genereret af AI) p√• forskellige steder p√• tv√¶rs af flere clouds, og du skal tage h√∏jde for eksisterende og kommende regler for datasikkerhed, styring og AI. For at beskytte dine data b√∏r du anvende nogle bedste praksisser og forholdsregler, s√•som:

- Brug cloudtjenester eller platforme, der tilbyder databeskyttelse og privatlivsfunktioner.
- Brug v√¶rkt√∏jer til datakvalitet og validering for at kontrollere dine data for fejl, inkonsistenser eller anomalier.
- Brug rammer for datastyring og etik for at sikre, at dine data bruges ansvarligt og gennemsigtigt.

### Efterligning af trusler fra den virkelige verden ‚Äì AI red teaming

Efterligning af trusler fra den virkelige verden betragtes nu som en standardpraksis i opbygningen af robuste AI-systemer ved at anvende lignende v√¶rkt√∏jer, taktikker og procedurer for at identificere risici for systemer og teste forsvarernes reaktion.
> Praksissen med AI red teaming har udviklet sig til at f√• en bredere betydning: det omfatter ikke kun at unders√∏ge sikkerhedss√•rbarheder, men ogs√• at afd√¶kke andre systemfejl, s√•som generering af potentielt skadeligt indhold. AI-systemer medf√∏rer nye risici, og red teaming er centralt for at forst√• disse nye risici, s√•som prompt injection og produktion af uunderbyggede indhold. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.da.png)]()

Nedenfor er n√∏gleindsigter, der har formet Microsofts AI Red Team-program.

1. **Omfattende r√¶kkevidde af AI Red Teaming:**  
   AI red teaming omfatter nu b√•de sikkerheds- og Responsible AI (RAI)-resultater. Traditionelt fokuserede red teaming p√• sikkerhedsaspekter og betragtede modellen som en angrebsvektor (f.eks. at stj√¶le den underliggende model). AI-systemer introducerer dog nye sikkerhedss√•rbarheder (f.eks. prompt injection, forgiftning), som kr√¶ver s√¶rlig opm√¶rksomhed. Udover sikkerhed unders√∏ger AI red teaming ogs√• retf√¶rdighedssp√∏rgsm√•l (f.eks. stereotyper) og skadeligt indhold (f.eks. glorificering af vold). Tidlig identifikation af disse problemer g√∏r det muligt at prioritere forsvarsindsatser.

2. **Ondsindede og harml√∏se fejl:**  
   AI red teaming tager h√∏jde for fejl b√•de fra ondsindede og harml√∏se perspektiver. For eksempel, n√•r vi red teamer den nye Bing, unders√∏ger vi ikke kun, hvordan ondsindede modstandere kan undergrave systemet, men ogs√• hvordan almindelige brugere kan st√∏de p√• problematisk eller skadeligt indhold. I mods√¶tning til traditionel sikkerhedsred teaming, der prim√¶rt fokuserer p√• ondsindede akt√∏rer, tager AI red teaming h√∏jde for et bredere spektrum af brugertyper og potentielle fejl.

3. **AI-systemers dynamiske natur:**  
   AI-applikationer udvikler sig konstant. I store sprogmodel-applikationer tilpasser udviklere sig l√∏bende til √¶ndrede krav. Kontinuerlig red teaming sikrer vedvarende √•rv√•genhed og tilpasning til nye risici.

AI red teaming er ikke altomfattende og b√∏r betragtes som et supplement til yderligere kontroller som [role-based access control (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) og omfattende datastyringsl√∏sninger. Det skal underst√∏tte en sikkerhedsstrategi, der fokuserer p√• at anvende sikre og ansvarlige AI-l√∏sninger, som tager h√∏jde for privatliv og sikkerhed, samtidig med at man str√¶ber efter at minimere bias, skadeligt indhold og misinformation, der kan underminere brugernes tillid.

Her er en liste over yderligere l√¶sning, der kan hj√¶lpe dig med bedre at forst√•, hvordan red teaming kan hj√¶lpe med at identificere og mindske risici i dine AI-systemer:

- [Planning red teaming for large language models (LLMs) and their applications](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [What is the OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming - A Key Practice for Building Safer and More Responsible AI Solutions](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en vidensbase over taktikker og teknikker, som modstandere bruger i virkelige angreb p√• AI-systemer.

## Videnstest

Hvad kunne v√¶re en god tilgang til at opretholde dataintegritet og forhindre misbrug?

1. Hav st√¶rke rollebaserede kontroller for dataadgang og datastyring  
1. Implementer og revider datam√¶rkning for at forhindre fejlagtig repr√¶sentation eller misbrug af data  
1. S√∏rg for, at din AI-infrastruktur underst√∏tter indholdsfiltrering

A:1, Selvom alle tre er gode anbefalinger, vil det at sikre, at du tildeler de rette dataadgangsrettigheder til brugerne, v√¶re et stort skridt mod at forhindre manipulation og fejlagtig repr√¶sentation af de data, som LLM‚Äôer bruger.

## üöÄ Udfordring

L√¶s mere om, hvordan du kan [styre og beskytte f√∏lsomme oplysninger](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) i AI-√¶raen.

## Godt arbejde, forts√¶t din l√¶ring

Efter at have gennemf√∏rt denne lektion, kan du tjekke vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts√¶tte med at styrke din viden om Generativ AI!

G√• videre til Lektion 14, hvor vi ser p√• [the Generative AI Application Lifecycle](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, bedes du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det oprindelige dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os intet ansvar for misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.