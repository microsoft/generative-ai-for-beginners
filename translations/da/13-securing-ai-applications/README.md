<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T19:09:30+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "da"
}
-->
# Sikring af dine generative AI-applikationer

[![Sikring af dine generative AI-applikationer](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.da.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Introduktion

Denne lektion vil d칝kke:

- Sikkerhed i konteksten af AI-systemer.
- Almindelige risici og trusler mod AI-systemer.
- Metoder og overvejelser for at sikre AI-systemer.

## L칝ringsm친l

Efter at have gennemf칮rt denne lektion vil du have forst친else for:

- Trusler og risici mod AI-systemer.
- Almindelige metoder og praksisser til at sikre AI-systemer.
- Hvordan implementering af sikkerhedstest kan forhindre uventede resultater og tab af brugertillid.

## Hvad betyder sikkerhed i konteksten af generativ AI?

Efterh친nden som kunstig intelligens (AI) og maskinl칝ring (ML) teknologier i stigende grad p친virker vores liv, er det afg칮rende at beskytte ikke kun kundedata, men ogs친 selve AI-systemerne. AI/ML bruges i stigende grad til at underst칮tte beslutningsprocesser med h칮j v칝rdi i industrier, hvor forkerte beslutninger kan f친 alvorlige konsekvenser.

Her er nogle vigtige punkter at overveje:

- **Indvirkning af AI/ML**: AI/ML har betydelig indflydelse p친 dagligdagen, og det er derfor blevet essentielt at beskytte dem.
- **Sikkerhedsudfordringer**: Den indflydelse, som AI/ML har, kr칝ver opm칝rksomhed for at adressere behovet for at beskytte AI-baserede produkter mod sofistikerede angreb, uanset om de kommer fra trollere eller organiserede grupper.
- **Strategiske problemer**: Teknologiindustrien skal proaktivt tage fat p친 strategiske udfordringer for at sikre langsigtet kundesikkerhed og datasikkerhed.

Derudover er maskinl칝ringsmodeller generelt ude af stand til at skelne mellem skadelig input og harml칮se anomale data. En betydelig kilde til tr칝ningsdata stammer fra ukurerede, umodererede, offentlige datas칝t, som er 친bne for bidrag fra tredjeparter. Angribere beh칮ver ikke kompromittere datas칝t, n친r de frit kan bidrage til dem. Over tid bliver lavtillids skadelige data til h칮jtillids betroede data, hvis datastrukturen/formatet forbliver korrekt.

Derfor er det kritisk at sikre integriteten og beskyttelsen af de datalagre, som dine modeller bruger til at tr칝ffe beslutninger.

## Forst친else af trusler og risici ved AI

N친r det kommer til AI og relaterede systemer, er datapoisoning den mest betydelige sikkerhedstrussel i dag. Datapoisoning opst친r, n친r nogen bevidst 칝ndrer de oplysninger, der bruges til at tr칝ne en AI, hvilket f친r den til at beg친 fejl. Dette skyldes manglen p친 standardiserede metoder til detektion og afhj칝lpning, kombineret med vores afh칝ngighed af utrov칝rdige eller ukurerede offentlige datas칝t til tr칝ning. For at opretholde dataintegritet og forhindre en fejlbeh칝ftet tr칝ningsproces er det afg칮rende at spore oprindelsen og afstamningen af dine data. Ellers g칝lder det gamle ordsprog "skrald ind, skrald ud", hvilket f칮rer til kompromitteret modelpr칝station.

Her er eksempler p친, hvordan datapoisoning kan p친virke dine modeller:

1. **Label Flipping**: I en bin칝r klassifikationsopgave 칝ndrer en modstander bevidst etiketterne p친 en lille del af tr칝ningsdataene. For eksempel bliver harml칮se pr칮ver m칝rket som skadelige, hvilket f친r modellen til at l칝re forkerte associationer.\
   **Eksempel**: Et spamfilter klassificerer legitime e-mails som spam p친 grund af manipulerede etiketter.
2. **Feature Poisoning**: En angriber 칝ndrer subtilt funktioner i tr칝ningsdataene for at introducere bias eller vildlede modellen.\
   **Eksempel**: Tilf칮jelse af irrelevante n칮gleord til produktbeskrivelser for at manipulere anbefalingssystemer.
3. **Data Injection**: Indspr칮jtning af skadelige data i tr칝ningss칝ttet for at p친virke modellens adf칝rd.\
   **Eksempel**: Introduktion af falske brugeranmeldelser for at sk칝vvride sentimentanalyse-resultater.
4. **Backdoor Attacks**: En modstander inds칝tter et skjult m칮nster (backdoor) i tr칝ningsdataene. Modellen l칝rer at genkende dette m칮nster og opf칮rer sig skadeligt, n친r det aktiveres.\
   **Eksempel**: Et ansigtsgenkendelsessystem tr칝net med bagd칮rbilleder, der fejlagtigt identificerer en bestemt person.

MITRE Corporation har skabt [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en vidensbase over taktikker og teknikker anvendt af modstandere i virkelige angreb p친 AI-systemer.

> Der er et stigende antal s친rbarheder i AI-aktiverede systemer, da inkorporeringen af AI 칮ger angrebsfladen for eksisterende systemer ud over traditionelle cyberangreb. Vi udviklede ATLAS for at 칮ge bevidstheden om disse unikke og udviklende s친rbarheder, da det globale samfund i stigende grad inkorporerer AI i forskellige systemer. ATLAS er modelleret efter MITRE ATT&CK춽-rammen, og dens taktikker, teknikker og procedurer (TTP'er) er komplement칝re til dem i ATT&CK.

Ligesom MITRE ATT&CK춽-rammen, som er meget brugt i traditionel cybersikkerhed til planl칝gning af avancerede trusselsimuleringer, giver ATLAS et let s칮gbart s칝t TTP'er, der kan hj칝lpe med bedre at forst친 og forberede sig p친 at forsvare sig mod nye angreb.

Derudover har Open Web Application Security Project (OWASP) skabt en "[Top 10-liste](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" over de mest kritiske s친rbarheder fundet i applikationer, der anvender LLM'er. Listen fremh칝ver risici ved trusler som den f칮rn칝vnte datapoisoning samt andre som:

- **Prompt Injection**: En teknik, hvor angribere manipulerer en Large Language Model (LLM) gennem n칮je udformede inputs, hvilket f친r den til at opf칮re sig uden for sin tilsigtede adf칝rd.
- **Forsyningsk칝des친rbarheder**: De komponenter og software, der udg칮r applikationerne, der bruges af en LLM, s친som Python-moduler eller eksterne datas칝t, kan selv kompromitteres, hvilket f칮rer til uventede resultater, introducerede bias og endda s친rbarheder i den underliggende infrastruktur.
- **Overafh칝ngighed**: LLM'er er fejlbeh칝ftede og har v칝ret tilb칮jelige til at hallucinere, hvilket resulterer i un칮jagtige eller usikre resultater. I flere dokumenterede tilf칝lde har folk taget resultaterne for gode varer, hvilket har f칮rt til utilsigtede negative konsekvenser i den virkelige verden.

Microsoft Cloud Advocate Rod Trent har skrevet en gratis e-bog, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), der g친r dybt ind i disse og andre fremvoksende AI-trusler og giver omfattende vejledning om, hvordan man bedst tackler disse scenarier.

## Sikkerhedstest for AI-systemer og LLM'er

Kunstig intelligens (AI) transformerer forskellige dom칝ner og industrier og tilbyder nye muligheder og fordele for samfundet. Men AI medf칮rer ogs친 betydelige udfordringer og risici, s친som databeskyttelse, bias, manglende forklarbarhed og potentiel misbrug. Derfor er det afg칮rende at sikre, at AI-systemer er sikre og ansvarlige, hvilket betyder, at de overholder etiske og juridiske standarder og kan stole p친 af brugere og interessenter.

Sikkerhedstest er processen med at evaluere sikkerheden af et AI-system eller LLM ved at identificere og udnytte deres s친rbarheder. Dette kan udf칮res af udviklere, brugere eller tredjepartsrevisorer, afh칝ngigt af form친let og omfanget af testen. Nogle af de mest almindelige metoder til sikkerhedstest for AI-systemer og LLM'er er:

- **Datasanering**: Dette er processen med at fjerne eller anonymisere f칮lsomme eller private oplysninger fra tr칝ningsdataene eller inputtet til et AI-system eller LLM. Datasanering kan hj칝lpe med at forhindre datal칝kage og skadelig manipulation ved at reducere eksponeringen af fortrolige eller personlige data.
- **Adversarial testing**: Dette er processen med at generere og anvende fjendtlige eksempler p친 inputtet eller outputtet af et AI-system eller LLM for at evaluere dets robusthed og modstandsdygtighed mod fjendtlige angreb. Adversarial testing kan hj칝lpe med at identificere og afhj칝lpe s친rbarheder og svagheder i et AI-system eller LLM, der kan udnyttes af angribere.
- **Modelverifikation**: Dette er processen med at verificere korrektheden og fuldst칝ndigheden af modelparametrene eller arkitekturen for et AI-system eller LLM. Modelverifikation kan hj칝lpe med at opdage og forhindre modeltyveri ved at sikre, at modellen er beskyttet og autentificeret.
- **Outputvalidering**: Dette er processen med at validere kvaliteten og p친lideligheden af outputtet fra et AI-system eller LLM. Outputvalidering kan hj칝lpe med at opdage og rette skadelig manipulation ved at sikre, at outputtet er konsistent og n칮jagtigt.

OpenAI, en leder inden for AI-systemer, har oprettet en r칝kke _sikkerhedsevalueringer_ som en del af deres red teaming-netv칝rksinitiativ, der sigter mod at teste output fra AI-systemer i h친b om at bidrage til AI-sikkerhed.

> Evalueringer kan variere fra simple Q&A-tests til mere komplekse simuleringer. Som konkrete eksempler er her pr칮veevalueringer udviklet af OpenAI til at evaluere AI-adf칝rd fra flere vinkler:

#### Overtalelse

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system narre et andet AI-system til at sige et hemmeligt ord?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system overbevise et andet AI-system om at donere penge?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system p친virke et andet AI-systems st칮tte til et politisk forslag?

#### Steganografi (skjulte beskeder)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system sende hemmelige beskeder uden at blive opdaget af et andet AI-system?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system komprimere og dekomprimere beskeder for at muligg칮re skjulte beskeder?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system koordinere med et andet AI-system uden direkte kommunikation?

### AI-sikkerhed

Det er afg칮rende, at vi str칝ber efter at beskytte AI-systemer mod skadelige angreb, misbrug eller utilsigtede konsekvenser. Dette inkluderer at tage skridt til at sikre sikkerheden, p친lideligheden og trov칝rdigheden af AI-systemer, s친som:

- Sikring af de data og algoritmer, der bruges til at tr칝ne og k칮re AI-modeller
- Forebyggelse af uautoriseret adgang, manipulation eller sabotage af AI-systemer
- Identifikation og afhj칝lpning af bias, diskrimination eller etiske problemer i AI-systemer
- Sikring af ansvarlighed, gennemsigtighed og forklarbarhed i AI-beslutninger og handlinger
- Tilpasning af AI-systemers m친l og v칝rdier med menneskers og samfundets

AI-sikkerhed er vigtig for at sikre integriteten, tilg칝ngeligheden og fortroligheden af AI-systemer og data. Nogle af udfordringerne og mulighederne ved AI-sikkerhed er:

- Mulighed: Inkorporering af AI i cybersikkerhedsstrategier, da det kan spille en afg칮rende rolle i at identificere trusler og forbedre responstider. AI kan hj칝lpe med at automatisere og forst칝rke detektion og afhj칝lpning af cyberangreb, s친som phishing, malware eller ransomware.
- Udfordring: AI kan ogs친 bruges af modstandere til at lancere sofistikerede angreb, s친som at generere falsk eller vildledende indhold, udgive sig for at v칝re brugere eller udnytte s친rbarheder i AI-systemer. Derfor har AI-udviklere et unikt ansvar for at designe systemer, der er robuste og modstandsdygtige mod misbrug.

### Databeskyttelse

LLM'er kan udg칮re risici for privatlivets fred og sikkerheden af de data, de bruger. For eksempel kan LLM'er potentielt huske og l칝kke f칮lsomme oplysninger fra deres tr칝ningsdata, s친som personlige navne, adresser, adgangskoder eller kreditkortnumre. De kan ogs친 manipuleres eller angribes af skadelige akt칮rer, der 칮nsker at udnytte deres s친rbarheder eller bias. Derfor er det vigtigt at v칝re opm칝rksom p친 disse risici og tage passende foranstaltninger for at beskytte de data, der bruges med LLM'er. Der er flere trin, du kan tage for at beskytte de data, der bruges med LLM'er. Disse trin inkluderer:

- **Begr칝nsning af m칝ngden og typen af data, der deles med LLM'er**: Del kun de data, der er n칮dvendige og relevante for de tilsigtede form친l, og undg친 at dele data, der er f칮lsomme, fortrolige eller personlige. Brugere b칮r ogs친 anonymisere eller kryptere de data, de deler med LLM'er, s친som ved at fjerne eller maskere identificerende oplysninger eller bruge sikre kommunikationskanaler.
- **Verificering af de data, som LLM'er genererer**: Kontroller altid n칮jagtigheden og kvaliteten af det output, der genereres af LLM'er, for at sikre, at det ikke indeholder u칮nskede eller upassende oplysninger.
- **Rapportering og alarmering af eventuelle databrud eller h칝ndelser**: V칝r opm칝rksom p친 mist칝nkelige eller unormale aktiviteter eller adf칝rd fra LLM'er, s친som generering af tekster, der er irrelevante, un칮jagtige, st칮dende eller skadelige. Dette kan v칝re en indikation p친 et databrud eller en sikkerhedsh칝ndelse.

Datasikkerhed, governance og overholdelse er kritisk for enhver organisation, der 칮nsker at udnytte data og AI's kraft i et multi-cloud-milj칮. At sikre og styre alle dine data er en kompleks og mangefacetteret opgave. Du skal sikre og styre forskellige typer data (strukturerede, ustrukturerede og data genereret af AI) p친 forskellige lokationer p친 tv칝rs af flere clouds, og du skal tage h칮jde for eksisterende og fremtidige datasikkerheds-, governance- og AI-regler. For at beskytte dine data skal du vedtage nogle bedste praksisser og forholdsregler, s친som:

- Brug cloud-tjenester eller platforme, der tilbyder databeskyttelses- og privatlivsfunktioner.
- Brug v칝rkt칮jer til datakvalitet og validering for at kontrollere dine data for fejl, uoverensstemmelser eller anomalier.
- Brug rammer for datastyring og etik for at sikre, at dine data bruges p친 en ansvarlig og gennemsigtig m친de.

### Simulering af virkelige trusler - AI red teaming
At efterligne trusler fra den virkelige verden betragtes nu som en standardpraksis i opbygningen af robuste AI-systemer ved at anvende lignende v칝rkt칮jer, taktikker og procedurer for at identificere risici for systemer og teste forsvarernes respons.

> Praksis med AI red teaming har udviklet sig til at f친 en mere udvidet betydning: det d칝kker ikke kun s칮gning efter sikkerhedss친rbarheder, men ogs친 s칮gning efter andre systemfejl, s친som generering af potentielt skadeligt indhold. AI-systemer medf칮rer nye risici, og red teaming er centralt for at forst친 disse nye risici, s친som prompt injection og produktion af ubegrundet indhold. - [Microsoft AI Red Team bygger fremtidens sikrere AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Vejledning og ressourcer til red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.da.png)]()

Nedenfor er n칮gleindsigter, der har formet Microsofts AI Red Team-program.

1. **Udvidet omfang af AI Red Teaming:**
   AI red teaming omfatter nu b친de sikkerhed og ansvarlig AI (RAI) resultater. Traditionelt fokuserede red teaming p친 sikkerhedsaspekter og behandlede modellen som en vektor (f.eks. tyveri af den underliggende model). Men AI-systemer introducerer nye sikkerhedss친rbarheder (f.eks. prompt injection, poisoning), som kr칝ver s칝rlig opm칝rksomhed. Ud over sikkerhed unders칮ger AI red teaming ogs친 retf칝rdighedsproblemer (f.eks. stereotyper) og skadeligt indhold (f.eks. glorificering af vold). Tidlig identifikation af disse problemer g칮r det muligt at prioritere forsvarsinvesteringer.
2. **Ondsindede og harml칮se fejl:**
   AI red teaming tager h칮jde for fejl fra b친de ondsindede og harml칮se perspektiver. For eksempel, n친r vi red teamer den nye Bing, unders칮ger vi ikke kun, hvordan ondsindede akt칮rer kan undergrave systemet, men ogs친 hvordan almindelige brugere kan st칮de p친 problematisk eller skadeligt indhold. I mods칝tning til traditionel sikkerhedsred teaming, som prim칝rt fokuserer p친 ondsindede akt칮rer, tager AI red teaming h칮jde for et bredere spektrum af personas og potentielle fejl.
3. **Dynamisk natur af AI-systemer:**
   AI-applikationer udvikler sig konstant. I applikationer med store sprogmodeller tilpasser udviklere sig til skiftende krav. Kontinuerlig red teaming sikrer l칮bende 친rv친genhed og tilpasning til udviklende risici.

AI red teaming er ikke altomfattende og b칮r betragtes som et supplement til yderligere kontroller s친som [rollebaseret adgangskontrol (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) og omfattende datastyringsl칮sninger. Det er beregnet til at supplere en sikkerhedsstrategi, der fokuserer p친 at anvende sikre og ansvarlige AI-l칮sninger, der tager h칮jde for privatliv og sikkerhed, samtidig med at man str칝ber efter at minimere bias, skadeligt indhold og misinformation, der kan underminere brugerens tillid.

Her er en liste over yderligere l칝sning, der kan hj칝lpe dig med bedre at forst친, hvordan red teaming kan hj칝lpe med at identificere og afb칮de risici i dine AI-systemer:

- [Planl칝gning af red teaming for store sprogmodeller (LLMs) og deres applikationer](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Hvad er OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - En n칮glepraksis for at bygge sikrere og mere ansvarlige AI-l칮sninger](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en vidensbase over taktikker og teknikker anvendt af modstandere i virkelige angreb p친 AI-systemer.

## Videnscheck

Hvad kunne v칝re en god tilgang til at opretholde dataintegritet og forhindre misbrug?

1. Have st칝rke rollebaserede kontroller for dataadgang og datastyring  
1. Implementere og revidere datam칝rkning for at forhindre datamisrepr칝sentation eller misbrug  
1. Sikre, at din AI-infrastruktur underst칮tter indholdsfiltrering  

A:1, Selvom alle tre er gode anbefalinger, vil det at sikre, at du tildeler de korrekte dataadgangsrettigheder til brugere, g칮re meget for at forhindre manipulation og misrepr칝sentation af de data, der bruges af LLM'er.

## 游 Udfordring

L칝s mere om, hvordan du kan [styre og beskytte f칮lsomme oplysninger](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) i AI's tidsalder.

## Godt arbejde, forts칝t din l칝ring

Efter at have afsluttet denne lektion, kan du tjekke vores [Generative AI Learning-samling](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts칝tte med at opbygge din viden om Generative AI!

G친 videre til Lektion 14, hvor vi vil se p친 [livscyklussen for generative AI-applikationer](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 n칮jagtighed, skal du v칝re opm칝rksom p친, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller fejltolkninger, der opst친r som f칮lge af brugen af denne overs칝ttelse.