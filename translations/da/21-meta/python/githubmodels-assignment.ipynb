{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byg med Meta-familien af modeller\n",
    "\n",
    "## Introduktion\n",
    "\n",
    "Denne lektion dækker:\n",
    "\n",
    "- Gennemgang af de to vigtigste modeller i Meta-familien – Llama 3.1 og Llama 3.2\n",
    "- Forståelse af anvendelsesmuligheder og scenarier for hver model\n",
    "- Eksempel på kode, der viser de unikke egenskaber ved hver model\n",
    "\n",
    "## Meta-familien af modeller\n",
    "\n",
    "I denne lektion ser vi nærmere på 2 modeller fra Meta-familien eller \"Llama-flokken\" – Llama 3.1 og Llama 3.2\n",
    "\n",
    "Disse modeller findes i forskellige varianter og kan hentes på Github Model marketplace. Her kan du læse mere om, hvordan du bruger Github Models til at [prototyping med AI-modeller](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Modelvarianter:\n",
    "- Llama 3.1 – 70B Instruct\n",
    "- Llama 3.1 – 405B Instruct\n",
    "- Llama 3.2 – 11B Vision Instruct\n",
    "- Llama 3.2 – 90B Vision Instruct\n",
    "\n",
    "*Bemærk: Llama 3 findes også på Github Models, men den bliver ikke gennemgået i denne lektion*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Med 405 milliarder parametre hører Llama 3.1 til i kategorien for open source LLM.\n",
    "\n",
    "Denne version er en opgradering af den tidligere Llama 3 og tilbyder:\n",
    "\n",
    "- Større kontekstvindue - 128k tokens mod 8k tokens\n",
    "- Større maks. output tokens - 4096 mod 2048\n",
    "- Bedre flersproget support - på grund af flere træningstokens\n",
    "\n",
    "Disse forbedringer gør Llama 3.1 i stand til at håndtere mere komplekse brugsscenarier, når man bygger GenAI-applikationer, herunder:\n",
    "- Indbygget funktionkald - muligheden for at kalde eksterne værktøjer og funktioner uden for LLM-arbejdsflowet\n",
    "- Bedre RAG-ydeevne - takket være det større kontekstvindue\n",
    "- Generering af syntetiske data - muligheden for at skabe effektiv data til opgaver som finjustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Function Calling\n",
    "\n",
    "Llama 3.1 er blevet finjusteret til at være mere effektiv til at foretage funktions- eller værktøjskald. Den har også to indbyggede værktøjer, som modellen kan identificere som nødvendige at bruge baseret på brugerens prompt. Disse værktøjer er:\n",
    "\n",
    "- **Brave Search** – Kan bruges til at hente opdateret information som vejret ved at udføre en websøgning\n",
    "- **Wolfram Alpha** – Kan bruges til mere komplekse matematiske beregninger, så du ikke behøver at skrive dine egne funktioner.\n",
    "\n",
    "Du kan også oprette dine egne brugerdefinerede værktøjer, som LLM kan kalde.\n",
    "\n",
    "I kodeeksemplet nedenfor:\n",
    "\n",
    "- Vi definerer de tilgængelige værktøjer (brave_search, wolfram_alpha) i systemprompten.\n",
    "- Sender en brugerprompt, der spørger om vejret i en bestemt by.\n",
    "- LLM’en vil svare med et værktøjskald til Brave Search-værktøjet, som vil se sådan ud: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Bemærk: Dette eksempel foretager kun værktøjskaldet. Hvis du ønsker at få resultaterne, skal du oprette en gratis konto på Brave API-siden og selv definere funktionen.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Selvom Llama 3.1 er en LLM, har den en begrænsning, når det gælder multimodalitet. Det vil sige, at den ikke kan bruge forskellige typer input som f.eks. billeder som prompts og give svar. Denne evne er en af hovedfunktionerne i Llama 3.2. Disse funktioner omfatter også:\n",
    "\n",
    "- Multimodalitet – har evnen til at evaluere både tekst- og billedprompts\n",
    "- Små til mellemstore varianter (11B og 90B) – dette giver fleksible muligheder for implementering,\n",
    "- Kun tekst-varianter (1B og 3B) – dette gør det muligt at implementere modellen på edge-/mobillenheder og giver lav latenstid\n",
    "\n",
    "Den multimodale understøttelse er et stort skridt i verden af open source-modeller. Eksemplet nedenfor tager både et billede og en tekstprompt for at få en analyse af billedet fra Llama 3.2 90B.\n",
    "\n",
    "### Multimodal understøttelse med Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Læringen stopper ikke her, fortsæt rejsen\n",
    "\n",
    "Når du har gennemført denne lektion, kan du tage et kig på vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at fortsætte med at udbygge din viden om Generativ AI!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, skal du være opmærksom på, at automatiske oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel, menneskelig oversættelse. Vi påtager os intet ansvar for misforståelser eller fejltolkninger, der måtte opstå ved brug af denne oversættelse.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:44:13+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}