<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-07-09T17:44:24+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "da"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.da.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Finjustering af dit LLM

Brugen af store sprogmodeller til at bygge generative AI-applikationer medf√∏rer nye udfordringer. En vigtig problemstilling er at sikre svarenes kvalitet (n√∏jagtighed og relevans) i det indhold, modellen genererer som svar p√• en given brugerforesp√∏rgsel. I tidligere lektioner har vi diskuteret teknikker som prompt engineering og retrieval-augmented generation, der fors√∏ger at l√∏se problemet ved at _√¶ndre prompt-inputtet_ til den eksisterende model.

I dagens lektion ser vi p√• en tredje teknik, **finjustering**, som fors√∏ger at tackle udfordringen ved at _genuddanne selve modellen_ med yderligere data. Lad os dykke ned i detaljerne.

## L√¶ringsm√•l

Denne lektion introducerer konceptet finjustering af fortr√¶nede sprogmodeller, udforsker fordelene og udfordringerne ved denne tilgang og giver vejledning i, hvorn√•r og hvordan man kan bruge finjustering til at forbedre ydeevnen af dine generative AI-modeller.

N√•r du har gennemf√∏rt denne lektion, b√∏r du kunne besvare f√∏lgende sp√∏rgsm√•l:

- Hvad er finjustering af sprogmodeller?
- Hvorn√•r og hvorfor er finjustering nyttigt?
- Hvordan kan jeg finjustere en fortr√¶net model?
- Hvad er begr√¶nsningerne ved finjustering?

Klar? Lad os komme i gang.

## Illustreret guide

Vil du have et overblik over, hvad vi skal gennemg√•, inden vi g√•r i dybden? Se denne illustrerede guide, der beskriver l√¶ringsrejsen for denne lektion ‚Äì fra at l√¶re de grundl√¶ggende begreber og motivationen for finjustering til at forst√• processen og bedste praksis for at udf√∏re finjusteringsopgaven. Det er et sp√¶ndende emne at udforske, s√• glem ikke at tjekke [Ressourcer](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) siden for yderligere links, der kan st√∏tte din selvstyrede l√¶ringsrejse!

![Illustreret guide til finjustering af sprogmodeller](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.da.png)

## Hvad er finjustering af sprogmodeller?

Per definition er store sprogmodeller _fortr√¶net_ p√• store m√¶ngder tekst hentet fra forskellige kilder, herunder internettet. Som vi har l√¶rt i tidligere lektioner, har vi brug for teknikker som _prompt engineering_ og _retrieval-augmented generation_ for at forbedre kvaliteten af modellens svar p√• brugerens sp√∏rgsm√•l ("prompter").

En popul√¶r prompt-engineering teknik indeb√¶rer at give modellen mere vejledning om, hvad der forventes i svaret, enten ved at give _instruktioner_ (eksplicit vejledning) eller _give den nogle f√• eksempler_ (implicit vejledning). Dette kaldes _few-shot learning_, men det har to begr√¶nsninger:

- Modellens token-gr√¶nser kan begr√¶nse antallet af eksempler, du kan give, og dermed begr√¶nse effektiviteten.
- Omkostningerne ved tokens kan g√∏re det dyrt at tilf√∏je eksempler til hver prompt og begr√¶nse fleksibiliteten.

Finjustering er en almindelig praksis i maskinl√¶ringssystemer, hvor man tager en fortr√¶net model og genuddanner den med nye data for at forbedre dens ydeevne p√• en specifik opgave. I forbindelse med sprogmodeller kan vi finjustere den fortr√¶nede model _med et kurateret s√¶t eksempler til en given opgave eller anvendelsesomr√•de_ for at skabe en **tilpasset model**, der kan v√¶re mere pr√¶cis og relevant for netop denne opgave eller dom√¶ne. En ekstra fordel ved finjustering er, at det ogs√• kan reducere antallet af eksempler, der er n√∏dvendige for few-shot learning ‚Äì hvilket mindsker tokenforbruget og de tilknyttede omkostninger.

## Hvorn√•r og hvorfor b√∏r vi finjustere modeller?

I _denne_ sammenh√¶ng, n√•r vi taler om finjustering, refererer vi til **superviseret** finjustering, hvor genuddannelsen sker ved at **tilf√∏je nye data**, som ikke var en del af det oprindelige tr√¶ningsdatas√¶t. Dette adskiller sig fra en usuperviseret finjusteringsmetode, hvor modellen genuddannes p√• de oprindelige data, men med forskellige hyperparametre.

Det vigtigste at huske er, at finjustering er en avanceret teknik, der kr√¶ver et vist niveau af ekspertise for at opn√• de √∏nskede resultater. Hvis det g√∏res forkert, kan det ikke give de forventede forbedringer og kan endda forringe modellens ydeevne for dit m√•lrettede dom√¶ne.

S√• f√∏r du l√¶rer "hvordan" man finjusterer sprogmodeller, skal du vide "hvorfor" du b√∏r v√¶lge denne vej, og "hvorn√•r" du skal starte finjusteringsprocessen. Start med at stille dig selv disse sp√∏rgsm√•l:

- **Brugssag**: Hvad er din _brugssag_ for finjustering? Hvilket aspekt af den nuv√¶rende fortr√¶nede model √∏nsker du at forbedre?
- **Alternativer**: Har du pr√∏vet _andre teknikker_ for at opn√• de √∏nskede resultater? Brug dem til at skabe en baseline til sammenligning.
  - Prompt engineering: Pr√∏v teknikker som few-shot prompting med eksempler p√• relevante prompt-svar. Evaluer svarenes kvalitet.
  - Retrieval Augmented Generation: Pr√∏v at supplere prompts med s√∏geresultater hentet fra dine data. Evaluer svarenes kvalitet.
- **Omkostninger**: Har du identificeret omkostningerne ved finjustering?
  - Mulighed for finjustering ‚Äì er den fortr√¶nede model tilg√¶ngelig for finjustering?
  - Arbejdsm√¶ngde ‚Äì til forberedelse af tr√¶ningsdata, evaluering og forbedring af modellen.
  - Beregning ‚Äì til at k√∏re finjusteringsjob og implementere den finjusterede model.
  - Data ‚Äì adgang til tilstr√¶kkeligt kvalitetsdata for at opn√• effekt ved finjustering.
- **Fordele**: Har du bekr√¶ftet fordelene ved finjustering?
  - Kvalitet ‚Äì overgik den finjusterede model baseline?
  - Omkostninger ‚Äì reducerer det tokenforbruget ved at forenkle prompts?
  - Udvidelsesmuligheder ‚Äì kan du genbruge basismodellen til nye dom√¶ner?

Ved at besvare disse sp√∏rgsm√•l b√∏r du kunne afg√∏re, om finjustering er den rette tilgang for din brugssag. Ideelt set er tilgangen kun gyldig, hvis fordelene opvejer omkostningerne. N√•r du har besluttet dig for at forts√¶tte, er det tid til at t√¶nke over _hvordan_ du kan finjustere den fortr√¶nede model.

Vil du have flere indsigter om beslutningsprocessen? Se [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Hvordan kan vi finjustere en fortr√¶net model?

For at finjustere en fortr√¶net model skal du have:

- en fortr√¶net model, der kan finjusteres
- et datas√¶t til brug ved finjustering
- et tr√¶ningsmilj√∏ til at k√∏re finjusteringsjobbet
- et hostingmilj√∏ til at implementere den finjusterede model

## Finjustering i praksis

F√∏lgende ressourcer giver trin-for-trin vejledninger, der guider dig gennem et konkret eksempel med en udvalgt model og et kurateret datas√¶t. For at arbejde med disse vejledninger skal du have en konto hos den specifikke udbyder samt adgang til den relevante model og datas√¶t.

| Udbyder     | Vejledning                                                                                                                                                                    | Beskrivelse                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ----------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI      | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)               | L√¶r at finjustere en `gpt-35-turbo` til et specifikt dom√¶ne ("recipe assistant") ved at forberede tr√¶ningsdata, k√∏re finjusteringsjobbet og bruge den finjusterede model til inferens.                                                                                                                                                                                                                                             |
| Azure OpenAI| [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | L√¶r at finjustere en `gpt-35-turbo-0613` model **p√• Azure** ved at oprette og uploade tr√¶ningsdata, k√∏re finjusteringsjobbet samt implementere og bruge den nye model.                                                                                                                                                                                                                                                             |
| Hugging Face| [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                              | Dette blogindl√¶g guider dig gennem finjustering af en _√•ben LLM_ (f.eks. `CodeLlama 7B`) ved hj√¶lp af [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteket og [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) med √•bne [datas√¶t](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) p√• Hugging Face. |
|             |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| ü§ó AutoTrain| [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                        | AutoTrain (eller AutoTrain Advanced) er et python-bibliotek udviklet af Hugging Face, som muligg√∏r finjustering for mange forskellige opgaver, herunder LLM finjustering. AutoTrain er en no-code l√∏sning, og finjustering kan udf√∏res i din egen cloud, p√• Hugging Face Spaces eller lokalt. Det underst√∏tter b√•de en webbaseret GUI, CLI og tr√¶ning via yaml-konfigurationsfiler.                                                                                 |
|             |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                 |

## Opgave

V√¶lg en af vejledningerne ovenfor og gennemf√∏r den. _Vi kan komme til at gengive en version af disse vejledninger i Jupyter Notebooks i dette repo som reference. Brug venligst de originale kilder direkte for at f√• de nyeste versioner_.

## Godt arbejde! Forts√¶t din l√¶ring.

Efter at have gennemf√∏rt denne lektion, kan du tjekke vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts√¶tte med at udvikle din viden om Generativ AI!

Tillykke!! Du har gennemf√∏rt den sidste lektion i v2-serien for dette kursus! Stop ikke med at l√¶re og bygge videre. \*\*Tjek [RESSOURCER](RESOURCES.md?WT.mc_id=academic-105485-koreyst) siden for en liste over yderligere forslag til netop dette emne.

Vores v1-serie af lektioner er ogs√• blevet opdateret med flere opgaver og koncepter. S√• tag et √∏jeblik til at opfriske din viden ‚Äì og del gerne [dine sp√∏rgsm√•l og feedback](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) for at hj√¶lpe os med at forbedre disse lektioner for f√¶llesskabet.

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, bedes du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det oprindelige dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os intet ansvar for misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.