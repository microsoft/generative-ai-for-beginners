<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "807f0d9fc1747e796433534e1be6a98a",
  "translation_date": "2025-10-17T19:13:18+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "da"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.da.png)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# Finjustering af din LLM

At bruge store sprogmodeller til at bygge generative AI-applikationer medf칮rer nye udfordringer. En vigtig problemstilling er at sikre kvaliteten af de svar (n칮jagtighed og relevans), som modellen genererer til en given brugerforesp칮rgsel. I tidligere lektioner har vi diskuteret teknikker som prompt engineering og retrieval-augmented generation, der fors칮ger at l칮se problemet ved _at 칝ndre prompt-input_ til den eksisterende model.

I dagens lektion diskuterer vi en tredje teknik, **finjustering**, som fors칮ger at tackle udfordringen ved _at genuddanne selve modellen_ med yderligere data. Lad os dykke ned i detaljerne.

## L칝ringsm친l

Denne lektion introducerer konceptet finjustering af fortr칝nede sprogmodeller, udforsker fordelene og udfordringerne ved denne tilgang og giver vejledning om, hvorn친r og hvordan man kan bruge finjustering til at forbedre ydeevnen af dine generative AI-modeller.

Ved slutningen af denne lektion b칮r du kunne besvare f칮lgende sp칮rgsm친l:

- Hvad er finjustering af sprogmodeller?
- Hvorn친r og hvorfor er finjustering nyttigt?
- Hvordan kan jeg finjustere en fortr칝net model?
- Hvad er begr칝nsningerne ved finjustering?

Klar? Lad os komme i gang.

## Illustreret guide

Vil du have det store overblik over, hvad vi d칝kker, f칮r vi dykker ned? Tjek denne illustrerede guide, der beskriver l칝ringsrejsen for denne lektion - fra at l칝re de grundl칝ggende begreber og motivationen for finjustering til at forst친 processen og bedste praksis for at udf칮re finjusteringsopgaven. Dette er et fascinerende emne at udforske, s친 glem ikke at tjekke [Ressourcer](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) siden for yderligere links til at st칮tte din selvstyrede l칝ringsrejse!

![Illustreret guide til finjustering af sprogmodeller](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.da.png)

## Hvad er finjustering af sprogmodeller?

Per definition er store sprogmodeller _fortr칝nede_ p친 store m칝ngder tekst hentet fra forskellige kilder, herunder internettet. Som vi har l칝rt i tidligere lektioner, har vi brug for teknikker som _prompt engineering_ og _retrieval-augmented generation_ for at forbedre kvaliteten af modellens svar p친 brugerens sp칮rgsm친l ("prompts").

En popul칝r prompt-engineering teknik involverer at give modellen mere vejledning om, hvad der forventes i svaret, enten ved at give _instruktioner_ (eksplicit vejledning) eller _give den nogle eksempler_ (implicit vejledning). Dette kaldes _few-shot learning_, men det har to begr칝nsninger:

- Modellens token-gr칝nser kan begr칝nse antallet af eksempler, du kan give, og begr칝nse effektiviteten.
- Modellens token-omkostninger kan g칮re det dyrt at tilf칮je eksempler til hver prompt og begr칝nse fleksibiliteten.

Finjustering er en almindelig praksis i maskinl칝ringssystemer, hvor vi tager en fortr칝net model og genuddanner den med nye data for at forbedre dens ydeevne p친 en specifik opgave. I konteksten af sprogmodeller kan vi finjustere den fortr칝nede model _med et kurateret s칝t eksempler til en given opgave eller applikationsdom칝ne_ for at skabe en **tilpasset model**, der kan v칝re mere pr칝cis og relevant for den specifikke opgave eller det specifikke dom칝ne. En sidegevinst ved finjustering er, at det ogs친 kan reducere antallet af eksempler, der er n칮dvendige for few-shot learning - hvilket reducerer tokenforbrug og relaterede omkostninger.

## Hvorn친r og hvorfor b칮r vi finjustere modeller?

I _denne_ kontekst, n친r vi taler om finjustering, refererer vi til **superviseret** finjustering, hvor genuddannelsen sker ved **at tilf칮je nye data**, der ikke var en del af det oprindelige tr칝ningsdatas칝t. Dette er anderledes end en usuperviseret finjusteringsmetode, hvor modellen genuddannes p친 det oprindelige data, men med forskellige hyperparametre.

Det vigtigste at huske er, at finjustering er en avanceret teknik, der kr칝ver et vist niveau af ekspertise for at opn친 de 칮nskede resultater. Hvis det g칮res forkert, kan det muligvis ikke give de forventede forbedringer og kan endda forringe modellens ydeevne for dit m친lrettede dom칝ne.

S친 f칮r du l칝rer "hvordan" man finjusterer sprogmodeller, skal du vide "hvorfor" du b칮r tage denne vej, og "hvorn친r" du skal starte processen med finjustering. Start med at stille dig selv disse sp칮rgsm친l:

- **Brugsscenarie**: Hvad er dit _brugsscenarie_ for finjustering? Hvilket aspekt af den nuv칝rende fortr칝nede model 칮nsker du at forbedre?
- **Alternativer**: Har du pr칮vet _andre teknikker_ for at opn친 de 칮nskede resultater? Brug dem til at skabe en baseline for sammenligning.
  - Prompt engineering: Pr칮v teknikker som few-shot prompting med eksempler p친 relevante prompt-svar. Evaluer kvaliteten af svarene.
  - Retrieval Augmented Generation: Pr칮v at udvide prompts med foresp칮rgselsresultater hentet ved at s칮ge i dine data. Evaluer kvaliteten af svarene.
- **Omkostninger**: Har du identificeret omkostningerne ved finjustering?
  - Justerbarhed - er den fortr칝nede model tilg칝ngelig for finjustering?
  - Indsats - for at forberede tr칝ningsdata, evaluere og forfine modellen.
  - Beregning - for at k칮re finjusteringsjobs og implementere den finjusterede model.
  - Data - adgang til tilstr칝kkelige kvalitets-eksempler for finjusteringsp친virkning.
- **Fordele**: Har du bekr칝ftet fordelene ved finjustering?
  - Kvalitet - overgik den finjusterede model baseline?
  - Omkostninger - reducerer det tokenforbrug ved at forenkle prompts?
  - Udvidelsesmuligheder - kan du genbruge basismodellen til nye dom칝ner?

Ved at besvare disse sp칮rgsm친l b칮r du kunne afg칮re, om finjustering er den rigtige tilgang for dit brugsscenarie. Ideelt set er tilgangen kun gyldig, hvis fordelene opvejer omkostningerne. N친r du beslutter dig for at forts칝tte, er det tid til at t칝nke over _hvordan_ du kan finjustere den fortr칝nede model.

Vil du have flere indsigter i beslutningsprocessen? Se [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Hvordan kan vi finjustere en fortr칝net model?

For at finjustere en fortr칝net model skal du have:

- en fortr칝net model til finjustering
- et datas칝t til brug for finjustering
- et tr칝ningsmilj칮 til at k칮re finjusteringsjobbet
- et hostingmilj칮 til at implementere den finjusterede model

## Finjustering i praksis

F칮lgende ressourcer giver trin-for-trin vejledninger til at guide dig gennem et reelt eksempel ved hj칝lp af en udvalgt model med et kurateret datas칝t. For at arbejde gennem disse vejledninger skal du have en konto hos den specifikke udbyder samt adgang til den relevante model og datas칝t.

| Udbyder      | Vejledning                                                                                                                                                                    | Beskrivelse                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                | L칝r at finjustere en `gpt-35-turbo` til et specifikt dom칝ne ("opskriftassistent") ved at forberede tr칝ningsdata, k칮re finjusteringsjobbet og bruge den finjusterede model til inferens.                                                                                                                                                                                                                                              |
| Azure OpenAI | [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | L칝r at finjustere en `gpt-35-turbo-0613` model **p친 Azure** ved at tage skridt til at oprette og uploade tr칝ningsdata, k칮re finjusteringsjobbet. Implementer og brug den nye model.                                                                                                                                                                                                                                                 |
| Hugging Face | [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Denne blogpost guider dig gennem finjustering af en _친ben LLM_ (fx `CodeLlama 7B`) ved hj칝lp af [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteket & [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) med 친bne [datas칝t](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) p친 Hugging Face. |
|              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 游뱅 AutoTrain | [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                         | AutoTrain (eller AutoTrain Advanced) er et Python-bibliotek udviklet af Hugging Face, der muligg칮r finjustering for mange forskellige opgaver, herunder LLM finjustering. AutoTrain er en no-code l칮sning, og finjustering kan udf칮res i din egen cloud, p친 Hugging Face Spaces eller lokalt. Det underst칮tter b친de en webbaseret GUI, CLI og tr칝ning via yaml-konfigurationsfiler.                                                       |
|              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Opgave

V칝lg en af vejledningerne ovenfor og gennemg친 dem. _Vi kan muligvis replikere en version af disse vejledninger i Jupyter Notebooks i dette repo kun til reference. Brug venligst de originale kilder direkte for at f친 de nyeste versioner_.

## Godt arbejde! Forts칝t din l칝ring.

Efter at have afsluttet denne lektion, tjek vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts칝tte med at opbygge din viden om Generative AI!

Tillykke!! Du har afsluttet den sidste lektion fra v2-serien for dette kursus! Stop ikke med at l칝re og bygge. \*\*Tjek [RESSOURCER](RESOURCES.md?WT.mc_id=academic-105485-koreyst) siden for en liste over yderligere forslag til netop dette emne.

Vores v1-serie af lektioner er ogs친 blevet opdateret med flere opgaver og begreber. S친 tag et 칮jeblik til at genopfriske din viden - og venligst [del dine sp칮rgsm친l og feedback](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) for at hj칝lpe os med at forbedre disse lektioner for f칝llesskabet.

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 n칮jagtighed, skal det bem칝rkes, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller fejltolkninger, der opst친r som f칮lge af brugen af denne overs칝ttelse.