[![Open Source Models](../../../translated_images/da/18-lesson-banner.f30176815b1a5074.webp)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# Finjustering af dit LLM

Brugen af store sprogmodeller til at bygge generative AI-applikationer kommer med nye udfordringer. Et n√∏gleproblem er at sikre responskvaliteten (n√∏jagtighed og relevans) i det indhold, som modellen genererer til en given brugerforesp√∏rgsel. I tidligere lektioner har vi diskuteret teknikker som prompt engineering og retrieval-augmented generation, der fors√∏ger at l√∏se problemet ved at _√¶ndre promptinputtet_ til den eksisterende model.

I dagens lektion diskuterer vi en tredje teknik, **finjustering**, som fors√∏ger at adressere udfordringen ved at _genuddanne selve modellen_ med yderligere data. Lad os g√• i detaljer.

## L√¶ringsm√•l

Denne lektion introducerer konceptet finjustering for fortr√¶nede sprogmodeller, udforsker fordelene og udfordringerne ved denne tilgang og giver vejledning om, hvorn√•r og hvordan man bruger finjustering til at forbedre ydeevnen af dine generative AI-modeller.

Ved slutningen af denne lektion b√∏r du kunne besvare f√∏lgende sp√∏rgsm√•l:

- Hvad er finjustering for sprogmodeller?
- Hvorn√•r og hvorfor er finjustering nyttig?
- Hvordan kan jeg finjustere en fortr√¶net model?
- Hvad er begr√¶nsningerne ved finjustering?

Klar? Lad os komme i gang.

## Illustreret guide

Vil du have overblikket over, hvad vi skal d√¶kke, inden vi dykker ned? Se denne illustrerede guide, der beskriver l√¶ringsrejsen for denne lektion ‚Äì fra at l√¶re kernekoncepter og motivation for finjustering til at forst√• processen og bedste praksis for udf√∏relse af finjusteringsopgaven. Dette er et fascinerende emne at udforske, s√• glem ikke at tjekke [Ressourcer](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) siden for yderligere links, der st√∏tter din selvstyrede l√¶ringsrejse!

![Illustreret guide til finjustering af sprogmodeller](../../../translated_images/da/18-fine-tuning-sketchnote.11b21f9ec8a70346.webp)

## Hvad er finjustering for sprogmodeller?

Efter definition er store sprogmodeller _fortr√¶net_ p√• store m√¶ngder tekst hentet fra forskellige kilder, inklusive internettet. Som vi har l√¶rt i tidligere lektioner, har vi brug for teknikker som _prompt engineering_ og _retrieval-augmented generation_ for at forbedre kvaliteten af modellens svar p√• brugerens sp√∏rgsm√•l ("prompts").

En popul√¶r prompt-engineering teknik indeb√¶rer at give modellen mere vejledning om, hvad der forventes i svaret, enten ved at give _instruktioner_ (eksplicit vejledning) eller _give den nogle f√• eksempler_ (implicit vejledning). Dette kaldes _few-shot learning_, men det har to begr√¶nsninger:

- Modellens token-gr√¶nser kan begr√¶nse antallet af eksempler, du kan give, og dermed begr√¶nse effektiviteten.
- Modellens token-omkostninger kan g√∏re det dyrt at tilf√∏je eksempler til hver prompt og begr√¶nse fleksibiliteten.

Finjustering er en almindelig praksis inden for maskinl√¶ringssystemer, hvor vi tager en fortr√¶net model og genuddanner den med nye data for at forbedre dens ydeevne p√• en specifik opgave. I konteksten af sprogmodeller kan vi finjustere den fortr√¶nede model _med et kurateret s√¶t eksempler til en given opgave eller anvendelsesomr√•de_ for at skabe en **specialmodel**, som kan v√¶re mere n√∏jagtig og relevant for netop den opgave eller det dom√¶ne. En sidegevinst ved finjustering er, at det ogs√• kan reducere antallet af eksempler, der er n√∏dvendige for few-shot learning ‚Äì hvilket mindsker token-brug og relaterede omkostninger.

## Hvorn√•r og hvorfor skal vi finjustere modeller?

I _denne_ kontekst, n√•r vi taler om finjustering, refererer vi til **overv√•get** finjustering, hvor genuddannelsen sker ved at **tilf√∏je nye data**, som ikke var en del af det oprindelige tr√¶ningsdatas√¶t. Dette adskiller sig fra en ikke-overv√•get finjusteringsmetode, hvor modellen genuddannes p√• det oprindelige datas√¶t, men med forskellige hyperparametre.

Det vigtigste at huske er, at finjustering er en avanceret teknik, der kr√¶ver et vist niveau af ekspertise for at opn√• de √∏nskede resultater. Hvis det udf√∏res forkert, kan det undlade at give de forventede forbedringer og endda forringe modellens ydeevne for dit m√•lrettede dom√¶ne.

S√• inden du l√¶rer "hvordan" man finjusterer sprogmodeller, skal du vide, "hvorfor" du skal v√¶lge denne metode, og "hvorn√•r" du skal starte finjusteringsprocessen. Start med at stille dig selv disse sp√∏rgsm√•l:

- **Use case**: Hvad er dit _brugsscenarie_ for finjustering? Hvilket aspekt af den nuv√¶rende fortr√¶nede model √∏nsker du at forbedre?
- **Alternativer**: Har du pr√∏vet _andre teknikker_ for at opn√• de √∏nskede resultater? Brug dem som baseline til sammenligning.
  - Prompt engineering: Pr√∏v teknikker som few-shot prompting med eksempler p√• relevante prompt-svar. Evaluer kvaliteten af svarene.
  - Retrieval Augmented Generation: Pr√∏v at forst√¶rke prompts med s√∏geresultater udvundet fra dine data. Evaluer kvaliteten af svarene.
- **Omkostninger**: Har du identificeret omkostningerne ved finjustering?
  - Justerbarhed ‚Äì er den fortr√¶nede model tilg√¶ngelig for finjustering?
  - Indsats ‚Äì til forberedelse af tr√¶ningsdata, evaluering og forfining af modellen.
  - Beregning ‚Äì til k√∏rsel af finjusteringsjob og implementering af finjusteret model.
  - Data ‚Äì adgang til tilstr√¶kkeligt kvalitets-eksempler for at f√• effekt af finjustering.
- **Fordele**: Har du bekr√¶ftet fordelene ved finjustering?
  - Kvalitet ‚Äì overgik den finjusterede model baseline?
  - Omkostning ‚Äì reducerer den token-brug ved at forenkle prompts?
  - Udvidelsesmulighed ‚Äì kan du genanvende basismodellen til nye dom√¶ner?

Ved at besvare disse sp√∏rgsm√•l b√∏r du kunne beslutte, om finjustering er den rette tilgang til dit brugsscenarie. Ideelt set er tilgangen kun gyldig, hvis fordelene opvejer omkostningerne. N√•r du beslutter at forts√¶tte, er det tid til at t√¶nke p√• _hvordan_ du kan finjustere den fortr√¶nede model.

Vil du have flere indsigter om beslutningsprocessen? Se [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Hvordan kan vi finjustere en fortr√¶net model?

For at finjustere en fortr√¶net model skal du have:

- en fortr√¶net model til finjustering
- et datas√¶t til brug ved finjustering
- et tr√¶ningsmilj√∏ til at k√∏re finjusteringsjobbet
- et hostingmilj√∏ til at implementere den finjusterede model

## Finjustering i praksis

F√∏lgende ressourcer giver trin-for-trin vejledninger, der f√∏rer dig gennem et konkret eksempel med en udvalgt model og et kurateret datas√¶t. For at arbejde med disse vejledninger har du brug for en konto hos den specifikke udbyder samt adgang til den relevante model og datas√¶t.

| Udbyder     | Vejledning                                                                                                                                                                    | Beskrivelse                                                                                                                                                                                                                                                                                                                                                                                                                      |
| ------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)               | L√¶r at finjustere en `gpt-35-turbo` til et specifikt dom√¶ne ("recipe assistant") ved at forberede tr√¶ningsdata, k√∏re finjusteringsjobbet og bruge den finjusterede model til inferens.                                                                                                                                                                                                                                          |
| Azure OpenAI | [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | L√¶r at finjustere en `gpt-35-turbo-0613` model **p√• Azure** ved at tage skridt til at oprette og uploade tr√¶ningsdata, k√∏re finjusteringsjobbet. Implementer og brug den nye model.                                                                                                                                                                                                                                               |
| Hugging Face | [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                              | Dette blogindl√¶g guider dig gennem finjustering af en _√•ben LLM_ (f.eks. `CodeLlama 7B`) ved hj√¶lp af [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteket & [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) med √•bne [datas√¶t](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) p√• Hugging Face. |
|              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ü§ó AutoTrain | [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                          | AutoTrain (eller AutoTrain Advanced) er et python-bibliotek udviklet af Hugging Face, som muligg√∏r finjustering for mange forskellige opgaver inklusive LLM finjustering. AutoTrain er en no-code l√∏sning, og finjustering kan udf√∏res i din egen cloud, p√• Hugging Face Spaces eller lokalt. Det underst√∏tter b√•de en webbaseret GUI, CLI og tr√¶ning via yaml-konfigurationsfiler.                                                               |
|              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ü¶• Unsloth   | [Fine-tuning LLMs with Unsloth](https://github.com/unslothai/unsloth)                                                                                                          | Unsloth er et open-source framework, der underst√∏tter LLM finjustering og reinforcement learning (RL). Unsloth str√∏mliner lokal tr√¶ning, evaluering og deployment med klar-til-brug [notebooks](https://github.com/unslothai/notebooks). Det underst√∏tter ogs√• tekst-til-tale (TTS), BERT og multimodale modeller. For at komme i gang, l√¶s deres trin-for-trin [Finjustering af LLM'er Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide).                            |
|              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
## Opgave

V√¶lg en af vejledningerne ovenfor og gennemg√• den. _Vi kan kopiere en version af disse vejledninger i Jupyter Notebooks i dette repo som reference. Brug venligst de originale kilder direkte for at f√• de nyeste versioner_.

## Godt arbejde! Forts√¶t din l√¶ring.

Efter at have gennemf√∏rt denne lektion, kan du tjekke vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts√¶tte med at styrke din generative AI-viden!

Tillykke!! Du har gennemf√∏rt den sidste lektion i v2-serien til dette kursus! Stop ikke med at l√¶re og bygge videre. \*\*Tjek [RESSOURCER](RESOURCES.md?WT.mc_id=academic-105485-koreyst) siden for en liste over yderligere forslag til lige netop dette emne.

Vores v1-serie af lektioner er ogs√• blevet opdateret med flere opgaver og koncepter. S√• tag et √∏jeblik til at friske din viden op ‚Äì og del venligst [dine sp√∏rgsm√•l og feedback](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) for at hj√¶lpe os med at forbedre disse lektioner for f√¶llesskabet.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfraskrivelse**:
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, bedes du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os intet ansvar for misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->