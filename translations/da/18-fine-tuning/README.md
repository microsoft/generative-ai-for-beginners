<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-05-20T07:50:13+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "da"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.8487555c3e3225eefc1dc84e72c8e00bce1ee76db867a080628fb0fbb04aa0d2.da.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Finjustering af din LLM

Brugen af store sprogmodeller til at bygge generative AI-applikationer kommer med nye udfordringer. Et centralt problem er at sikre kvaliteten af svarene (n칮jagtighed og relevans) i indholdet, som modellen genererer for en given brugerforesp칮rgsel. I tidligere lektioner har vi diskuteret teknikker som prompt engineering og retrieval-augmented generation, der fors칮ger at l칮se problemet ved at _칝ndre prompt input_ til den eksisterende model.

I dagens lektion diskuterer vi en tredje teknik, **finjustering**, som fors칮ger at im칮deg친 udfordringen ved _at genoptr칝ne selve modellen_ med yderligere data. Lad os dykke ned i detaljerne.

## L칝ringsm친l

Denne lektion introducerer konceptet finjustering for forudtr칝nede sprogmodeller, udforsker fordelene og udfordringerne ved denne tilgang og giver vejledning om, hvorn친r og hvordan man bruger finjustering til at forbedre ydeevnen af dine generative AI-modeller.

Ved slutningen af denne lektion skal du kunne besvare f칮lgende sp칮rgsm친l:

- Hvad er finjustering for sprogmodeller?
- Hvorn친r, og hvorfor, er finjustering nyttig?
- Hvordan kan jeg finjustere en forudtr칝net model?
- Hvad er begr칝nsningerne ved finjustering?

Klar? Lad os komme i gang.

## Illustreret Guide

Vil du have det store billede af, hvad vi vil d칝kke, f칮r vi dykker ind? Se denne illustrerede guide, der beskriver l칝ringsrejsen for denne lektion - fra at l칝re de grundl칝ggende begreber og motivationen for finjustering, til at forst친 processen og bedste praksis for at udf칮re finjusteringsopgaven. Dette er et fascinerende emne at udforske, s친 glem ikke at tjekke [Ressourcer](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) siden for yderligere links til at st칮tte din selvstyrede l칝ringsrejse!

![Illustreret Guide til Finjustering af Sprogmodeller](../../../translated_images/18-fine-tuning-sketchnote.92733966235199dd260184b1aae3a84b877c7496bc872d8e63ad6fa2dd96bafc.da.png)

## Hvad er finjustering for sprogmodeller?

Per definition er store sprogmodeller _forudtr칝nede_ p친 store m칝ngder tekst hentet fra diverse kilder, herunder internettet. Som vi har l칝rt i tidligere lektioner, har vi brug for teknikker som _prompt engineering_ og _retrieval-augmented generation_ for at forbedre kvaliteten af modellens svar p친 brugerens sp칮rgsm친l ("prompts").

En popul칝r prompt-engineering teknik involverer at give modellen mere vejledning om, hvad der forventes i svaret enten ved at give _instruktioner_ (eksplicit vejledning) eller _give den nogle eksempler_ (implicit vejledning). Dette kaldes _few-shot learning_, men det har to begr칝nsninger:

- Model token gr칝nser kan begr칝nse antallet af eksempler, du kan give, og begr칝nse effektiviteten.
- Model token omkostninger kan g칮re det dyrt at tilf칮je eksempler til hver prompt og begr칝nse fleksibiliteten.

Finjustering er en almindelig praksis i maskinl칝ringssystemer, hvor vi tager en forudtr칝net model og genoptr칝ner den med nye data for at forbedre dens ydeevne p친 en specifik opgave. I konteksten af sprogmodeller kan vi finjustere den forudtr칝nede model _med et kurateret s칝t eksempler for en given opgave eller applikationsdom칝ne_ for at skabe en **custom model**, der m친ske er mere pr칝cis og relevant for den specifikke opgave eller dom칝ne. En sidefordel ved finjustering er, at det ogs친 kan reducere antallet af eksempler, der er n칮dvendige for few-shot learning - hvilket reducerer tokenbrug og relaterede omkostninger.

## Hvorn친r og hvorfor skal vi finjustere modeller?

I _denne_ kontekst, n친r vi taler om finjustering, henviser vi til **supervised** finjustering, hvor genoptr칝ningen sker ved **tilf칮jelse af nye data**, der ikke var en del af det originale tr칝ningsdatas칝t. Dette er forskelligt fra en unsupervised finjustering tilgang, hvor modellen genoptr칝nes p친 de originale data, men med forskellige hyperparametre.

Det vigtigste at huske er, at finjustering er en avanceret teknik, der kr칝ver et vist niveau af ekspertise for at opn친 de 칮nskede resultater. Hvis det g칮res forkert, kan det m친ske ikke give de forventede forbedringer og kan endda forringe modellens ydeevne for dit m친lrettede dom칝ne.

S친 f칮r du l칝rer "hvordan" man finjusterer sprogmodeller, skal du vide "hvorfor" du skal tage denne vej, og "hvorn친r" du skal starte processen med finjustering. Start med at stille dig selv disse sp칮rgsm친l:

- **Use Case**: Hvad er din _use case_ for finjustering? Hvilken aspekt af den nuv칝rende forudtr칝nede model 칮nsker du at forbedre?
- **Alternatives**: Har du pr칮vet _andre teknikker_ for at opn친 de 칮nskede resultater? Brug dem til at skabe en baseline til sammenligning.
  - Prompt engineering: Pr칮v teknikker som few-shot prompting med eksempler p친 relevante prompt svar. Evaluer kvaliteten af svarene.
  - Retrieval Augmented Generation: Pr칮v at udvide prompts med foresp칮rgselsresultater hentet ved at s칮ge i dine data. Evaluer kvaliteten af svarene.
- **Costs**: Har du identificeret omkostningerne ved finjustering?
  - Tunability - er den forudtr칝nede model tilg칝ngelig for finjustering?
  - Effort - for at forberede tr칝ningsdata, evaluere & forfine model.
  - Compute - for at k칮re finjusteringsjob og implementere finjusteret model
  - Data - adgang til tilstr칝kkelige kvalitets eksempler for finjusteringsp친virkning
- **Benefits**: Har du bekr칝ftet fordelene ved finjustering?
  - Quality - overgik den finjusterede model baseline?
  - Cost - reducerer det tokenbrug ved at forenkle prompts?
  - Extensibility - kan du genbruge basismodellen til nye dom칝ner?

Ved at besvare disse sp칮rgsm친l b칮r du kunne beslutte, om finjustering er den rigtige tilgang for din use case. Ideelt set er tilgangen kun gyldig, hvis fordelene opvejer omkostningerne. N친r du beslutter dig for at forts칝tte, er det tid til at t칝nke over _hvordan_ du kan finjustere den forudtr칝nede model.

Vil du have flere indsigter i beslutningsprocessen? Se [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Hvordan kan vi finjustere en forudtr칝net model?

For at finjustere en forudtr칝net model skal du have:

- en forudtr칝net model til finjustering
- et datas칝t til brug for finjustering
- et tr칝ningsmilj칮 til at k칮re finjusteringsjobbet
- et hostingmilj칮 til at implementere den finjusterede model

## Finjustering i aktion

F칮lgende ressourcer giver trin-for-trin vejledninger til at guide dig gennem et reelt eksempel ved hj칝lp af en valgt model med et kurateret datas칝t. For at arbejde gennem disse vejledninger skal du have en konto hos den specifikke udbyder, sammen med adgang til den relevante model og datas칝t.

| Udbyder      | Tutorial                                                                                                                                                                       | Beskrivelse                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                | L칝r at finjustere en `gpt-35-turbo` for et specifikt dom칝ne ("recipe assistant") ved at forberede tr칝ningsdata, k칮re finjusteringsjobbet og bruge den finjusterede model til inferens.                                                                                                                                                                                                                                              |
| Azure OpenAI | [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | L칝r at finjustere en `gpt-35-turbo-0613` model **p친 Azure** ved at tage skridt til at oprette & uploade tr칝ningsdata, k칮re finjusteringsjobbet. Implementer & brug den nye model.                                                                                                                                                                                                                                                                 |
| Hugging Face | [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Dette blogindl칝g guider dig i finjustering af en _open LLM_ (fx `CodeLlama 7B`) ved hj칝lp af [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteket & [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) med 친bne [datasets](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) p친 Hugging Face. |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 游뱅 AutoTrain | [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                         | AutoTrain (eller AutoTrain Advanced) er et python-bibliotek udviklet af Hugging Face, der tillader finjustering for mange forskellige opgaver, inklusive LLM finjustering. AutoTrain er en no-code l칮sning, og finjustering kan g칮res i din egen cloud, p친 Hugging Face Spaces eller lokalt. Det underst칮tter b친de en web-baseret GUI, CLI og tr칝ning via yaml konfigurationsfiler.                                                                               |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Opgave

V칝lg en af vejledningerne ovenfor og gennemg친 dem. _Vi kan muligvis replikere en version af disse vejledninger i Jupyter Notebooks i dette repo kun til reference. Brug venligst de originale kilder direkte for at f친 de nyeste versioner_.

## Godt arbejde! Forts칝t din l칝ring.

Efter at have afsluttet denne lektion, tjek vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts칝tte med at opbygge din viden om Generativ AI!

Tillykke!! Du har gennemf칮rt den sidste lektion fra v2 serien for dette kursus! Stop ikke med at l칝re og bygge. \*\*Tjek [RESSOURCER](RESOURCES.md?WT.mc_id=academic-105485-koreyst) siden for en liste over yderligere forslag til netop dette emne.

Vores v1 serie af lektioner er ogs친 blevet opdateret med flere opgaver og koncepter. S친 tag et 칮jeblik til at opfriske din viden - og venligst [del dine sp칮rgsm친l og feedback](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) for at hj칝lpe os med at forbedre disse lektioner for f칝llesskabet.

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Mens vi bestr칝ber os p친 at sikre n칮jagtighed, bedes du v칝re opm칝rksom p친, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller fejltolkninger, der m친tte opst친 ved brug af denne overs칝ttelse.