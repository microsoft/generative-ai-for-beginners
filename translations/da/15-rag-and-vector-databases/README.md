# Retrieval Augmented Generation (RAG) og Vektordatabaser

[![Retrieval Augmented Generation (RAG) and Vector Databases](../../../translated_images/da/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

I s√∏geapplikationslektionen l√¶rte vi kort, hvordan man integrerer egne data i Large Language Models (LLMs). I denne lektion vil vi dykke dybere ned i begreberne omkring at forankre dine data i din LLM-applikation, mekanikkerne i processen og metoderne til lagring af data, herunder b√•de embeddings og tekst.

> **Video Kommer Snart**

## Introduktion

I denne lektion vil vi d√¶kke f√∏lgende:

- En introduktion til RAG, hvad det er, og hvorfor det bruges inden for AI (kunstig intelligens).

- Forst√•else af, hvad vektordatabaser er, og hvordan man opretter en til vores applikation.

- Et praktisk eksempel p√•, hvordan man integrerer RAG i en applikation.

## L√¶ringsm√•l

Efter at have gennemf√∏rt denne lektion vil du kunne:

- Forklare betydningen af RAG i dataudtr√¶k og behandling.

- Ops√¶tte en RAG-applikation og forankre dine data til en LLM.

- Effektiv integration af RAG og vektordatabaser i LLM-applikationer.

## Vores scenarie: forbedring af vores LLMs med egne data

Til denne lektion √∏nsker vi at tilf√∏je vores egne notater til uddannelsesstartuppen, hvilket g√∏r det muligt for chatbotten at f√• mere information om de forskellige emner. Brug af de notater, vi har, vil elever kunne studere bedre og forst√• de forskellige emner, hvilket g√∏r det nemmere at forberede sig til deres eksamener. For at skabe vores scenarie vil vi bruge:

- `Azure OpenAI:` den LLM, vi vil bruge til at oprette vores chatbot

- `AI for beginners' lesson on Neural Networks:` dette vil v√¶re de data, vi forankrer vores LLM p√•

- `Azure AI Search` og `Azure Cosmos DB:` vektordatabaser til at gemme vores data og oprette et s√∏geindeks

Brugere vil kunne oprette √∏vequizzer ud fra deres notater, revisionsflashkort og opsummere det til koncise overblik. For at komme i gang, lad os se p√•, hvad RAG er, og hvordan det fungerer:

## Retrieval Augmented Generation (RAG)

En LLM-drevet chatbot behandler brugerens prompts for at generere svar. Den er designet til at v√¶re interaktiv og engagerer sig med brugere om en bred vifte af emner. Dog er dens svar begr√¶nset til den kontekst, der er givet, og det grundl√¶ggende tr√¶ningsdata. For eksempel har GPT-4 en vidensafsk√¶ring i september 2021, hvilket betyder, at den mangler viden om begivenheder, der er sket efter denne periode. Derudover udelukker dataene, som bruges til at tr√¶ne LLM'er, fortrolig information som personlige notater eller en virksomheds produktmanual.

### Hvordan RAG (Retrieval Augmented Generation) fungerer

![drawing showing how RAGs work](../../../translated_images/da/how-rag-works.f5d0ff63942bd3a6.webp)

Antag, at du √∏nsker at implementere en chatbot, der laver quizzer ud fra dine notater; du vil have brug for en forbindelse til vidensbasen. Dette er, hvor RAG kommer til unds√¶tning. RAG‚Äôer fungerer som f√∏lger:

- **Vidensbase:** F√∏r udtr√¶kning skal disse dokumenter indtastes og forbehandles, typisk ved at opdele store dokumenter i mindre bidder, omdanne dem til tekstembedding og gemme dem i en database.

- **Brugerforesp√∏rgsel:** brugeren stiller et sp√∏rgsm√•l

- **Udtr√¶kning:** N√•r en bruger stiller et sp√∏rgsm√•l, henter embeddings-modellen relevant information fra vores vidensbase for at give mere kontekst, som inkorporeres i prompten.

- **Forst√¶rket generering:** LLM‚Äôen forbedrer sit svar baseret p√• de hentede data. Det g√∏r det muligt, at det genererede svar ikke kun baseres p√• forudtr√¶nede data men ogs√• relevant information fra den tilf√∏jede kontekst. De hentede data bruges til at forst√¶rke LLM‚Äôens svar. LLM‚Äôen returnerer derefter et svar p√• brugerens sp√∏rgsm√•l.

![drawing showing how RAGs architecture](../../../translated_images/da/encoder-decode.f2658c25d0eadee2.webp)

Arkitekturen for RAG‚Äôer implementeres ved hj√¶lp af transformere best√•ende af to dele: en encoder og en decoder. For eksempel, n√•r en bruger stiller et sp√∏rgsm√•l, 'kodes' inputteksten til vektorer, der fanger ordenes betydning, og vektorerne 'afkodes' i vores dokumentindeks og genererer ny tekst baseret p√• brugerens foresp√∏rgsel. LLM‚Äôen bruger b√•de en encoder-decoder-model til at generere output.

To tilgange ved implementering af RAG if√∏lge det foresl√•ede papir: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) er:

- **_RAG-Sequence_** bruger hentede dokumenter til at forudsige det bedst mulige svar p√• en brugersp√∏rgsel

- **RAG-Token** bruger dokumenter til at generere den n√¶ste token, og henter derefter flere for at besvare brugerens foresp√∏rgsel

### Hvorfor skulle du bruge RAG?

- **Informationsrigdom:** sikrer, at tekstsvar er opdaterede og aktuelle. Det forbedrer derfor pr√¶stationen p√• dom√¶nespecifikke opgaver ved at f√• adgang til intern vidensbase.

- Reducerer fabrikation ved at bruge **verificerbare data** i vidensbasen til at give kontekst til brugerforesp√∏rgsler.

- Det er **omkostningseffektivt**, da de er mere √∏konomiske sammenlignet med at finjustere en LLM.

## Oprettelse af en vidensbase

Vores applikation baseres p√• vores personlige data, dvs. Neural Network-lektionen i AI For Beginners pensum.

### Vektordatabaser

En vektordatabase er, i mods√¶tning til traditionelle databaser, en specialiseret database designet til at lagre, h√•ndtere og s√∏ge i indlejrede vektorer. Den lagrer numeriske repr√¶sentationer af dokumenter. Opdeling af data til numeriske embeddings g√∏r det lettere for vores AI-system at forst√• og behandle dataene.

Vi gemmer vores embeddings i vektordatabaser, da LLM‚Äôer har en begr√¶nsning p√• antallet af tokens, de accepterer som input. Da du ikke kan give hele embeddings til en LLM, skal vi opdele dem i bidder, og n√•r en bruger stiller et sp√∏rgsm√•l, returneres de embeddings, der mest ligner sp√∏rgsm√•let, sammen med prompten. Opdeling reducerer ogs√• omkostningerne ved antallet af tokens, der passerer gennem en LLM.

Nogle popul√¶re vektordatabaser inkluderer Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant og DeepLake. Du kan oprette en Azure Cosmos DB-model ved hj√¶lp af Azure CLI med f√∏lgende kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fra tekst til embeddings

F√∏r vi lagrer vores data, skal vi konvertere det til vektor-embeddings, f√∏r det gemmes i databasen. Hvis du arbejder med store dokumenter eller lange tekster, kan du opdele dem baseret p√• forventede foresp√∏rgsler. Opdeling kan ske p√• s√¶tningsniveau eller afsnitsniveau. Da opdeling udleder betydninger fra ordene omkring dem, kan du tilf√∏je anden kontekst til en bid, for eksempel ved at tilf√∏je dokumentets titel eller inkludere noget tekst f√∏r eller efter bidet. Du kan opdele dataene som f√∏lger:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Hvis det sidste stykke ikke n√•ede den minimale l√¶ngde, tilf√∏j det alligevel
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N√•r de er opdelt, kan vi indlejre vores tekst ved hj√¶lp af forskellige embedding-modeller. Nogle modeller, du kan bruge, inkluderer: word2vec, ada-002 fra OpenAI, Azure Computer Vision og mange flere. Valget af model afh√¶nger af de sprog, du bruger, typen af indkodet indhold (tekst/billeder/lyd), st√∏rrelsen p√• input, den kan kode, og l√¶ngden p√• embedding-outputtet.

Et eksempel p√• embedded tekst ved hj√¶lp af OpenAI‚Äôs `text-embedding-ada-002` model er:
![an embedding of the word cat](../../../translated_images/da/cat.74cbd7946bc9ca38.webp)

## Udtr√¶kning og vektors√∏gning

N√•r en bruger stiller et sp√∏rgsm√•l, omdanner retrieveren det til en vektor ved hj√¶lp af foresp√∏rgselsencoderen, den s√∏ger derefter gennem vores dokument-s√∏geindeks efter relevante vektorer i dokumentet, der relaterer til input. N√•r det er gjort, konverterer den b√•de inputvektoren og dokumentvektorerne til tekst og sender det gennem LLM‚Äôen.

### Udtr√¶kning

Udtr√¶kning sker, n√•r systemet pr√∏ver hurtigt at finde de dokumenter i indekset, der opfylder s√∏gekriterierne. M√•let med retrieveren er at hente dokumenter, som vil blive brugt til at give kontekst og forankre LLM‚Äôen p√• dine data.

Der er flere m√•der at udf√∏re s√∏gning i vores database p√•, s√•som:

- **S√∏gning efter n√∏gleord** ‚Äì bruges til teksts√∏gninger

- **Vektors√∏gning** ‚Äì konverterer dokumenter fra tekst til vektorrepr√¶sentationer ved hj√¶lp af embedding-modeller, hvilket tillader en **semantisk s√∏gning** baseret p√• ordenes betydning. Udtr√¶kning sker ved at foresp√∏rge de dokumenter, hvis vektorrepr√¶sentationer er t√¶ttest p√• brugerens sp√∏rgsm√•l.

- **Hybrid** ‚Äì en kombination af b√•de s√∏gning efter n√∏gleord og vektors√∏gning.

En udfordring ved udtr√¶kning opst√•r, n√•r der ikke er noget tilsvarende svar p√• foresp√∏rgslen i databasen; systemet vil s√• returnere den bedst mulige information, det kan finde. Du kan dog bruge taktikker som at s√¶tte den maksimale afstand for relevans eller bruge hybrid s√∏gning, der kombinerer b√•de n√∏gleords- og vektors√∏gning. I denne lektion vil vi bruge hybrid s√∏gning, en kombination af b√•de vektor- og n√∏gleordss√∏gning. Vi gemmer vores data i en dataframe med kolonner, der indeholder b√•de bidder og embeddings.

### Vektorligning

Retrieveren s√∏ger igennem vidensdatabasen efter embeddings, der ligger t√¶t p√• hinanden, den n√¶rmeste nabo, da de er tekster, der ligner hinanden. I scenariet hvor en bruger stiller en foresp√∏rgsel, bliver den f√∏rst embedded og derefter matchet med lignende embeddings. Den mest almindelige m√•ling til at finde, hvor ens forskellige vektorer er, er cosinus-lighed, som baseres p√• vinklen mellem to vektorer.

Vi kan m√•le lighed ved hj√¶lp af andre alternativer s√•som Euklidisk afstand, som er den direkte linje mellem de to vektors endepunkter, og prikprodukt, som m√•ler summen af produkterne af tilsvarende elementer i to vektorer.

### S√∏geindeks

N√•r vi udtr√¶kker, skal vi bygge et s√∏geindeks for vores vidensbase, f√∏r vi udf√∏rer s√∏gning. Et indeks gemmer vores embeddings og kan hurtigt finde de mest lignende bidder, selv i en stor database. Vi kan oprette vores indeks lokalt ved hj√¶lp af:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Opret s√∏geindekset
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# For at foresp√∏rge indekset kan du bruge kneighbors-metoden
distances, indices = nbrs.kneighbors(embeddings)
```

### Omrangering

N√•r du har forespurgt databasen, kan det v√¶re n√∏dvendigt at sortere resultaterne fra de mest relevante. En omrangering LLM bruger maskinl√¶ring til at forbedre relevansen af s√∏geresultater ved at ordne dem fra mest relevante. Ved brug af Azure AI Search sker omrangering automatisk ved hj√¶lp af en semantisk omrangering. Et eksempel p√•, hvordan omrangering fungerer ved brug af n√¶rmeste naboer:

```python
# Find de mest lignende dokumenter
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Udskriv de mest lignende dokumenter
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## At samle det hele

Det sidste trin er at tilf√∏je vores LLM i blandingen for at kunne f√• svar, der er forankret i vores data. Vi kan implementere det som f√∏lger:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Konverter sp√∏rgsm√•let til en foresp√∏rgselsvektor
    query_vector = create_embeddings(user_input)

    # Find de mest lignende dokumenter
    distances, indices = nbrs.kneighbors([query_vector])

    # tilf√∏j dokumenter til foresp√∏rgslen for at give kontekst
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # kombiner historikken og brugerens input
    history.append(user_input)

    # opret et beskedobjekt
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # brug chatf√¶rdigg√∏relse til at generere et svar
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluering af vores applikation

### Evalueringsm√•l

- Kvalitet af leverede svar med sikring af, at det lyder naturligt, flydende og menneskeligt

- Forankring i data: evaluere om svaret kom fra leverede dokumenter

- Relevans: evaluering af om svaret matcher og er relateret til det stillede sp√∏rgsm√•l

- Flydende ‚Äì om svaret giver grammatisk mening

## Brugstilf√¶lde for brug af RAG (Retrieval Augmented Generation) og vektordatabaser

Der findes mange forskellige brugstilf√¶lde, hvor funktionskald kan forbedre din app s√•som:

- Sp√∏rgsm√•l og svar: forankring af virksomhedens data til en chat, som kan bruges af medarbejdere til at stille sp√∏rgsm√•l.

- Anbefalingssystemer: hvor du kan oprette et system, der matcher de mest lignende v√¶rdier, f.eks. film, restauranter og meget mere.

- Chatbot-tjenester: du kan gemme chat-historik og personligg√∏re samtalen baseret p√• brugerdata.

- Billeds√∏gning baseret p√• vektor-embeddings, nyttigt ved billedgenkendelse og anomalies√∏gning.

## Resum√©

Vi har d√¶kket de grundl√¶ggende omr√•der af RAG fra tilf√∏jelse af vores data til applikationen, brugerforesp√∏rgslen og output. For at forenkle oprettelsen af RAG kan du bruge frameworks som Semanti Kernel, Langchain eller Autogen.

## Opgave

For at forts√¶tte din l√¶ring om Retrieval Augmented Generation (RAG) kan du bygge:

- Opbygge et frontend til applikationen ved hj√¶lp af det framework, du foretr√¶kker

- Benytte et framework, enten LangChain eller Semantic Kernel, og genskabe din applikation.

Tillykke med at have gennemf√∏rt lektionen üëè.

## L√¶ringen stopper ikke her, forts√¶t rejsen

Efter at have gennemf√∏rt denne lektion, se vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts√¶tte med at forbedre din viden om Generativ AI!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfraskrivelse**:
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, bedes du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det oprindelige dokument p√• dets modersm√•l b√∏r betragtes som den autoritative kilde. For vigtig information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os intet ansvar for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->