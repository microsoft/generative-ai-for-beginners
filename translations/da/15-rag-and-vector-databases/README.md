<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-05-20T01:21:03+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "da"
}
-->
# Retrieval Augmented Generation (RAG) og Vektordatabaser

[![Retrieval Augmented Generation (RAG) og Vektordatabaser](../../../translated_images/15-lesson-banner.799d0cd2229970edb365f6667a4c7b3a0f526eb8698baa7d2e05c3bd49a5d83f.da.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

I lektionen om s칮geapplikationer l칝rte vi kort, hvordan man integrerer sine egne data i Store Sproglige Modeller (LLMs). I denne lektion vil vi dykke dybere ned i begreberne om at forankre dine data i din LLM-applikation, processens mekanik og metoder til at lagre data, herunder b친de embeddings og tekst.

> **Video kommer snart**

## Introduktion

I denne lektion vil vi d칝kke f칮lgende:

- En introduktion til RAG, hvad det er, og hvorfor det bruges i AI (kunstig intelligens).

- Forst친else af, hvad vektordatabaser er, og hvordan man opretter en til vores applikation.

- Et praktisk eksempel p친, hvordan man integrerer RAG i en applikation.

## L칝ringsm친l

Efter at have gennemf칮rt denne lektion, vil du v칝re i stand til at:

- Forklare betydningen af RAG i datahentning og -behandling.

- Ops칝tte en RAG-applikation og forankre dine data til en LLM.

- Effektiv integration af RAG og Vektordatabaser i LLM-applikationer.

## Vores scenarie: forbedring af vores LLM'er med vores egne data

For denne lektion 칮nsker vi at tilf칮je vores egne noter til uddannelsesstartuppen, hvilket giver chatbotten mulighed for at f친 mere information om de forskellige emner. Ved at bruge de noter, vi har, vil eleverne kunne studere bedre og forst친 de forskellige emner, hvilket g칮r det lettere at repetere til deres eksamener. For at skabe vores scenarie vil vi bruge:

- `Azure OpenAI:` den LLM, vi vil bruge til at skabe vores chatbot

- `AI for beginners' lesson on Neural Networks`: dette vil v칝re de data, vi forankrer vores LLM p친

- `Azure AI Search` og `Azure Cosmos DB:` vektordatabase til at lagre vores data og skabe et s칮geindeks

Brugerne vil kunne lave 칮velsesquizzer fra deres noter, revisionsflashkort og opsummere dem til pr칝cise oversigter. For at komme i gang, lad os se p친, hvad RAG er, og hvordan det fungerer:

## Retrieval Augmented Generation (RAG)

En LLM-drevet chatbot behandler brugerens foresp칮rgsler for at generere svar. Den er designet til at v칝re interaktiv og engagerer sig med brugere om en bred vifte af emner. Dog er dens svar begr칝nset til den kontekst, der gives, og dens grundl칝ggende tr칝ningsdata. For eksempel er GPT-4's vidensafsk칝ring september 2021, hvilket betyder, at den mangler viden om begivenheder, der er sket efter denne periode. Derudover udelukker de data, der bruges til at tr칝ne LLM'er, fortrolige oplysninger som personlige noter eller en virksomheds produktmanual.

### Hvordan RAGs (Retrieval Augmented Generation) fungerer

![tegning der viser, hvordan RAGs fungerer](../../../translated_images/how-rag-works.d87a7ed9c30f43126bb9e8e259be5d66e16cd1fef65374e6914746ba9bfb0b2f.da.png)

Antag, at du vil implementere en chatbot, der laver quizzer fra dine noter, s친 har du brug for en forbindelse til vidensbasen. Det er her, RAG kommer til unds칝tning. RAGs fungerer som f칮lger:

- **Vidensbase:** F칮r hentning skal disse dokumenter indtages og forbehandles, typisk ved at nedbryde store dokumenter i mindre stykker, omdanne dem til tekstembedding og lagre dem i en database.

- **Brugerforesp칮rgsel:** brugeren stiller et sp칮rgsm친l

- **Hentning:** N친r en bruger stiller et sp칮rgsm친l, henter embedding-modellen relevant information fra vores vidensbase for at give mere kontekst, der vil blive indarbejdet i prompten.

- **Forst칝rket generering:** LLM'en forbedrer sit svar baseret p친 de hentede data. Det g칮r det muligt for det genererede svar ikke kun at v칝re baseret p친 fortr칝nede data, men ogs친 relevant information fra den tilf칮jede kontekst. De hentede data bruges til at forst칝rke LLM'ens svar. LLM'en returnerer derefter et svar p친 brugerens sp칮rgsm친l.

![tegning der viser RAGs arkitektur](../../../translated_images/encoder-decode.75eebc7093ccefec17568eebc80d3d0b831ecf2ea204566377a04c77a5a57ebb.da.png)

Arkitekturen for RAGs implementeres ved hj칝lp af transformere, der best친r af to dele: en encoder og en decoder. For eksempel, n친r en bruger stiller et sp칮rgsm친l, 'encodes' inputteksten til vektorer, der fanger betydningen af ordene, og vektorerne 'decodes' til vores dokumentindeks og genererer ny tekst baseret p친 brugerens foresp칮rgsel. LLM'en bruger b친de en encoder-decoder-model til at generere output.

To tilgange ved implementering af RAG if칮lge det foresl친ede papir: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) er:

- **_RAG-Sequence_** bruger hentede dokumenter til at forudsige det bedst mulige svar p친 en brugerforesp칮rgsel

- **RAG-Token** bruger dokumenter til at generere den n칝ste token og derefter hente dem for at besvare brugerens foresp칮rgsel

### Hvorfor ville du bruge RAGs?

- **Informationsrigdom:** sikrer, at tekstsvar er opdaterede og aktuelle. Det forbedrer derfor ydeevnen p친 dom칝nespecifikke opgaver ved at f친 adgang til den interne vidensbase.

- Reducerer fabrikation ved at bruge **verificerbare data** i vidensbasen til at give kontekst til brugerforesp칮rgsler.

- Det er **omkostningseffektivt**, da de er mere 칮konomiske sammenlignet med at finjustere en LLM.

## Oprettelse af en vidensbase

Vores applikation er baseret p친 vores personlige data, dvs. lektionen om Neurale Netv칝rk i AI For Beginners-kurset.

### Vektordatabaser

En vektordatabase, i mods칝tning til traditionelle databaser, er en specialiseret database designet til at lagre, administrere og s칮ge i indlejrede vektorer. Den lagrer numeriske repr칝sentationer af dokumenter. Nedbrydning af data til numeriske embeddings g칮r det lettere for vores AI-system at forst친 og behandle dataene.

Vi lagrer vores embeddings i vektordatabaser, da LLM'er har en gr칝nse for antallet af tokens, de accepterer som input. Da du ikke kan sende hele embeddings til en LLM, bliver vi n칮dt til at opdele dem i stykker, og n친r en bruger stiller et sp칮rgsm친l, returneres de embeddings, der ligner sp칮rgsm친let, sammen med prompten. Chunking reducerer ogs친 omkostningerne ved antallet af tokens, der sendes gennem en LLM.

Nogle popul칝re vektordatabaser inkluderer Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant og DeepLake. Du kan oprette en Azure Cosmos DB-model ved hj칝lp af Azure CLI med f칮lgende kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fra tekst til embeddings

F칮r vi lagrer vores data, skal vi konvertere dem til vektorembeddings, inden de lagres i databasen. Hvis du arbejder med store dokumenter eller lange tekster, kan du opdele dem baseret p친 de foresp칮rgsler, du forventer. Chunking kan g칮res p친 s칝tningsniveau eller p친 afsnitsniveau. Da chunking afleder betydninger fra ordene omkring dem, kan du tilf칮je en anden kontekst til et stykke, for eksempel ved at tilf칮je dokumenttitlen eller inkludere noget tekst f칮r eller efter stykket. Du kan opdele dataene som f칮lger:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N친r de er opdelt, kan vi derefter embedde vores tekst ved hj칝lp af forskellige embedding-modeller. Nogle modeller, du kan bruge, inkluderer: word2vec, ada-002 fra OpenAI, Azure Computer Vision og mange flere. Valget af model afh칝nger af de sprog, du bruger, typen af indhold, der kodes (tekst/billeder/lyd), st칮rrelsen af inputtet, det kan kode, og l칝ngden af embedding-outputtet.

Et eksempel p친 embedded tekst ved hj칝lp af OpenAI's `text-embedding-ada-002` model er:
![en embedding af ordet kat](../../../translated_images/cat.3db013cbca4fd5d90438ea7b312ad0364f7686cf79931ab15cd5922151aea53e.da.png)

## Hentning og vektors칮gning

N친r en bruger stiller et sp칮rgsm친l, omdanner retrieveren det til en vektor ved hj칝lp af foresp칮rgselsencoderen, og den s칮ger derefter gennem vores dokuments칮geindeks efter relevante vektorer i dokumentet, der er relateret til inputtet. N친r det er gjort, konverterer den b친de inputvektoren og dokumentvektorerne til tekst og sender det gennem LLM'en.

### Hentning

Hentning sker, n친r systemet fors칮ger hurtigt at finde dokumenterne fra indekset, der opfylder s칮gekriterierne. Retrieverens m친l er at f친 dokumenter, der vil blive brugt til at give kontekst og forankre LLM'en p친 dine data.

Der er flere m친der at udf칮re s칮gning inden for vores database, s친som:

- **N칮gleordss칮gning** - bruges til teksts칮gninger

- **Semantisk s칮gning** - bruger den semantiske betydning af ord

- **Vektors칮gning** - konverterer dokumenter fra tekst til vektorrepr칝sentationer ved hj칝lp af embedding-modeller. Hentning vil ske ved at foresp칮rge dokumenterne, hvis vektorrepr칝sentationer er t칝ttest p친 brugerens sp칮rgsm친l.

- **Hybrid** - en kombination af b친de n칮gleord og vektors칮gning.

En udfordring ved hentning opst친r, n친r der ikke er noget lignende svar p친 foresp칮rgslen i databasen, vil systemet derefter returnere den bedste information, de kan f친. Dog kan du bruge taktikker som at indstille den maksimale afstand for relevans eller bruge hybrid s칮gning, der kombinerer b친de n칮gleord og vektors칮gning. I denne lektion vil vi bruge hybrid s칮gning, en kombination af b친de vektor- og n칮gleordss칮gning. Vi vil lagre vores data i en dataframe med kolonner, der indeholder stykkerne samt embeddings.

### Vektorsimilaritet

Retrieveren vil s칮ge gennem vidensdatabasen efter embeddings, der er t칝t sammen, den n칝rmeste nabo, da de er tekster, der er ens. I scenariet, hvor en bruger stiller en foresp칮rgsel, bliver den f칮rst embeddet og derefter matchet med lignende embeddings. Den almindelige m친ling, der bruges til at finde ud af, hvor ens forskellige vektorer er, er cosinus-similaritet, der er baseret p친 vinklen mellem to vektorer.

Vi kan m친le similaritet ved hj칝lp af andre alternativer, vi kan bruge, er euklidisk afstand, som er den lige linje mellem vektorens endepunkter og prikprodukt, der m친ler summen af produkterne af de tilsvarende elementer i to vektorer.

### S칮geindeks

N친r vi foretager hentning, skal vi opbygge et s칮geindeks for vores vidensbase, f칮r vi udf칮rer s칮gning. Et indeks vil lagre vores embeddings og hurtigt kunne hente de mest lignende stykker, selv i en stor database. Vi kan oprette vores indeks lokalt ved hj칝lp af:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Re-ranking

N친r du har forespurgt databasen, kan du muligvis have brug for at sortere resultaterne fra de mest relevante. En reranking LLM bruger maskinl칝ring til at forbedre relevansen af s칮geresultaterne ved at ordne dem fra de mest relevante. Ved hj칝lp af Azure AI Search udf칮res reranking automatisk for dig ved hj칝lp af en semantisk reranker. Et eksempel p친, hvordan reranking fungerer ved hj칝lp af n칝rmeste naboer:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## At samle det hele

Det sidste trin er at tilf칮je vores LLM til blandingen for at kunne f친 svar, der er forankret i vores data. Vi kan implementere det som f칮lger:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluering af vores applikation

### Evalueringsmetrikker

- Kvaliteten af de leverede svar sikrer, at det lyder naturligt, flydende og menneskeligt

- Forankring af dataene: evaluering af, om svaret kom fra de leverede dokumenter

- Relevans: evaluering af, om svaret matcher og er relateret til det stillede sp칮rgsm친l

- Flydende - om svaret giver mening grammatisk

## Anvendelsestilf칝lde for brug af RAG (Retrieval Augmented Generation) og vektordatabaser

Der er mange forskellige anvendelsestilf칝lde, hvor funktionskald kan forbedre din app, s친som:

- Sp칮rgsm친l og svar: forankring af dine virksomhedsdata til en chat, der kan bruges af medarbejdere til at stille sp칮rgsm친l.

- Anbefalingssystemer: hvor du kan skabe et system, der matcher de mest lignende v칝rdier, f.eks. film, restauranter og mange flere.

- Chatbot-tjenester: du kan gemme chathistorik og personalisere samtalen baseret p친 brugerdata.

- Billeds칮gning baseret p친 vektorembeddings, nyttig ved billedgenkendelse og anomalidetektion.

## Resum칠

Vi har d칝kket de grundl칝ggende omr친der af RAG fra at tilf칮je vores data til applikationen, brugerforesp칮rgslen og output. For at forenkle oprettelsen af RAG kan du bruge rammer som Semanti Kernel, Langchain eller Autogen.

## Opgave

For at forts칝tte din l칝ring om Retrieval Augmented Generation (RAG) kan du bygge:

- Byg en front-end til applikationen ved hj칝lp af den ramme, du v칝lger

- Brug en ramme, enten LangChain eller Semantic Kernel, og genskab din applikation.

Tillykke med at have gennemf칮rt lektionen 游녪.

## L칝ring stopper ikke her, forts칝t rejsen

Efter at have gennemf칮rt denne lektion, tjek vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts칝tte med at forbedre din viden om Generativ AI!

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 n칮jagtighed, skal du v칝re opm칝rksom p친, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi er ikke ansvarlige for misforst친elser eller fejltolkninger, der m친tte opst친 som f칮lge af brugen af denne overs칝ttelse.