<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-07-09T16:14:04+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "da"
}
-->
# Retrieval Augmented Generation (RAG) og vektordatabaser

[![Retrieval Augmented Generation (RAG) og vektordatabaser](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.da.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

I lektionen om s√∏geapplikationer l√¶rte vi kort, hvordan du kan integrere dine egne data i Large Language Models (LLMs). I denne lektion dykker vi dybere ned i begreberne omkring at forankre dine data i din LLM-applikation, mekanismerne i processen og metoderne til at gemme data, herunder b√•de embeddings og tekst.

> **Video kommer snart**

## Introduktion

I denne lektion vil vi gennemg√• f√∏lgende:

- En introduktion til RAG, hvad det er, og hvorfor det bruges inden for AI (kunstig intelligens).

- Forst√•else af, hvad vektordatabaser er, og hvordan man opretter en til vores applikation.

- Et praktisk eksempel p√•, hvordan man integrerer RAG i en applikation.

## L√¶ringsm√•l

Efter at have gennemf√∏rt denne lektion vil du kunne:

- Forklare betydningen af RAG i datahentning og -behandling.

- Ops√¶tte en RAG-applikation og forankre dine data til en LLM.

- Effektivt integrere RAG og vektordatabaser i LLM-applikationer.

## Vores scenarie: forbedring af vores LLM‚Äôer med egne data

I denne lektion √∏nsker vi at tilf√∏je vores egne noter til uddannelsesstartuppen, hvilket g√∏r det muligt for chatbotten at f√• mere information om de forskellige emner. Ved at bruge de noter, vi har, vil eleverne kunne studere bedre og forst√• de forskellige emner, hvilket g√∏r det nemmere at forberede sig til eksamener. Til at skabe vores scenarie vil vi bruge:

- `Azure OpenAI:` den LLM, vi bruger til at skabe vores chatbot

- `AI for beginners' lesson on Neural Networks:` dette bliver de data, vi forankrer vores LLM p√•

- `Azure AI Search` og `Azure Cosmos DB:` vektordatabaser til at gemme vores data og oprette et s√∏geindeks

Brugere vil kunne lave √∏vequizzer ud fra deres noter, repetitionsflashcards og opsummere det til korte oversigter. For at komme i gang, lad os se p√•, hvad RAG er, og hvordan det fungerer:

## Retrieval Augmented Generation (RAG)

En LLM-drevet chatbot behandler brugerforesp√∏rgsler for at generere svar. Den er designet til at v√¶re interaktiv og engagerer sig med brugere om mange forskellige emner. Dog er dens svar begr√¶nset til den kontekst, der gives, og dens grundl√¶ggende tr√¶ningsdata. For eksempel har GPT-4 en vidensgr√¶nse i september 2021, hvilket betyder, at den ikke kender til begivenheder, der er sket efter denne dato. Derudover udelukker de data, der bruges til at tr√¶ne LLM‚Äôer, fortrolige oplysninger som personlige noter eller en virksomheds produktmanual.

### Hvordan RAGs (Retrieval Augmented Generation) fungerer

![tegning der viser, hvordan RAGs fungerer](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.da.png)

Antag, at du vil implementere en chatbot, der laver quizzer ud fra dine noter; s√• har du brug for en forbindelse til vidensbasen. Her kommer RAG til unds√¶tning. RAGs fungerer p√• f√∏lgende m√•de:

- **Vidensbase:** F√∏r hentning skal disse dokumenter indtastes og forbehandles, typisk ved at opdele store dokumenter i mindre bidder, omdanne dem til tekst-embedding og gemme dem i en database.

- **Brugerforesp√∏rgsel:** brugeren stiller et sp√∏rgsm√•l

- **Hentning:** N√•r en bruger stiller et sp√∏rgsm√•l, henter embeddings-modellen relevant information fra vores vidensbase for at give mere kontekst, som indg√•r i prompten.

- **Forst√¶rket generering:** LLM‚Äôen forbedrer sit svar baseret p√• de hentede data. Det g√∏r, at det genererede svar ikke kun baseres p√• forudtr√¶nede data, men ogs√• p√• relevant information fra den tilf√∏jede kontekst. De hentede data bruges til at forst√¶rke LLM‚Äôens svar. LLM‚Äôen returnerer derefter et svar p√• brugerens sp√∏rgsm√•l.

![tegning der viser RAGs arkitektur](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.da.png)

Arkitekturen for RAGs implementeres ved hj√¶lp af transformers, der best√•r af to dele: en encoder og en decoder. For eksempel, n√•r en bruger stiller et sp√∏rgsm√•l, bliver inputteksten 'kodet' til vektorer, der fanger betydningen af ordene, og vektorerne 'afkodes' til vores dokumentindeks og genererer ny tekst baseret p√• brugerens foresp√∏rgsel. LLM‚Äôen bruger b√•de en encoder-decoder-model til at generere output.

To tilgange til implementering af RAG if√∏lge den foresl√•ede artikel: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) er:

- **_RAG-Sequence_** bruger hentede dokumenter til at forudsige det bedst mulige svar p√• en brugerforesp√∏rgsel

- **RAG-Token** bruger dokumenter til at generere det n√¶ste token og henter dem derefter for at besvare brugerens foresp√∏rgsel

### Hvorfor bruge RAGs?

- **Informationsrigdom:** sikrer, at tekstsvar er opdaterede og aktuelle. Det forbedrer derfor pr√¶stationen p√• dom√¶nespecifikke opgaver ved at f√• adgang til den interne vidensbase.

- Reducerer fabrikation ved at bruge **verificerbare data** i vidensbasen til at give kontekst til brugerforesp√∏rgsler.

- Det er **omkostningseffektivt**, da det er billigere end at finjustere en LLM.

## Oprettelse af en vidensbase

Vores applikation er baseret p√• vores personlige data, dvs. lektionen om neurale netv√¶rk i AI For Beginners-kurset.

### Vektordatabaser

En vektordatabase er, i mods√¶tning til traditionelle databaser, en specialiseret database designet til at gemme, h√•ndtere og s√∏ge i indlejrede vektorer. Den gemmer numeriske repr√¶sentationer af dokumenter. Ved at opdele data til numeriske embeddings bliver det lettere for vores AI-system at forst√• og behandle dataene.

Vi gemmer vores embeddings i vektordatabaser, da LLM‚Äôer har en gr√¶nse for, hvor mange tokens de kan modtage som input. Da du ikke kan sende hele embedding‚Äôen til en LLM, skal vi opdele dem i bidder, og n√•r en bruger stiller et sp√∏rgsm√•l, returneres de embeddings, der bedst matcher sp√∏rgsm√•let, sammen med prompten. Opdeling i bidder reducerer ogs√• omkostningerne ved antallet af tokens, der sendes gennem en LLM.

Nogle popul√¶re vektordatabaser inkluderer Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant og DeepLake. Du kan oprette en Azure Cosmos DB-model ved hj√¶lp af Azure CLI med f√∏lgende kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fra tekst til embeddings

F√∏r vi gemmer vores data, skal vi konvertere dem til vektor-embeddings, inden de gemmes i databasen. Hvis du arbejder med store dokumenter eller lange tekster, kan du opdele dem i bidder baseret p√• de foresp√∏rgsler, du forventer. Opdeling kan ske p√• s√¶tningsniveau eller afsnitsniveau. Da opdeling udleder betydning fra de omkringliggende ord, kan du tilf√∏je ekstra kontekst til en bid, for eksempel ved at tilf√∏je dokumentets titel eller inkludere noget tekst f√∏r eller efter bidet. Du kan opdele dataene som f√∏lger:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N√•r de er opdelt, kan vi derefter indlejre vores tekst ved hj√¶lp af forskellige embedding-modeller. Nogle modeller, du kan bruge, inkluderer: word2vec, ada-002 fra OpenAI, Azure Computer Vision og mange flere. Valget af model afh√¶nger af de sprog, du bruger, typen af indhold (tekst/billeder/lyd), st√∏rrelsen af input, den kan kode, og l√¶ngden af embedding-outputtet.

Et eksempel p√• indlejret tekst ved brug af OpenAI‚Äôs `text-embedding-ada-002` model er:
![en embedding af ordet kat](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.da.png)

## Hentning og vektors√∏gning

N√•r en bruger stiller et sp√∏rgsm√•l, omdanner retrieveren det til en vektor ved hj√¶lp af query-encoderen, hvorefter den s√∏ger i vores dokumentindeks efter relevante vektorer i dokumentet, der relaterer til inputtet. N√•r det er gjort, konverterer den b√•de inputvektoren og dokumentvektorerne til tekst og sender det gennem LLM‚Äôen.

### Hentning

Hentning sker, n√•r systemet hurtigt pr√∏ver at finde dokumenter i indekset, der opfylder s√∏gekriterierne. M√•let med retrieveren er at finde dokumenter, der kan bruges til at give kontekst og forankre LLM‚Äôen i dine data.

Der er flere m√•der at udf√∏re s√∏gning i vores database p√•, s√•som:

- **N√∏gleordss√∏gning** ‚Äì bruges til teksts√∏gninger

- **Semantisk s√∏gning** ‚Äì bruger den semantiske betydning af ord

- **Vektors√∏gning** ‚Äì konverterer dokumenter fra tekst til vektorrepr√¶sentationer ved hj√¶lp af embedding-modeller. Hentning sker ved at foresp√∏rge dokumenter, hvis vektorrepr√¶sentationer er t√¶ttest p√• brugerens sp√∏rgsm√•l.

- **Hybrid** ‚Äì en kombination af b√•de n√∏gleordss√∏gning og vektors√∏gning.

En udfordring ved hentning opst√•r, n√•r der ikke findes et lignende svar p√• foresp√∏rgslen i databasen; systemet vil s√• returnere den bedste information, det kan finde. Du kan dog bruge taktikker som at s√¶tte en maksimal afstand for relevans eller bruge hybrid s√∏gning, der kombinerer b√•de n√∏gleord og vektors√∏gning. I denne lektion bruger vi hybrid s√∏gning, en kombination af b√•de vektor- og n√∏gleordss√∏gning. Vi gemmer vores data i en dataframe med kolonner, der indeholder b√•de bidder og embeddings.

### Vektorlignelighed

Retrieveren s√∏ger i vidensdatabasen efter embeddings, der ligger t√¶t p√• hinanden, den n√¶rmeste nabo, da det er tekster, der ligner hinanden. I scenariet, hvor en bruger stiller et sp√∏rgsm√•l, bliver det f√∏rst embedded og derefter matchet med lignende embeddings. Den mest almindelige m√•ling, der bruges til at finde, hvor ens forskellige vektorer er, er cosinuslignelighed, som baseres p√• vinklen mellem to vektorer.

Vi kan ogs√• m√•le lighed ved hj√¶lp af andre metoder som Euclidean distance, der er den lige linje mellem vektorendepunkter, og dot product, som m√•ler summen af produkterne af tilsvarende elementer i to vektorer.

### S√∏geindeks

N√•r vi laver hentning, skal vi opbygge et s√∏geindeks for vores vidensbase, f√∏r vi udf√∏rer s√∏gning. Et indeks gemmer vores embeddings og kan hurtigt hente de mest lignende bidder, selv i en stor database. Vi kan oprette vores indeks lokalt ved hj√¶lp af:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Omrangering

N√•r du har forespurgt databasen, kan det v√¶re n√∏dvendigt at sortere resultaterne efter relevans. En omrangering-LLM bruger maskinl√¶ring til at forbedre relevansen af s√∏geresultater ved at sortere dem fra mest til mindst relevant. Ved brug af Azure AI Search sker omrangering automatisk for dig ved hj√¶lp af en semantisk omrangering. Et eksempel p√•, hvordan omrangering fungerer ved brug af n√¶rmeste naboer:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## At samle det hele

Det sidste skridt er at tilf√∏je vores LLM i processen for at kunne f√• svar, der er forankret i vores data. Vi kan implementere det som f√∏lger:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluering af vores applikation

### Evalueringsmetrikker

- Kvaliteten af de leverede svar, der sikrer, at de lyder naturlige, flydende og menneskelige

- Forankring af data: vurdering af, om svaret stammer fra de leverede dokumenter

- Relevans: vurdering af, om svaret matcher og relaterer til det stillede sp√∏rgsm√•l

- Flydende sprog ‚Äì om svaret giver grammatisk mening

## Anvendelsestilf√¶lde for RAG (Retrieval Augmented Generation) og vektordatabaser

Der findes mange forskellige anvendelsestilf√¶lde, hvor funktionskald kan forbedre din app, s√•som:

- Sp√∏rgsm√•l og svar: forankring af virksomhedens data til en chat, som medarbejdere kan bruge til at stille sp√∏rgsm√•l.

- Anbefalingssystemer: hvor du kan skabe et system, der matcher de mest lignende v√¶rdier, f.eks. film, restauranter og meget mere.

- Chatbot-tjenester: du kan gemme chat-historik og personligg√∏re samtalen baseret p√• brugerdata.

- Billeds√∏gning baseret p√• vektor-embeddings, nyttigt ved billedgenkendelse og anomalidetektion.

## Resum√©

Vi har d√¶kket de grundl√¶ggende omr√•der af RAG fra tilf√∏jelse af vores data til applikationen, brugerforesp√∏rgslen og output. For at forenkle oprettelsen af RAG kan du bruge frameworks som Semantic Kernel, Langchain eller Autogen.

## Opgave

For at forts√¶tte din l√¶ring om Retrieval Augmented Generation (RAG) kan du bygge:

- Byg et front-end til applikationen ved hj√¶lp af det framework, du foretr√¶kker

- Brug et framework, enten LangChain eller Semantic Kernel, og genskab din applikation.

Tillykke med at have gennemf√∏rt lektionen üëè.

## L√¶ringen stopper ikke her, forts√¶t rejsen

Efter at have gennemf√∏rt denne lektion, kan du tjekke vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts√¶tte med at udvikle din viden om Generativ AI!

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, bedes du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det oprindelige dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os intet ansvar for misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.