<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b4b0266fbadbba7ded891b6485adc66d",
  "translation_date": "2025-10-17T19:10:51+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "da"
}
-->
# Retrieval Augmented Generation (RAG) og Vektordatabaser

[![Retrieval Augmented Generation (RAG) og Vektordatabaser](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.da.png)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

I lektionen om s칮geapplikationer l칝rte vi kort, hvordan man integrerer egne data i store sprogmodeller (LLMs). I denne lektion vil vi g친 dybere ind i begreberne omkring at forankre dine data i din LLM-applikation, mekanikken i processen og metoderne til at lagre data, herunder b친de embeddings og tekst.

> **Video kommer snart**

## Introduktion

I denne lektion vil vi d칝kke f칮lgende:

- En introduktion til RAG, hvad det er, og hvorfor det bruges i kunstig intelligens (AI).

- Forst친else af, hvad vektordatabaser er, og hvordan man opretter en til vores applikation.

- Et praktisk eksempel p친, hvordan man integrerer RAG i en applikation.

## L칝ringsm친l

Efter at have gennemf칮rt denne lektion vil du kunne:

- Forklare betydningen af RAG i datahentning og -behandling.

- Ops칝tte en RAG-applikation og forankre dine data til en LLM.

- Effektiv integration af RAG og vektordatabaser i LLM-applikationer.

## Vores scenarie: forbedring af vores LLMs med vores egne data

I denne lektion 칮nsker vi at tilf칮je vores egne noter til uddannelsesstartuppen, hvilket giver chatbotten mulighed for at f친 mere information om de forskellige emner. Ved at bruge de noter, vi har, vil eleverne kunne studere bedre og forst친 de forskellige emner, hvilket g칮r det lettere at forberede sig til deres eksamener. For at skabe vores scenarie vil vi bruge:

- `Azure OpenAI:` LLM'en, vi vil bruge til at skabe vores chatbot.

- `AI for beginners' lesson on Neural Networks:` dette vil v칝re de data, vi forankrer vores LLM p친.

- `Azure AI Search` og `Azure Cosmos DB:` vektordatabase til at lagre vores data og oprette en s칮geindeks.

Brugere vil kunne oprette 칮velsesquizzer fra deres noter, repetitionskort og opsummere dem til korte oversigter. For at komme i gang, lad os se p친, hvad RAG er, og hvordan det fungerer:

## Retrieval Augmented Generation (RAG)

En LLM-drevet chatbot behandler brugerens foresp칮rgsler for at generere svar. Den er designet til at v칝re interaktiv og engagerer sig med brugere om en bred vifte af emner. Dog er dens svar begr칝nset til den kontekst, der er givet, og dens grundl칝ggende tr칝ningsdata. For eksempel er GPT-4's viden afsk친ret i september 2021, hvilket betyder, at den mangler viden om begivenheder, der er sket efter denne periode. Derudover udelukker de data, der bruges til at tr칝ne LLM'er, fortrolige oplysninger som personlige noter eller en virksomheds produktmanual.

### Hvordan RAGs (Retrieval Augmented Generation) fungerer

![tegning der viser, hvordan RAGs fungerer](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.da.png)

Antag, at du vil implementere en chatbot, der opretter quizzer fra dine noter, s친 skal du have en forbindelse til vidensbasen. Her kommer RAG til unds칝tning. RAGs fungerer som f칮lger:

- **Vidensbase:** F칮r data kan hentes, skal dokumenterne indl칝ses og forbehandles, typisk ved at opdele store dokumenter i mindre stykker, transformere dem til tekstembeddings og lagre dem i en database.

- **Brugerforesp칮rgsel:** Brugeren stiller et sp칮rgsm친l.

- **Hentning:** N친r en bruger stiller et sp칮rgsm친l, henter embedding-modellen relevant information fra vores vidensbase for at give mere kontekst, som vil blive indarbejdet i prompten.

- **Forst칝rket generering:** LLM'en forbedrer sit svar baseret p친 de hentede data. Det g칮r det muligt for det genererede svar ikke kun at v칝re baseret p친 fortr칝nede data, men ogs친 relevant information fra den tilf칮jede kontekst. De hentede data bruges til at forst칝rke LLM'ens svar. LLM'en returnerer derefter et svar p친 brugerens sp칮rgsm친l.

![tegning der viser RAGs arkitektur](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.da.png)

Arkitekturen for RAGs implementeres ved hj칝lp af transformere, der best친r af to dele: en encoder og en decoder. For eksempel, n친r en bruger stiller et sp칮rgsm친l, bliver inputteksten 'kodet' til vektorer, der fanger betydningen af ord, og vektorerne bliver 'afkodet' til vores dokumentindeks og genererer ny tekst baseret p친 brugerens foresp칮rgsel. LLM'en bruger b친de en encoder-decoder-model til at generere output.

To tilgange ved implementering af RAG if칮lge det foresl친ede papir: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) er:

- **_RAG-Sequence_** bruger hentede dokumenter til at forudsige det bedst mulige svar p친 en brugerforesp칮rgsel.

- **RAG-Token** bruger dokumenter til at generere den n칝ste token og henter dem derefter for at besvare brugerens foresp칮rgsel.

### Hvorfor bruge RAGs?

- **Informationsrigdom:** sikrer, at tekstsvar er opdaterede og aktuelle. Det forbedrer derfor ydeevnen p친 dom칝nespecifikke opgaver ved at f친 adgang til den interne vidensbase.

- Reducerer fabrikation ved at bruge **verificerbare data** i vidensbasen til at give kontekst til brugerforesp칮rgsler.

- Det er **omkostningseffektivt**, da de er mere 칮konomiske sammenlignet med finjustering af en LLM.

## Oprettelse af en vidensbase

Vores applikation er baseret p친 vores personlige data, dvs. Neural Network-lektionen fra AI For Beginners-kurset.

### Vektordatabaser

En vektordatabase, i mods칝tning til traditionelle databaser, er en specialiseret database designet til at lagre, administrere og s칮ge i embedded vektorer. Den lagrer numeriske repr칝sentationer af dokumenter. At nedbryde data til numeriske embeddings g칮r det lettere for vores AI-system at forst친 og behandle dataene.

Vi lagrer vores embeddings i vektordatabaser, da LLM'er har en gr칝nse for antallet af tokens, de accepterer som input. Da du ikke kan sende hele embeddings til en LLM, skal vi opdele dem i mindre stykker, og n친r en bruger stiller et sp칮rgsm친l, vil embeddings, der ligner sp칮rgsm친let mest, blive returneret sammen med prompten. Opdeling reducerer ogs친 omkostningerne ved antallet af tokens, der sendes gennem en LLM.

Nogle popul칝re vektordatabaser inkluderer Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant og DeepLake. Du kan oprette en Azure Cosmos DB-model ved hj칝lp af Azure CLI med f칮lgende kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fra tekst til embeddings

F칮r vi lagrer vores data, skal vi konvertere dem til vektor embeddings, f칮r de lagres i databasen. Hvis du arbejder med store dokumenter eller lange tekster, kan du opdele dem baseret p친 de foresp칮rgsler, du forventer. Opdeling kan ske p친 s칝tningsniveau eller p친 afsnitsniveau. Da opdeling afleder betydninger fra ordene omkring dem, kan du tilf칮je noget anden kontekst til et stykke, for eksempel ved at tilf칮je dokumenttitlen eller inkludere noget tekst f칮r eller efter stykket. Du kan opdele dataene som f칮lger:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N친r de er opdelt, kan vi derefter embedde vores tekst ved hj칝lp af forskellige embedding-modeller. Nogle modeller, du kan bruge, inkluderer: word2vec, ada-002 fra OpenAI, Azure Computer Vision og mange flere. Valget af model afh칝nger af de sprog, du bruger, typen af indhold, der kodes (tekst/billeder/lyd), st칮rrelsen af input, det kan kode, og l칝ngden af embedding-outputtet.

Et eksempel p친 embedded tekst ved hj칝lp af OpenAI's `text-embedding-ada-002`-model er:
![en embedding af ordet kat](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.da.png)

## Hentning og vektors칮gning

N친r en bruger stiller et sp칮rgsm친l, transformerer retrieveren det til en vektor ved hj칝lp af foresp칮rgsels-encoderen, og den s칮ger derefter gennem vores dokumentindeks efter relevante vektorer i dokumentet, der er relateret til inputtet. N친r det er gjort, konverterer den b친de inputvektoren og dokumentvektorerne til tekst og sender det gennem LLM'en.

### Hentning

Hentning sker, n친r systemet fors칮ger hurtigt at finde dokumenterne fra indekset, der opfylder s칮gekriterierne. M친let med retrieveren er at f친 dokumenter, der vil blive brugt til at give kontekst og forankre LLM'en p친 dine data.

Der er flere m친der at udf칮re s칮gning inden for vores database, s친som:

- **N칮gleordss칮gning** - bruges til teksts칮gninger.

- **Semantisk s칮gning** - bruger den semantiske betydning af ord.

- **Vektors칮gning** - konverterer dokumenter fra tekst til vektorrepr칝sentationer ved hj칝lp af embedding-modeller. Hentning vil ske ved at foresp칮rge dokumenterne, hvis vektorrepr칝sentationer er t칝ttest p친 brugerens sp칮rgsm친l.

- **Hybrid** - en kombination af b친de n칮gleord og vektors칮gning.

En udfordring med hentning opst친r, n친r der ikke er noget lignende svar p친 foresp칮rgslen i databasen, systemet vil derefter returnere den bedste information, de kan f친, men du kan bruge taktikker som at indstille den maksimale afstand for relevans eller bruge hybrid s칮gning, der kombinerer b친de n칮gleord og vektors칮gning. I denne lektion vil vi bruge hybrid s칮gning, en kombination af b친de vektor- og n칮gleordss칮gning. Vi vil lagre vores data i en dataframe med kolonner, der indeholder stykkerne samt embeddings.

### Vektorlignendehed

Retrieveren vil s칮ge gennem vidensdatabasen efter embeddings, der er t칝t p친 hinanden, den n칝rmeste nabo, da de er tekster, der ligner hinanden. I scenariet, hvor en bruger stiller en foresp칮rgsel, embeddes den f칮rst og matches derefter med lignende embeddings. Den almindelige m친ling, der bruges til at finde ud af, hvor lignende forskellige vektorer er, er cosinuslignendehed, som er baseret p친 vinklen mellem to vektorer.

Vi kan m친le lignendehed ved hj칝lp af andre alternativer som Euklidisk afstand, som er den lige linje mellem vektorendepunkter, og prikprodukt, som m친ler summen af produkterne af tilsvarende elementer i to vektorer.

### S칮geindeks

N친r vi udf칮rer hentning, skal vi opbygge et s칮geindeks for vores vidensbase, f칮r vi udf칮rer s칮gning. Et indeks vil lagre vores embeddings og kan hurtigt hente de mest lignende stykker, selv i en stor database. Vi kan oprette vores indeks lokalt ved hj칝lp af:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Genrangering

N친r du har forespurgt databasen, skal du muligvis sortere resultaterne fra de mest relevante. En genrangerings-LLM bruger maskinl칝ring til at forbedre relevansen af s칮geresultater ved at ordne dem fra de mest relevante. Ved hj칝lp af Azure AI Search udf칮res genrangering automatisk for dig ved hj칝lp af en semantisk genrangerer. Et eksempel p친, hvordan genrangering fungerer ved hj칝lp af n칝rmeste naboer:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Samle det hele

Det sidste trin er at tilf칮je vores LLM til blandingen for at kunne f친 svar, der er forankret i vores data. Vi kan implementere det som f칮lger:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluering af vores applikation

### Evalueringsmetrikker

- Kvaliteten af de leverede svar, der sikrer, at de lyder naturlige, flydende og menneskelige.

- Forankring af dataene: evaluering af, om svaret kom fra de leverede dokumenter.

- Relevans: evaluering af, om svaret matcher og er relateret til det stillede sp칮rgsm친l.

- Flydende - om svaret giver mening grammatisk.

## Anvendelsesmuligheder for brug af RAG (Retrieval Augmented Generation) og vektordatabaser

Der er mange forskellige anvendelsesmuligheder, hvor funktionelle kald kan forbedre din app, s친som:

- Sp칮rgsm친l og svar: forankring af dine virksomhedsdata til en chat, der kan bruges af medarbejdere til at stille sp칮rgsm친l.

- Anbefalingssystemer: hvor du kan oprette et system, der matcher de mest lignende v칝rdier, f.eks. film, restauranter og meget mere.

- Chatbot-tjenester: du kan lagre chat-historik og personalisere samtalen baseret p친 brugerdata.

- Billeds칮gning baseret p친 vektorembeddings, nyttigt ved billedgenkendelse og anomali-detektion.

## Opsummering

Vi har d칝kket de grundl칝ggende omr친der af RAG fra at tilf칮je vores data til applikationen, brugerforesp칮rgslen og outputtet. For at forenkle oprettelsen af RAG kan du bruge frameworks som Semantic Kernel, Langchain eller Autogen.

## Opgave

For at forts칝tte din l칝ring om Retrieval Augmented Generation (RAG) kan du bygge:

- Byg en front-end til applikationen ved hj칝lp af det framework, du v칝lger.

- Brug et framework, enten LangChain eller Semantic Kernel, og genskab din applikation.

Tillykke med at have gennemf칮rt lektionen 游녪.

## L칝ring stopper ikke her, forts칝t rejsen

Efter at have gennemf칮rt denne lektion, kan du tjekke vores [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for at forts칝tte med at opbygge din viden om Generative AI!

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 n칮jagtighed, skal det bem칝rkes, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller fejltolkninger, der opst친r som f칮lge af brugen af denne overs칝ttelse.