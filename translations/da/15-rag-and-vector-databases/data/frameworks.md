<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:32:57+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "da"
}
-->
# Neural Network Frameworks

Som vi allerede har l√¶rt, for at kunne tr√¶ne neurale netv√¶rk effektivt, skal vi g√∏re to ting:

* At kunne arbejde med tensors, fx multiplicere, l√¶gge sammen og beregne funktioner som sigmoid eller softmax
* At kunne beregne gradienter af alle udtryk for at udf√∏re gradient descent optimering

Selvom `numpy`-biblioteket kan klare den f√∏rste del, har vi brug for en mekanisme til at beregne gradienter. I vores framework, som vi udviklede i det foreg√•ende afsnit, m√•tte vi manuelt programmere alle afledte funktioner inde i `backward`-metoden, som udf√∏rer backpropagation. Ideelt set b√∏r et framework give os mulighed for at beregne gradienter af *ethvert udtryk*, vi kan definere.

En anden vigtig ting er at kunne udf√∏re beregninger p√• GPU eller andre specialiserede beregningsenheder som TPU. Tr√¶ning af dybe neurale netv√¶rk kr√¶ver *mange* beregninger, og det er meget vigtigt at kunne parallelisere disse beregninger p√• GPU‚Äôer.

> ‚úÖ Begrebet 'parallelisere' betyder at fordele beregningerne over flere enheder.

I √∏jeblikket er de to mest popul√¶re neurale frameworks: TensorFlow og PyTorch. Begge tilbyder en lavniveau-API til at arbejde med tensors p√• b√•de CPU og GPU. Oven p√• lavniveau-API‚Äôen findes ogs√• en h√∏jere niveau API, kaldet Keras og PyTorch Lightning henholdsvis.

Low-Level API | TensorFlow| PyTorch  
--------------|-------------------------------------|--------------------------------  
High-level API| Keras| PyTorch  

**Low-level API‚Äôer** i begge frameworks giver dig mulighed for at bygge s√•kaldte **computational graphs**. Denne graf definerer, hvordan output (normalt tab-funktionen) beregnes ud fra givne inputparametre, og kan sendes til beregning p√• GPU, hvis det er tilg√¶ngeligt. Der findes funktioner til at differentiere denne computational graph og beregne gradienter, som derefter kan bruges til at optimere modelparametre.

**High-level API‚Äôer** betragter neurale netv√¶rk som en **sekvens af lag**, og g√∏r det meget nemmere at konstruere de fleste neurale netv√¶rk. Tr√¶ning af modellen kr√¶ver normalt, at data forberedes, hvorefter man kalder en `fit`-funktion for at udf√∏re tr√¶ningen.

High-level API‚Äôen g√∏r det muligt at bygge typiske neurale netv√¶rk meget hurtigt uden at bekymre sig om mange detaljer. Samtidig giver low-level API‚Äôen langt mere kontrol over tr√¶ningsprocessen, og derfor bruges den ofte i forskning, n√•r man arbejder med nye neurale netv√¶rksarkitekturer.

Det er ogs√• vigtigt at forst√•, at du kan bruge begge API‚Äôer sammen, fx kan du udvikle din egen netv√¶rkslagsarkitektur ved hj√¶lp af low-level API‚Äôen og derefter bruge den i et st√∏rre netv√¶rk, der er konstrueret og tr√¶net med high-level API‚Äôen. Eller du kan definere et netv√¶rk med high-level API‚Äôen som en sekvens af lag og derefter bruge din egen low-level tr√¶ningsl√∏kke til at udf√∏re optimering. Begge API‚Äôer bygger p√• de samme grundl√¶ggende koncepter og er designet til at fungere godt sammen.

## Learning

I dette kursus tilbyder vi det meste af indholdet b√•de for PyTorch og TensorFlow. Du kan v√¶lge dit foretrukne framework og kun gennemg√• de tilh√∏rende notebooks. Hvis du er i tvivl om, hvilket framework du skal v√¶lge, kan du l√¶se nogle diskussioner p√• internettet om **PyTorch vs. TensorFlow**. Du kan ogs√• kigge p√• begge frameworks for at f√• en bedre forst√•else.

Hvor det er muligt, vil vi bruge High-Level API‚Äôer for enkelhedens skyld. Vi mener dog, det er vigtigt at forst√•, hvordan neurale netv√¶rk fungerer helt fra bunden, s√• i starten arbejder vi med low-level API og tensors. Hvis du derimod vil hurtigt i gang og ikke √∏nsker at bruge meget tid p√• at l√¶re disse detaljer, kan du springe dem over og g√• direkte til high-level API-notebooks.

## ‚úçÔ∏è √òvelser: Frameworks

Forts√¶t din l√¶ring i f√∏lgende notebooks:

Low-Level API | TensorFlow+Keras Notebook | PyTorch  
--------------|-------------------------------------|--------------------------------  
High-level API| Keras | *PyTorch Lightning*

N√•r du har mestret frameworks, lad os opsummere begrebet overfitting.

# Overfitting

Overfitting er et ekstremt vigtigt begreb inden for maskinl√¶ring, og det er meget vigtigt at forst√• det korrekt!

Overvej f√∏lgende problem med at tiln√¶rme 5 punkter (repr√¶senteret ved `x` p√• graferne nedenfor):

!linear | overfit  
-------------------------|--------------------------  
**Line√¶r model, 2 parametre** | **Ikke-line√¶r model, 7 parametre**  
Tr√¶ningsfejl = 5.3 | Tr√¶ningsfejl = 0  
Valideringsfejl = 5.1 | Valideringsfejl = 20  

* Til venstre ser vi en god line√¶r tiln√¶rmelse. Fordi antallet af parametre er passende, fanger modellen den underliggende fordeling af punkterne korrekt.
* Til h√∏jre er modellen for kraftfuld. Fordi vi kun har 5 punkter, og modellen har 7 parametre, kan den tilpasse sig s√•dan, at den g√•r igennem alle punkterne, hvilket giver en tr√¶ningsfejl p√• 0. Det forhindrer dog modellen i at forst√• det korrekte m√∏nster bag dataene, og derfor er valideringsfejlen meget h√∏j.

Det er meget vigtigt at finde den rette balance mellem modellens kompleksitet (antal parametre) og antallet af tr√¶ningspr√∏ver.

## Hvorfor overfitting opst√•r

  * Ikke nok tr√¶ningsdata  
  * For kraftfuld model  
  * For meget st√∏j i inputdata  

## Hvordan man opdager overfitting

Som du kan se p√• grafen ovenfor, kan overfitting opdages ved en meget lav tr√¶ningsfejl og en h√∏j valideringsfejl. Normalt vil b√•de tr√¶nings- og valideringsfejl falde under tr√¶ningen, men p√• et tidspunkt kan valideringsfejlen stoppe med at falde og begynde at stige. Det er et tegn p√• overfitting og en indikator for, at vi sandsynligvis b√∏r stoppe tr√¶ningen p√• dette tidspunkt (eller i det mindste tage et snapshot af modellen).

overfitting

## Hvordan man forhindrer overfitting

Hvis du kan se, at overfitting opst√•r, kan du g√∏re en af f√∏lgende:

 * √òge m√¶ngden af tr√¶ningsdata  
 * Mindske modellens kompleksitet  
 * Bruge en form for regularisering, som Dropout, som vi vil se n√¶rmere p√• senere.

## Overfitting og Bias-Variance Tradeoff

Overfitting er faktisk et tilf√¶lde af et mere generelt problem i statistik kaldet Bias-Variance Tradeoff. Hvis vi ser p√• mulige fejlkilder i vores model, kan vi identificere to typer fejl:

* **Bias-fejl** skyldes, at vores algoritme ikke kan fange sammenh√¶ngen i tr√¶ningsdata korrekt. Det kan skyldes, at modellen ikke er kraftfuld nok (**underfitting**).
* **Variance-fejl** skyldes, at modellen tilpasser sig st√∏j i inputdata i stedet for meningsfulde sammenh√¶nge (**overfitting**).

Under tr√¶ningen falder bias-fejlen (efterh√•nden som modellen l√¶rer at tiln√¶rme data), mens variance-fejlen stiger. Det er vigtigt at stoppe tr√¶ningen ‚Äì enten manuelt (n√•r vi opdager overfitting) eller automatisk (ved at indf√∏re regularisering) ‚Äì for at forhindre overfitting.

## Konklusion

I denne lektion har du l√¶rt om forskellene mellem de forskellige API‚Äôer i de to mest popul√¶re AI-frameworks, TensorFlow og PyTorch. Derudover har du l√¶rt om et meget vigtigt emne, overfitting.

## üöÄ Udfordring

I de tilh√∏rende notebooks finder du 'opgaver' nederst; arbejd dig igennem notebooks og l√∏s opgaverne.

## Review & Selvstudie

Lav lidt research om f√∏lgende emner:

- TensorFlow  
- PyTorch  
- Overfitting  

Stil dig selv f√∏lgende sp√∏rgsm√•l:

- Hvad er forskellen p√• TensorFlow og PyTorch?  
- Hvad er forskellen p√• overfitting og underfitting?  

## Opgave

I dette laboratorium skal du l√∏se to klassifikationsproblemer ved hj√¶lp af enkelt- og flerlags fuldt forbundne netv√¶rk ved brug af PyTorch eller TensorFlow.

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, bedes du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det oprindelige dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os intet ansvar for misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.