<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:47:30+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "da"
}
-->
# Introduktion til Neurale NetvÃ¦rk. Multi-Layered Perceptron

I det forrige afsnit lÃ¦rte du om den simpleste model for neurale netvÃ¦rk â€“ en enkeltlags perceptron, en lineÃ¦r to-klasses klassifikationsmodel.

I dette afsnit udvider vi denne model til en mere fleksibel ramme, der giver os mulighed for at:

* udfÃ¸re **multi-klasse klassifikation** ud over to-klasses
* lÃ¸se **regressionsproblemer** ud over klassifikation
* adskille klasser, der ikke er lineÃ¦rt separable

Vi vil ogsÃ¥ udvikle vores eget modulÃ¦re framework i Python, som gÃ¸r det muligt at konstruere forskellige neurale netvÃ¦rksarkitekturer.

## Formalisering af MaskinlÃ¦ring

Lad os starte med at formalisere maskinlÃ¦ringsproblemet. Antag, at vi har et trÃ¦ningsdatasÃ¦t **X** med labels **Y**, og vi skal bygge en model *f*, der kan lave de mest prÃ¦cise forudsigelser. Kvaliteten af forudsigelserne mÃ¥les ved en **loss-funktion** â„’. FÃ¸lgende loss-funktioner bruges ofte:

* Til regressionsproblemer, hvor vi skal forudsige et tal, kan vi bruge **absolut fejl** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, eller **kvadreret fejl** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Til klassifikation bruger vi **0-1 loss** (som i praksis svarer til modellens **nÃ¸jagtighed**), eller **logistisk loss**.

For en enkeltlags perceptron var funktionen *f* defineret som en lineÃ¦r funktion *f(x)=wx+b* (her er *w* vÃ¦gtmatricen, *x* er inputfeature-vektoren, og *b* er bias-vektoren). For forskellige neurale netvÃ¦rksarkitekturer kan denne funktion antage en mere kompleks form.

> I tilfÃ¦lde af klassifikation er det ofte Ã¸nskeligt at fÃ¥ sandsynligheder for de tilsvarende klasser som netvÃ¦rkets output. For at omdanne vilkÃ¥rlige tal til sandsynligheder (f.eks. for at normalisere output) bruger vi ofte **softmax**-funktionen Ïƒ, og funktionen *f* bliver *f(x)=Ïƒ(wx+b)*

I definitionen af *f* ovenfor kaldes *w* og *b* for **parametre** Î¸=âŸ¨*w,b*âŸ©. Givet datasÃ¦ttet âŸ¨**X**,**Y**âŸ© kan vi beregne den samlede fejl pÃ¥ hele datasÃ¦ttet som en funktion af parametrene Î¸.

> âœ… **MÃ¥let med trÃ¦ning af neurale netvÃ¦rk er at minimere fejlen ved at variere parametrene Î¸**

## Gradient Descent Optimering

Der findes en velkendt metode til funktionsoptimering kaldet **gradient descent**. Ideen er, at vi kan beregne en afledt funktion (i flerdimensionelle tilfÃ¦lde kaldet **gradient**) af loss-funktionen med hensyn til parametrene, og variere parametrene pÃ¥ en mÃ¥de, sÃ¥ fejlen mindskes. Dette kan formaliseres som fÃ¸lger:

* Initialiser parametrene med nogle tilfÃ¦ldige vÃ¦rdier w<sup>(0)</sup>, b<sup>(0)</sup>
* Gentag fÃ¸lgende trin mange gange:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Under trÃ¦ningen forventes optimeringstrinene at blive beregnet ud fra hele datasÃ¦ttet (husk at loss beregnes som summen over alle trÃ¦ningsprÃ¸ver). I praksis tager vi dog smÃ¥ portioner af datasÃ¦ttet kaldet **minibatches**, og beregner gradienterne ud fra et delmÃ¦ngde af data. Fordi delmÃ¦ngden vÃ¦lges tilfÃ¦ldigt hver gang, kaldes denne metode **stokastisk gradient descent** (SGD).

## Multi-Layered Perceptrons og Backpropagation

Et enkeltlags netvÃ¦rk, som vi har set ovenfor, kan klassificere lineÃ¦rt separable klasser. For at bygge en mere kompleks model kan vi kombinere flere lag i netvÃ¦rket. Matematisk betyder det, at funktionen *f* fÃ¥r en mere kompleks form og beregnes i flere trin:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Her er Î± en **non-lineÃ¦r aktiveringsfunktion**, Ïƒ er softmax-funktionen, og parametrene Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradient descent-algoritmen forbliver den samme, men det bliver svÃ¦rere at beregne gradienterne. Ved hjÃ¦lp af kÃ¦dereglen for differentiation kan vi beregne afledte som:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… KÃ¦dereglen bruges til at beregne afledte af loss-funktionen med hensyn til parametrene.

BemÃ¦rk, at den venstre del af alle disse udtryk er den samme, og derfor kan vi effektivt beregne afledte ved at starte fra loss-funktionen og bevÃ¦ge os "baglÃ¦ns" gennem beregningsgrafen. Derfor kaldes metoden til trÃ¦ning af et multi-lags perceptron for **backpropagation**, eller blot 'backprop'.

> TODO: billedhenvisning

> âœ… Vi vil gennemgÃ¥ backprop i meget stÃ¸rre detaljer i vores notebook-eksempel.

## Konklusion

I denne lektion har vi bygget vores eget neurale netvÃ¦rksbibliotek, og vi har brugt det til en simpel todimensionel klassifikationsopgave.

## ğŸš€ Udfordring

I den tilhÃ¸rende notebook skal du implementere dit eget framework til at bygge og trÃ¦ne multi-lags perceptrons. Du vil kunne se i detaljer, hvordan moderne neurale netvÃ¦rk fungerer.

FortsÃ¦t til OwnFramework-notebooken og arbejd dig igennem den.

## Gennemgang & Selvstudie

Backpropagation er en almindelig algoritme brugt i AI og ML, som er vÃ¦rd at studere nÃ¦rmere.

## Opgave

I dette laboratorium skal du bruge det framework, du har konstrueret i denne lektion, til at lÃ¸se MNIST hÃ¥ndskrevne cifre-klassifikation.

* Instruktioner
* Notebook

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjÃ¦lp af AI-oversÃ¦ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestrÃ¦ber os pÃ¥ nÃ¸jagtighed, bedes du vÃ¦re opmÃ¦rksom pÃ¥, at automatiserede oversÃ¦ttelser kan indeholde fejl eller unÃ¸jagtigheder. Det oprindelige dokument pÃ¥ dets oprindelige sprog bÃ¸r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversÃ¦ttelse. Vi pÃ¥tager os intet ansvar for misforstÃ¥elser eller fejltolkninger, der opstÃ¥r som fÃ¸lge af brugen af denne oversÃ¦ttelse.