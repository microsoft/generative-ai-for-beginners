{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "В этом уроке мы рассмотрим:\n",
    "- Что такое вызов функции и где он применяется\n",
    "- Как создать вызов функции с помощью OpenAI\n",
    "- Как интегрировать вызов функции в приложение\n",
    "\n",
    "## Цели обучения\n",
    "\n",
    "После завершения этого урока вы будете знать и понимать:\n",
    "\n",
    "- Для чего используется вызов функции\n",
    "- Как настроить вызов функции с помощью сервиса OpenAI\n",
    "- Как разрабатывать эффективные вызовы функций для вашего приложения\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Понимание вызова функций\n",
    "\n",
    "В этом уроке мы хотим создать функцию для нашего образовательного стартапа, которая позволит пользователям с помощью чат-бота находить технические курсы. Мы будем рекомендовать курсы, подходящие их уровню знаний, текущей должности и интересующим технологиям.\n",
    "\n",
    "Для этого мы используем комбинацию:\n",
    " - `OpenAI` для создания чата с пользователем\n",
    " - `Microsoft Learn Catalog API` для поиска курсов по запросу пользователя\n",
    " - `Function Calling` для передачи запроса пользователя в функцию, которая выполнит API-запрос.\n",
    "\n",
    "Для начала давайте разберёмся, зачем вообще использовать вызов функций:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # получаем новый ответ от GPT, который уже видит ответ функции\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем нужен Function Calling\n",
    "\n",
    "Если вы уже проходили другие уроки этого курса, вы, вероятно, понимаете, насколько мощными могут быть большие языковые модели (LLM). Надеюсь, вы также заметили и их ограничения.\n",
    "\n",
    "Function Calling — это функция в OpenAI Service, созданная для решения следующих проблем:\n",
    "\n",
    "Несогласованный формат ответов:\n",
    "- До появления function calling ответы от языковой модели были неструктурированными и непоследовательными. Разработчикам приходилось писать сложный код для проверки и обработки всех возможных вариантов вывода.\n",
    "\n",
    "Ограниченная интеграция с внешними данными:\n",
    "- Раньше было сложно использовать данные из других частей приложения в чате с моделью.\n",
    "\n",
    "Благодаря стандартизации формата ответов и возможности легко интегрировать внешние данные, function calling упрощает разработку и уменьшает необходимость в дополнительной проверке данных.\n",
    "\n",
    "Пользователи не могли получить ответы вроде «Какая сейчас погода в Стокгольме?». Это связано с тем, что модели ограничены временем, на котором они были обучены.\n",
    "\n",
    "Давайте рассмотрим пример ниже, который иллюстрирует эту проблему:\n",
    "\n",
    "Допустим, мы хотим создать базу данных студентов, чтобы предлагать им подходящие курсы. Ниже приведены два описания студентов, которые очень похожи по содержанию данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим отправить это LLM для разбора данных. В дальнейшем это можно будет использовать в нашем приложении для отправки в API или хранения в базе данных.\n",
    "\n",
    "Давайте создадим два одинаковых запроса, в которых мы укажем LLM, какая информация нас интересует:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим отправить это LLM для разбора частей, важных для нашего продукта. Таким образом, мы можем создать два идентичных запроса, чтобы дать инструкции LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После создания этих двух подсказок мы отправим их в LLM с помощью `openai.ChatCompletion`. Мы сохраняем подсказку в переменной `messages` и назначаем роль `user`. Это делается для имитации сообщения от пользователя, написанного в чат-бот.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя подсказки одинаковые, а описания похожи, мы можем получить разные форматы свойства `Grades`.\n",
    "\n",
    "Если выполнить ячейку выше несколько раз, формат может быть как `3.7`, так и `3.7 GPA`.\n",
    "\n",
    "Это происходит потому, что LLM принимает неструктурированные данные в виде текстовой подсказки и также возвращает неструктурированные данные. Нам нужен структурированный формат, чтобы мы знали, чего ожидать при сохранении или использовании этих данных.\n",
    "\n",
    "Используя функциональные вызовы, мы можем быть уверены, что получаем обратно структурированные данные. При использовании функциональных вызовов LLM на самом деле не вызывает и не выполняет никакие функции. Вместо этого мы создаём структуру, которой LLM должен следовать в своих ответах. Затем мы используем эти структурированные ответы, чтобы понять, какую функцию запускать в наших приложениях.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Диаграмма потока вызова функции](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.ru.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры использования вызова функций\n",
    "\n",
    "**Вызов внешних инструментов**  \n",
    "Чат-боты отлично отвечают на вопросы пользователей. С помощью вызова функций чат-боты могут использовать сообщения пользователей для выполнения определённых задач. Например, студент может попросить чат-бота: \"Отправь письмо моему преподавателю с просьбой о дополнительной помощи по этому предмету\". Для этого можно вызвать функцию `send_email(to: string, body: string)`\n",
    "\n",
    "**Создание API или запросов к базе данных**  \n",
    "Пользователи могут находить информацию, используя естественный язык, который преобразуется в форматированный запрос или API-запрос. Например, учитель может спросить: \"Кто из студентов сдал последнее задание?\", что может вызвать функцию с именем `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Создание структурированных данных**  \n",
    "Пользователи могут взять текстовый блок или CSV-файл и использовать LLM для извлечения из него важной информации. Например, студент может преобразовать статью из Википедии о мирных соглашениях, чтобы создать AI-карточки для запоминания. Это можно сделать с помощью функции `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Создание первого вызова функции\n",
    "\n",
    "Процесс создания вызова функции включает 3 основных шага:\n",
    "1. Вызовите Chat Completions API с перечнем ваших функций и сообщением пользователя\n",
    "2. Прочитайте ответ модели, чтобы выполнить действие, например, вызвать функцию или API\n",
    "3. Сделайте еще один вызов к Chat Completions API с ответом вашей функции, чтобы использовать эту информацию для создания ответа пользователю.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flow of a Function Call](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.ru.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Элементы вызова функции\n",
    "\n",
    "#### Ввод пользователя\n",
    "\n",
    "Первым шагом является создание сообщения пользователя. Это сообщение можно задать динамически, получив значение из текстового поля, или указать его здесь вручную. Если вы впервые работаете с API Chat Completions, необходимо определить `role` и `content` сообщения.\n",
    "\n",
    "`role` может быть либо `system` (создание правил), `assistant` (модель), либо `user` (конечный пользователь). Для вызова функции мы укажем роль как `user` и приведём пример вопроса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание функций.\n",
    "\n",
    "Далее мы определим функцию и её параметры. Здесь мы будем использовать только одну функцию под названием `search_courses`, но вы можете создать несколько функций.\n",
    "\n",
    "**Важно**: Функции включаются в системное сообщение для LLM и учитываются в общем количестве доступных вам токенов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Определения**\n",
    "\n",
    "Структура определения функции имеет несколько уровней, у каждого из которых есть свои свойства. Вот разбор вложенной структуры:\n",
    "\n",
    "**Свойства функции верхнего уровня:**\n",
    "\n",
    "`name` - Имя функции, которую мы хотим вызвать.\n",
    "\n",
    "`description` - Описание того, как работает функция. Здесь важно быть конкретным и понятным.\n",
    "\n",
    "`parameters` - Список значений и формат, которые вы хотите получить в ответе от модели.\n",
    "\n",
    "**Свойства объекта parameters:**\n",
    "\n",
    "`type` - Тип данных объекта параметров (обычно \"object\")\n",
    "\n",
    "`properties` - Список конкретных значений, которые модель будет использовать в своем ответе\n",
    "\n",
    "**Свойства отдельного параметра:**\n",
    "\n",
    "`name` - Неявно определяется по ключу свойства (например, \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Тип данных этого конкретного параметра (например, \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - Описание конкретного параметра\n",
    "\n",
    "**Необязательные свойства:**\n",
    "\n",
    "`required` - Массив, в котором перечислены параметры, обязательные для выполнения вызова функции\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вызов функции\n",
    "После того как функция определена, теперь нужно включить её в запрос к Chat Completion API. Для этого мы добавляем `functions` в запрос. В данном случае это `functions=functions`.\n",
    "\n",
    "Также есть возможность установить параметр `function_call` в значение `auto`. Это значит, что мы позволим LLM самостоятельно определить, какую функцию вызвать, исходя из сообщения пользователя, вместо того чтобы указывать это вручную.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на ответ и обратим внимание на его форматирование:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Вы можете увидеть, что вызывается функция с определённым именем, и из сообщения пользователя LLM смог найти данные для заполнения аргументов этой функции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Интеграция вызовов функций в приложение.\n",
    "\n",
    "После того как мы протестировали отформатированный ответ от LLM, теперь мы можем интегрировать это в приложение.\n",
    "\n",
    "### Управление процессом\n",
    "\n",
    "Чтобы интегрировать это в наше приложение, давайте выполним следующие шаги:\n",
    "\n",
    "Сначала вызовем сервисы OpenAI и сохраним сообщение в переменную с именем `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы определим функцию, которая будет вызывать API Microsoft Learn для получения списка курсов:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве лучшей практики мы сначала проверим, хочет ли модель вызвать функцию. После этого мы создадим одну из доступных функций и сопоставим её с вызываемой функцией.\n",
    "Затем мы возьмём аргументы функции и сопоставим их с аргументами из LLM.\n",
    "\n",
    "Наконец, мы добавим сообщение о вызове функции и значения, которые были возвращены сообщением `search_courses`. Это даст LLM всю необходимую информацию для ответа пользователю на естественном языке.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание по программированию\n",
    "\n",
    "Отличная работа! Чтобы продолжить изучение вызова функций OpenAI, вы можете создать: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Больше параметров функции, которые могут помочь учащимся находить больше курсов. Доступные параметры API можно найти здесь:\n",
    " - Создайте еще один вызов функции, который принимает больше информации от учащегося, например, его родной язык\n",
    " - Реализуйте обработку ошибок на случай, если вызов функции и/или API не возвращает подходящих курсов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от ответственности**:  \nЭтот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T20:43:00+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "ru"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}