{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение \n",
    "\n",
    "В этом уроке будут рассмотрены: \n",
    "- Что такое вызов функции и случаи его использования \n",
    "- Как создать вызов функции с помощью OpenAI \n",
    "- Как интегрировать вызов функции в приложение \n",
    "\n",
    "## Цели обучения \n",
    "\n",
    "После завершения этого урока вы будете знать, как и понимать: \n",
    "\n",
    "- Цель использования вызова функции \n",
    "- Настройка вызова функции с использованием сервиса OpenAI \n",
    "- Проектирование эффективных вызовов функций для сценария использования вашего приложения\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Понимание вызовов функций\n",
    "\n",
    "Для этого урока мы хотим создать функцию для нашего образовательного стартапа, которая позволит пользователям использовать чат-бота для поиска технических курсов. Мы будем рекомендовать курсы, соответствующие их уровню навыков, текущей роли и интересующей технологии.\n",
    "\n",
    "Для этого мы будем использовать комбинацию:\n",
    " - `OpenAI` для создания чат-опыта для пользователя\n",
    " - `Microsoft Learn Catalog API` для помощи пользователям в поиске курсов на основе запроса пользователя\n",
    " - `Function Calling` для обработки запроса пользователя и отправки его в функцию для выполнения API-запроса.\n",
    "\n",
    "Для начала давайте посмотрим, почему мы хотим использовать вызов функций в первую очередь:\n",
    "\n",
    "print(\"Сообщения в следующем запросе:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # получить новый ответ от GPT, который может видеть ответ функции\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Почему вызов функций\n",
    "\n",
    "Если вы прошли любой другой урок в этом курсе, вы, вероятно, понимаете мощь использования больших языковых моделей (LLM). Надеюсь, вы также видите некоторые их ограничения.\n",
    "\n",
    "Вызов функций — это функция сервиса OpenAI, разработанная для решения следующих задач:\n",
    "\n",
    "Несогласованное форматирование ответов:\n",
    "- До появления вызова функций ответы большой языковой модели были неструктурированными и непоследовательными. Разработчикам приходилось писать сложный код валидации для обработки каждого варианта вывода.\n",
    "\n",
    "Ограниченная интеграция с внешними данными:\n",
    "- До этой функции было сложно включать данные из других частей приложения в контекст чата.\n",
    "\n",
    "Стандартизируя форматы ответов и обеспечивая бесшовную интеграцию с внешними данными, вызов функций упрощает разработку и снижает необходимость дополнительной логики валидации.\n",
    "\n",
    "Пользователи не могли получить ответы на вопросы вроде «Какая сейчас погода в Стокгольме?». Это связано с тем, что модели ограничены временем, на которое были обучены данные.\n",
    "\n",
    "Рассмотрим пример ниже, иллюстрирующий эту проблему:\n",
    "\n",
    "Допустим, мы хотим создать базу данных студентов, чтобы предлагать им подходящие курсы. Ниже приведены два описания студентов, которые очень похожи по содержащимся в них данным.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим отправить это в LLM для анализа данных. Это позже можно будет использовать в нашем приложении для отправки на API или сохранения в базе данных.\n",
    "\n",
    "Давайте создадим два идентичных запроса, в которых мы укажем LLM, какую информацию мы хотим получить:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим отправить это в LLM, чтобы разобрать части, важные для нашего продукта. Таким образом, мы можем создать два идентичных запроса для инструктажа LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После создания этих двух подсказок мы отправим их в LLM с помощью `openai.ChatCompletion`. Мы сохраняем подсказку в переменной `messages` и назначаем роль `user`. Это имитирует сообщение от пользователя, написанное для чат-бота.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем отправить оба запроса в LLM и изучить полученный ответ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя подсказки одинаковы, а описания похожи, мы можем получить разные форматы свойства `Grades`.\n",
    "\n",
    "Если запустить приведённую выше ячейку несколько раз, формат может быть `3.7` или `3.7 GPA`.\n",
    "\n",
    "Это происходит потому, что LLM принимает неструктурированные данные в виде написанной подсказки и также возвращает неструктурированные данные. Нам нужен структурированный формат, чтобы знать, чего ожидать при сохранении или использовании этих данных.\n",
    "\n",
    "Используя функциональный вызов, мы можем убедиться, что получаем обратно структурированные данные. При использовании функционального вызова LLM фактически не вызывает и не выполняет никаких функций. Вместо этого мы создаём структуру, которой LLM должен следовать в своих ответах. Затем мы используем эти структурированные ответы, чтобы знать, какую функцию запускать в наших приложениях.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Диаграмма потока вызова функции](../../../../translated_images/Function-Flow.083875364af4f4bb.ru.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы можем взять то, что возвращается из функции, и отправить это обратно в LLM. LLM затем ответит, используя естественный язык, чтобы ответить на запрос пользователя.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сценарии использования вызовов функций\n",
    "\n",
    "**Вызов внешних инструментов**  \n",
    "Чат-боты отлично подходят для предоставления ответов на вопросы пользователей. С помощью вызова функций чат-боты могут использовать сообщения от пользователей для выполнения определённых задач. Например, студент может попросить чат-бота «Отправь письмо моему преподавателю с просьбой о дополнительной помощи по этому предмету». Это может привести к вызову функции `send_email(to: string, body: string)`.\n",
    "\n",
    "**Создание запросов к API или базе данных**  \n",
    "Пользователи могут находить информацию, используя естественный язык, который преобразуется в форматированный запрос или API-запрос. Примером может служить учитель, который спрашивает «Кто из студентов выполнил последнее задание», что может вызвать функцию с именем `get_completed(student_name: string, assignment: int, current_status: string)`.\n",
    "\n",
    "**Создание структурированных данных**  \n",
    "Пользователи могут взять блок текста или CSV и использовать LLM для извлечения из него важной информации. Например, студент может преобразовать статью из Википедии о мирных соглашениях для создания AI-флешкарт. Это можно сделать с помощью функции `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Создание вашего первого вызова функции\n",
    "\n",
    "Процесс создания вызова функции включает 3 основных шага:  \n",
    "1. Вызов API Chat Completions с списком ваших функций и сообщением пользователя  \n",
    "2. Чтение ответа модели для выполнения действия, например, вызова функции или API  \n",
    "3. Выполнение еще одного вызова API Chat Completions с ответом вашей функции, чтобы использовать эту информацию для создания ответа пользователю.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Поток вызова функции](../../../../translated_images/LLM-Flow.3285ed8caf4796d7.ru.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Элементы вызова функции \n",
    "\n",
    "#### Ввод пользователя \n",
    "\n",
    "Первым шагом является создание сообщения пользователя. Оно может быть динамически присвоено путем получения значения из текстового поля или вы можете присвоить значение здесь. Если вы впервые работаете с API Chat Completions, нам нужно определить `role` и `content` сообщения. \n",
    "\n",
    "`role` может быть либо `system` (создание правил), `assistant` (модель) или `user` (конечный пользователь). Для вызова функции мы присвоим это значение как `user` и пример вопроса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание функций.\n",
    "\n",
    "Далее мы определим функцию и параметры этой функции. Здесь мы используем только одну функцию под названием `search_courses`, но вы можете создавать несколько функций.\n",
    "\n",
    "**Важно**: Функции включаются в системное сообщение для LLM и учитываются в количестве доступных вам токенов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Определения** \n",
    "\n",
    "Структура определения функции имеет несколько уровней, каждый со своими свойствами. Вот разбор вложенной структуры:\n",
    "\n",
    "**Свойства функции верхнего уровня:**\n",
    "\n",
    "`name` - Имя функции, которую мы хотим вызвать. \n",
    "\n",
    "`description` - Описание того, как работает функция. Здесь важно быть конкретным и ясным. \n",
    "\n",
    "`parameters` - Список значений и формата, которые вы хотите, чтобы модель использовала в своем ответе. \n",
    "\n",
    "**Свойства объекта параметров:**\n",
    "\n",
    "`type` - Тип данных объекта параметров (обычно \"object\").\n",
    "\n",
    "`properties` - Список конкретных значений, которые модель будет использовать в своем ответе. \n",
    "\n",
    "**Свойства отдельных параметров:**\n",
    "\n",
    "`name` - Неявно определяется ключом свойства (например, \"role\", \"product\", \"level\").\n",
    "\n",
    "`type` - Тип данных этого конкретного параметра (например, \"string\", \"number\", \"boolean\"). \n",
    "\n",
    "`description` - Описание конкретного параметра. \n",
    "\n",
    "**Необязательные свойства:**\n",
    "\n",
    "`required` - Массив, перечисляющий параметры, которые обязательны для выполнения вызова функции. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вызов функции  \n",
    "После определения функции нам нужно включить её в вызов API Chat Completion. Мы делаем это, добавляя `functions` в запрос. В данном случае `functions=functions`.  \n",
    "\n",
    "Также есть опция установить `function_call` в значение `auto`. Это означает, что мы позволим LLM самостоятельно решить, какую функцию вызвать, исходя из сообщения пользователя, вместо того чтобы назначать это вручную.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на ответ и увидим, как он отформатирован:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Вы можете видеть, что вызывается имя функции, и из сообщения пользователя LLM смог найти данные, соответствующие аргументам функции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Интеграция вызовов функций в приложение. \n",
    "\n",
    "\n",
    "После того как мы протестировали отформатированный ответ от LLM, теперь мы можем интегрировать это в приложение. \n",
    "\n",
    "### Управление потоком \n",
    "\n",
    "Чтобы интегрировать это в наше приложение, давайте выполним следующие шаги: \n",
    "\n",
    "Сначала сделаем вызов к сервисам OpenAI и сохраним сообщение в переменную с именем `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы определим функцию, которая будет вызывать API Microsoft Learn для получения списка курсов:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве лучшей практики мы затем проверим, хочет ли модель вызвать функцию. После этого мы создадим одну из доступных функций и сопоставим её с вызываемой функцией.  \n",
    "Затем мы возьмём аргументы функции и сопоставим их с аргументами из LLM.\n",
    "\n",
    "Наконец, мы добавим сообщение о вызове функции и значения, которые были возвращены сообщением `search_courses`. Это даёт LLM всю необходимую информацию для  \n",
    "ответа пользователю на естественном языке.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы отправим обновленное сообщение в LLM, чтобы получить ответ на естественном языке вместо ответа в формате JSON API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача по программированию\n",
    "\n",
    "Отличная работа! Чтобы продолжить изучение вызова функций OpenAI, вы можете создать: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    " - Больше параметров функции, которые могут помочь учащимся найти больше курсов. Доступные параметры API можно найти здесь:  \n",
    " - Создайте еще один вызов функции, который принимает больше информации от учащегося, например, их родной язык  \n",
    " - Создайте обработку ошибок на случай, если вызов функции и/или вызов API не возвращает подходящих курсов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Отказ от ответственности**:  \nЭтот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, имейте в виду, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обращаться к профессиональному переводу, выполненному человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T08:50:17+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "ru"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}