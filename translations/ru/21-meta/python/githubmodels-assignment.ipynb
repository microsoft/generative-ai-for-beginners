{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с моделями семейства Meta\n",
    "\n",
    "## Введение\n",
    "\n",
    "В этом уроке мы рассмотрим:\n",
    "\n",
    "- Два основных представителя семейства Meta — Llama 3.1 и Llama 3.2\n",
    "- Сценарии и задачи, для которых подходит каждая из моделей\n",
    "- Пример кода, демонстрирующий уникальные возможности каждой модели\n",
    "\n",
    "## Семейство моделей Meta\n",
    "\n",
    "В этом уроке мы познакомимся с двумя моделями из семейства Meta, также известного как \"Llama Herd\" — Llama 3.1 и Llama 3.2\n",
    "\n",
    "Эти модели представлены в нескольких вариантах и доступны на маркетплейсе моделей Github. Подробнее о том, как использовать Github Models для [прототипирования с AI-моделями](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Варианты моделей:\n",
    "- Llama 3.1 — 70B Instruct\n",
    "- Llama 3.1 — 405B Instruct\n",
    "- Llama 3.2 — 11B Vision Instruct\n",
    "- Llama 3.2 — 90B Vision Instruct\n",
    "\n",
    "*Примечание: Llama 3 также доступна на Github Models, но в этом уроке она не рассматривается*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "С 405 миллиардами параметров Llama 3.1 относится к категории открытых LLM.\n",
    "\n",
    "Эта версия является улучшением по сравнению с предыдущей Llama 3 и предлагает:\n",
    "\n",
    "- Более широкое контекстное окно — 128k токенов против 8k токенов\n",
    "- Большее максимальное количество выходных токенов — 4096 против 2048\n",
    "- Улучшенная поддержка разных языков — благодаря увеличению объёма обучающих данных\n",
    "\n",
    "Это позволяет Llama 3.1 справляться с более сложными задачами при создании GenAI-приложений, включая:\n",
    "- Вызов функций на уровне модели — возможность обращаться к внешним инструментам и функциям вне стандартного процесса LLM\n",
    "- Улучшенная производительность RAG — благодаря увеличенному контекстному окну\n",
    "- Генерация синтетических данных — возможность создавать эффективные данные для задач, таких как дообучение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вызов нативных функций\n",
    "\n",
    "Llama 3.1 была дообучена для более эффективного использования функций и инструментов. В ней также есть два встроенных инструмента, которые модель может определить как необходимые для использования в зависимости от запроса пользователя. Эти инструменты:\n",
    "\n",
    "- **Brave Search** — может использоваться для получения актуальной информации, например, прогноза погоды, с помощью веб-поиска\n",
    "- **Wolfram Alpha** — подходит для более сложных математических вычислений, поэтому нет необходимости писать собственные функции.\n",
    "\n",
    "Вы также можете создавать свои собственные инструменты, которые LLM сможет вызывать.\n",
    "\n",
    "В примере кода ниже:\n",
    "\n",
    "- Мы определяем доступные инструменты (brave_search, wolfram_alpha) в системном промпте.\n",
    "- Отправляем пользовательский запрос о погоде в определённом городе.\n",
    "- LLM ответит вызовом инструмента Brave Search, который будет выглядеть так: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Примечание: В этом примере происходит только вызов инструмента. Если вы хотите получить результаты, вам нужно создать бесплатный аккаунт на странице Brave API и определить саму функцию.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Несмотря на то, что Llama 3.1 является LLM, у неё есть ограничение — отсутствие мультимодальности. То есть, она не может использовать разные типы входных данных, например, изображения, в качестве подсказок и давать на них ответы. Эта возможность стала одной из ключевых особенностей Llama 3.2. Среди новых функций также:\n",
    "\n",
    "- Мультимодальность — умеет обрабатывать как текстовые, так и визуальные подсказки\n",
    "- Варианты малых и средних размеров (11B и 90B) — это даёт гибкость при развертывании,\n",
    "- Только текстовые варианты (1B и 3B) — позволяют запускать модель на мобильных и edge-устройствах с низкой задержкой\n",
    "\n",
    "Поддержка мультимодальности — это значительный шаг вперёд для мира open source моделей. В примере кода ниже используется и изображение, и текстовая подсказка, чтобы получить анализ изображения от Llama 3.2 90B.\n",
    "\n",
    "### Мультимодальная поддержка в Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом обучение не заканчивается — продолжайте свой путь\n",
    "\n",
    "После завершения этого урока загляните в нашу [подборку по обучению генеративному ИИ](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), чтобы продолжить развивать свои знания в области генеративного искусственного интеллекта!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от ответственности**:  \nЭтот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:36:40+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "ru"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}