{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тонкая настройка моделей Open AI\n",
    "\n",
    "Этот блокнот основан на актуальных рекомендациях из документации [Fine Tuning](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst) от Open AI.\n",
    "\n",
    "Тонкая настройка улучшает работу базовых моделей для вашего приложения за счёт дополнительного обучения на данных и контексте, которые важны для конкретного случая использования. Обратите внимание, что такие техники, как _few shot learning_ и _retrieval augmented generation_, позволяют обогатить стандартный запрос релевантными данными для повышения качества. Однако эти подходы ограничены максимальным размером окна токенов выбранной базовой модели.\n",
    "\n",
    "При тонкой настройке мы фактически дообучаем саму модель на нужных данных (что позволяет использовать гораздо больше примеров, чем помещается в максимальное окно токенов) и развёртываем _кастомную_ версию модели, которой больше не нужно предоставлять примеры во время инференса. Это не только повышает эффективность проектирования запросов (у нас появляется больше свободы в использовании окна токенов для других целей), но и потенциально снижает затраты (за счёт уменьшения количества токенов, которые нужно отправлять модели при инференсе).\n",
    "\n",
    "Тонкая настройка включает 4 шага:\n",
    "1. Подготовить обучающие данные и загрузить их.\n",
    "1. Запустить обучение для получения дообученной модели.\n",
    "1. Оценить дообученную модель и доработать её для повышения качества.\n",
    "1. Развернуть дообученную модель для инференса, когда результат вас устроит.\n",
    "\n",
    "Обратите внимание, что не все базовые модели поддерживают тонкую настройку — [проверьте документацию OpenAI](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned?WT.mc_id=academic-105485-koreyst) для получения самой свежей информации. Также можно дообучать уже дообученную модель. В этом уроке мы будем использовать `gpt-35-turbo` в качестве целевой базовой модели для тонкой настройки.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 1.1: Подготовьте ваш датасет\n",
    "\n",
    "Давайте создадим чат-бота, который поможет вам разобраться в периодической таблице элементов, отвечая на вопросы о каждом элементе в форме лимерика. В _этом_ простом уроке мы просто создадим датасет для обучения модели с несколькими примерами ответов, чтобы показать ожидаемый формат данных. В реальном проекте вам потребуется собрать датасет с гораздо большим количеством примеров. Также вы можете использовать открытый датасет (по вашей предметной области), если такой существует, и отформатировать его для использования в дообучении.\n",
    "\n",
    "Поскольку мы работаем с `gpt-35-turbo` и ожидаем однократный ответ (chat completion), мы можем создать примеры, используя [этот рекомендуемый формат](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?WT.mc_id=academic-105485-koreyst), соответствующий требованиям OpenAI для chat completion. Если вы ожидаете диалог с несколькими репликами, используйте [формат для многотуровых примеров](https://platform.openai.com/docs/guides/fine-tuning/multi-turn-chat-examples?WT.mc_id=academic-105485-koreyst), который включает параметр `weight`, чтобы указать, какие сообщения использовать (или не использовать) в процессе дообучения.\n",
    "\n",
    "В этом уроке мы воспользуемся более простым однократным форматом. Данные представлены в [формате jsonl](https://jsonlines.org/?WT.mc_id=academic-105485-koreyst) — по одной записи на строку, каждая из которых оформлена как объект в формате JSON. В примере ниже показаны 2 записи — полный набор примеров (10 штук), который мы будем использовать для нашего урока по дообучению, смотрите в [training-data.jsonl](../../../../../18-fine-tuning/python/openai/training-data.jsonl). **Обратите внимание:** Каждая запись _должна_ быть определена в одной строке (не разбивайте на несколько строк, как это обычно делается в отформатированном JSON-файле)\n",
    "\n",
    "```json\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Gallium\"}, {\"role\": \"assistant\", \"content\": \"Gallium, oh gallium, so light - Melts in your hand, oh what a sight - At 86 degrees - Its liquid with ease - And in semiconductors, it's out of sight\"}]}\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Hydrogen\"}, {\"role\": \"assistant\", \"content\": \"Hydrogen, the first in the line - The lightest of all, so divine - It's in water, you see - And in stars, it's the key - The universe's most common sign\"}]}\n",
    "```\n",
    "\n",
    "В реальном проекте для хороших результатов потребуется гораздо больше примеров — здесь нужно найти баланс между качеством ответов и временем/стоимостью дообучения. Мы используем небольшой набор, чтобы быстро пройти процесс дообучения и показать, как это работает. Более сложный пример дообучения смотрите в [этом примере из OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 1.2 Загрузка вашего набора данных\n",
    "\n",
    "Загрузите данные с помощью Files API [как описано здесь](https://platform.openai.com/docs/guides/fine-tuning/upload-a-training-file). Обратите внимание, что для запуска этого кода необходимо сначала выполнить следующие шаги:\n",
    " - Установить Python-пакет `openai` (убедитесь, что используете версию >=0.28.0 для поддержки последних функций)\n",
    " - Установить переменную окружения `OPENAI_API_KEY` со своим ключом OpenAI API\n",
    "Чтобы узнать больше, ознакомьтесь с [руководством по настройке](./../../../00-course-setup/02-setup-local.md?WT.mc_id=academic-105485-koreyst), предоставленным для курса.\n",
    "\n",
    "Теперь запустите код для создания файла для загрузки из вашего локального файла JSONL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-JdAJcagdOTG6ACNlFWzuzmyV', bytes=4021, created_at=1715566183, filename='training-data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
      "Training File ID: file-JdAJcagdOTG6ACNlFWzuzmyV\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_file = client.files.create(\n",
    "  file=open(\"./training-data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(ft_file)\n",
    "print(\"Training File ID: \" + ft_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2.1: Создайте задачу дообучения с помощью SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', created_at=1715566184, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-EZ6ag0n0S6Zm8eV9BSWKmE6l', result_files=[], seed=830529052, status='validating_files', trained_tokens=None, training_file='file-JdAJcagdOTG6ACNlFWzuzmyV', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)\n",
      "Fine-tuning Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_filejob = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file.id, \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "print(ft_filejob)\n",
    "print(\"Fine-tuning Job ID: \" + ft_filejob.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2.2: Проверьте статус задания\n",
    "\n",
    "Вот что можно сделать с API `client.fine_tuning.jobs`:\n",
    "- `client.fine_tuning.jobs.list(limit=<n>)` — Показать последние n заданий по дообучению\n",
    "- `client.fine_tuning.jobs.retrieve(<job_id>)` — Получить подробную информацию о конкретном задании по дообучению\n",
    "- `client.fine_tuning.jobs.cancel(<job_id>)` — Отменить задание по дообучению\n",
    "- `client.fine_tuning.jobs.list_events(fine_tuning_job_id=<job_id>, limit=<b>)` — Показать до n событий из задания\n",
    "- `client.fine_tuning.jobs.create(model=\"gpt-35-turbo\", training_file=\"your-training-file.jsonl\", ...)`\n",
    "\n",
    "Первый шаг этого процесса — _проверка обучающего файла_, чтобы убедиться, что данные имеют правильный формат.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-GkWiDgZmOsuv4q5cSTEGscY6', created_at=1715566184, level='info', message='Validating training file: file-JdAJcagdOTG6ACNlFWzuzmyV', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-3899xdVTO3LN7Q7LkKLMJUnb', created_at=1715566184, level='info', message='Created fine-tuning job: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_filejob.id, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n",
      "Status: running\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "# Once the training data is validated\n",
    "# Track the job status to see if it is running and when it is complete\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(\"Trained Tokens:\", response.trained_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2.3: Отслеживайте события для мониторинга прогресса\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 85/100: training loss=0.14\n",
      "Step 86/100: training loss=0.00\n",
      "Step 87/100: training loss=0.00\n",
      "Step 88/100: training loss=0.07\n",
      "Step 89/100: training loss=0.00\n",
      "Step 90/100: training loss=0.00\n",
      "Step 91/100: training loss=0.00\n",
      "Step 92/100: training loss=0.00\n",
      "Step 93/100: training loss=0.00\n",
      "Step 94/100: training loss=0.00\n",
      "Step 95/100: training loss=0.08\n",
      "Step 96/100: training loss=0.05\n",
      "Step 97/100: training loss=0.00\n",
      "Step 98/100: training loss=0.00\n",
      "Step 99/100: training loss=0.00\n",
      "Step 100/100: training loss=0.00\n",
      "Checkpoint created at step 80 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyyF2:ckpt-step-80\n",
      "Checkpoint created at step 90 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyzhK:ckpt-step-90\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "# You can also track progress in a more granular way by checking for events\n",
    "# Refresh this code till you get the `The job has successfully completed` message\n",
    "response = client.fine_tuning.jobs.list_events(ft_filejob.id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2.4: Просмотрите статус в панели управления OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы также можете просмотреть статус, посетив сайт OpenAI и перейдя в раздел _Fine-tuning_ на платформе. Здесь отображается статус текущей задачи, а также можно отследить историю предыдущих запусков. На этом скриншоте видно, что предыдущий запуск завершился с ошибкой, а второй прошёл успешно. Для справки: это произошло потому, что в первом запуске использовался JSON-файл с некорректно отформатированными записями — после исправления второй запуск завершился успешно, и модель стала доступна для использования.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fine-tuned-model-status.563271727bf7bfba7e3f73a201f8712fae3cea1c08f7c7f12ca469c06d234122.ru.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы также можете просматривать сообщения о состоянии и метрики, прокручивая ниже на визуальной панели, как показано:\n",
    "\n",
    "| Сообщения | Метрики |\n",
    "|:---|:---|\n",
    "| ![Сообщения](../../../../../translated_images/fine-tuned-messages-panel.4ed0c2da5ea1313b3a706a66f66bf5007c379cd9219cfb74cb30c0b04b90c4c8.ru.png) |  ![Метрики](../../../../../translated_images/fine-tuned-metrics-panel.700d7e4995a652299584ab181536a6cfb67691a897a518b6c7a2aa0a17f1a30d.ru.png)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3.1: Получение ID и тестирование дообученной модели в коде\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the identity of the fine-tuned model once ready\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "fine_tuned_model_id = response.fine_tuned_model\n",
    "print(\"Fine-tuned Model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Strontium, a metal so bright - It's in fireworks, a dazzling sight - It's in bones, you see - And in tea, it's the key - It's the fortieth, so pure, that's the right\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# You can then use that model to generate completions from the SDK as shown\n",
    "# Or you can load that model into the OpenAI Playground (in the UI) to validate it from there.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Elle, a factual chatbot that answers questions about elements in the periodic table with a limerick\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Strontium\"},\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3.2: Загрузка и тестирование дообученной модели в Playground\n",
    "\n",
    "Теперь вы можете протестировать дообученную модель двумя способами. Во-первых, вы можете зайти в Playground и выбрать вашу новую дообученную модель из выпадающего списка моделей. Второй способ — воспользоваться опцией \"Playground\" в панели Fine-tuning (см. скриншот выше), которая открывает _сравнительный_ режим: версии базовой и дообученной модели отображаются рядом для быстрой оценки.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fine-tuned-playground-compare.56e06f0ad8922016497d39ced3d84ea296eec89073503f2bf346ec9718f913b5.ru.png)\n",
    "\n",
    "Просто заполните системный контекст, который использовался в ваших обучающих данных, и введите тестовый вопрос. Вы заметите, что обе стороны обновляются с одинаковым контекстом и вопросом. Запустите сравнение, и вы увидите разницу в ответах между ними. _Обратите внимание, что дообученная модель формирует ответ в том формате, который вы задали в примерах, а базовая модель просто следует системному промпту_.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fine-tuned-playground-launch.5a26495c983c6350c227e05700a47a89002d132949a56fa4ff37f266ebe997b2.ru.png)\n",
    "\n",
    "Также вы увидите, что сравнение показывает количество токенов для каждой модели и время, затраченное на инференс. **Этот конкретный пример очень простой и предназначен для демонстрации процесса, но не отражает реальный датасет или сценарий**. Вы можете заметить, что в обоих случаях количество токенов одинаковое (системный контекст и пользовательский запрос идентичны), но дообученная модель тратит больше времени на инференс (кастомная модель).\n",
    "\n",
    "В реальных сценариях вы не будете использовать такой учебный пример, а будете дообучать модель на реальных данных (например, каталог товаров для поддержки клиентов), где качество ответа будет заметно выше. В _таком_ случае, чтобы добиться аналогичного качества ответа от базовой модели, потребуется более сложная настройка промпта, что увеличит расход токенов и, возможно, время обработки инференса. _Чтобы попробовать это на практике, посмотрите примеры дообучения в OpenAI Cookbook._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от ответственности**:  \nЭтот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "69b527f1e605a10fb9c7e00ae841021d",
   "translation_date": "2025-08-25T21:30:23+00:00",
   "source_file": "18-fine-tuning/python/openai/oai-assignment.ipynb",
   "language_code": "ru"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}