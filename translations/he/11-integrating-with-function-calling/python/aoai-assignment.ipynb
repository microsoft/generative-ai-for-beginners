{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## הקדמה\n",
    "\n",
    "בשיעור זה נעסוק ב:\n",
    "- מהו קריאה לפונקציה ובאילו מקרים משתמשים בה\n",
    "- איך ליצור קריאה לפונקציה באמצעות Azure OpenAI\n",
    "- איך לשלב קריאה לפונקציה בתוך אפליקציה\n",
    "\n",
    "## מטרות למידה\n",
    "\n",
    "לאחר סיום השיעור תדעו ותבינו:\n",
    "\n",
    "- את המטרה של שימוש בקריאה לפונקציה\n",
    "- איך להגדיר קריאה לפונקציה באמצעות Azure Open AI Service\n",
    "- איך לעצב קריאות פונקציה יעילות עבור המקרה השימושי של האפליקציה שלכם\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## הבנת קריאות לפונקציות\n",
    "\n",
    "בשיעור הזה, נבנה פיצ'ר לסטארטאפ החינוכי שלנו שמאפשר למשתמשים להשתמש בצ'אטבוט כדי למצוא קורסים טכנולוגיים. נמליץ על קורסים שמתאימים לרמת הידע שלהם, לתפקיד הנוכחי ולטכנולוגיה שמעניינת אותם.\n",
    "\n",
    "כדי להשלים את זה נשתמש בשילוב של:\n",
    " - `Azure Open AI` כדי ליצור חוויית צ'אט למשתמש\n",
    " - `Microsoft Learn Catalog API` כדי לעזור למשתמשים למצוא קורסים לפי הבקשה שלהם\n",
    " - `Function Calling` כדי לקחת את השאילתה של המשתמש ולשלוח אותה לפונקציה שמבצעת את הבקשה ל-API.\n",
    "\n",
    "כדי להתחיל, בואו נבין למה בכלל נרצה להשתמש בקריאות לפונקציות:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # קבלת תגובה חדשה מ-GPT שבה הוא יכול לראות את התגובה של הפונקציה\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### למה להשתמש ב-Function Calling\n",
    "\n",
    "אם השלמת שיעור אחר בקורס הזה, כנראה שאתה כבר מבין את הכוח של שימוש במודלים גדולים לשפה (LLMs). סביר להניח שגם הבחנת בכמה מהמגבלות שלהם.\n",
    "\n",
    "Function Calling היא תכונה בשירות Azure Open AI שנועדה להתגבר על המגבלות הבאות:\n",
    "1) פורמט תגובה עקבי\n",
    "2) היכולת להשתמש בנתונים ממקורות אחרים של אפליקציה בתוך שיחה\n",
    "\n",
    "לפני שהייתה אפשרות ל-Function Calling, התגובות ממודל השפה היו לא מובנות ולא עקביות. מפתחים היו צריכים לכתוב קוד אימות מסובך כדי לוודא שהם יכולים להתמודד עם כל וריאציה של תגובה.\n",
    "\n",
    "משתמשים לא יכלו לקבל תשובות כמו \"מה מזג האוויר הנוכחי בסטוקהולם?\". הסיבה לכך היא שהמודלים היו מוגבלים לזמן שבו הנתונים שלהם אומנו.\n",
    "\n",
    "בואו נסתכל על הדוגמה למטה שממחישה את הבעיה הזו:\n",
    "\n",
    "נניח שאנחנו רוצים ליצור מסד נתונים של נתוני תלמידים כדי שנוכל להמליץ להם על הקורס המתאים. למטה יש לנו שתי תיאורים של תלמידים שמכילים נתונים דומים מאוד.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אנחנו רוצים לשלוח את זה למודל שפה גדול (LLM) כדי לנתח את הנתונים. מאוחר יותר נוכל להשתמש בזה באפליקציה שלנו כדי לשלוח את זה ל-API או לשמור את זה במסד נתונים.\n",
    "\n",
    "בואו ניצור שני פרומפטים זהים שבהם ננחה את ה-LLM אילו נתונים מעניינים אותנו:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אנחנו רוצים לשלוח את זה למודל שפה גדול כדי לנתח את החלקים שחשובים למוצר שלנו. כך נוכל ליצור שני הנחיות זהות כדי להנחות את המודל:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "לאחר יצירת שני ההנחיות הללו, נשלח אותן ל-LLM באמצעות ‎`openai.ChatCompletion`‎. אנו שומרים את ההנחיה במשתנה ‎`messages`‎ ומקצים את התפקיד ל-‎`user`‎. זאת כדי לדמות הודעה ממשתמש הנכתבת לצ'אטבוט.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עכשיו אנחנו יכולים לשלוח את שתי הבקשות ל-LLM ולבחון את התגובה שנקבל.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "למרות שההנחיות זהות והתיאורים דומים, אנחנו יכולים לקבל פורמטים שונים של המאפיין `Grades`.\n",
    "\n",
    "אם תריץ את התא למעלה כמה פעמים, הפורמט יכול להיות `3.7` או `3.7 GPA`.\n",
    "\n",
    "הסיבה לכך היא שהמודל הלשוני מקבל נתונים לא מובנים בצורה של הנחיה כתובה וגם מחזיר נתונים לא מובנים. אנחנו צריכים פורמט מובנה כדי שנדע למה לצפות כשאנחנו שומרים או משתמשים בנתונים האלה.\n",
    "\n",
    "באמצעות קריאה פונקציונלית, אפשר לוודא שמקבלים נתונים מובנים בחזרה. כשמשתמשים בקריאה פונקציונלית, המודל הלשוני לא באמת מפעיל או מריץ פונקציות. במקום זאת, אנחנו יוצרים מבנה שהמודל צריך לעקוב אחריו בתשובות שלו. לאחר מכן, אנחנו משתמשים בתשובות המובנות כדי לדעת איזו פונקציה להפעיל באפליקציות שלנו.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![דיאגרמת זרימת קריאות פונקציה](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.he.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### שימושים עיקריים לקריאות לפונקציות\n",
    "\n",
    "**שימוש בכלים חיצוניים**  \n",
    "צ'אטבוטים מצוינים במתן תשובות לשאלות של משתמשים. בעזרת קריאות לפונקציות, הצ'אטבוטים יכולים להשתמש בהודעות מהמשתמשים כדי לבצע משימות מסוימות. לדוגמה, סטודנט יכול לבקש מהצ'אטבוט \"שלח מייל למרצה שלי וכתוב שאני צריך עזרה נוספת בנושא הזה\". זה יכול להפעיל קריאה לפונקציה `send_email(to: string, body: string)`\n",
    "\n",
    "**יצירת שאילתות ל-API או למסדי נתונים**  \n",
    "משתמשים יכולים למצוא מידע בעזרת שפה טבעית שמומרת לשאילתה או בקשה ל-API בפורמט מתאים. לדוגמה, מורה יכול לשאול \"מי התלמידים שסיימו את המשימה האחרונה\", מה שיכול להפעיל פונקציה בשם `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**יצירת נתונים מובנים**  \n",
    "משתמשים יכולים לקחת קטע טקסט או קובץ CSV ולהיעזר ב-LLM כדי להוציא ממנו מידע חשוב. לדוגמה, סטודנט יכול להמיר ערך מוויקיפדיה על הסכמי שלום לכרטיסיות לימוד מבוססות בינה מלאכותית. זה יכול להתבצע בעזרת פונקציה בשם `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. יצירת קריאת הפונקציה הראשונה שלך\n",
    "\n",
    "תהליך יצירת קריאת פונקציה כולל שלושה שלבים עיקריים:\n",
    "1. קריאה ל-Chat Completions API עם רשימת הפונקציות שלך והודעה מהמשתמש\n",
    "2. קריאת התגובה של המודל כדי לבצע פעולה, כלומר להריץ פונקציה או קריאה ל-API\n",
    "3. ביצוע קריאה נוספת ל-Chat Completions API עם התגובה מהפונקציה שלך, כדי להשתמש במידע הזה ליצירת תגובה למשתמש.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![זרימת קריאה לפונקציה](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.he.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### מרכיבי קריאה לפונקציה\n",
    "\n",
    "#### קלט מהמשתמש\n",
    "\n",
    "השלב הראשון הוא ליצור הודעה מהמשתמש. אפשר להקצות אותה באופן דינמי על ידי קבלת הערך מקלט טקסט, או להכניס ערך כאן. אם זו הפעם הראשונה שלך לעבוד עם Chat Completions API, צריך להגדיר את ה-`role` ואת ה-`content` של ההודעה.\n",
    "\n",
    "ה-`role` יכול להיות או `system` (הגדרת כללים), `assistant` (המודל) או `user` (המשתמש הסופי). עבור קריאה לפונקציה, נגדיר זאת כ-`user` ונשתמש בשאלה לדוגמה.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### יצירת פונקציות.\n",
    "\n",
    "כעת נגדיר פונקציה ואת הפרמטרים שלה. כאן נשתמש בפונקציה אחת בלבד בשם `search_courses`, אבל אפשר ליצור כמה פונקציות שתרצו.\n",
    "\n",
    "**חשוב**: פונקציות נכללות בהודעת המערכת ל-LLM והן יחשבו כחלק ממספר הטוקנים הזמינים לכם.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**הגדרות**\n",
    "\n",
    "`name` - השם של הפונקציה שברצוננו שתקרא.\n",
    "\n",
    "`description` - זהו תיאור של איך הפונקציה פועלת. כאן חשוב להיות מדויקים וברורים.\n",
    "\n",
    "`parameters` - רשימה של ערכים והפורמט שתרצה שהמודל יחזיר בתשובתו.\n",
    "\n",
    "`type` - סוג הנתונים שבו הערכים יאוחסנו.\n",
    "\n",
    "`properties` - רשימה של הערכים הספציפיים שהמודל ישתמש בהם בתשובתו.\n",
    "\n",
    "`name` - שם המאפיין שהמודל ישתמש בו בתשובה המפורמטת שלו.\n",
    "\n",
    "`type` - סוג הנתונים של מאפיין זה.\n",
    "\n",
    "`description` - תיאור של המאפיין הספציפי.\n",
    "\n",
    "**אופציונלי**\n",
    "\n",
    "`required` - מאפיין שחובה לספק כדי שהקריאה לפונקציה תושלם.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ביצוע קריאה לפונקציה\n",
    "לאחר שהגדרנו פונקציה, כעת עלינו לכלול אותה בקריאה ל-Chat Completion API. אנחנו עושים זאת על ידי הוספת `functions` לבקשה. במקרה הזה `functions=functions`.\n",
    "\n",
    "יש גם אפשרות להגדיר את `function_call` ל-`auto`. המשמעות היא שניתן למודל השפה להחליט איזו פונקציה יש לקרוא על סמך ההודעה של המשתמש, במקום שנקבע זאת בעצמנו.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עכשיו בואו נסתכל על התגובה ונראה איך היא מעוצבת:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "אתם יכולים לראות ששם הפונקציה נקרא ומההודעה של המשתמש, המודל הצליח למצוא את הנתונים שמתאימים לארגומנטים של הפונקציה.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. שילוב קריאות לפונקציות בתוך אפליקציה\n",
    "\n",
    "אחרי שבדקנו את התגובה המעוצבת מה-LLM, עכשיו אפשר לשלב אותה בתוך אפליקציה.\n",
    "\n",
    "### ניהול זרימת העבודה\n",
    "\n",
    "כדי לשלב את זה באפליקציה שלנו, נבצע את הצעדים הבאים:\n",
    "\n",
    "ראשית, נבצע קריאה לשירותי Open AI ונשמור את ההודעה במשתנה בשם `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עכשיו נגדיר את הפונקציה שתקרא ל-API של Microsoft Learn כדי לקבל רשימת קורסים:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "כמנהג מומלץ, נבדוק אם המודל רוצה לקרוא לפונקציה. לאחר מכן, ניצור אחת מהפונקציות הזמינות ונתאים אותה לפונקציה שנקראה.\n",
    "לאחר מכן ניקח את הארגומנטים של הפונקציה ונמפה אותם לארגומנטים מה-LLM.\n",
    "\n",
    "לבסוף, נוסיף את הודעת קריאת הפונקציה ואת הערכים שהוחזרו מההודעה `search_courses`. זה נותן ל-LLM את כל המידע שהוא צריך\n",
    "כדי להגיב למשתמש בשפה טבעית.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## אתגר קוד\n",
    "\n",
    "עבודה מצוינת! כדי להמשיך ללמוד על Azure Open AI Function Calling, אתם יכולים לבנות: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    " - להוסיף עוד פרמטרים לפונקציה שיכולים לעזור ללומדים למצוא קורסים נוספים. תוכלו למצוא את כל הפרמטרים הזמינים של ה-API כאן:  \n",
    " - ליצור קריאה נוספת לפונקציה שלוקחת מידע נוסף מהלומד, כמו שפת האם שלו  \n",
    " - להוסיף טיפול בשגיאות כאשר קריאת הפונקציה ו/או קריאת ה-API לא מחזירה קורסים מתאימים\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\nCertainly! Here is your text translated into Hebrew:\n\n**כתב ויתור**:  \nמסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש להיות מודעים לכך שתרגומים אוטומטיים עשויים להכיל שגיאות או אי-דיוקים. המסמך המקורי בשפת המקור ייחשב כמקור הסמכותי. למידע קריטי, מומלץ לפנות לתרגום מקצועי על ידי אדם. איננו אחראים לכל אי-הבנה או פרשנות שגויה הנובעת מהשימוש בתרגום זה.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-08-25T20:21:57+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "he"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}