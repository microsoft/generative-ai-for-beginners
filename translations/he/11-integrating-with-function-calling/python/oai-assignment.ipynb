{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## מבוא\n",
    "\n",
    "שיעור זה יכסה:\n",
    "- מהי קריאת פונקציה ומקרי השימוש שלה\n",
    "- כיצד ליצור קריאת פונקציה באמצעות OpenAI\n",
    "- כיצד לשלב קריאת פונקציה באפליקציה\n",
    "\n",
    "## מטרות הלמידה\n",
    "\n",
    "לאחר סיום שיעור זה תדע כיצד ותבין:\n",
    "\n",
    "- המטרה בשימוש בקריאת פונקציה\n",
    "- הגדרת קריאת פונקציה באמצעות שירות OpenAI\n",
    "- עיצוב קריאות פונקציה יעילות למקרי השימוש של האפליקציות שלך\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## הבנת קריאות לפונקציות\n",
    "\n",
    "בשיעור זה, אנו רוצים לבנות תכונה לסטארטאפ החינוכי שלנו שמאפשרת למשתמשים להשתמש בצ'אטבוט כדי למצוא קורסים טכניים. נמליץ על קורסים שמתאימים לרמת המיומנות שלהם, לתפקיד הנוכחי ולתחום הטכנולוגיה שמעניין אותם.\n",
    "\n",
    "כדי להשלים זאת נשתמש בשילוב של:\n",
    " - `OpenAI` ליצירת חווית צ'אט למשתמש\n",
    " - `Microsoft Learn Catalog API` כדי לעזור למשתמשים למצוא קורסים בהתבסס על בקשת המשתמש\n",
    " - `Function Calling` כדי לקחת את השאילתה של המשתמש ולשלוח אותה לפונקציה שתבצע את בקשת ה-API.\n",
    "\n",
    "כדי להתחיל, בואו נסתכל למה נרצה להשתמש בקריאת פונקציה מלכתחילה:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # לקבל תגובה חדשה מ-GPT שבה הוא יכול לראות את תגובת הפונקציה\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### למה קריאת פונקציה\n",
    "\n",
    "אם השלמתם כל שיעור אחר בקורס הזה, סביר להניח שאתם מבינים את הכוח שבשימוש במודלים לשוניים גדולים (LLMs). מקווים שגם תוכלו לראות כמה מהמגבלות שלהם.\n",
    "\n",
    "קריאת פונקציה היא תכונה של שירות OpenAI שנועדה להתמודד עם האתגרים הבאים:\n",
    "\n",
    "עיצוב תגובות לא עקבי:\n",
    "- לפני קריאת פונקציה, התגובות ממודל שפה גדול היו לא מובנות ולא עקביות. המפתחים נאלצו לכתוב קוד אימות מורכב כדי להתמודד עם כל וריאציה בתוצאה.\n",
    "\n",
    "אינטגרציה מוגבלת עם נתונים חיצוניים:\n",
    "- לפני תכונה זו, היה קשה לשלב נתונים מחלקים אחרים של אפליקציה בהקשר של שיחה.\n",
    "\n",
    "על ידי סטנדרטיזציה של פורמטי תגובה ואפשרות אינטגרציה חלקה עם נתונים חיצוניים, קריאת פונקציה מפשטת את הפיתוח ומפחיתה את הצורך בלוגיקת אימות נוספת.\n",
    "\n",
    "משתמשים לא יכלו לקבל תשובות כמו \"מה מזג האוויר הנוכחי בסטוקהולם?\". זאת מכיוון שהמודלים היו מוגבלים לזמן שבו הנתונים אומנו.\n",
    "\n",
    "בואו נסתכל על הדוגמה למטה שממחישה את הבעיה הזו:\n",
    "\n",
    "נניח שאנחנו רוצים ליצור מסד נתונים של נתוני סטודנטים כדי שנוכל להציע להם את הקורס המתאים. למטה יש לנו שתי תיאורים של סטודנטים שהם מאוד דומים בנתונים שהם מכילים.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אנחנו רוצים לשלוח את זה ל-LLM כדי לנתח את הנתונים. זה יכול לשמש מאוחר יותר באפליקציה שלנו כדי לשלוח את זה ל-API או לאחסן את זה בבסיס נתונים.\n",
    "\n",
    "בואו ניצור שני פרומפטים זהים שבהם ננחה את ה-LLM לגבי המידע שמעניין אותנו:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אנחנו רוצים לשלוח את זה ל-LLM כדי לנתח את החלקים החשובים למוצר שלנו. כך נוכל ליצור שני פרומפטים זהים להנחות את ה-LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "לאחר יצירת שני ההנחיות הללו, נשלח אותן ל-LLM באמצעות `openai.ChatCompletion`. אנו מאחסנים את ההנחיה במשתנה `messages` ומקצים את התפקיד ל-`user`. זאת כדי לחקות הודעה ממשתמש שנכתבת לצ'אטבוט.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עכשיו אנחנו יכולים לשלוח את שתי הבקשות ל-LLM ולבחון את התגובה שנקבל.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "למרות שההנחיות זהות והתיאורים דומים, אנו יכולים לקבל פורמטים שונים של המאפיין `Grades`.\n",
    "\n",
    "אם תריץ את התא שלמעלה מספר פעמים, הפורמט יכול להיות `3.7` או `3.7 GPA`.\n",
    "\n",
    "זה נובע מכך ש-LLM מקבל נתונים לא מובנים בצורת ההנחיה הכתובה ומחזיר גם נתונים לא מובנים. אנו צריכים פורמט מובנה כדי לדעת למה לצפות בעת אחסון או שימוש בנתונים אלו.\n",
    "\n",
    "על ידי שימוש בקריאה פונקציונלית, אנו יכולים לוודא שאנו מקבלים חזרה נתונים מובנים. כאשר משתמשים בקריאה פונקציונלית, ה-LLM בפועל לא קורא או מריץ פונקציות כלשהן. במקום זאת, אנו יוצרים מבנה שה-LLM צריך לעקוב אחריו בתגובותיו. לאחר מכן אנו משתמשים בתגובות המובנות הללו כדי לדעת איזו פונקציה להריץ באפליקציות שלנו.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![דיאגרמת זרימת קריאת פונקציה](../../../../translated_images/Function-Flow.083875364af4f4bb.he.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אנו יכולים אז לקחת את מה שמוחזר מהפונקציה ולשלוח זאת חזרה ל-LLM. ה-LLM יגיב אז בשפה טבעית כדי לענות על שאלת המשתמש.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### מקרים לשימוש בקריאות לפונקציות\n",
    "\n",
    "**קריאה לכלים חיצוניים**  \n",
    "צ'אטבוטים מצוינים במתן תשובות לשאלות משתמשים. באמצעות קריאת פונקציות, הצ'אטבוטים יכולים להשתמש בהודעות מהמשתמשים כדי להשלים משימות מסוימות. לדוגמה, סטודנט יכול לבקש מהצ'אטבוט \"שלח מייל למרצה שלי ואמר שאני צריך עזרה נוספת בנושא הזה\". זה יכול לבצע קריאת פונקציה ל-`send_email(to: string, body: string)`\n",
    "\n",
    "**יצירת שאילתות API או מסד נתונים**  \n",
    "משתמשים יכולים למצוא מידע באמצעות שפה טבעית שהופכת לשאילתה מעוצבת או בקשת API. דוגמה לכך יכולה להיות מורה שמבקש \"מי התלמידים שסיימו את המשימה האחרונה\" שיכולה לקרוא לפונקציה בשם `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**יצירת נתונים מובנים**  \n",
    "משתמשים יכולים לקחת בלוק טקסט או CSV ולהשתמש ב-LLM כדי לחלץ מידע חשוב ממנו. לדוגמה, סטודנט יכול להמיר מאמר מוויקיפדיה על הסכמי שלום ליצירת כרטיסיות AI ללמידה. זה יכול להיעשות באמצעות פונקציה בשם `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. יצירת קריאת הפונקציה הראשונה שלך\n",
    "\n",
    "תהליך יצירת קריאת פונקציה כולל 3 שלבים עיקריים:  \n",
    "1. קריאה ל-Chat Completions API עם רשימת הפונקציות שלך והודעת משתמש  \n",
    "2. קריאת תגובת המודל לביצוע פעולה כלומר ביצוע פונקציה או קריאת API  \n",
    "3. ביצוע קריאה נוספת ל-Chat Completions API עם התגובה מהפונקציה שלך כדי להשתמש במידע זה ליצירת תגובה למשתמש.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![זרימת קריאת פונקציה](../../../../translated_images/LLM-Flow.3285ed8caf4796d7.he.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### אלמנטים של קריאת פונקציה\n",
    "\n",
    "#### קלט משתמשים\n",
    "\n",
    "השלב הראשון הוא ליצור הודעת משתמש. ניתן להקצות זאת באופן דינמי על ידי לקיחת הערך של שדה טקסט או ניתן להקצות ערך כאן. אם זו הפעם הראשונה שלך לעבוד עם ה-Chat Completions API, עלינו להגדיר את ה-`role` ואת ה-`content` של ההודעה.\n",
    "\n",
    "ה-`role` יכול להיות או `system` (יצירת כללים), `assistant` (המודל) או `user` (המשתמש הקצה). עבור קריאת פונקציה, נקצה זאת כ-`user` ודוגמה לשאלה.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### יצירת פונקציות.\n",
    "\n",
    "בהמשך נגדיר פונקציה ואת הפרמטרים של אותה פונקציה. נשתמש כאן בפונקציה אחת בלבד שנקראת `search_courses` אבל ניתן ליצור מספר פונקציות.\n",
    "\n",
    "**חשוב**: פונקציות נכללות בהודעת המערכת ל-LLM וייכללו בכמות הטוקנים הזמינה שיש לך.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**הגדרות**\n",
    "\n",
    "מבנה הגדרת הפונקציה כולל רמות מרובות, כל אחת עם התכונות שלה. הנה פירוט של המבנה המקונן:\n",
    "\n",
    "**תכונות פונקציה ברמה העליונה:**\n",
    "\n",
    "`name` - שם הפונקציה שאנו רוצים שתקרא.\n",
    "\n",
    "`description` - זוהי התיאור של אופן פעולת הפונקציה. כאן חשוב להיות מדויק וברור.\n",
    "\n",
    "`parameters` - רשימת ערכים ופורמט שברצונך שהמודל יפיק בתגובתו.\n",
    "\n",
    "**תכונות אובייקט הפרמטרים:**\n",
    "\n",
    "`type` - סוג הנתונים של אובייקט הפרמטרים (בדרך כלל \"object\").\n",
    "\n",
    "`properties` - רשימת הערכים הספציפיים שהמודל ישתמש בהם בתגובתו.\n",
    "\n",
    "**תכונות פרמטר יחיד:**\n",
    "\n",
    "`name` - מוגדר במפורש על ידי מפתח התכונה (למשל, \"role\", \"product\", \"level\").\n",
    "\n",
    "`type` - סוג הנתונים של פרמטר זה (למשל, \"string\", \"number\", \"boolean\").\n",
    "\n",
    "`description` - תיאור הפרמטר הספציפי.\n",
    "\n",
    "**תכונות אופציונליות:**\n",
    "\n",
    "`required` - מערך המפרט אילו פרמטרים נדרשים כדי שהקריאה לפונקציה תושלם.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ביצוע קריאת הפונקציה  \n",
    "לאחר שהגדרנו פונקציה, כעת עלינו לכלול אותה בקריאה ל-Chat Completion API. אנו עושים זאת על ידי הוספת `functions` לבקשה. במקרה זה `functions=functions`.  \n",
    "\n",
    "קיימת גם אפשרות להגדיר את `function_call` ל-`auto`. משמעות הדבר היא שנאפשר ל-LLM להחליט איזו פונקציה יש לקרוא בהתבסס על הודעת המשתמש במקום להקצות זאת בעצמנו.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "עכשיו בואו נסתכל על התגובה ונראה כיצד היא מעוצבת:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "ניתן לראות ששמו של הפונקציה נקרא וממסר המשתמש, ה-LLM הצליח למצוא את הנתונים המתאימים לארגומנטים של הפונקציה.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.שילוב קריאות פונקציה באפליקציה. \n",
    "\n",
    "\n",
    "לאחר שבדקנו את התגובה המעוצבת מה-LLM, כעת נוכל לשלב זאת באפליקציה. \n",
    "\n",
    "### ניהול הזרימה \n",
    "\n",
    "כדי לשלב זאת באפליקציה שלנו, ננקוט בצעדים הבאים: \n",
    "\n",
    "ראשית, נבצע את הקריאה לשירותי OpenAI ונאחסן את ההודעה במשתנה בשם `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "כעת נגדיר את הפונקציה שתתקשר ל-API של Microsoft Learn כדי לקבל רשימת קורסים:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "כפרקטיקה מומלצת, נבדוק לאחר מכן אם המודל רוצה לקרוא לפונקציה. לאחר מכן, ניצור אחת מהפונקציות הזמינות ונתאים אותה לפונקציה שנקראת.  \n",
    "לאחר מכן, ניקח את הפרמטרים של הפונקציה ונמפה אותם לפרמטרים מה-LLM.\n",
    "\n",
    "לבסוף, נוסיף את הודעת קריאת הפונקציה ואת הערכים שהוחזרו מהודעת `search_courses`. זה נותן ל-LLM את כל המידע שהוא צריך כדי  \n",
    "להגיב למשתמש בשפה טבעית.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "כעת נשלח את ההודעה המעודכנת ל-LLM כדי שנוכל לקבל תגובה בשפה טבעית במקום תגובת JSON בפורמט API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## אתגר קוד\n",
    "\n",
    "עבודה מצוינת! כדי להמשיך בלמידתך של קריאת פונקציות OpenAI תוכל לבנות: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    " - פרמטרים נוספים של הפונקציה שעשויים לעזור ללומדים למצוא קורסים נוספים. ניתן למצוא את פרמטרי ה-API הזמינים כאן:  \n",
    " - צור קריאת פונקציה נוספת שלוקחת מידע נוסף מהלומד כמו שפת האם שלו  \n",
    " - צור טיפול בשגיאות כאשר קריאת הפונקציה ו/או קריאת ה-API לא מחזירה קורסים מתאימים כלשהם  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**כתב ויתור**:  \nמסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון כי תרגומים אוטומטיים עלולים להכיל שגיאות או אי-דיוקים. המסמך המקורי בשפת המקור שלו נחשב למקור הסמכותי. למידע קריטי מומלץ להשתמש בתרגום מקצועי על ידי אדם. אנו לא נושאים באחריות לכל אי-הבנה או פרשנות שגויה הנובעת משימוש בתרגום זה.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T10:52:06+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "he"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}