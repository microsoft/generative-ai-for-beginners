{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# כיוונון עדין של מודלים של Open AI\n",
    "\n",
    "מחברת זו מבוססת על ההנחיות העדכניות המסופקות בתיעוד [Fine Tuning](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst) של Open AI.\n",
    "\n",
    "כיוונון עדין משפר את הביצועים של מודלים בסיסיים עבור היישום שלך על ידי אימון חוזר עם נתונים נוספים והקשר הרלוונטי למקרה או לתרחיש הספציפי הזה. שים לב שטכניקות הנדסת פרומפט כמו _למידה עם דוגמאות מועטות_ ו-_יצירה משופרת באמצעות אחזור_ מאפשרות לך לשפר את הפרומפט ברירת המחדל עם נתונים רלוונטיים לשיפור האיכות. עם זאת, גישות אלו מוגבלות על ידי גודל חלון הטוקנים המקסימלי של המודל הבסיסי המיועד.\n",
    "\n",
    "עם כיוונון עדין, אנו למעשה מאמנים מחדש את המודל עצמו עם הנתונים הנדרשים (מה שמאפשר לנו להשתמש בהרבה יותר דוגמאות ממה שיכול להתאים בחלון הטוקנים המקסימלי) - ומפרסמים גרסה _מותאמת אישית_ של המודל שלא זקוקה עוד לספק דוגמאות בזמן ההסקה. זה לא רק משפר את היעילות של עיצוב הפרומפט שלנו (יש לנו יותר גמישות בשימוש בחלון הטוקנים לדברים אחרים) אלא גם עשוי לשפר את העלויות שלנו (על ידי הפחתת מספר הטוקנים שעלינו לשלוח למודל בזמן ההסקה).\n",
    "\n",
    "לכיוונון עדין יש 4 שלבים:\n",
    "1. הכנת נתוני האימון והעלאתם.\n",
    "1. הרצת משימת האימון לקבלת מודל מכוונן עדין.\n",
    "1. הערכת המודל המכוין עדין וחזרה לשיפור האיכות.\n",
    "1. פריסת המודל המכוין עדין להסקה כאשר מרוצים.\n",
    "\n",
    "שים לב שלא כל המודלים הבסיסיים תומכים בכיוונון עדין - [בדוק את תיעוד OpenAI](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned?WT.mc_id=academic-105485-koreyst) למידע העדכני ביותר. ניתן גם לכוונן עדין מודל שכבר כוונן עדין בעבר. במדריך זה, נשתמש ב-`gpt-35-turbo` כמודל הבסיסי המיועד לכיוונון עדין. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### שלב 1.1: הכנת מערך הנתונים שלך\n",
    "\n",
    "בואו נבנה צ'אטבוט שיעזור לך להבין את הטבלה המחזורית של היסודות על ידי מענה על שאלות לגבי יסוד בלימריק. במדריך הפשוט _הזה_ ניצור רק מערך נתונים לאימון המודל עם כמה דוגמאות תגובה המדגימות את הפורמט הצפוי של הנתונים. במקרה שימוש אמיתי, תצטרך ליצור מערך נתונים עם הרבה יותר דוגמאות. ייתכן גם שתוכל להשתמש במערך נתונים פתוח (לתחום היישום שלך) אם קיים כזה, ולעצב אותו מחדש לשימוש בכוונון עדין.\n",
    "\n",
    "מכיוון שאנו מתמקדים ב-`gpt-35-turbo` ומחפשים תגובה בסיבוב יחיד (השלמת שיחה), נוכל ליצור דוגמאות באמצעות [הפורמט המוצע הזה](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?WT.mc_id=academic-105485-koreyst) המשקף את דרישות השלמת השיחה של OpenAI. אם אתה מצפה לתוכן שיחה רב-סיבובי, תשתמש ב-[פורמט הדוגמאות הרב-סיבובי](https://platform.openai.com/docs/guides/fine-tuning/multi-turn-chat-examples?WT.mc_id=academic-105485-koreyst) הכולל פרמטר `weight` כדי לסמן אילו הודעות יש להשתמש בהן (או לא) בתהליך הכוונון העדין.\n",
    "\n",
    "נשתמש כאן בפורמט הפשוט של סיבוב יחיד למדריך שלנו. הנתונים הם בפורמט [jsonl](https://jsonlines.org/?WT.mc_id=academic-105485-koreyst) עם רשומה אחת בכל שורה, שכל אחת מיוצגת כאובייקט בפורמט JSON. הקטע למטה מציג 2 רשומות כדוגמה - ראה את [training-data.jsonl](../../../../../18-fine-tuning/python/openai/training-data.jsonl) למערך דוגמאות מלא (10 דוגמאות) שנשתמש בו במדריך הכוונון העדין שלנו. **הערה:** כל רשומה _חייבת_ להיות מוגדרת בשורה אחת (ולא מפוצלת על פני שורות כפי שנהוג בקובץ JSON מעוצב).\n",
    "\n",
    "```json\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Gallium\"}, {\"role\": \"assistant\", \"content\": \"Gallium, oh gallium, so light - Melts in your hand, oh what a sight - At 86 degrees - Its liquid with ease - And in semiconductors, it's out of sight\"}]}\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Hydrogen\"}, {\"role\": \"assistant\", \"content\": \"Hydrogen, the first in the line - The lightest of all, so divine - It's in water, you see - And in stars, it's the key - The universe's most common sign\"}]}\n",
    "```\n",
    "  \n",
    "בשימוש אמיתי תזדקק למערך דוגמאות גדול בהרבה לתוצאות טובות - הפשרה תהיה בין איכות התגובות לבין הזמן/העלויות של הכוונון העדין. אנו משתמשים במערך קטן כדי שנוכל להשלים את הכוונון במהירות ולהמחיש את התהליך. ראה [דוגמת OpenAI Cookbook הזו](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst) למדריך כוונון עדין מורכב יותר.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### שלב 1.2 העלאת מערך הנתונים שלך\n",
    "\n",
    "העלה את הנתונים באמצעות ממשק ה-API של הקבצים [כמתואר כאן](https://platform.openai.com/docs/guides/fine-tuning/upload-a-training-file). שים לב שכדי להריץ את הקוד הזה, עליך לבצע את השלבים הבאים קודם:\n",
    " - התקנת חבילת הפייתון `openai` (ודא שאתה משתמש בגרסה >=0.28.0 עבור התכונות העדכניות ביותר)\n",
    " - הגדרת משתנה הסביבה `OPENAI_API_KEY` למפתח ה-API של OpenAI שלך\n",
    "למידע נוסף, ראה את [מדריך ההתקנה](./../../../00-course-setup/02-setup-local.md?WT.mc_id=academic-105485-koreyst) שסופק לקורס.\n",
    "\n",
    "כעת, הרץ את הקוד כדי ליצור קובץ להעלאה מקובץ ה-JSONL המקומי שלך.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-JdAJcagdOTG6ACNlFWzuzmyV', bytes=4021, created_at=1715566183, filename='training-data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
      "Training File ID: file-JdAJcagdOTG6ACNlFWzuzmyV\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_file = client.files.create(\n",
    "  file=open(\"./training-data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(ft_file)\n",
    "print(\"Training File ID: \" + ft_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### שלב 2.1: צור את משימת הכוונון המדויק עם ה-SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', created_at=1715566184, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-EZ6ag0n0S6Zm8eV9BSWKmE6l', result_files=[], seed=830529052, status='validating_files', trained_tokens=None, training_file='file-JdAJcagdOTG6ACNlFWzuzmyV', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)\n",
      "Fine-tuning Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_filejob = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file.id, \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "print(ft_filejob)\n",
    "print(\"Fine-tuning Job ID: \" + ft_filejob.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### שלב 2.2: בדוק את מצב העבודה\n",
    "\n",
    "הנה כמה דברים שאתה יכול לעשות עם ה-API של `client.fine_tuning.jobs`:\n",
    "- `client.fine_tuning.jobs.list(limit=<n>)` - רשום את n עבודות הכוונון האחרונות\n",
    "- `client.fine_tuning.jobs.retrieve(<job_id>)` - קבל פרטים של עבודת כוונון ספציפית\n",
    "- `client.fine_tuning.jobs.cancel(<job_id>)` - בטל עבודת כוונון\n",
    "- `client.fine_tuning.jobs.list_events(fine_tuning_job_id=<job_id>, limit=<b>)` - רשום עד n אירועים מהעבודה\n",
    "- `client.fine_tuning.jobs.create(model=\"gpt-35-turbo\", training_file=\"your-training-file.jsonl\", ...)`\n",
    "\n",
    "השלב הראשון בתהליך הוא _אימות קובץ האימון_ כדי לוודא שהנתונים בפורמט הנכון.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-GkWiDgZmOsuv4q5cSTEGscY6', created_at=1715566184, level='info', message='Validating training file: file-JdAJcagdOTG6ACNlFWzuzmyV', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-3899xdVTO3LN7Q7LkKLMJUnb', created_at=1715566184, level='info', message='Created fine-tuning job: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_filejob.id, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n",
      "Status: running\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "# Once the training data is validated\n",
    "# Track the job status to see if it is running and when it is complete\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(\"Trained Tokens:\", response.trained_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### שלב 2.3: עקוב אחר אירועים כדי לנטר התקדמות\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 85/100: training loss=0.14\n",
      "Step 86/100: training loss=0.00\n",
      "Step 87/100: training loss=0.00\n",
      "Step 88/100: training loss=0.07\n",
      "Step 89/100: training loss=0.00\n",
      "Step 90/100: training loss=0.00\n",
      "Step 91/100: training loss=0.00\n",
      "Step 92/100: training loss=0.00\n",
      "Step 93/100: training loss=0.00\n",
      "Step 94/100: training loss=0.00\n",
      "Step 95/100: training loss=0.08\n",
      "Step 96/100: training loss=0.05\n",
      "Step 97/100: training loss=0.00\n",
      "Step 98/100: training loss=0.00\n",
      "Step 99/100: training loss=0.00\n",
      "Step 100/100: training loss=0.00\n",
      "Checkpoint created at step 80 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyyF2:ckpt-step-80\n",
      "Checkpoint created at step 90 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyzhK:ckpt-step-90\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "# You can also track progress in a more granular way by checking for events\n",
    "# Refresh this code till you get the `The job has successfully completed` message\n",
    "response = client.fine_tuning.jobs.list_events(ft_filejob.id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### שלב 2.4: הצגת הסטטוס בלוח הבקרה של OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אתה יכול גם לצפות במצב על ידי ביקור באתר OpenAI וחקר של מדור _Fine-tuning_ בפלטפורמה. זה יראה לך את מצב העבודה הנוכחית, ויאפשר לך גם לעקוב אחרי היסטוריית הריצות של ביצועי עבודות קודמות. בתמונה זו, ניתן לראות שהריצה הקודמת נכשלה, והריצה השנייה הצליחה. להקשר, זה קרה כאשר הריצה הראשונה השתמשה בקובץ JSON עם רשומות בפורמט שגוי - לאחר התיקון, הריצה השנייה הושלמה בהצלחה והפכה את המודל לזמין לשימוש.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/he/fine-tuned-model-status.563271727bf7bfba.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אתה יכול גם לצפות בהודעות הסטטוס ובמדדים על ידי גלילה למטה עוד בלוח המחוונים הוויזואלי כפי שמוצג:\n",
    "\n",
    "| הודעות | מדדים |\n",
    "|:---|:---|\n",
    "| ![Messages](../../../../../translated_images/he/fine-tuned-messages-panel.4ed0c2da5ea1313b.png) |  ![Metrics](../../../../../translated_images/he/fine-tuned-metrics-panel.700d7e4995a65229.png)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### שלב 3.1: שליפת מזהה ובדיקת מודל מכוונן בקוד\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the identity of the fine-tuned model once ready\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "fine_tuned_model_id = response.fine_tuned_model\n",
    "print(\"Fine-tuned Model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Strontium, a metal so bright - It's in fireworks, a dazzling sight - It's in bones, you see - And in tea, it's the key - It's the fortieth, so pure, that's the right\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# You can then use that model to generate completions from the SDK as shown\n",
    "# Or you can load that model into the OpenAI Playground (in the UI) to validate it from there.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Elle, a factual chatbot that answers questions about elements in the periodic table with a limerick\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Strontium\"},\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### שלב 3.2: טעינת ובדיקת מודל מותאם אישית ב-Playground\n",
    "\n",
    "כעת ניתן לבדוק את המודל המותאם בשתי דרכים. ראשית, ניתן לבקר ב-Playground ולהשתמש בתפריט הנפתח Models כדי לבחור את המודל המותאם החדש מתוך האפשרויות המופיעות. האפשרות השנייה היא להשתמש באפשרות \"Playground\" המוצגת בלוח Fine-tuning (ראה צילום המסך למעלה) אשר מפעילה את תצוגת ה_השוואה_ הבאה המציגה את גרסאות המודל הבסיסי והמודל המותאם זה לצד זה להערכה מהירה.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/he/fine-tuned-playground-compare.56e06f0ad8922016.png)\n",
    "\n",
    "פשוט מלא את הקשר המערכת ששימש בנתוני האימון שלך וספק את שאלת הבדיקה שלך. תבחין ששני הצדדים מתעדכנים עם אותו הקשר ושאלה זהים. הפעל את ההשוואה ותראה את ההבדל בתגובות ביניהם. _שים לב כיצד המודל המותאם מציג את התגובה בפורמט שסיפקת בדוגמאות שלך בעוד שהמודל הבסיסי פשוט עוקב אחרי הפקודה של המערכת_.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/he/fine-tuned-playground-launch.5a26495c983c6350.png)\n",
    "\n",
    "תבחין שגם ההשוואה מספקת את ספירת הטוקנים עבור כל מודל, ואת הזמן שנדרש להסקת המסקנות. **דוגמה ספציפית זו היא פשוטה ומטרתה להראות את התהליך אך אינה משקפת מערך נתונים או תרחיש אמיתי**. ייתכן שתבחין ששני הדוגמאות מציגות את אותו מספר טוקנים (הקשר המערכת וההנחיה זהים) כאשר המודל המותאם לוקח יותר זמן להסקת מסקנות (מודל מותאם אישית).\n",
    "\n",
    "בתרחישים אמיתיים, לא תשתמש בדוגמה פשוטה כזו, אלא תתאים את המודל לנתונים אמיתיים (למשל, קטלוג מוצרים לשירות לקוחות) שבהם איכות התגובה תהיה ברורה הרבה יותר. בהקשר _זה_, קבלת איכות תגובה שווה ערך עם המודל הבסיסי תדרוש הנדסת פקודות מותאמת אישית שתגדיל את השימוש בטוקנים ואולי גם את זמן העיבוד להסקת מסקנות. _כדי לנסות זאת, עיין בדוגמאות ההתאמה האישית בספר הבישול של OpenAI כדי להתחיל._\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**כתב ויתור**:  \nמסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון כי תרגומים אוטומטיים עלולים להכיל שגיאות או אי-דיוקים. המסמך המקורי בשפת המקור שלו הוא המקור הסמכותי. למידע קריטי מומלץ להשתמש בתרגום מקצועי על ידי אדם. אנו לא נושאים באחריות לכל אי-הבנה או פרשנות שגויה הנובעת משימוש בתרגום זה.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "1725725c956564056baf895e6ca92aa5",
   "translation_date": "2025-12-19T10:52:56+00:00",
   "source_file": "18-fine-tuning/python/openai/oai-assignment.ipynb",
   "language_code": "he"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}