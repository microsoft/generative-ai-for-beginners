{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# בנייה עם משפחת המודלים של Meta\n",
    "\n",
    "## הקדמה\n",
    "\n",
    "בשיעור זה נעסוק ב:\n",
    "\n",
    "- היכרות עם שני המודלים המרכזיים ממשפחת Meta – Llama 3.1 ו-Llama 3.2\n",
    "- הבנת מקרי השימוש והתרחישים המתאימים לכל מודל\n",
    "- דוגמת קוד המדגימה את היכולות הייחודיות של כל מודל\n",
    "\n",
    "## משפחת המודלים של Meta\n",
    "\n",
    "בשיעור זה נחקור שני מודלים ממשפחת Meta, או \"עדר ה-Llama\" – Llama 3.1 ו-Llama 3.2\n",
    "\n",
    "למודלים אלו יש גרסאות שונות, והם זמינים בשוק המודלים של Github. תוכלו למצוא מידע נוסף על שימוש במודלים של Github ל[פיתוח אב-טיפוס עם מודלים של בינה מלאכותית](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "גרסאות המודלים:\n",
    "- Llama 3.1 – 70B Instruct\n",
    "- Llama 3.1 – 405B Instruct\n",
    "- Llama 3.2 – 11B Vision Instruct\n",
    "- Llama 3.2 – 90B Vision Instruct\n",
    "\n",
    "*הערה: Llama 3 זמין גם הוא במודלים של Github, אך לא יידון בשיעור זה*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "עם 405 מיליארד פרמטרים, Llama 3.1 נכנס לקטגוריית מודלי השפה הפתוחים (LLM) בקוד פתוח.\n",
    "\n",
    "הגרסה הזו היא שדרוג לגרסה הקודמת Llama 3 ומציעה:\n",
    "\n",
    "- חלון הקשר גדול יותר – 128k טוקנים לעומת 8k טוקנים\n",
    "- כמות מקסימלית גדולה יותר של טוקנים לפלט – 4096 לעומת 2048\n",
    "- תמיכה רב-לשונית משופרת – בזכות הגדלת כמות הטוקנים באימון\n",
    "\n",
    "היכולות האלו מאפשרות ל-Llama 3.1 להתמודד עם מקרי שימוש מורכבים יותר בפיתוח יישומי GenAI, כולל:\n",
    "- קריאה לפונקציות באופן מובנה – היכולת להפעיל כלים ופונקציות חיצוניות מחוץ לזרימת העבודה של ה-LLM\n",
    "- ביצועי RAG טובים יותר – בזכות חלון הקשר הרחב יותר\n",
    "- יצירת נתונים סינתטיים – היכולת ליצור נתונים יעילים למשימות כמו כיוונון עדין\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### קריאה לפונקציות מובנות\n",
    "\n",
    "Llama 3.1 עברה כיוונון נוסף כדי להיות יעילה יותר בביצוע קריאות לפונקציות או כלים. בנוסף, יש לה שני כלים מובנים שהמודל יודע לזהות מתי יש צורך להשתמש בהם לפי הבקשה של המשתמש. הכלים הם:\n",
    "\n",
    "- **Brave Search** - ניתן להשתמש בו כדי לקבל מידע עדכני כמו מזג האוויר על ידי חיפוש באינטרנט\n",
    "- **Wolfram Alpha** - ניתן להשתמש בו לחישובים מתמטיים מורכבים יותר, כך שאין צורך לכתוב פונקציות משלך.\n",
    "\n",
    "ניתן גם ליצור כלים מותאמים אישית שה-LLM יוכל לקרוא להם.\n",
    "\n",
    "בדוגמה הקודמת למטה:\n",
    "\n",
    "- אנחנו מגדירים את הכלים הזמינים (brave_search, wolfram_alpha) בהנחיית המערכת.\n",
    "- שולחים בקשה מהמשתמש ששואלת על מזג האוויר בעיר מסוימת.\n",
    "- ה-LLM יגיב בקריאה לכלי Brave Search, שתיראה כך: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*הערה: בדוגמה זו מתבצעת רק קריאה לכלי. אם תרצה לקבל את התוצאות, תצטרך ליצור חשבון חינמי בדף ה-API של Brave ולהגדיר את הפונקציה בעצמך.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "למרות ש-Llama 3.1 הוא מודל שפה גדול, אחת המגבלות שלו היא חוסר תמיכה במולטימודאליות. כלומר, היכולת להשתמש בסוגי קלט שונים כמו תמונות כהנחיה ולקבל תגובות בהתאם. היכולת הזו היא אחת התכונות המרכזיות של Llama 3.2. תכונות נוספות כוללות:\n",
    "\n",
    "- מולטימודאליות – מסוגל להעריך גם הנחיות טקסט וגם תמונה\n",
    "- וריאציות בגודל קטן עד בינוני (11B ו-90B) – מאפשר אפשרויות פריסה גמישות,\n",
    "- וריאציות טקסט בלבד (1B ו-3B) – מאפשרות להריץ את המודל על מכשירים ניידים/קצה ומספקות השהיה נמוכה\n",
    "\n",
    "התמיכה במולטימודאליות מהווה צעד משמעותי בעולם המודלים בקוד פתוח. בדוגמה הבאה, הקוד מקבל גם תמונה וגם הנחיית טקסט כדי לקבל ניתוח של התמונה מ-Llama 3.2 90B.\n",
    "\n",
    "### תמיכה מולטימודאלית עם Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## הלמידה לא מסתיימת כאן, המשיכו במסע\n",
    "\n",
    "לאחר שסיימתם את השיעור הזה, בקרו ב-[אוסף הלמידה של בינה מלאכותית גנרטיבית](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) כדי להמשיך להעמיק את הידע שלכם בבינה מלאכותית גנרטיבית!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\nCertainly! Here is your text translated into Hebrew:\n\n**כתב ויתור**:  \nמסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש להיות מודעים לכך שתרגומים אוטומטיים עשויים להכיל שגיאות או אי-דיוקים. המסמך המקורי בשפת המקור ייחשב כמקור הסמכותי. למידע קריטי, מומלץ לפנות לתרגום מקצועי על ידי אדם. איננו אחראים לכל אי-הבנה או פרשנות שגויה הנובעת מהשימוש בתרגום זה.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:45:44+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "he"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}