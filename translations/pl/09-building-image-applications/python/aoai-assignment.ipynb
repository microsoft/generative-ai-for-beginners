{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Budowanie aplikacji do generowania obrazów\n",
    "\n",
    "LLM-y to nie tylko generowanie tekstu. Możliwe jest także tworzenie obrazów na podstawie opisów tekstowych. Obrazy jako dodatkowa modalność mogą być niezwykle przydatne w wielu dziedzinach, takich jak MedTech, architektura, turystyka, tworzenie gier i wiele innych. W tym rozdziale przyjrzymy się dwóm najpopularniejszym modelom generowania obrazów: DALL-E i Midjourney.\n",
    "\n",
    "## Wprowadzenie\n",
    "\n",
    "W tej lekcji omówimy:\n",
    "\n",
    "- Generowanie obrazów i dlaczego jest to przydatne.\n",
    "- DALL-E i Midjourney – czym są i jak działają.\n",
    "- Jak zbudować aplikację do generowania obrazów.\n",
    "\n",
    "## Cele nauki\n",
    "\n",
    "Po ukończeniu tej lekcji będziesz w stanie:\n",
    "\n",
    "- Zbudować aplikację do generowania obrazów.\n",
    "- Określić granice swojej aplikacji za pomocą meta promptów.\n",
    "- Pracować z DALL-E i Midjourney.\n",
    "\n",
    "## Dlaczego warto budować aplikację do generowania obrazów?\n",
    "\n",
    "Aplikacje do generowania obrazów to świetny sposób na poznanie możliwości Sztucznej Inteligencji Generatywnej. Mogą być wykorzystywane na przykład do:\n",
    "\n",
    "- **Edycji i syntezy obrazów**. Możesz generować obrazy do różnych zastosowań, takich jak edycja czy synteza obrazów.\n",
    "\n",
    "- **Zastosowania w różnych branżach**. Mogą być używane do generowania obrazów dla wielu branż, takich jak MedTech, turystyka, tworzenie gier i wiele innych.\n",
    "\n",
    "## Scenariusz: Edu4All\n",
    "\n",
    "W ramach tej lekcji będziemy kontynuować pracę z naszym startupem Edu4All. Uczniowie będą tworzyć obrazy do swoich prac zaliczeniowych – to, jakie to będą obrazy, zależy od nich. Mogą to być ilustracje do własnej bajki, stworzenie nowej postaci do opowiadania lub pomoc w wizualizacji pomysłów i koncepcji.\n",
    "\n",
    "Oto przykład, co uczniowie Edu4All mogliby wygenerować podczas lekcji o zabytkach:\n",
    "\n",
    "![Startup Edu4All, lekcja o zabytkach, Wieża Eiffla](../../../../translated_images/startup.94d6b79cc4bb3f5afbf6e2ddfcf309aa5d1e256b5f30cc41d252024eaa9cc5dc.pl.png)\n",
    "\n",
    "używając promptu takiego jak\n",
    "\n",
    "> \"Pies obok Wieży Eiffla w porannym świetle\"\n",
    "\n",
    "## Czym są DALL-E i Midjourney?\n",
    "\n",
    "[DALL-E](https://openai.com/dall-e-2?WT.mc_id=academic-105485-koreyst) i [Midjourney](https://www.midjourney.com/?WT.mc_id=academic-105485-koreyst) to dwa najpopularniejsze modele generowania obrazów, które pozwalają tworzyć obrazy na podstawie promptów.\n",
    "\n",
    "### DALL-E\n",
    "\n",
    "Zacznijmy od DALL-E, czyli modelu AI generatywnej, który tworzy obrazy na podstawie opisów tekstowych.\n",
    "\n",
    "> [DALL-E to połączenie dwóch modeli: CLIP i diffused attention](https://towardsdatascience.com/openais-dall-e-and-clip-101-a-brief-introduction-3a4367280d4e?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "- **CLIP** to model, który generuje embeddingi, czyli numeryczne reprezentacje danych, zarówno z obrazów, jak i tekstu.\n",
    "\n",
    "- **Diffused attention** to model, który generuje obrazy na podstawie embeddingów. DALL-E jest trenowany na zbiorze danych zawierającym obrazy i teksty, dzięki czemu potrafi tworzyć obrazy na podstawie opisów tekstowych. Na przykład, DALL-E może wygenerować obraz kota w kapeluszu lub psa z irokezem.\n",
    "\n",
    "### Midjourney\n",
    "\n",
    "Midjourney działa podobnie jak DALL-E – generuje obrazy na podstawie promptów tekstowych. Midjourney również pozwala tworzyć obrazy na podstawie promptów takich jak „kot w kapeluszu” czy „pies z irokezem”.\n",
    "\n",
    "![Obraz wygenerowany przez Midjourney, mechaniczny gołąb](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Rupert_Breheny_mechanical_dove_eca144e7-476d-4976-821d-a49c408e4f36.png/440px-Rupert_Breheny_mechanical_dove_eca144e7-476d-4976-821d-a49c408e4f36.png?WT.mc_id=academic-105485-koreyst)\n",
    "\n",
    "*Źródło: Wikipedia, obraz wygenerowany przez Midjourney*\n",
    "\n",
    "## Jak działają DALL-E i Midjourney\n",
    "\n",
    "Najpierw [DALL-E](https://arxiv.org/pdf/2102.12092.pdf?WT.mc_id=academic-105485-koreyst). DALL-E to model AI generatywnej oparty na architekturze transformera z *autoregresyjnym transformerem*.\n",
    "\n",
    "*Autoregresyjny transformer* określa, w jaki sposób model generuje obrazy na podstawie opisów tekstowych – generuje jeden piksel na raz, a następnie wykorzystuje już wygenerowane piksele do stworzenia kolejnego. Proces ten przechodzi przez wiele warstw sieci neuronowej, aż obraz zostanie ukończony.\n",
    "\n",
    "Dzięki temu procesowi DALL-E kontroluje atrybuty, obiekty, cechy i inne elementy obrazu, który generuje. Jednak DALL-E 2 i 3 dają jeszcze większą kontrolę nad tworzonym obrazem,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tworzenie swojej pierwszej aplikacji do generowania obrazów\n",
    "\n",
    "Co jest potrzebne, aby zbudować aplikację do generowania obrazów? Potrzebujesz następujących bibliotek:\n",
    "\n",
    "- **python-dotenv** – zdecydowanie zalecamy użycie tej biblioteki, aby przechowywać swoje dane poufne w pliku *.env*, z dala od kodu.\n",
    "- **openai** – ta biblioteka służy do komunikacji z API OpenAI.\n",
    "- **pillow** – do pracy z obrazami w Pythonie.\n",
    "- **requests** – ułatwia wykonywanie zapytań HTTP.\n",
    "\n",
    "\n",
    "1. Utwórz plik *.env* z następującą zawartością:\n",
    "\n",
    "    ```text\n",
    "    AZURE_OPENAI_ENDPOINT=<your endpoint>\n",
    "    AZURE_OPENAI_API_KEY=<your key>\n",
    "    ```\n",
    "\n",
    "    Te informacje znajdziesz w Azure Portal dla swojego zasobu w sekcji „Keys and Endpoint”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Zbierz powyższe biblioteki w pliku o nazwie *requirements.txt* w następujący sposób:\n",
    "\n",
    "    ```text\n",
    "    python-dotenv\n",
    "    openai\n",
    "    pillow\n",
    "    requests\n",
    "    ```\n",
    "\n",
    "1. Następnie utwórz środowisko wirtualne i zainstaluj biblioteki:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# create virtual env\n",
    "! python3 -m venv venv\n",
    "# activate environment\n",
    "! source venv/bin/activate\n",
    "# install libraries\n",
    "# pip install -r requirements.txt, if using a requirements.txt file \n",
    "! pip install python-dotenv openai pillow requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> Dla systemu Windows użyj poniższych poleceń, aby utworzyć i aktywować swoje wirtualne środowisko:\n",
    "\n",
    "    ```bash\n",
    "    python3 -m venv venv\n",
    "    venv\\Scripts\\activate.bat\n",
    "    ```\n",
    "\n",
    "1. Dodaj poniższy kod do pliku o nazwie *app.py*:\n",
    "\n",
    "    ```python\n",
    "    import openai\n",
    "    import os\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    import dotenv\n",
    "    \n",
    "    # import dotenv\n",
    "    dotenv.load_dotenv()\n",
    "    \n",
    "    # Get endpoint and key from environment variables\n",
    "    openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "    openai.api_key = os.environ['AZURE_OPENAI_API_KEY']     \n",
    "    \n",
    "    # Assign the API version (DALL-E is currently supported for the 2023-06-01-preview API version only)\n",
    "    openai.api_version = '2023-06-01-preview'\n",
    "    openai.api_type = 'azure'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Create an image by using the image generation API\n",
    "        generation_response = openai.Image.create(\n",
    "            prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n",
    "            size='1024x1024',\n",
    "            n=2,\n",
    "            temperature=0,\n",
    "        )\n",
    "        # Set the directory for the stored image\n",
    "        image_dir = os.path.join(os.curdir, 'images')\n",
    "    \n",
    "        # If the directory doesn't exist, create it\n",
    "        if not os.path.isdir(image_dir):\n",
    "            os.mkdir(image_dir)\n",
    "    \n",
    "        # Initialize the image path (note the filetype should be png)\n",
    "        image_path = os.path.join(image_dir, 'generated-image.png')\n",
    "    \n",
    "        # Retrieve the generated image\n",
    "        image_url = generation_response[\"data\"][0][\"url\"]  # extract image URL from response\n",
    "        generated_image = requests.get(image_url).content  # download the image\n",
    "        with open(image_path, \"wb\") as image_file:\n",
    "            image_file.write(generated_image)\n",
    "    \n",
    "        # Display the image in the default image viewer\n",
    "        image = Image.open(image_path)\n",
    "        image.show()\n",
    "    \n",
    "    # catch exceptions\n",
    "    except openai.InvalidRequestError as err:\n",
    "        print(err)\n",
    "\n",
    "    ```\n",
    "\n",
    "Wyjaśnijmy ten kod:\n",
    "\n",
    "- Najpierw importujemy potrzebne biblioteki, w tym bibliotekę OpenAI, dotenv, requests oraz Pillow.\n",
    "\n",
    "    ```python\n",
    "    import openai\n",
    "    import os\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    import dotenv\n",
    "    ```\n",
    "\n",
    "- Następnie ładujemy zmienne środowiskowe z pliku *.env*.\n",
    "\n",
    "    ```python\n",
    "    # import dotenv\n",
    "    dotenv.load_dotenv()\n",
    "    ```\n",
    "\n",
    "- Potem ustawiamy endpoint, klucz do API OpenAI, wersję oraz typ.\n",
    "\n",
    "    ```python\n",
    "    # Get endpoint and key from environment variables\n",
    "    openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "    openai.api_key = os.environ['AZURE_OPENAI_API_KEY'] \n",
    "\n",
    "    # add version and type, Azure specific\n",
    "    openai.api_version = '2023-06-01-preview'\n",
    "    openai.api_type = 'azure'\n",
    "    ```\n",
    "\n",
    "- Następnie generujemy obraz:\n",
    "\n",
    "    ```python\n",
    "    # Create an image by using the image generation API\n",
    "    generation_response = openai.Image.create(\n",
    "        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n",
    "        size='1024x1024',\n",
    "        n=2,\n",
    "        temperature=0,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    Powyższy kod zwraca obiekt JSON, który zawiera adres URL wygenerowanego obrazu. Możemy użyć tego adresu, aby pobrać obraz i zapisać go do pliku.\n",
    "\n",
    "- Na końcu otwieramy obraz i wyświetlamy go w domyślnej przeglądarce obrazów:\n",
    "\n",
    "    ```python\n",
    "    image = Image.open(image_path)\n",
    "    image.show()\n",
    "    ```\n",
    "\n",
    "### Więcej szczegółów na temat generowania obrazu\n",
    "\n",
    "Przyjrzyjmy się dokładniej kodowi, który generuje obraz:\n",
    "\n",
    "```python\n",
    "generation_response = openai.Image.create(\n",
    "        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here\n",
    "        size='1024x1024',\n",
    "        n=2,\n",
    "        temperature=0,\n",
    "    )\n",
    "```\n",
    "\n",
    "- **prompt** to tekst, na podstawie którego generowany jest obraz. W tym przypadku używamy promptu \"Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils\".\n",
    "- **size** to rozmiar generowanego obrazu. W tym przypadku generujemy obraz o wymiarach 1024x1024 pikseli.\n",
    "- **n** to liczba generowanych obrazów. W tym przypadku generujemy dwa obrazy.\n",
    "- **temperature** to parametr, który kontroluje losowość wyników modelu generatywnego AI. Temperatura to wartość od 0 do 1, gdzie 0 oznacza, że wynik jest deterministyczny, a 1 – że jest losowy. Domyślna wartość to 0.7.\n",
    "\n",
    "Jest jeszcze więcej rzeczy, które możesz zrobić z obrazami – omówimy je w kolejnej sekcji.\n",
    "\n",
    "## Dodatkowe możliwości generowania obrazów\n",
    "\n",
    "Do tej pory widziałeś, jak można wygenerować obraz za pomocą kilku linijek kodu w Pythonie. Jednak z obrazami można zrobić znacznie więcej.\n",
    "\n",
    "Możesz także:\n",
    "\n",
    "- **Wykonywać edycje**. Podając istniejący obraz, maskę oraz prompt, możesz zmienić obraz. Na przykład możesz dodać coś do wybranego fragmentu obrazu. Wyobraź sobie nasz obrazek z królikiem – możesz dodać królikowi kapelusz. Robisz to, podając obraz, maskę (wskazującą obszar do zmiany) oraz tekstowy prompt opisujący, co ma zostać zrobione.\n",
    "\n",
    "    ```python\n",
    "    response = openai.Image.create_edit(\n",
    "      image=open(\"base_image.png\", \"rb\"),\n",
    "      mask=open(\"mask.png\", \"rb\"),\n",
    "      prompt=\"An image of a rabbit with a hat on its head.\",\n",
    "      n=1,\n",
    "      size=\"1024x1024\"\n",
    "    )\n",
    "    image_url = response['data'][0]['url']\n",
    "    ```\n",
    "\n",
    "    Obraz bazowy zawierałby tylko królika, a na końcowym obrazie królik miałby już kapelusz.\n",
    "\n",
    "- **Tworzyć wariacje**.\n",
    "    Zajrzyj do naszego [notatnika OpenAI po więcej informacji](./oai-assignment.ipynb?WT.mc_id=academic-105485-koreyst).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Zastrzeżenie**:  \nTen dokument został przetłumaczony przy użyciu usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było poprawne, należy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być traktowany jako źródło nadrzędne. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnych usług tłumacza. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "d5c5bc8d857e5cb63e313beac9fa6402",
   "translation_date": "2025-08-25T19:16:07+00:00",
   "source_file": "09-building-image-applications/python/aoai-assignment.ipynb",
   "language_code": "pl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}