<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b2651fb16bcfbc62b8e518751ed90fdb",
  "translation_date": "2025-10-18T00:49:36+00:00",
  "source_file": "05-advanced-prompts/README.md",
  "language_code": "pl"
}
-->
# Tworzenie zaawansowanych promptÃ³w

[![Tworzenie zaawansowanych promptÃ³w](../../../translated_images/05-lesson-banner.522610fd4a2cd82dbed66bb7e6fe104ed6da172e085dbb4d9100b28dc73ed435.pl.png)](https://youtu.be/BAjzkaCdRok?si=NmUIyRf7-cDgbjtt)

Podsumujmy najwaÅ¼niejsze informacje z poprzedniego rozdziaÅ‚u:

> InÅ¼ynieria promptÃ³w to proces, w ktÃ³rym **kierujemy model w stronÄ™ bardziej trafnych odpowiedzi**, dostarczajÄ…c bardziej uÅ¼yteczne instrukcje lub kontekst.

Pisanie promptÃ³w skÅ‚ada siÄ™ z dwÃ³ch etapÃ³w: konstruowania promptu poprzez dostarczenie odpowiedniego kontekstu oraz _optymalizacji_, czyli stopniowego ulepszania promptu.

Na tym etapie mamy juÅ¼ podstawowe zrozumienie, jak pisaÄ‡ prompty, ale musimy zagÅ‚Ä™biÄ‡ siÄ™ w temat. W tym rozdziale przejdziesz od testowania rÃ³Å¼nych promptÃ³w do zrozumienia, dlaczego jeden prompt jest lepszy od drugiego. Nauczysz siÄ™, jak konstruowaÄ‡ prompty, stosujÄ…c podstawowe techniki, ktÃ³re moÅ¼na zastosowaÄ‡ w kaÅ¼dym LLM.

## Wprowadzenie

W tym rozdziale omÃ³wimy nastÄ™pujÄ…ce tematy:

- Rozszerzenie wiedzy na temat inÅ¼ynierii promptÃ³w poprzez zastosowanie rÃ³Å¼nych technik.
- Konfigurowanie promptÃ³w w celu uzyskania zrÃ³Å¼nicowanych wynikÃ³w.

## Cele nauki

Po ukoÅ„czeniu tej lekcji bÄ™dziesz w stanie:

- ZastosowaÄ‡ techniki inÅ¼ynierii promptÃ³w, ktÃ³re poprawiajÄ… wyniki.
- TworzyÄ‡ prompty generujÄ…ce wyniki zrÃ³Å¼nicowane lub deterministyczne.

## InÅ¼ynieria promptÃ³w

InÅ¼ynieria promptÃ³w to proces tworzenia promptÃ³w, ktÃ³re generujÄ… poÅ¼Ä…dane wyniki. InÅ¼ynieria promptÃ³w to coÅ› wiÄ™cej niÅ¼ tylko pisanie tekstowego promptu. Nie jest to dyscyplina inÅ¼ynierska, ale raczej zestaw technik, ktÃ³re moÅ¼na zastosowaÄ‡, aby osiÄ…gnÄ…Ä‡ zamierzony rezultat.

### PrzykÅ‚ad promptu

WeÅºmy podstawowy prompt, taki jak ten:

> Wygeneruj 10 pytaÅ„ dotyczÄ…cych geografii.

W tym promptcie faktycznie stosujesz zestaw rÃ³Å¼nych technik.

RozÅ‚Ã³Å¼my go na czÄ™Å›ci.

- **Kontekst**, okreÅ›lasz, Å¼e ma dotyczyÄ‡ "geografii".
- **Ograniczenie wyniku**, chcesz, aby byÅ‚o nie wiÄ™cej niÅ¼ 10 pytaÅ„.

### Ograniczenia prostych promptÃ³w

MoÅ¼esz, ale nie musisz, uzyskaÄ‡ poÅ¼Ä…dany wynik. Pytania zostanÄ… wygenerowane, ale geografia to szeroki temat i moÅ¼esz nie uzyskaÄ‡ tego, czego oczekujesz z nastÄ™pujÄ…cych powodÃ³w:

- **Szeroki temat**, nie wiesz, czy bÄ™dzie to dotyczyÄ‡ krajÃ³w, stolic, rzek itd.
- **Format**, co jeÅ›li chciaÅ‚byÅ›, aby pytania miaÅ‚y okreÅ›lony format?

Jak widaÄ‡, jest wiele rzeczy do rozwaÅ¼enia podczas tworzenia promptÃ³w.

Do tej pory widzieliÅ›my prosty przykÅ‚ad promptu, ale generatywna sztuczna inteligencja jest zdolna do znacznie wiÄ™cej, aby pomÃ³c ludziom w rÃ³Å¼nych rolach i branÅ¼ach. Przyjrzyjmy siÄ™ teraz podstawowym technikom.

### Techniki tworzenia promptÃ³w

Najpierw musimy zrozumieÄ‡, Å¼e tworzenie promptÃ³w jest _wÅ‚aÅ›ciwoÅ›ciÄ… emergentnÄ…_ LLM, co oznacza, Å¼e nie jest to funkcja wbudowana w model, ale raczej coÅ›, co odkrywamy podczas jego uÅ¼ywania.

Istnieje kilka podstawowych technik, ktÃ³re moÅ¼emy zastosowaÄ‡ do tworzenia promptÃ³w w LLM. Przyjrzyjmy siÄ™ im.

- **Zero-shot prompting**, to najprostsza forma tworzenia promptÃ³w. Polega na pojedynczym promptcie, ktÃ³ry prosi o odpowiedÅº od LLM wyÅ‚Ä…cznie na podstawie jego danych treningowych.
- **Few-shot prompting**, ten rodzaj promptu prowadzi LLM, dostarczajÄ…c 1 lub wiÄ™cej przykÅ‚adÃ³w, na ktÃ³rych moÅ¼e siÄ™ oprzeÄ‡, aby wygenerowaÄ‡ odpowiedÅº.
- **Chain-of-thought**, ten rodzaj promptu instruuje LLM, jak rozÅ‚oÅ¼yÄ‡ problem na kroki.
- **Generated knowledge**, aby poprawiÄ‡ odpowiedÅº promptu, moÅ¼esz dostarczyÄ‡ dodatkowe fakty lub wiedzÄ™ wygenerowanÄ… w ramach promptu.
- **Least to most**, podobnie jak chain-of-thought, ta technika polega na rozbiciu problemu na seriÄ™ krokÃ³w, ktÃ³re nastÄ™pnie sÄ… wykonywane w okreÅ›lonej kolejnoÅ›ci.
- **Self-refine**, ta technika polega na krytycznej ocenie odpowiedzi LLM i poproszeniu go o jej poprawienie.
- **Maieutic prompting**, tutaj chodzi o upewnienie siÄ™, Å¼e odpowiedÅº LLM jest poprawna, proszÄ…c go o wyjaÅ›nienie rÃ³Å¼nych czÄ™Å›ci odpowiedzi. Jest to forma self-refine.

### Zero-shot prompting

Ten styl promptu jest bardzo prosty, skÅ‚ada siÄ™ z pojedynczego promptu. Ta technika jest prawdopodobnie tym, czego uÅ¼ywasz, gdy zaczynasz uczyÄ‡ siÄ™ o LLM. Oto przykÅ‚ad:

- Prompt: "Co to jest algebra?"
- OdpowiedÅº: "Algebra to gaÅ‚Ä…Åº matematyki zajmujÄ…ca siÄ™ badaniem symboli matematycznych i zasad ich manipulacji."

### Few-shot prompting

Ten styl promptu pomaga modelowi, dostarczajÄ…c kilka przykÅ‚adÃ³w wraz z zapytaniem. SkÅ‚ada siÄ™ z pojedynczego promptu z dodatkowymi danymi specyficznymi dla zadania. Oto przykÅ‚ad:

- Prompt: "Napisz wiersz w stylu Szekspira. Oto kilka przykÅ‚adÃ³w sonetÃ³w Szekspira:
  Sonet 18: 'Czy mam ciÄ™ porÃ³wnaÄ‡ do letniego dnia? JesteÅ› bardziej piÄ™kny i bardziej umiarkowany...'
  Sonet 116: 'Niech nie bÄ™dzie przeszkÃ³d w maÅ‚Å¼eÅ„stwie prawdziwych umysÅ‚Ã³w. MiÅ‚oÅ›Ä‡ nie jest miÅ‚oÅ›ciÄ…, ktÃ³ra zmienia siÄ™, gdy zmiana siÄ™ pojawia...'
  Sonet 132: 'Twoje oczy kocham, a one, jakby litujÄ…c siÄ™ nade mnÄ…, ZnajÄ…c twoje serce, drÄ™czÄ… mnie pogardÄ…,...'
  Teraz napisz sonet o piÄ™knie ksiÄ™Å¼yca."
- OdpowiedÅº: "Na niebie ksiÄ™Å¼yc Å‚agodnie lÅ›ni, W srebrzystym blasku, ktÃ³ry rzuca swÄ… Å‚askÄ™,..."

PrzykÅ‚ady dostarczajÄ… LLM kontekstu, formatu lub stylu poÅ¼Ä…danego wyniku. PomagajÄ… modelowi zrozumieÄ‡ konkretne zadanie i generowaÄ‡ bardziej trafne odpowiedzi.

### Chain-of-thought

Chain-of-thought to bardzo interesujÄ…ca technika, poniewaÅ¼ polega na przeprowadzeniu LLM przez seriÄ™ krokÃ³w. PomysÅ‚ polega na instruowaniu LLM w taki sposÃ³b, aby zrozumiaÅ‚, jak coÅ› zrobiÄ‡. RozwaÅ¼my nastÄ™pujÄ…cy przykÅ‚ad, z i bez chain-of-thought:

    - Prompt: "Alicja ma 5 jabÅ‚ek, rzuca 3 jabÅ‚ka, daje 2 Bobowi, a Bob oddaje jedno, ile jabÅ‚ek ma Alicja?"
    - OdpowiedÅº: 5

LLM odpowiada 5, co jest bÅ‚Ä™dne. Poprawna odpowiedÅº to 1 jabÅ‚ko, biorÄ…c pod uwagÄ™ obliczenia (5 - 3 - 2 + 1 = 1).

Jak moÅ¼emy nauczyÄ‡ LLM, aby zrobiÅ‚ to poprawnie?

SprÃ³bujmy chain-of-thought. Zastosowanie chain-of-thought oznacza:

1. Podaj LLM podobny przykÅ‚ad.
1. PokaÅ¼ obliczenia i jak je poprawnie wykonaÄ‡.
1. Dostarcz oryginalny prompt.

Oto jak to wyglÄ…da:

- Prompt: "Lisa ma 7 jabÅ‚ek, rzuca 1 jabÅ‚ko, daje 4 jabÅ‚ka Bartowi, a Bart oddaje jedno:
  7 -1 = 6
  6 -4 = 2
  2 +1 = 3  
  Alicja ma 5 jabÅ‚ek, rzuca 3 jabÅ‚ka, daje 2 Bobowi, a Bob oddaje jedno, ile jabÅ‚ek ma Alicja?"
  OdpowiedÅº: 1

ZauwaÅ¼, jak piszemy znacznie dÅ‚uÅ¼sze prompty z innym przykÅ‚adem, obliczeniami, a nastÄ™pnie oryginalnym promptem i dochodzimy do poprawnej odpowiedzi 1.

Jak widaÄ‡, chain-of-thought to bardzo potÄ™Å¼na technika.

### Generated knowledge

CzÄ™sto, gdy chcesz skonstruowaÄ‡ prompt, chcesz to zrobiÄ‡, uÅ¼ywajÄ…c danych swojej firmy. Chcesz, aby czÄ™Å›Ä‡ promptu pochodziÅ‚a z firmy, a druga czÄ™Å›Ä‡ byÅ‚a faktycznym promptem, ktÃ³ry CiÄ™ interesuje.

Na przykÅ‚ad, jeÅ›li pracujesz w branÅ¼y ubezpieczeniowej, TwÃ³j prompt moÅ¼e wyglÄ…daÄ‡ tak:

```text
{{company}}: {{company_name}}
{{products}}:
{{products_list}}
Please suggest an insurance given the following budget and requirements:
Budget: {{budget}}
Requirements: {{requirements}}
```

PowyÅ¼ej widzisz, jak prompt jest skonstruowany za pomocÄ… szablonu. W szablonie znajduje siÄ™ kilka zmiennych, oznaczonych jako `{{variable}}`, ktÃ³re zostanÄ… zastÄ…pione rzeczywistymi wartoÅ›ciami z API firmy.

Oto przykÅ‚ad, jak moÅ¼e wyglÄ…daÄ‡ prompt po zastÄ…pieniu zmiennych treÅ›ciÄ… z Twojej firmy:

```text
Insurance company: ACME Insurance
Insurance products (cost per month):
- Car, cheap, 500 USD
- Car, expensive, 1100 USD
- Home, cheap, 600 USD
- Home, expensive, 1200 USD
- Life, cheap, 100 USD

Please suggest an insurance given the following budget and requirements:
Budget: $1000
Requirements: Car, Home, and Life insurance
```

Uruchomienie tego promptu w LLM wygeneruje odpowiedÅº takÄ… jak ta:

```output
Given the budget and requirements, we suggest the following insurance package from ACME Insurance:
- Car, cheap, 500 USD
- Home, cheap, 600 USD
- Life, cheap, 100 USD
Total cost: $1,200 USD
```

Jak widaÄ‡, sugeruje rÃ³wnieÅ¼ ubezpieczenie na Å¼ycie, czego nie powinien. Ten wynik wskazuje, Å¼e musimy zoptymalizowaÄ‡ prompt, zmieniajÄ…c go tak, aby byÅ‚ bardziej precyzyjny. Po kilku _prÃ³bach i bÅ‚Ä™dach_ dochodzimy do nastÄ™pujÄ…cego promptu:

```text
Insurance company: ACME Insurance
Insurance products (cost per month):
- type: Car, cheap, cost: 500 USD
- type: Car, expensive, cost: 1100 USD
- type: Home, cheap, cost: 600 USD
- type: Home, expensive, cost: 1200 USD
- type: Life, cheap, cost: 100 USD

Please suggest an insurance given the following budget and requirements:
Budget: $1000 restrict choice to types: Car, Home
```

ZauwaÅ¼, jak dodanie _typu_ i _kosztu_, a takÅ¼e uÅ¼ycie sÅ‚owa kluczowego _ograniczenie_ pomaga LLM zrozumieÄ‡, czego chcemy.

Teraz otrzymujemy nastÄ™pujÄ…cÄ… odpowiedÅº:

```output
Given the budget and requirements, we suggest the Car, Cheap insurance product which costs 500 USD per month.
```

Celem tego przykÅ‚adu byÅ‚o pokazanie, Å¼e nawet jeÅ›li uÅ¼ywamy podstawowej techniki, takiej jak _generated knowledge_, w wiÄ™kszoÅ›ci przypadkÃ³w nadal musimy zoptymalizowaÄ‡ prompt, aby uzyskaÄ‡ poÅ¼Ä…dany wynik.

### Least-to-most

PomysÅ‚ na Least-to-most prompting polega na rozbiciu wiÄ™kszego problemu na podproblemy. W ten sposÃ³b pomagasz LLM "pokonaÄ‡" wiÄ™kszy problem. Dobrym przykÅ‚adem moÅ¼e byÄ‡ analiza danych, gdzie moÅ¼esz poprosiÄ‡ LLM o podzielenie problemu w nastÄ™pujÄ…cy sposÃ³b:

> Prompt: Jak przeprowadziÄ‡ analizÄ™ danych w 5 krokach?

Twoja odpowiedÅº od asystenta AI:

1. Zbierz dane
1. OczyÅ›Ä‡ dane
1. Przeanalizuj dane
1. ZrÃ³b wykres danych
1. Przedstaw dane

### Self-refine, krytyka wynikÃ³w

W przypadku generatywnej sztucznej inteligencji i LLM nie moÅ¼na w peÅ‚ni ufaÄ‡ wynikom. Musisz je zweryfikowaÄ‡. W koÅ„cu LLM prezentuje tylko to, co jest najbardziej prawdopodobne do powiedzenia, a nie to, co jest poprawne. Dlatego dobrym pomysÅ‚em jest poproszenie LLM o krytykÄ™ wÅ‚asnych wynikÃ³w, co prowadzi nas do techniki self-refine.

Jak to dziaÅ‚a? PostÄ™puj zgodnie z nastÄ™pujÄ…cymi krokami:

1. PoczÄ…tkowy prompt, w ktÃ³rym prosisz LLM o rozwiÄ…zanie problemu.
1. LLM odpowiada.
1. Krytykujesz odpowiedÅº i prosisz AI o poprawÄ™.
1. LLM odpowiada ponownie, tym razem uwzglÄ™dniajÄ…c krytykÄ™ i sugerujÄ…c rozwiÄ…zania, ktÃ³re wymyÅ›liÅ‚.

MoÅ¼esz powtarzaÄ‡ ten proces tyle razy, ile chcesz.

Oto przykÅ‚ad uÅ¼ycia tej techniki:

> Prompt: "StwÃ³rz Python Web API z trasami products i customers"

OdpowiedÅº AI:

```python
import flask
app = Flask(__name__)

@app.route('/products')
def products():
    return 'Products'

@app.route('/customers')
def customers():
    return 'Customers'
```

> Prompt: zaproponuj 3 ulepszenia powyÅ¼szego kodu

OdpowiedÅº AI:

```text
Sure, here are three suggestions to improve the code:

Import the correct module: The first line should import Flask instead of flask. The correct import statement should be from flask import Flask.

Add a main function: Add a if __name__ == '__main__': block to the end of the file to run the application when the file is executed.

Use f-strings: Use f-strings to format the return statements in the route functions. For example, return 'Products' can be changed to return f'This is the {request.path} page'.
```

```python
from flask import Flask, request

app = Flask(__name__)

@app.route('/products')
def products():
    return f'This is the {request.path} page'

@app.route('/customers')
def customers():
    return f'This is the {request.path} page'

if __name__ == '__main__':
    app.run()
```

Jak widaÄ‡, powyÅ¼sza odpowiedÅº AI poprawia pierwszy zaproponowany kod dziÄ™ki krytyce pierwszej odpowiedzi.

### Maieutic prompting

Maieutic prompting to technika podobna do self-refine, ale bardziej polega na proszeniu LLM o wyjaÅ›nienie swoich odpowiedzi. Celem jest zmniejszenie niespÃ³jnoÅ›ci w wynikach LLM, aby upewniÄ‡ siÄ™, Å¼e dochodzi do poprawnej odpowiedzi. Proces wyglÄ…da nastÄ™pujÄ…co:

1. PoproÅ› LLM o odpowiedÅº na pytanie.
1. Dla kaÅ¼dej czÄ™Å›ci odpowiedzi poproÅ› LLM o bardziej szczegÃ³Å‚owe wyjaÅ›nienie.
1. JeÅ›li wystÄ™pujÄ… niespÃ³jnoÅ›ci, odrzuÄ‡ czÄ™Å›ci, ktÃ³re sÄ… niespÃ³jne.

Powtarzaj kroki 2 i 3, aÅ¼ przejdziesz przez wszystkie czÄ™Å›ci i bÄ™dziesz zadowolony z odpowiedzi.

Oto przykÅ‚ad promptu:

> prompt: Jak mogÄ™ stworzyÄ‡ plan kryzysowy, aby zÅ‚agodziÄ‡ skutki pandemii w 5 krokach?
> OdpowiedÅº LLM:

```text
1. Identify the risks
1. Identify the stakeholders
1. Identify the resources
1. Identify the actions
1. Identify the communication channels
```

Zidentyfikowano 5 krokÃ³w, ale czy moÅ¼emy stwierdziÄ‡, Å¼e sÄ… poprawne? Zapytajmy LLM o wyjaÅ›nienie kaÅ¼dego kroku:

> prompt: WyjaÅ›nij pierwszy krok bardziej szczegÃ³Å‚owo, jakie sÄ… szczegÃ³Å‚owe ryzyka zwiÄ…zane z pandemiÄ…?

```text
The risks are the following:

- Loss of life
- Loss of business
- Loss of reputation
- Loss of customers
- Loss of employees
```

Na tym etapie chcemy upewniÄ‡ siÄ™, Å¼e LLM jest poprawny, wiÄ™c prosimy go o wyjaÅ›nienie pierwszego ryzyka bardziej szczegÃ³Å‚owo i mamy nadziejÄ™, Å¼e jest zgodne z powyÅ¼szÄ… odpowiedziÄ…:

> prompt: W pandemii, jakie jest najwiÄ™ksze ryzyko i dlaczego?

```text
The biggest risk is loss of life because it's the most important thing to protect.
```

> Jakie sÄ… dwa najwiÄ™ksze ryzyka w pandemii?

```text
The two biggest risks are loss of life and loss of business.
```

Na tym etapie LLM jest spÃ³jny i wspomina "Å¼ycie" i "biznes" jako dwa najwiÄ™ksze ryzyka. MoÅ¼emy teraz przejÅ›Ä‡ do nastÄ™pnego kroku i czuÄ‡ siÄ™ doÅ›Ä‡ pewnie. Jednak nie powinniÅ›my Å›lepo ufaÄ‡ LLM, zawsze naleÅ¼y weryfikowaÄ‡ wyniki.

## ZrÃ³Å¼nicowanie wynikÃ³w

LLM sÄ… z natury niedeterministyczne, co oznacza, Å¼e za kaÅ¼dym razem, gdy uruchomisz ten sam prompt, moÅ¼esz otrzymaÄ‡ rÃ³Å¼ne wyniki. SprÃ³buj na przykÅ‚ad tego promptu:

> "Wygeneruj kod dla Python Web API"

```python
# Import necessary modules
import flask
from flask import request, jsonify

# Create a Flask app
app = flask.Flask(__name__)

# Create a route for the API
@app.route('/api', methods=['GET'])
def api():
    # Get the data from the request
    data = request.args

    # Process the data
    result = process_data(data)

    # Return the result as JSON
    return jsonify(result)

# Function to process the data
def process_data(data):
    # Do something with the data
    result = data + 1

    # Return the result
    return result

# Run the app
if __name__ == '__main__':
    app.run()

```

Uruchomienie tego samego promptu ponownie generuje nieco innÄ… odpowiedÅº:

```python
#import necessary packages
import flask
from flask import request, jsonify

#create the Flask application
app = flask.Flask(__name__)
app.config["DEBUG"] = True

#create a list of books
books = [
    {'id': 0,
     'title': 'A Fire Upon the Deep',
     'author': 'Vernor Vinge',
     'first_sentence': 'The coldsleep itself was dreamless.',
     'year_published': '1992'},
    {'id': 1,
     'title': 'The Ones Who Walk Away From Omelas',
     'author': 'Ursula K. Le Guin',
     'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.',
     'published': '1973'},
    {'id': 2,
     'title': 'Dhalgren',
     'author': 'Samuel R. Delany',
     'first_sentence': 'to wound the autumnal city.',
     'published': '1975'}
]

#create an endpoint for the API
@app.route('/', methods=['GET'])
def home():
    return '''<h1>Books API</h1>
<p>A prototype API for retrieving books.</p>'''

#create an endpoint to return all books
@app.route('/api/v1/resources/books/all', methods=['GET'])
def api_all():
    return jsonify(books)

#create an endpoint to return a single book
@app.route('/api/v1/resources/books', methods=['GET'])
def api_id():
    #check if an ID was provided as part of the URL
    #if ID is provided, assign it to a variable
    #if no ID is provided, display an error
    if 'id' in request.args:
        id = int(request.args['id'])
    else:
        return "Error: No id field provided. Please specify an id."

    #create an empty list for our results
    results = []

    #loop through the data and match results that fit the requested ID
    #IDs are unique, but other fields might return many results
    for book in books:
        if book['id'] == id:
            results.append(book)

    #use the jsonify function from Flask to convert our list of
    #Python dictionaries to the JSON format
    return jsonify(results)

app.run()
```

> Czy zrÃ³Å¼nicowane wyniki sÄ… problemem?

To zaleÅ¼y od tego, co prÃ³bujesz osiÄ…gnÄ…Ä‡. JeÅ›li chcesz uzyskaÄ‡ konkretnÄ… odpowiedÅº, to jest to problem. JeÅ›li akceptujesz zrÃ³Å¼nicowane wyniki, takie jak "Wygeneruj dowolne 3 pytania dotyczÄ…ce geografii", to nie jest to problem.

### UÅ¼ycie temperatury do zrÃ³Å¼nicowania wynikÃ³w

Ok, wiÄ™c zdecydowaliÅ›my, Å¼e chcemy ograniczyÄ‡ wyniki, aby byÅ‚y bardziej przewidywalne, czyli bardziej deterministyczne. Jak to zrobiÄ‡?

Temperatura to wartoÅ›Ä‡ miÄ™dzy 0 a 1, gdzie 0 oznacza najbardziej deterministyczne wyniki, a 1 najbardziej zrÃ³Å¼nicowane. DomyÅ›lna wartoÅ›Ä‡ to 0.7. Zobaczmy, co siÄ™ stanie przy dwÃ³ch uruchomieniach tego samego promptu z ustawionÄ… temperaturÄ… na 0.1:

> "Wygeneruj kod dla Python Web API"

```python
#import necessary libraries
import flask
from flask import request, jsonify

#create an instance of the Flask class
app = flask.Flask(__name__)

#create an endpoint for the API
@app.route('/api/v1/endpoint', methods=['GET'])
def api_endpoint():
    #get the request data
    data = request.get_json()

    #process the data
    result = process_data(data)

    #return the result
    return jsonify(result)

#function to process the data
def process_data(data):
    #process the data
    result = {'result': 'success'}

    #return the result
    return result

#run the Flask app
if __name__ == '__main__':
    app.run()

```

Ponowne uruchomienie promptu daje taki wynik:

```python
#import necessary libraries
import flask
from flask import request, jsonify

#create a Flask app
app = flask.Flask(__name__)

#create a route for the API
@app.route('/api', methods=['GET'])
def api_call():
    #get the data from the request
    data = request.get_json()

    #process the data
    result = process_data(data)

    #return the result
    return jsonify(result)

#define the process_data function
def process_data(data):
    #do something with the data
    result = data + 1

    #return the result
    return result

#run the app
if __name__ == '__main__':
    app.run()

```

RÃ³Å¼nica miÄ™dzy tymi dwoma wynikami jest minimalna. Tym razem zrÃ³bmy odwrotnie, ustawmy temperaturÄ™ na 0.9:

```python
# Import necessary libraries
import flask
from flask import request, jsonify

# Create a Flask app
app = flask.Flask(__name__)

# Create a route for the API
@app.route('/api', methods=['GET'])
def api_call():
    # Get the data from the request
    data = request.args

    # Process the data
    result = process_data(data)

    # Return the result
    return jsonify(result)

# Function to process the data
def process_data(data):
    # Do something with the data
    result = data + 1

    # Return the result
    return result

# Run the app
if __name__ == '__main__':
    app.run()

```

i drugie podejÅ›cie przy wartoÅ›ci temperatury 0.9:

```python
import flask
from flask import request, jsonify

# create the Flask app
app = flask.Flask(__name__)
app.config['DEBUG'] = True

# create some test data
books = [
    {'id': 0, 'title': 'A Fire Upon The Deep', 'author': 'Vernor Vinge', 'first_sentence': 'The coldsleep itself was dreamless.', 'year_published': '1992'},
    {'id': 1, 'title': 'The Ones Who Walk Away From Omelas', 'author': 'Ursula K. Le Guin', 'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.', 'published': '1973'},
    {'id': 2, 'title': 'Dhalgren', 'author': 'Samuel R. Delany', 'first_sentence': 'to wound the autumnal city.', 'published': '1975'}
]

# create an endpoint
@app.route('/', methods=['GET'])
def home():
    return '''<h1>Welcome to our book API!</h1>'''

@app.route('/api/v1/resources/books

```

Jak widaÄ‡, wyniki nie mogÅ‚yby byÄ‡ bardziej zrÃ³Å¼nicowane.

> PamiÄ™taj, Å¼e istnieje wiÄ™cej parametrÃ³w, ktÃ³re moÅ¼esz zmieniÄ‡, aby zrÃ³Å¼nicowaÄ‡ wyniki, takich jak top-k, top-p, kara za powtÃ³rzenia, kara za dÅ‚ugoÅ›Ä‡ i kara za rÃ³Å¼norodnoÅ›Ä‡, ale sÄ… one poza zakresem tego programu nauczania.

## Dobre praktyki

Istnieje wiele praktyk, ktÃ³re moÅ¼esz zastosowaÄ‡, aby osiÄ…gnÄ…Ä‡ zamierzony efekt. Z czasem, korzystajÄ…c z techniki promptingu, wypracujesz swÃ³j wÅ‚asny styl.

OprÃ³cz technik, ktÃ³re omÃ³wiliÅ›my, warto wziÄ…Ä‡ pod uwagÄ™ kilka dobrych praktyk podczas tworzenia promptÃ³w dla LLM.

Oto kilka dobrych praktyk, ktÃ³re warto rozwaÅ¼yÄ‡:

- **OkreÅ›l kontekst**. Kontekst ma znaczenie â€“ im wiÄ™cej szczegÃ³Å‚Ã³w podasz, takich jak dziedzina, temat itd., tym lepiej.
- Ogranicz wynik. JeÅ›li chcesz okreÅ›lonÄ… liczbÄ™ elementÃ³w lub konkretnÄ… dÅ‚ugoÅ›Ä‡, wyraÅºnie to zaznacz.
- **OkreÅ›l zarÃ³wno co, jak i jak**. PamiÄ™taj, aby wspomnieÄ‡ zarÃ³wno, co chcesz, jak i jak chcesz to osiÄ…gnÄ…Ä‡, na przykÅ‚ad: "StwÃ³rz Python Web API z trasami dla produktÃ³w i klientÃ³w, podziel je na 3 pliki".
- **UÅ¼ywaj szablonÃ³w**. CzÄ™sto bÄ™dziesz chciaÅ‚ wzbogaciÄ‡ swoje prompty o dane z Twojej firmy. UÅ¼ywaj szablonÃ³w, aby to zrobiÄ‡. Szablony mogÄ… zawieraÄ‡ zmienne, ktÃ³re zastÄ™pujesz rzeczywistymi danymi.
- **Pisownia ma znaczenie**. LLM moÅ¼e dostarczyÄ‡ poprawnÄ… odpowiedÅº, ale jeÅ›li piszesz poprawnie, otrzymasz lepszÄ… odpowiedÅº.

## Zadanie

Oto kod w Pythonie pokazujÄ…cy, jak zbudowaÄ‡ prosty API za pomocÄ… Flask:

```python
from flask import Flask, request

app = Flask(__name__)

@app.route('/')
def hello():
    name = request.args.get('name', 'World')
    return f'Hello, {name}!'

if __name__ == '__main__':
    app.run()
```

UÅ¼yj asystenta AI, takiego jak GitHub Copilot lub ChatGPT, i zastosuj technikÄ™ "self-refine", aby ulepszyÄ‡ kod.

## RozwiÄ…zanie

SprÃ³buj rozwiÄ…zaÄ‡ zadanie, dodajÄ…c odpowiednie prompty do kodu.

> [!TIP]
> SformuÅ‚uj prompt, aby poprosiÄ‡ o ulepszenie. Dobrym pomysÅ‚em jest ograniczenie liczby ulepszeÅ„. MoÅ¼esz rÃ³wnieÅ¼ poprosiÄ‡ o ulepszenie w okreÅ›lony sposÃ³b, na przykÅ‚ad pod kÄ…tem architektury, wydajnoÅ›ci, bezpieczeÅ„stwa itd.

[RozwiÄ…zanie](../../../05-advanced-prompts/python/aoai-solution.py)

## Sprawdzenie wiedzy

Dlaczego warto uÅ¼ywaÄ‡ promptingu typu chain-of-thought? PokaÅ¼ 1 poprawnÄ… odpowiedÅº i 2 niepoprawne odpowiedzi.

1. Aby nauczyÄ‡ LLM, jak rozwiÄ…zaÄ‡ problem.
1. B, Aby nauczyÄ‡ LLM znajdowania bÅ‚Ä™dÃ³w w kodzie.
1. C, Aby instruowaÄ‡ LLM, aby wymyÅ›liÅ‚o rÃ³Å¼ne rozwiÄ…zania.

A: 1, poniewaÅ¼ chain-of-thought polega na pokazaniu LLM, jak rozwiÄ…zaÄ‡ problem, dostarczajÄ…c mu seriÄ™ krokÃ³w, podobnych problemÃ³w i sposobÃ³w ich rozwiÄ…zania.

## ğŸš€ Wyzwanie

WÅ‚aÅ›nie uÅ¼yÅ‚eÅ› techniki self-refine w zadaniu. WeÅº dowolny program, ktÃ³ry stworzyÅ‚eÅ›, i zastanÃ³w siÄ™, jakie ulepszenia chciaÅ‚byÅ› wprowadziÄ‡. NastÄ™pnie uÅ¼yj techniki self-refine, aby zastosowaÄ‡ proponowane zmiany. Co sÄ…dzisz o wyniku â€“ lepszy czy gorszy?

## Åšwietna robota! Kontynuuj naukÄ™

Po ukoÅ„czeniu tej lekcji, sprawdÅº naszÄ… [kolekcjÄ™ nauki o generatywnej AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby dalej rozwijaÄ‡ swojÄ… wiedzÄ™ o generatywnej AI!

PrzejdÅº do Lekcji 6, gdzie zastosujemy naszÄ… wiedzÄ™ o inÅ¼ynierii promptÃ³w, [budujÄ…c aplikacje do generowania tekstu](../06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst).

---

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ staramy siÄ™ zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za wiarygodne ÅºrÃ³dÅ‚o. W przypadku informacji krytycznych zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.