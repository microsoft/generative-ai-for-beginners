{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy notatnik został wygenerowany automatycznie przez GitHub Copilot Chat i jest przeznaczony wyłącznie do wstępnej konfiguracji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wprowadzenie do inżynierii promptów\n",
    "Inżynieria promptów to proces projektowania i optymalizacji promptów do zadań związanych z przetwarzaniem języka naturalnego. Obejmuje wybór odpowiednich promptów, dostosowywanie ich parametrów oraz ocenę ich skuteczności. Inżynieria promptów jest kluczowa dla osiągnięcia wysokiej dokładności i wydajności modeli NLP. W tej sekcji przyjrzymy się podstawom inżynierii promptów, korzystając z modeli OpenAI do eksploracji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ćwiczenie 1: Tokenizacja\n",
    "Poznaj tokenizację z użyciem tiktoken, szybkiego otwartoźródłowego tokenizera od OpenAI\n",
    "Zobacz [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb?WT.mc_id=academic-105485-koreyst) po więcej przykładów.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE:\n",
    "# 1. Run the exercise as is first\n",
    "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Define the prompt you want tokenized\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "# Set the model you want encoding for\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Encode the text - gives you the tokens in integer form\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens);\n",
    "\n",
    "# Decode the integers to see what the text versions look like\n",
    "[encoding.decode_single_token_bytes(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ćwiczenie 2: Sprawdź konfigurację klucza API OpenAI\n",
    "\n",
    "Uruchom poniższy kod, aby upewnić się, że Twój endpoint OpenAI został poprawnie skonfigurowany. Kod wykonuje proste zapytanie i sprawdza odpowiedź. Wpisz `oh say can you see`, a odpowiedź powinna być w stylu `by the dawn's early light..`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OpenAI SDK was updated on Nov 8, 2023 with new guidance for migration\n",
    "# See: https://github.com/openai/openai-python/discussions/742\n",
    "\n",
    "## Updated\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\"\n",
    "\n",
    "## Updated\n",
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]       \n",
    "    response = client.chat.completions.create(   \n",
    "        model=deployment,                                         \n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "## ---------- Call the helper method\n",
    "\n",
    "### 1. Set primary content or prompt text\n",
    "text = f\"\"\"\n",
    "oh say can you see\n",
    "\"\"\"\n",
    "\n",
    "### 2. Use that in the prompt template below\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ćwiczenie 3: Zmyślenia\n",
    "Sprawdź, co się dzieje, gdy poprosisz LLM o wygenerowanie odpowiedzi na temat, który może nie istnieć, albo na temat, o którym model może nie mieć wiedzy, bo nie był częścią jego zbioru danych treningowych (np. nowsze informacje). Zobacz, jak zmienia się odpowiedź, jeśli użyjesz innego polecenia lub innego modelu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Set the text for simple prompt or primary content\n",
    "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
    "## Run the completion \n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the Martian War of 2076.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ćwiczenie 4: Oparte na instrukcji\n",
    "Użyj zmiennej \"text\", aby ustawić główną treść,\n",
    "a zmiennej \"prompt\", aby podać instrukcję dotyczącą tej treści.\n",
    "\n",
    "Tutaj prosimy model, aby streścił tekst w sposób zrozumiały dla ucznia drugiej klasy szkoły podstawowej.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Example\n",
    "# https://platform.openai.com/playground/p/default-summarize\n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize content you are provided with for a second-grade student.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ćwiczenie 5: Złożona podpowiedź\n",
    "Spróbuj zapytania, które zawiera wiadomości systemowe, użytkownika i asystenta  \n",
    "System ustawia kontekst asystenta  \n",
    "Wiadomości użytkownika i asystenta tworzą kontekst wieloetapowej rozmowy\n",
    "\n",
    "Zwróć uwagę, że osobowość asystenta została ustawiona na „sarkastyczną” w kontekście systemowym.  \n",
    "Spróbuj użyć innego kontekstu osobowości. Albo wypróbuj inną serię wiadomości wejściowych/wyjściowych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ćwiczenie: Zbadaj swoją intuicję\n",
    "Powyższe przykłady pokazują schematy, które możesz wykorzystać do tworzenia własnych promptów (prostych, złożonych, instrukcji itp.) – spróbuj wymyślić inne ćwiczenia, aby sprawdzić pozostałe pomysły, o których rozmawialiśmy, takie jak przykłady, wskazówki i inne.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Zastrzeżenie**:  \nTen dokument został przetłumaczony przy użyciu usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby zapewnić poprawność tłumaczenia, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być traktowany jako źródło nadrzędne. W przypadku informacji o kluczowym znaczeniu zalecamy skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "coopTranslator": {
   "original_hash": "ec9eedd4f3981f097d4bd34c849cb8b6",
   "translation_date": "2025-08-25T13:42:23+00:00",
   "source_file": "04-prompt-engineering-fundamentals/python/oai-assignment.ipynb",
   "language_code": "pl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}