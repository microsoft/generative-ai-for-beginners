<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "13084c6321a2092841b9a081b29497ba",
  "translation_date": "2025-05-19T09:25:57+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "pl"
}
-->
# Odpowiedzialne Wykorzystanie Generatywnej AI

> _Kliknij powy偶szy obrazek, aby obejrze wideo tej lekcji_

atwo jest zafascynowa si sztuczn inteligencj, a szczeg贸lnie generatywn AI, ale musisz zastanowi si, jak u偶ywa jej odpowiedzialnie. Trzeba rozwa偶y takie rzeczy, jak zapewnienie, 偶e wyniki s sprawiedliwe, nie szkodz i inne. Ten rozdzia ma na celu dostarczenie kontekstu, co nale偶y rozwa偶y i jak podj aktywne kroki, aby poprawi swoje korzystanie z AI.

## Wprowadzenie

Ta lekcja obejmie:

- Dlaczego powiniene priorytetowo traktowa Odpowiedzialn AI przy tworzeniu aplikacji Generatywnej AI.
- G贸wne zasady Odpowiedzialnej AI i jak odnosz si one do Generatywnej AI.
- Jak wdra偶a te zasady Odpowiedzialnej AI w praktyce poprzez strategie i narzdzia.

## Cele nauki

Po ukoczeniu tej lekcji bdziesz wiedzie:

- Jak wa偶na jest Odpowiedzialna AI przy tworzeniu aplikacji Generatywnej AI.
- Kiedy myle i stosowa g贸wne zasady Odpowiedzialnej AI przy tworzeniu aplikacji Generatywnej AI.
- Jakie narzdzia i strategie s dostpne, aby wprowadzi koncepcj Odpowiedzialnej AI w 偶ycie.

## Zasady Odpowiedzialnej AI

Ekscytacja Generatywn AI nigdy nie bya wiksza. Ta ekscytacja przycigna wielu nowych deweloper贸w, uwag i finansowanie do tej przestrzeni. Chocia偶 jest to bardzo pozytywne dla ka偶dego, kto chce budowa produkty i firmy z wykorzystaniem Generatywnej AI, wa偶ne jest, aby postpowa odpowiedzialnie.

W caym tym kursie skupiamy si na budowaniu naszego startupu i naszego produktu edukacyjnego AI. U偶yjemy zasad Odpowiedzialnej AI: Sprawiedliwo, Wczanie, Niezawodno/Bezpieczestwo, Bezpieczestwo i Prywatno, Przejrzysto i Odpowiedzialno. Dziki tym zasadom zbadamy, jak odnosz si one do naszego wykorzystania Generatywnej AI w naszych produktach.

## Dlaczego Powiniene Priorytetowo Traktowa Odpowiedzialn AI

Podczas budowania produktu, podejcie zorientowane na czowieka, kt贸re uwzgldnia najlepsze interesy u偶ytkownika, prowadzi do najlepszych wynik贸w.

Unikalno Generatywnej AI polega na jej mocy tworzenia pomocnych odpowiedzi, informacji, wskaz贸wek i treci dla u偶ytkownik贸w. Mo偶na to zrobi bez wielu rcznych krok贸w, co mo偶e prowadzi do bardzo imponujcych wynik贸w. Bez odpowiedniego planowania i strategii, niestety, mo偶e to r贸wnie偶 prowadzi do szkodliwych wynik贸w dla u偶ytkownik贸w, produktu i caego spoeczestwa.

Przyjrzyjmy si niekt贸rym (cho nie wszystkim) z tych potencjalnie szkodliwych wynik贸w:

### Halucynacje

Halucynacje to termin u偶ywany do opisania sytuacji, gdy LLM generuje tre, kt贸ra jest cakowicie bezsensowna lub co, co wiemy, 偶e jest faktycznie bdne na podstawie innych 藕r贸de informacji.

We藕my na przykad, 偶e budujemy funkcj dla naszego startupu, kt贸ra pozwala studentom zadawa modelowi pytania historyczne. Student zadaje pytanie `Who was the sole survivor of Titanic?`

Model generuje odpowied藕 tak jak poni偶ej:

To jest bardzo pewna siebie i szczeg贸owa odpowied藕. Niestety, jest nieprawidowa. Nawet przy minimalnym badaniu mo偶na by odkry, 偶e wicej ni偶 jedna osoba prze偶ya katastrof Titanica. Dla studenta, kt贸ry dopiero zaczyna bada ten temat, ta odpowied藕 mo偶e by na tyle przekonujca, 偶e nie zostanie zakwestionowana i potraktowana jako fakt. Konsekwencje tego mog prowadzi do tego, 偶e system AI bdzie niewiarygodny i negatywnie wpynie na reputacj naszego startupu.

Z ka偶d iteracj danego LLM widzimy popraw wydajnoci w zakresie minimalizowania halucynacji. Nawet przy tej poprawie, jako tw贸rcy aplikacji i u偶ytkownicy, musimy by wiadomi tych ogranicze.

### Szkodliwe Treci

Om贸wilimy w poprzedniej sekcji, kiedy LLM generuje nieprawidowe lub bezsensowne odpowiedzi. Innym ryzykiem, kt贸re musimy mie na uwadze, jest sytuacja, gdy model odpowiada szkodliwymi treciami.

Szkodliwe treci mo偶na zdefiniowa jako:

- Dostarczanie instrukcji lub zachcanie do samookalecze lub krzywdzenia okrelonych grup.
- Treci nienawistne lub poni偶ajce.
- Kierowanie planowaniem jakiegokolwiek rodzaju ataku lub akt贸w przemocy.
- Dostarczanie instrukcji, jak znale藕 nielegalne treci lub popenia nielegalne czyny.
- Wywietlanie treci o charakterze seksualnym.

Dla naszego startupu chcemy mie pewno, 偶e mamy odpowiednie narzdzia i strategie, aby zapobiec widocznoci tego typu treci przez student贸w.

### Brak Sprawiedliwoci

Sprawiedliwo definiuje si jako "zapewnienie, 偶e system AI jest wolny od uprzedze i dyskryminacji oraz 偶e traktuje wszystkich sprawiedliwie i r贸wno". W wiecie Generatywnej AI chcemy upewni si, 偶e wykluczajce wiatopogldy marginalizowanych grup nie s wzmacniane przez output modelu.

Tego typu wyniki nie tylko niszcz pozytywne dowiadczenia produktowe dla naszych u偶ytkownik贸w, ale tak偶e powoduj dalsze szkody spoeczne. Jako tw贸rcy aplikacji powinnimy zawsze mie na uwadze szerok i zr贸偶nicowan baz u偶ytkownik贸w podczas budowania rozwiza z Generatywn AI.

## Jak Odpowiedzialnie Wykorzystywa Generatywn AI

Teraz, gdy zidentyfikowalimy znaczenie Odpowiedzialnej Generatywnej AI, przyjrzyjmy si 4 krokom, kt贸re mo偶emy podj, aby odpowiedzialnie budowa nasze rozwizania AI:

### Mierzenie Potencjalnych Szk贸d

W testowaniu oprogramowania testujemy oczekiwane dziaania u偶ytkownika na aplikacji. Podobnie, testowanie r贸偶norodnego zestawu podpowiedzi, kt贸rych u偶ytkownicy najprawdopodobniej u偶yj, jest dobrym sposobem na zmierzenie potencjalnych szk贸d.

Poniewa偶 nasz startup buduje produkt edukacyjny, dobrze byoby przygotowa list podpowiedzi zwizanych z edukacj. Mogoby to obejmowa pewien przedmiot, fakty historyczne i podpowiedzi dotyczce 偶ycia studenckiego.

### Ograniczanie Potencjalnych Szk贸d

Teraz czas znale藕 sposoby, w jakie mo偶emy zapobiec lub ograniczy potencjalne szkody spowodowane przez model i jego odpowiedzi. Mo偶emy na to spojrze w 4 r贸偶nych warstwach:

- **Model**. Wyb贸r odpowiedniego modelu do odpowiedniego przypadku u偶ycia. Wiksze i bardziej zo偶one modele, takie jak GPT-4, mog powodowa wiksze ryzyko szkodliwych treci, gdy s stosowane do mniejszych i bardziej specyficznych przypadk贸w u偶ycia. U偶ywanie danych treningowych do dopasowania r贸wnie偶 zmniejsza ryzyko szkodliwych treci.

- **System Bezpieczestwa**. System bezpieczestwa to zestaw narzdzi i konfiguracji na platformie obsugujcej model, kt贸re pomagaj ogranicza szkody. Przykadem tego jest system filtrowania treci w usudze Azure OpenAI. Systemy powinny r贸wnie偶 wykrywa ataki jailbreak i niepo偶dane dziaania, takie jak 偶dania od bot贸w.

- **Metaprompt**. Metaprompt i uzasadnienie to sposoby, w jakie mo偶emy kierowa lub ogranicza model w oparciu o pewne zachowania i informacje. Mogoby to polega na u偶ywaniu danych wejciowych systemu do okrelenia pewnych ogranicze modelu. Dodatkowo, dostarczanie wynik贸w bardziej zwizanych z zakresem lub dziedzin systemu.

Mo偶e to r贸wnie偶 polega na u偶ywaniu technik takich jak Retrieval Augmented Generation (RAG), aby model pobiera informacje tylko z wybranych zaufanych 藕r贸de. Jest p贸藕niej w tym kursie lekcja dotyczca [budowania aplikacji wyszukiwania](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Dowiadczenie U偶ytkownika**. Ostatnia warstwa to miejsce, gdzie u偶ytkownik wchodzi w bezporedni interakcj z modelem poprzez interfejs naszej aplikacji w jaki spos贸b. W ten spos贸b mo偶emy zaprojektowa UI/UX, aby ograniczy u偶ytkownika w typach danych wejciowych, kt贸re mo偶e wysa do modelu, a tak偶e tekst贸w lub obraz贸w wywietlanych u偶ytkownikowi. Podczas wdra偶ania aplikacji AI musimy r贸wnie偶 by przejrzystymi na temat tego, co nasza aplikacja Generatywnej AI mo偶e i czego nie mo偶e zrobi.

Mamy ca lekcj powicon [Projektowaniu UX dla Aplikacji AI](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Ewaluacja modelu**. Praca z LLM mo偶e by wyzwaniem, poniewa偶 nie zawsze mamy kontrol nad danymi, na kt贸rych model by trenowany. Niemniej jednak, zawsze powinnimy ocenia wydajno i wyniki modelu. Wci偶 wa偶ne jest mierzenie dokadnoci modelu, podobiestwa, uzasadnienia i trafnoci wyniku. To pomaga zapewni przejrzysto i zaufanie dla interesariuszy i u偶ytkownik贸w.

### Operowanie Odpowiedzialnym Rozwizaniem Generatywnej AI

Budowanie praktyki operacyjnej wok贸 aplikacji AI to ostatni etap. Obejmuje to wsp贸prac z innymi czciami naszego startupu, takimi jak Dzia Prawny i Bezpieczestwa, aby upewni si, 偶e jestemy zgodni z wszystkimi politykami regulacyjnymi. Przed uruchomieniem chcemy r贸wnie偶 budowa plany dotyczce dostarczania, obsugi incydent贸w i wycofywania, aby zapobiec jakimkolwiek szkodom dla naszych u偶ytkownik贸w.

## Narzdzia

Chocia偶 praca nad opracowaniem Odpowiedzialnych Rozwiza AI mo偶e wydawa si trudna, jest to praca warta wysiku. W miar jak obszar Generatywnej AI ronie, wicej narzdzi pomagajcych deweloperom efektywnie integrowa odpowiedzialno w ich przepywy pracy bdzie si rozwija. Na przykad, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) mo偶e pom贸c w wykrywaniu szkodliwych treci i obraz贸w za pomoc 偶dania API.

## Sprawdzenie wiedzy

Na co musisz zwr贸ci uwag, aby zapewni odpowiedzialne u偶ycie AI?

1. 呕e odpowied藕 jest poprawna.
2. Szkodliwe u偶ycie, 偶e AI nie jest wykorzystywana do cel贸w przestpczych.
3. Zapewnienie, 偶e AI jest wolna od uprzedze i dyskryminacji.

Odpowied藕: 2 i 3 s poprawne. Odpowiedzialna AI pomaga rozwa偶y, jak agodzi szkodliwe skutki i uprzedzenia oraz wicej.

##  Wyzwanie

Przeczytaj o [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) i zobacz, co mo偶esz zaadaptowa do swojego u偶ycia.

## wietna Robota, Kontynuuj Nauczanie

Po ukoczeniu tej lekcji, sprawd藕 nasz [Kolekcj Nauki o Generatywnej AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby kontynuowa podnoszenie swojej wiedzy na temat Generatywnej AI!

Przejd藕 do Lekcji 4, gdzie przyjrzymy si [Podstawom In偶ynierii Podpowiedzi](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

**Zastrze偶enie**:  
Ten dokument zosta przetumaczony za pomoc usugi tumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chocia偶 staramy si o dokadno, prosimy mie na uwadze, 偶e automatyczne tumaczenia mog zawiera bdy lub niecisoci. Oryginalny dokument w jego rodzimym jzyku powinien by uznawany za autorytatywne 藕r贸do. W przypadku istotnych informacji zaleca si profesjonalne tumaczenie przez czowieka. Nie ponosimy odpowiedzialnoci za jakiekolwiek nieporozumienia lub bdne interpretacje wynikajce z u偶ycia tego tumaczenia.