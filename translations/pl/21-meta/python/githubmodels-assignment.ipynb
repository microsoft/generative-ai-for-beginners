{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Budowanie z modelami rodziny Meta\n",
    "\n",
    "## Wprowadzenie\n",
    "\n",
    "W tej lekcji omówimy:\n",
    "\n",
    "- Poznanie dwóch głównych modeli z rodziny Meta – Llama 3.1 i Llama 3.2\n",
    "- Zrozumienie zastosowań i scenariuszy dla każdego z modeli\n",
    "- Przykład kodu pokazujący unikalne cechy każdego modelu\n",
    "\n",
    "## Rodzina modeli Meta\n",
    "\n",
    "W tej lekcji przyjrzymy się dwóm modelom z rodziny Meta, czyli „Llama Herd” – Llama 3.1 i Llama 3.2\n",
    "\n",
    "Modele te występują w różnych wariantach i są dostępne na rynku modeli Github. Więcej informacji o korzystaniu z modeli Github do [prototypowania z modelami AI](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Warianty modeli:\n",
    "- Llama 3.1 – 70B Instruct\n",
    "- Llama 3.1 – 405B Instruct\n",
    "- Llama 3.2 – 11B Vision Instruct\n",
    "- Llama 3.2 – 90B Vision Instruct\n",
    "\n",
    "*Uwaga: Llama 3 jest również dostępny na Github Models, ale nie będzie omawiany w tej lekcji*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Dzięki 405 miliardom parametrów, Llama 3.1 zalicza się do kategorii otwartoźródłowych modeli LLM.\n",
    "\n",
    "Ten model to ulepszona wersja wcześniejszego Llama 3, oferująca:\n",
    "\n",
    "- Większe okno kontekstu – 128 tys. tokenów zamiast 8 tys.\n",
    "- Większa maksymalna liczba tokenów wyjściowych – 4096 zamiast 2048\n",
    "- Lepsze wsparcie dla wielu języków – dzięki zwiększonej liczbie tokenów treningowych\n",
    "\n",
    "Te usprawnienia pozwalają Llama 3.1 obsługiwać bardziej złożone przypadki użycia podczas budowania aplikacji GenAI, w tym:\n",
    "- Natywne wywoływanie funkcji – możliwość korzystania z zewnętrznych narzędzi i funkcji poza standardowym przepływem LLM\n",
    "- Lepsza wydajność RAG – dzięki większemu oknu kontekstu\n",
    "- Generowanie syntetycznych danych – możliwość tworzenia skutecznych danych do zadań takich jak dostrajanie modelu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wywoływanie funkcji natywnych\n",
    "\n",
    "Llama 3.1 została dostrojona, aby skuteczniej wykonywać wywołania funkcji lub narzędzi. Model posiada także dwa wbudowane narzędzia, które potrafi samodzielnie rozpoznać jako potrzebne na podstawie polecenia użytkownika. Są to:\n",
    "\n",
    "- **Brave Search** – Może być używane do uzyskiwania aktualnych informacji, takich jak pogoda, poprzez wyszukiwanie w internecie\n",
    "- **Wolfram Alpha** – Może służyć do bardziej złożonych obliczeń matematycznych, dzięki czemu nie trzeba pisać własnych funkcji.\n",
    "\n",
    "Możesz również tworzyć własne, niestandardowe narzędzia, które LLM będzie mógł wywoływać.\n",
    "\n",
    "W poniższym przykładzie kodu:\n",
    "\n",
    "- Definiujemy dostępne narzędzia (brave_search, wolfram_alpha) w poleceniu systemowym.\n",
    "- Wysyłamy polecenie użytkownika z pytaniem o pogodę w konkretnym mieście.\n",
    "- LLM odpowie wywołaniem narzędzia Brave Search, które będzie wyglądać tak: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Uwaga: Ten przykład wykonuje jedynie wywołanie narzędzia. Jeśli chcesz uzyskać wyniki, musisz założyć darmowe konto na stronie Brave API i samodzielnie zdefiniować funkcję.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Mimo że Llama 3.1 jest dużym modelem językowym, jej ograniczeniem jest brak obsługi multimodalności. Chodzi o możliwość korzystania z różnych typów danych wejściowych, takich jak obrazy, jako podpowiedzi i generowanie odpowiedzi. Ta funkcja jest jednym z głównych atutów Llama 3.2. Do nowych możliwości należą także:\n",
    "\n",
    "- Multimodalność – potrafi analizować zarówno tekst, jak i obrazy jako podpowiedzi\n",
    "- Wersje o małych i średnich rozmiarach (11B i 90B) – zapewniają elastyczne opcje wdrożenia,\n",
    "- Wersje tylko tekstowe (1B i 3B) – pozwalają na wdrożenie modelu na urządzeniach brzegowych lub mobilnych i gwarantują niskie opóźnienia\n",
    "\n",
    "Obsługa multimodalności to duży krok naprzód w świecie otwartych modeli. Przykład kodu poniżej pokazuje, jak można użyć zarówno obrazu, jak i tekstu jako podpowiedzi, aby uzyskać analizę obrazu z Llama 3.2 90B.\n",
    "\n",
    "### Obsługa multimodalności w Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nauka nie kończy się tutaj, kontynuuj swoją podróż\n",
    "\n",
    "Po ukończeniu tej lekcji zajrzyj do naszej [kolekcji materiałów o Generative AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby dalej rozwijać swoją wiedzę na temat Generative AI!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Zastrzeżenie**:  \nTen dokument został przetłumaczony przy użyciu usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było poprawne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być traktowany jako źródło nadrzędne. W przypadku informacji o kluczowym znaczeniu zalecamy skorzystanie z profesjonalnych usług tłumaczeniowych. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:42:35+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "pl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}