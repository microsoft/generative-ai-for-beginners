{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Rozdział 7: Tworzenie aplikacji czatowych\n",
    "## Szybki start z Github Models API\n",
    "\n",
    "Ten notatnik został zaadaptowany z [Repozytorium Przykładów Azure OpenAI](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst), które zawiera notatniki umożliwiające dostęp do usług [Azure OpenAI](notebook-azure-openai.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Przegląd  \n",
    "\"Duże modele językowe to funkcje, które zamieniają tekst na tekst. Po otrzymaniu wejściowego ciągu tekstu, duży model językowy stara się przewidzieć, jaki tekst pojawi się dalej\"(1). Ten notatnik \"szybki start\" wprowadzi użytkowników w podstawowe pojęcia związane z LLM, kluczowe wymagania pakietów potrzebnych do rozpoczęcia pracy z AML, da łagodne wprowadzenie do projektowania promptów oraz pokaże kilka krótkich przykładów różnych zastosowań.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Spis treści  \n",
    "\n",
    "[Przegląd](../../../../07-building-chat-applications/python)  \n",
    "[Jak korzystać z usługi OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[1. Tworzenie własnej usługi OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[2. Instalacja](../../../../07-building-chat-applications/python)    \n",
    "[3. Dane uwierzytelniające](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Przykłady użycia](../../../../07-building-chat-applications/python)    \n",
    "[1. Podsumowanie tekstu](../../../../07-building-chat-applications/python)  \n",
    "[2. Klasyfikacja tekstu](../../../../07-building-chat-applications/python)  \n",
    "[3. Generowanie nowych nazw produktów](../../../../07-building-chat-applications/python)  \n",
    "[4. Dostosowanie klasyfikatora](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Odnośniki](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Zbuduj swój pierwszy prompt  \n",
    "To krótkie ćwiczenie wprowadzi Cię w podstawy wysyłania promptów do modelu w Github Models na przykładzie prostego zadania \"podsumowanie\".\n",
    "\n",
    "\n",
    "**Kroki**:  \n",
    "1. Zainstaluj bibliotekę `azure-ai-inference` w swoim środowisku Pythona, jeśli jeszcze tego nie zrobiłeś.  \n",
    "2. Załaduj standardowe biblioteki pomocnicze i skonfiguruj poświadczenia do Github Models.  \n",
    "3. Wybierz model odpowiedni do Twojego zadania  \n",
    "4. Stwórz prosty prompt dla modelu  \n",
    "5. Wyślij swoje zapytanie do API modelu!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Zainstaluj `azure-ai-inference`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Wybór odpowiedniego modelu  \n",
    "Modele GPT-3.5-turbo lub GPT-4 potrafią rozumieć i generować naturalny język.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Projektowanie promptów  \n",
    "\n",
    "„Magia dużych modeli językowych polega na tym, że poprzez uczenie się minimalizowania błędów przewidywania na ogromnych ilościach tekstu, modele te ostatecznie przyswajają sobie pojęcia przydatne do tych przewidywań. Na przykład uczą się takich rzeczy jak”(1):\n",
    "\n",
    "* jak poprawnie pisać\n",
    "* jak działa gramatyka\n",
    "* jak parafrazować\n",
    "* jak odpowiadać na pytania\n",
    "* jak prowadzić rozmowę\n",
    "* jak pisać w wielu językach\n",
    "* jak programować\n",
    "* itd.\n",
    "\n",
    "#### Jak sterować dużym modelem językowym  \n",
    "„Spośród wszystkich danych wejściowych do dużego modelu językowego, zdecydowanie największy wpływ ma tekstowy prompt”(1).\n",
    "\n",
    "Duże modele językowe można nakłonić do generowania odpowiedzi na kilka sposobów:\n",
    "\n",
    "Instrukcja: Powiedz modelowi, czego oczekujesz  \n",
    "Uzupełnienie: Skłoń model do dokończenia rozpoczętego tekstu  \n",
    "Demonstracja: Pokaż modelowi, czego chcesz, poprzez:  \n",
    "Kilka przykładów w promptcie  \n",
    "Setki lub tysiące przykładów w zbiorze danych do fine-tuningu\n",
    "\n",
    "#### Istnieją trzy podstawowe zasady tworzenia promptów:\n",
    "\n",
    "**Pokaż i powiedz**. Jasno określ, czego oczekujesz – poprzez instrukcje, przykłady lub ich połączenie. Jeśli chcesz, by model posortował listę alfabetycznie lub sklasyfikował akapit pod względem sentymentu, pokaż mu, o co chodzi.\n",
    "\n",
    "**Dostarcz dobre dane**. Jeśli chcesz zbudować klasyfikator lub nauczyć model podążać za wzorcem, upewnij się, że masz wystarczająco dużo przykładów. Sprawdź dokładnie swoje przykłady – model zwykle jest na tyle „inteligentny”, by przeoczyć drobne błędy ortograficzne i udzielić odpowiedzi, ale może też uznać je za celowe, co wpłynie na rezultat.\n",
    "\n",
    "**Sprawdź ustawienia.** Parametry temperature i top_p decydują o tym, jak bardzo model jest zdeterminowany w generowaniu odpowiedzi. Jeśli oczekujesz jednej poprawnej odpowiedzi, ustaw je niżej. Jeśli zależy Ci na większej różnorodności odpowiedzi, możesz je podnieść. Najczęstszy błąd to mylenie tych ustawień z „inteligencją” lub „kreatywnością” modelu.\n",
    "\n",
    "Źródło: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Podsumuj tekst  \n",
    "#### Wyzwanie  \n",
    "Podsumuj tekst, dodając na końcu fragmentu „tl;dr:”. Zwróć uwagę, jak model potrafi wykonać wiele zadań bez dodatkowych instrukcji. Możesz eksperymentować z bardziej opisowymi poleceniami niż tl;dr, aby zmienić zachowanie modelu i dostosować otrzymywane podsumowania(3).  \n",
    "\n",
    "Najnowsze badania wykazały znaczne postępy w wielu zadaniach i benchmarkach NLP dzięki wstępnemu trenowaniu na dużych zbiorach tekstów, a następnie dostrajaniu pod konkretne zadanie. Chociaż architektura jest zazwyczaj niezależna od zadania, ta metoda nadal wymaga specjalnych zbiorów danych do dostrajania, liczących tysiące lub dziesiątki tysięcy przykładów. Dla porównania, ludzie potrafią zazwyczaj wykonać nowe zadanie językowe na podstawie zaledwie kilku przykładów lub prostych instrukcji – z czym obecne systemy NLP wciąż mają trudności. Pokazujemy tutaj, że zwiększanie rozmiaru modeli językowych znacznie poprawia ich uniwersalność i skuteczność w zadaniach z niewielką liczbą przykładów, czasem osiągając poziom konkurencyjny wobec wcześniejszych, zaawansowanych metod dostrajania. \n",
    "\n",
    "\n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Ćwiczenia dla różnych zastosowań  \n",
    "1. Podsumuj tekst  \n",
    "2. Sklasyfikuj tekst  \n",
    "3. Wymyśl nowe nazwy produktów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Klasyfikacja tekstu  \n",
    "#### Wyzwanie  \n",
    "Przypisz elementy do kategorii podanych w czasie wnioskowania. W poniższym przykładzie zarówno kategorie, jak i tekst do sklasyfikowania, są podane w poleceniu (*playground_reference).\n",
    "\n",
    "Zapytanie klienta: Dzień dobry, jeden z klawiszy na mojej klawiaturze laptopa niedawno się zepsuł i będę potrzebować zamiennika:\n",
    "\n",
    "Przypisana kategoria:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Generuj nowe nazwy produktów\n",
    "#### Wyzwanie\n",
    "Stwórz nazwy produktów na podstawie przykładowych słów. W poleceniu zawarliśmy informacje o produkcie, dla którego generujemy nazwy. Podajemy też podobny przykład, aby pokazać wzór, jaki chcemy uzyskać. Ustawiliśmy również wysoką wartość temperatury, aby zwiększyć losowość i uzyskać bardziej innowacyjne odpowiedzi.\n",
    "\n",
    "Opis produktu: Domowy shaker do robienia koktajli mlecznych  \n",
    "Słowa kluczowe: szybki, zdrowy, kompaktowy.  \n",
    "Nazwy produktów: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Opis produktu: Para butów, które dopasowują się do każdego rozmiaru stopy.  \n",
    "Słowa kluczowe: adaptacyjny, dopasowanie, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Odnośniki  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Przykłady OpenAI Studio](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Najlepsze praktyki dostrajania GPT-3 do klasyfikacji tekstu](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Po więcej pomocy  \n",
    "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Współtwórcy\n",
    "* [Chew-Yean Yam](https://www.linkedin.com/in/cyyam/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Zastrzeżenie**:  \nTen dokument został przetłumaczony przy użyciu usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby zapewnić dokładność, należy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za źródło wiążące. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "3eeb8e5cad61b52a8366f6259a53ed49",
   "translation_date": "2025-08-25T17:35:58+00:00",
   "source_file": "07-building-chat-applications/python/githubmodels-assignment.ipynb",
   "language_code": "pl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}