{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Rozdział 7: Tworzenie aplikacji czatujących\n",
    "## Szybki start z API OpenAI\n",
    "\n",
    "Ten notatnik został zaadaptowany z [Repozytorium Przykładów Azure OpenAI](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst), które zawiera notatniki korzystające z usług [Azure OpenAI](notebook-azure-openai.ipynb).\n",
    "\n",
    "Pythonowe API OpenAI działa również z modelami Azure OpenAI, z kilkoma drobnymi zmianami. Więcej o różnicach przeczytasz tutaj: [Jak przełączać się między punktami końcowymi OpenAI i Azure OpenAI w Pythonie](https://learn.microsoft.com/azure/ai-services/openai/how-to/switching-endpoints?WT.mc_id=academic-109527-jasmineg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Przegląd  \n",
    "\"Duże modele językowe to funkcje, które zamieniają tekst na tekst. Po otrzymaniu wejściowego ciągu tekstu, duży model językowy stara się przewidzieć, jaki tekst pojawi się dalej\"(1). Ten notatnik \"szybki start\" wprowadzi użytkowników w podstawowe pojęcia związane z LLM, kluczowe wymagania pakietów potrzebnych do rozpoczęcia pracy z AML, da łagodne wprowadzenie do projektowania promptów oraz pokaże kilka krótkich przykładów różnych zastosowań.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Spis treści  \n",
    "\n",
    "[Przegląd](../../../../07-building-chat-applications/python)  \n",
    "[Jak korzystać z usługi OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[1. Tworzenie własnej usługi OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[2. Instalacja](../../../../07-building-chat-applications/python)    \n",
    "[3. Dane uwierzytelniające](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Przykłady użycia](../../../../07-building-chat-applications/python)    \n",
    "[1. Podsumowanie tekstu](../../../../07-building-chat-applications/python)  \n",
    "[2. Klasyfikacja tekstu](../../../../07-building-chat-applications/python)  \n",
    "[3. Generowanie nowych nazw produktów](../../../../07-building-chat-applications/python)  \n",
    "[4. Dostosowanie klasyfikatora](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Odnośniki](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Stwórz swój pierwszy prompt  \n",
    "To krótkie ćwiczenie wprowadzi Cię w podstawy wysyłania promptów do modelu OpenAI na przykładzie prostego zadania „podsumowanie”.\n",
    "\n",
    "\n",
    "**Kroki**:  \n",
    "1. Zainstaluj bibliotekę OpenAI w swoim środowisku python  \n",
    "2. Załaduj standardowe biblioteki pomocnicze i ustaw typowe dane uwierzytelniające OpenAI dla utworzonej przez Ciebie usługi OpenAI  \n",
    "3. Wybierz model do swojego zadania  \n",
    "4. Stwórz prosty prompt dla modelu  \n",
    "5. Wyślij swoje zapytanie do API modelu!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\")\n",
    "assert API_KEY, \"ERROR: OpenAI Key is missing\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Wybór odpowiedniego modelu  \n",
    "Modele GPT-3.5-turbo lub GPT-4 potrafią rozumieć i generować naturalny język.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Projektowanie promptów  \n",
    "\n",
    "„Magia dużych modeli językowych polega na tym, że poprzez trenowanie na minimalizowanie błędów przewidywania na ogromnych ilościach tekstu, modele te uczą się pojęć przydatnych do tych przewidywań. Na przykład uczą się takich rzeczy jak”(1):\n",
    "\n",
    "* jak poprawnie pisać\n",
    "* jak działa gramatyka\n",
    "* jak parafrazować\n",
    "* jak odpowiadać na pytania\n",
    "* jak prowadzić rozmowę\n",
    "* jak pisać w wielu językach\n",
    "* jak programować\n",
    "* itd.\n",
    "\n",
    "#### Jak sterować dużym modelem językowym  \n",
    "„Spośród wszystkich danych wejściowych do dużego modelu językowego, zdecydowanie największy wpływ ma tekstowy prompt”(1).\n",
    "\n",
    "Duże modele językowe można nakłonić do generowania odpowiedzi na kilka sposobów:\n",
    "\n",
    "Instrukcja: Powiedz modelowi, czego oczekujesz\n",
    "Uzupełnienie: Skłonienie modelu do dokończenia rozpoczętego tekstu\n",
    "Demonstracja: Pokaż modelowi, czego chcesz, poprzez:\n",
    "Kilka przykładów w promptcie\n",
    "Setki lub tysiące przykładów w zbiorze danych do fine-tuningu\n",
    "\n",
    "\n",
    "\n",
    "#### Istnieją trzy podstawowe zasady tworzenia promptów:\n",
    "\n",
    "**Pokaż i powiedz**. Wyraźnie określ, czego oczekujesz — poprzez instrukcje, przykłady lub ich kombinację. Jeśli chcesz, by model uporządkował listę według alfabetu lub sklasyfikował akapit pod względem sentymentu, pokaż mu, że właśnie o to chodzi.\n",
    "\n",
    "**Dostarcz dobre dane**. Jeśli próbujesz zbudować klasyfikator lub chcesz, by model podążał za określonym wzorcem, upewnij się, że masz wystarczająco dużo przykładów. Sprawdź swoje przykłady — model zazwyczaj jest na tyle „inteligentny”, by przeoczyć drobne błędy ortograficzne i wygenerować odpowiedź, ale może też uznać, że to celowe i wpłynie to na rezultat.\n",
    "\n",
    "**Sprawdź ustawienia.** Parametry temperature i top_p kontrolują, jak bardzo model jest deterministyczny podczas generowania odpowiedzi. Jeśli oczekujesz odpowiedzi, gdzie jest tylko jedna poprawna, ustaw te wartości niżej. Jeśli zależy Ci na bardziej zróżnicowanych odpowiedziach, możesz ustawić je wyżej. Najczęstszy błąd to mylenie tych ustawień z kontrolą „inteligencji” lub „kreatywności” modelu.\n",
    "\n",
    "\n",
    "Źródło: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Podsumuj tekst  \n",
    "#### Wyzwanie  \n",
    "Podsumuj tekst, dodając na końcu fragmentu „tl;dr:”. Zwróć uwagę, jak model potrafi wykonać wiele zadań bez dodatkowych instrukcji. Możesz eksperymentować z bardziej opisowymi poleceniami niż tl;dr, aby zmienić zachowanie modelu i dostosować otrzymywane podsumowania(3).  \n",
    "\n",
    "Najnowsze badania wykazały znaczne postępy w wielu zadaniach i benchmarkach NLP dzięki wstępnemu trenowaniu na dużym korpusie tekstów, a następnie dostrajaniu pod konkretne zadanie. Chociaż architektura jest zazwyczaj niezależna od zadania, ta metoda nadal wymaga specjalnych zbiorów danych do dostrajania, liczących tysiące lub dziesiątki tysięcy przykładów. Dla porównania, ludzie potrafią zazwyczaj wykonać nowe zadanie językowe na podstawie zaledwie kilku przykładów lub prostych instrukcji – z czym obecne systemy NLP wciąż mają trudności. Pokazujemy tutaj, że zwiększanie rozmiaru modeli językowych znacznie poprawia ich uniwersalność i skuteczność w zadaniach z niewielką liczbą przykładów, czasem osiągając poziom konkurencyjny wobec wcześniejszych, zaawansowanych metod dostrajania. \n",
    "\n",
    "\n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Ćwiczenia dla różnych zastosowań  \n",
    "1. Podsumuj tekst  \n",
    "2. Sklasyfikuj tekst  \n",
    "3. Wymyśl nowe nazwy produktów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Klasyfikacja tekstu  \n",
    "#### Wyzwanie  \n",
    "Przypisz elementy do kategorii podanych w czasie wnioskowania. W poniższym przykładzie zarówno kategorie, jak i tekst do sklasyfikowania, są podane w poleceniu (*playground_reference).\n",
    "\n",
    "Zapytanie klienta: Dzień dobry, jeden z klawiszy na mojej klawiaturze laptopa niedawno się zepsuł i będę potrzebować zamiennika:\n",
    "\n",
    "Przypisana kategoria:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Generuj nowe nazwy produktów\n",
    "#### Wyzwanie\n",
    "Stwórz nazwy produktów na podstawie przykładowych słów. W poleceniu zawarliśmy informacje o produkcie, dla którego generujemy nazwy. Podajemy także podobny przykład, aby pokazać wzór, jaki chcemy uzyskać. Ustawiliśmy również wysoką wartość temperatury, aby zwiększyć losowość i uzyskać bardziej innowacyjne odpowiedzi.\n",
    "\n",
    "Opis produktu: Domowy shaker do robienia koktajli mlecznych  \n",
    "Słowa kluczowe: szybki, zdrowy, kompaktowy.  \n",
    "Nazwy produktów: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Opis produktu: Para butów, które dopasowują się do każdego rozmiaru stopy.  \n",
    "Słowa kluczowe: adaptacyjne, dopasowanie, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Źródła  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Przykłady z OpenAI Studio](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Najlepsze praktyki dostrajania GPT-3 do klasyfikacji tekstu](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Po więcej pomocy  \n",
    "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Współtwórcy\n",
    "* Louis Li\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Zastrzeżenie**:  \nTen dokument został przetłumaczony przy użyciu usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby zapewnić poprawność tłumaczenia, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za źródło nadrzędne. W przypadku informacji o kluczowym znaczeniu zalecamy skorzystanie z profesjonalnych usług tłumaczeniowych. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "1854bdde1dd56b366f1f6c9122f7d304",
   "translation_date": "2025-08-25T18:15:14+00:00",
   "source_file": "07-building-chat-applications/python/oai-assignment.ipynb",
   "language_code": "pl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}