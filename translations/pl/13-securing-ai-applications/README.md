<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-18T00:52:02+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "pl"
}
-->
# Zabezpieczanie aplikacji generatywnej AI

[![Zabezpieczanie aplikacji generatywnej AI](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.pl.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Wprowadzenie

W tej lekcji omÃ³wimy:

- BezpieczeÅ„stwo w kontekÅ›cie systemÃ³w AI.
- Typowe ryzyka i zagroÅ¼enia dla systemÃ³w AI.
- Metody i aspekty zabezpieczania systemÃ³w AI.

## Cele nauki

Po ukoÅ„czeniu tej lekcji bÄ™dziesz rozumieÄ‡:

- ZagroÅ¼enia i ryzyka zwiÄ…zane z systemami AI.
- Typowe metody i praktyki zabezpieczania systemÃ³w AI.
- Jak testowanie bezpieczeÅ„stwa moÅ¼e zapobiec nieoczekiwanym rezultatom i utracie zaufania uÅ¼ytkownikÃ³w.

## Co oznacza bezpieczeÅ„stwo w kontekÅ›cie generatywnej AI?

W miarÄ™ jak technologie sztucznej inteligencji (AI) i uczenia maszynowego (ML) coraz bardziej wpÅ‚ywajÄ… na nasze Å¼ycie, kluczowe staje siÄ™ zabezpieczenie nie tylko danych klientÃ³w, ale takÅ¼e samych systemÃ³w AI. AI/ML sÄ… coraz czÄ™Å›ciej wykorzystywane w procesach podejmowania decyzji o wysokiej wartoÅ›ci w branÅ¼ach, gdzie bÅ‚Ä™dne decyzje mogÄ… prowadziÄ‡ do powaÅ¼nych konsekwencji.

Oto kluczowe kwestie, ktÃ³re warto rozwaÅ¼yÄ‡:

- **WpÅ‚yw AI/ML**: AI/ML majÄ… znaczÄ…cy wpÅ‚yw na codzienne Å¼ycie, dlatego ich ochrona staÅ‚a siÄ™ niezbÄ™dna.
- **Wyzwania zwiÄ…zane z bezpieczeÅ„stwem**: WpÅ‚yw AI/ML wymaga odpowiedniej uwagi, aby chroniÄ‡ produkty oparte na AI przed zaawansowanymi atakami, zarÃ³wno ze strony trolli, jak i zorganizowanych grup.
- **Problemy strategiczne**: BranÅ¼a technologiczna musi proaktywnie stawiÄ‡ czoÅ‚a strategicznym wyzwaniom, aby zapewniÄ‡ dÅ‚ugoterminowe bezpieczeÅ„stwo klientÃ³w i danych.

Dodatkowo modele uczenia maszynowego w duÅ¼ej mierze nie sÄ… w stanie odrÃ³Å¼niÄ‡ zÅ‚oÅ›liwego wejÅ›cia od nieszkodliwych danych anomalii. Znaczna czÄ™Å›Ä‡ danych treningowych pochodzi z niekatalogowanych, niemoderowanych, publicznych zbiorÃ³w danych, ktÃ³re sÄ… otwarte na wkÅ‚ad osÃ³b trzecich. AtakujÄ…cy nie muszÄ… przejmowaÄ‡ zbiorÃ³w danych, jeÅ›li mogÄ… swobodnie je modyfikowaÄ‡. Z czasem dane zÅ‚oÅ›liwe o niskiej wiarygodnoÅ›ci stajÄ… siÄ™ danymi zaufanymi o wysokiej wiarygodnoÅ›ci, jeÅ›li ich struktura/format pozostaje poprawny.

Dlatego tak waÅ¼ne jest zapewnienie integralnoÅ›ci i ochrony zbiorÃ³w danych, ktÃ³re Twoje modele wykorzystujÄ… do podejmowania decyzji.

## Zrozumienie zagroÅ¼eÅ„ i ryzyk zwiÄ…zanych z AI

W kontekÅ›cie AI i powiÄ…zanych systemÃ³w, zatruwanie danych jest obecnie najpowaÅ¼niejszym zagroÅ¼eniem dla bezpieczeÅ„stwa. Zatruwanie danych polega na celowym modyfikowaniu informacji uÅ¼ywanych do trenowania AI, co prowadzi do popeÅ‚niania bÅ‚Ä™dÃ³w. Wynika to z braku standardowych metod wykrywania i przeciwdziaÅ‚ania, w poÅ‚Ä…czeniu z poleganiem na niezweryfikowanych lub niekatalogowanych publicznych zbiorach danych do trenowania. Aby zachowaÄ‡ integralnoÅ›Ä‡ danych i zapobiec wadliwemu procesowi treningowemu, kluczowe jest Å›ledzenie pochodzenia i historii danych. W przeciwnym razie sprawdza siÄ™ stare powiedzenie â€Å›mieci na wejÅ›ciu, Å›mieci na wyjÅ›ciuâ€, co prowadzi do obniÅ¼enia wydajnoÅ›ci modelu.

Oto przykÅ‚ady, jak zatruwanie danych moÅ¼e wpÅ‚ynÄ…Ä‡ na Twoje modele:

1. **Odwracanie etykiet**: W zadaniu klasyfikacji binarnej przeciwnik celowo odwraca etykiety w niewielkiej czÄ™Å›ci danych treningowych. Na przykÅ‚ad prÃ³bki nieszkodliwe sÄ… oznaczane jako zÅ‚oÅ›liwe, co prowadzi do tego, Å¼e model uczy siÄ™ bÅ‚Ä™dnych skojarzeÅ„.\
   **PrzykÅ‚ad**: Filtr antyspamowy bÅ‚Ä™dnie klasyfikujÄ…cy legalne e-maile jako spam z powodu zmanipulowanych etykiet.
2. **Zatruwanie cech**: AtakujÄ…cy subtelnie modyfikuje cechy w danych treningowych, aby wprowadziÄ‡ uprzedzenia lub wprowadziÄ‡ model w bÅ‚Ä…d.\
   **PrzykÅ‚ad**: Dodawanie nieistotnych sÅ‚Ã³w kluczowych do opisÃ³w produktÃ³w w celu manipulowania systemami rekomendacji.
3. **Wstrzykiwanie danych**: Wprowadzanie zÅ‚oÅ›liwych danych do zestawu treningowego w celu wpÅ‚ywania na zachowanie modelu.\
   **PrzykÅ‚ad**: Wprowadzenie faÅ‚szywych recenzji uÅ¼ytkownikÃ³w w celu znieksztaÅ‚cenia wynikÃ³w analizy sentymentu.
4. **Ataki typu backdoor**: Przeciwnik wprowadza ukryty wzÃ³r (backdoor) do danych treningowych. Model uczy siÄ™ rozpoznawaÄ‡ ten wzÃ³r i zachowuje siÄ™ zÅ‚oÅ›liwie, gdy zostanie wywoÅ‚any.\
   **PrzykÅ‚ad**: System rozpoznawania twarzy trenowany na obrazach z backdoorami, ktÃ³ry bÅ‚Ä™dnie identyfikuje konkretnÄ… osobÄ™.

MITRE Corporation stworzyÅ‚a [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), bazÄ™ wiedzy o taktykach i technikach stosowanych przez przeciwnikÃ³w w rzeczywistych atakach na systemy AI.

> Liczba luk w systemach opartych na AI roÅ›nie, poniewaÅ¼ wÅ‚Ä…czenie AI zwiÄ™ksza powierzchniÄ™ ataku istniejÄ…cych systemÃ³w poza tradycyjne cyberataki. OpracowaliÅ›my ATLAS, aby zwiÄ™kszyÄ‡ Å›wiadomoÅ›Ä‡ tych unikalnych i ewoluujÄ…cych luk, poniewaÅ¼ globalna spoÅ‚ecznoÅ›Ä‡ coraz czÄ™Å›ciej wÅ‚Ä…cza AI do rÃ³Å¼nych systemÃ³w. ATLAS wzorowany jest na frameworku MITRE ATT&CKÂ® i jego taktyki, techniki oraz procedury (TTPs) sÄ… komplementarne do tych w ATT&CK.

Podobnie jak framework MITRE ATT&CKÂ®, ktÃ³ry jest szeroko stosowany w tradycyjnym cyberbezpieczeÅ„stwie do planowania zaawansowanych scenariuszy emulacji zagroÅ¼eÅ„, ATLAS dostarcza Å‚atwo przeszukiwalny zestaw TTPs, ktÃ³ry pomaga lepiej zrozumieÄ‡ i przygotowaÄ‡ siÄ™ do obrony przed nowymi atakami.

Dodatkowo, Open Web Application Security Project (OWASP) stworzyÅ‚ "[Top 10 listÄ™](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" najwaÅ¼niejszych luk w aplikacjach wykorzystujÄ…cych LLM. Lista podkreÅ›la ryzyko zagroÅ¼eÅ„, takich jak wspomniane wczeÅ›niej zatruwanie danych, a takÅ¼e inne, takie jak:

- **Wstrzykiwanie poleceÅ„**: technika, w ktÃ³rej atakujÄ…cy manipulujÄ… duÅ¼ym modelem jÄ™zykowym (LLM) za pomocÄ… starannie skonstruowanych danych wejÅ›ciowych, powodujÄ…c, Å¼e dziaÅ‚a on poza zamierzonym zakresem.
- **Luki w Å‚aÅ„cuchu dostaw**: Komponenty i oprogramowanie, ktÃ³re skÅ‚adajÄ… siÄ™ na aplikacje uÅ¼ywane przez LLM, takie jak moduÅ‚y Python czy zewnÄ™trzne zbiory danych, mogÄ… byÄ‡ same w sobie podatne na ataki, prowadzÄ…c do nieoczekiwanych rezultatÃ³w, wprowadzonych uprzedzeÅ„, a nawet luk w podstawowej infrastrukturze.
- **Nadmierne poleganie**: LLM sÄ… omylne i majÄ… tendencjÄ™ do generowania bÅ‚Ä™dnych lub niebezpiecznych wynikÃ³w. W kilku udokumentowanych przypadkach ludzie przyjmowali wyniki za pewnik, co prowadziÅ‚o do niezamierzonych negatywnych konsekwencji w rzeczywistoÅ›ci.

Rod Trent, ekspert Microsoft Cloud Advocate, napisaÅ‚ darmowy ebook, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), ktÃ³ry szczegÃ³Å‚owo omawia te i inne pojawiajÄ…ce siÄ™ zagroÅ¼enia zwiÄ…zane z AI oraz dostarcza obszernych wskazÃ³wek, jak najlepiej radziÄ‡ sobie z tymi scenariuszami.

## Testowanie bezpieczeÅ„stwa systemÃ³w AI i LLM

Sztuczna inteligencja (AI) zmienia rÃ³Å¼ne dziedziny i branÅ¼e, oferujÄ…c nowe moÅ¼liwoÅ›ci i korzyÅ›ci dla spoÅ‚eczeÅ„stwa. Jednak AI stawia rÃ³wnieÅ¼ znaczÄ…ce wyzwania i ryzyka, takie jak prywatnoÅ›Ä‡ danych, uprzedzenia, brak przejrzystoÅ›ci oraz potencjalne naduÅ¼ycia. Dlatego kluczowe jest zapewnienie, Å¼e systemy AI sÄ… bezpieczne i odpowiedzialne, co oznacza, Å¼e przestrzegajÄ… standardÃ³w etycznych i prawnych oraz mogÄ… byÄ‡ zaufane przez uÅ¼ytkownikÃ³w i interesariuszy.

Testowanie bezpieczeÅ„stwa to proces oceny bezpieczeÅ„stwa systemu AI lub LLM poprzez identyfikacjÄ™ i wykorzystanie jego luk. MoÅ¼e byÄ‡ przeprowadzane przez deweloperÃ³w, uÅ¼ytkownikÃ³w lub zewnÄ™trznych audytorÃ³w, w zaleÅ¼noÅ›ci od celu i zakresu testowania. NiektÃ³re z najczÄ™stszych metod testowania bezpieczeÅ„stwa systemÃ³w AI i LLM to:

- **Sanityzacja danych**: Proces usuwania lub anonimizowania wraÅ¼liwych lub prywatnych informacji z danych treningowych lub wejÅ›ciowych systemu AI lub LLM. Sanityzacja danych moÅ¼e pomÃ³c zapobiec wyciekom danych i zÅ‚oÅ›liwym manipulacjom poprzez ograniczenie ekspozycji poufnych lub osobistych danych.
- **Testowanie przeciwdziaÅ‚ajÄ…ce atakom**: Proces generowania i stosowania przykÅ‚adÃ³w przeciwdziaÅ‚ajÄ…cych atakom na dane wejÅ›ciowe lub wyjÅ›ciowe systemu AI lub LLM w celu oceny jego odpornoÅ›ci na ataki. Testowanie to moÅ¼e pomÃ³c w identyfikacji i Å‚agodzeniu luk i sÅ‚aboÅ›ci systemu AI lub LLM, ktÃ³re mogÄ… byÄ‡ wykorzystane przez atakujÄ…cych.
- **Weryfikacja modelu**: Proces weryfikacji poprawnoÅ›ci i kompletnoÅ›ci parametrÃ³w modelu lub architektury systemu AI lub LLM. Weryfikacja modelu moÅ¼e pomÃ³c w wykrywaniu i zapobieganiu kradzieÅ¼y modelu poprzez zapewnienie jego ochrony i uwierzytelnienia.
- **Walidacja wynikÃ³w**: Proces walidacji jakoÅ›ci i wiarygodnoÅ›ci wynikÃ³w generowanych przez system AI lub LLM. Walidacja wynikÃ³w moÅ¼e pomÃ³c w wykrywaniu i korygowaniu zÅ‚oÅ›liwych manipulacji poprzez zapewnienie, Å¼e wyniki sÄ… spÃ³jne i dokÅ‚adne.

OpenAI, lider w dziedzinie systemÃ³w AI, ustanowiÅ‚o seriÄ™ _ocen bezpieczeÅ„stwa_ w ramach inicjatywy red teaming network, majÄ…cej na celu testowanie wynikÃ³w systemÃ³w AI w nadziei na przyczynienie siÄ™ do bezpieczeÅ„stwa AI.

> Oceny mogÄ… obejmowaÄ‡ proste testy pytaÅ„ i odpowiedzi, jak rÃ³wnieÅ¼ bardziej zÅ‚oÅ¼one symulacje. Oto przykÅ‚ady ocen opracowanych przez OpenAI w celu oceny zachowaÅ„ AI z rÃ³Å¼nych perspektyw:

#### Perswazja

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi nakÅ‚oniÄ‡ inny system AI do wypowiedzenia tajnego sÅ‚owa?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi przekonaÄ‡ inny system AI do przekazania pieniÄ™dzy?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi wpÅ‚ynÄ…Ä‡ na poparcie innego systemu AI dla propozycji politycznej?

#### Steganografia (ukryte wiadomoÅ›ci)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi przekazywaÄ‡ tajne wiadomoÅ›ci, nie bÄ™dÄ…c wykrytym przez inny system AI?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi kompresowaÄ‡ i dekompresowaÄ‡ wiadomoÅ›ci, umoÅ¼liwiajÄ…c ukrywanie tajnych wiadomoÅ›ci?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi wspÃ³Å‚pracowaÄ‡ z innym systemem AI bez bezpoÅ›redniej komunikacji?

### BezpieczeÅ„stwo AI

Konieczne jest, abyÅ›my dÄ…Å¼yli do ochrony systemÃ³w AI przed zÅ‚oÅ›liwymi atakami, naduÅ¼yciami lub niezamierzonymi konsekwencjami. Obejmuje to podejmowanie dziaÅ‚aÅ„ majÄ…cych na celu zapewnienie bezpieczeÅ„stwa, niezawodnoÅ›ci i wiarygodnoÅ›ci systemÃ³w AI, takich jak:

- Zabezpieczanie danych i algorytmÃ³w uÅ¼ywanych do trenowania i dziaÅ‚ania modeli AI.
- Zapobieganie nieautoryzowanemu dostÄ™powi, manipulacji lub sabotaÅ¼owi systemÃ³w AI.
- Wykrywanie i Å‚agodzenie uprzedzeÅ„, dyskryminacji lub problemÃ³w etycznych w systemach AI.
- Zapewnienie odpowiedzialnoÅ›ci, przejrzystoÅ›ci i wyjaÅ›nialnoÅ›ci decyzji i dziaÅ‚aÅ„ AI.
- Dopasowanie celÃ³w i wartoÅ›ci systemÃ³w AI do celÃ³w i wartoÅ›ci ludzi oraz spoÅ‚eczeÅ„stwa.

BezpieczeÅ„stwo AI jest kluczowe dla zapewnienia integralnoÅ›ci, dostÄ™pnoÅ›ci i poufnoÅ›ci systemÃ³w AI oraz danych. NiektÃ³re wyzwania i moÅ¼liwoÅ›ci zwiÄ…zane z bezpieczeÅ„stwem AI to:

- MoÅ¼liwoÅ›Ä‡: WÅ‚Ä…czenie AI do strategii cyberbezpieczeÅ„stwa, poniewaÅ¼ moÅ¼e odegraÄ‡ kluczowÄ… rolÄ™ w identyfikacji zagroÅ¼eÅ„ i poprawie czasu reakcji. AI moÅ¼e pomÃ³c w automatyzacji i usprawnieniu wykrywania oraz Å‚agodzenia cyberatakÃ³w, takich jak phishing, malware czy ransomware.
- Wyzwanie: AI moÅ¼e byÄ‡ rÃ³wnieÅ¼ wykorzystywane przez przeciwnikÃ³w do przeprowadzania zaawansowanych atakÃ³w, takich jak generowanie faÅ‚szywych lub wprowadzajÄ…cych w bÅ‚Ä…d treÅ›ci, podszywanie siÄ™ pod uÅ¼ytkownikÃ³w czy wykorzystywanie luk w systemach AI. Dlatego twÃ³rcy AI majÄ… szczegÃ³lnÄ… odpowiedzialnoÅ›Ä‡ za projektowanie systemÃ³w odpornych na naduÅ¼ycia.

### Ochrona danych

LLM mogÄ… stanowiÄ‡ zagroÅ¼enie dla prywatnoÅ›ci i bezpieczeÅ„stwa danych, ktÃ³re wykorzystujÄ…. Na przykÅ‚ad LLM mogÄ… potencjalnie zapamiÄ™tywaÄ‡ i ujawniaÄ‡ wraÅ¼liwe informacje z danych treningowych, takie jak imiona, adresy, hasÅ‚a czy numery kart kredytowych. MogÄ… rÃ³wnieÅ¼ byÄ‡ manipulowane lub atakowane przez zÅ‚oÅ›liwe podmioty, ktÃ³re chcÄ… wykorzystaÄ‡ ich luki lub uprzedzenia. Dlatego waÅ¼ne jest, aby byÄ‡ Å›wiadomym tych zagroÅ¼eÅ„ i podjÄ…Ä‡ odpowiednie kroki w celu ochrony danych uÅ¼ywanych z LLM. Oto kilka krokÃ³w, ktÃ³re moÅ¼na podjÄ…Ä‡, aby chroniÄ‡ dane uÅ¼ywane z LLM:

- **Ograniczenie iloÅ›ci i rodzaju danych udostÄ™pnianych LLM**: UdostÄ™pniaj tylko dane, ktÃ³re sÄ… niezbÄ™dne i istotne dla zamierzonych celÃ³w, unikajÄ…c udostÄ™pniania danych wraÅ¼liwych, poufnych lub osobistych. UÅ¼ytkownicy powinni rÃ³wnieÅ¼ anonimizowaÄ‡ lub szyfrowaÄ‡ dane, ktÃ³re udostÄ™pniajÄ… LLM, na przykÅ‚ad usuwajÄ…c lub maskujÄ…c wszelkie informacje identyfikujÄ…ce lub korzystajÄ…c z bezpiecznych kanaÅ‚Ã³w komunikacji.
- **Weryfikacja danych generowanych przez LLM**: Zawsze sprawdzaj dokÅ‚adnoÅ›Ä‡ i jakoÅ›Ä‡ wynikÃ³w generowanych przez LLM, aby upewniÄ‡ siÄ™, Å¼e nie zawierajÄ… niepoÅ¼Ä…danych lub nieodpowiednich informacji.
- **ZgÅ‚aszanie i alarmowanie o wszelkich naruszeniach danych lub incydentach**: BÄ…dÅº czujny na wszelkie podejrzane lub nietypowe dziaÅ‚ania lub zachowania LLM, takie jak generowanie tekstÃ³w, ktÃ³re sÄ… nieistotne, nieprawdziwe, obraÅºliwe lub szkodliwe. MoÅ¼e to byÄ‡ oznaka naruszenia danych lub incydentu bezpieczeÅ„stwa.

BezpieczeÅ„stwo danych, zarzÄ…dzanie nimi i zgodnoÅ›Ä‡ z przepisami sÄ… kluczowe dla kaÅ¼dej organizacji, ktÃ³ra chce wykorzystaÄ‡ potencjaÅ‚ danych i AI w Å›rodowisku wielochmurowym. Zabezpieczenie i zarzÄ…dzanie wszystkimi danymi to zÅ‚oÅ¼one i wieloaspektowe przedsiÄ™wziÄ™cie. Musisz zabezpieczyÄ‡ i zarzÄ…dzaÄ‡ rÃ³Å¼nymi typami danych (strukturalnymi, niestrukturalnymi i generowanymi przez AI) w rÃ³Å¼nych lokalizacjach w wielu chmurach, a takÅ¼e uwzglÄ™dniÄ‡ istniejÄ…ce i przyszÅ‚e przepisy dotyczÄ…ce bezpieczeÅ„stwa danych, zarzÄ…dzania nimi i regulacji AI. Aby chroniÄ‡ swoje dane, naleÅ¼y przyjÄ…Ä‡ najlepsze praktyki i Å›rodki ostroÅ¼noÅ›ci, takie jak:

- Korzystanie z usÅ‚ug chmurowych lub platform oferujÄ…cych funkcje ochrony danych i prywatnoÅ›ci.
- Korzystanie z narz
Symulowanie zagroÅ¼eÅ„ z rzeczywistego Å›wiata jest obecnie uznawane za standardowÄ… praktykÄ™ w budowaniu odpornych systemÃ³w AI poprzez stosowanie podobnych narzÄ™dzi, taktyk i procedur w celu identyfikacji ryzyk dla systemÃ³w oraz testowania reakcji obroÅ„cÃ³w.

> Praktyka red teamingu AI ewoluowaÅ‚a, przyjmujÄ…c bardziej rozszerzone znaczenie: obejmuje nie tylko badanie podatnoÅ›ci na zagroÅ¼enia bezpieczeÅ„stwa, ale takÅ¼e analizÄ™ innych awarii systemu, takich jak generowanie potencjalnie szkodliwych treÅ›ci. Systemy AI niosÄ… ze sobÄ… nowe ryzyka, a red teaming jest kluczowy dla zrozumienia tych nowych zagroÅ¼eÅ„, takich jak wstrzykiwanie poleceÅ„ czy generowanie treÅ›ci bez podstaw. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![WskazÃ³wki i zasoby dotyczÄ…ce red teamingu](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.pl.png)]()

PoniÅ¼ej znajdujÄ… siÄ™ kluczowe spostrzeÅ¼enia, ktÃ³re uksztaÅ‚towaÅ‚y program Microsoft AI Red Team.

1. **Rozszerzony zakres red teamingu AI:**
   Red teaming AI obejmuje teraz zarÃ³wno aspekty bezpieczeÅ„stwa, jak i wyniki zwiÄ…zane z Odpowiedzialnym AI (RAI). Tradycyjnie red teaming koncentrowaÅ‚ siÄ™ na aspektach bezpieczeÅ„stwa, traktujÄ…c model jako wektor (np. kradzieÅ¼ podstawowego modelu). Jednak systemy AI wprowadzajÄ… nowe podatnoÅ›ci na zagroÅ¼enia bezpieczeÅ„stwa (np. wstrzykiwanie poleceÅ„, zatruwanie danych), wymagajÄ…ce szczegÃ³lnej uwagi. Poza bezpieczeÅ„stwem, red teaming AI bada rÃ³wnieÅ¼ kwestie sprawiedliwoÅ›ci (np. stereotypy) oraz szkodliwe treÅ›ci (np. gloryfikacja przemocy). Wczesne wykrycie tych problemÃ³w pozwala na priorytetowe inwestycje w obronÄ™.
2. **ZÅ‚oÅ›liwe i niezamierzone awarie:**
   Red teaming AI uwzglÄ™dnia awarie zarÃ³wno z perspektywy zÅ‚oÅ›liwej, jak i niezamierzonej. Na przykÅ‚ad, podczas red teamingu nowego Bing, badamy nie tylko, jak zÅ‚oÅ›liwi przeciwnicy mogÄ… zakÅ‚Ã³ciÄ‡ dziaÅ‚anie systemu, ale takÅ¼e jak zwykli uÅ¼ytkownicy mogÄ… napotkaÄ‡ problematyczne lub szkodliwe treÅ›ci. W przeciwieÅ„stwie do tradycyjnego red teamingu bezpieczeÅ„stwa, ktÃ³ry koncentruje siÄ™ gÅ‚Ã³wnie na zÅ‚oÅ›liwych aktorach, red teaming AI uwzglÄ™dnia szerszy zakres person i potencjalnych awarii.
3. **Dynamiczny charakter systemÃ³w AI:**
   Aplikacje AI nieustannie siÄ™ rozwijajÄ…. W aplikacjach opartych na duÅ¼ych modelach jÄ™zykowych deweloperzy dostosowujÄ… siÄ™ do zmieniajÄ…cych siÄ™ wymagaÅ„. CiÄ…gÅ‚y red teaming zapewnia staÅ‚Ä… czujnoÅ›Ä‡ i adaptacjÄ™ do ewoluujÄ…cych zagroÅ¼eÅ„.

Red teaming AI nie jest wszechstronny i powinien byÄ‡ traktowany jako uzupeÅ‚nienie dodatkowych mechanizmÃ³w kontroli, takich jak [kontrola dostÄ™pu oparta na rolach (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) oraz kompleksowe rozwiÄ…zania zarzÄ…dzania danymi. Ma na celu uzupeÅ‚nienie strategii bezpieczeÅ„stwa, ktÃ³ra koncentruje siÄ™ na stosowaniu bezpiecznych i odpowiedzialnych rozwiÄ…zaÅ„ AI, uwzglÄ™dniajÄ…cych prywatnoÅ›Ä‡ i bezpieczeÅ„stwo, jednoczeÅ›nie dÄ…Å¼Ä…c do minimalizacji uprzedzeÅ„, szkodliwych treÅ›ci i dezinformacji, ktÃ³re mogÄ… osÅ‚abiÄ‡ zaufanie uÅ¼ytkownikÃ³w.

Oto lista dodatkowych materiaÅ‚Ã³w, ktÃ³re mogÄ… pomÃ³c lepiej zrozumieÄ‡, jak red teaming moÅ¼e pomÃ³c w identyfikacji i Å‚agodzeniu ryzyk w systemach AI:

- [Planowanie red teamingu dla duÅ¼ych modeli jÄ™zykowych (LLM) i ich aplikacji](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Czym jest OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [Red Teaming AI - Kluczowa praktyka w budowaniu bezpieczniejszych i bardziej odpowiedzialnych rozwiÄ…zaÅ„ AI](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), baza wiedzy o taktykach i technikach stosowanych przez przeciwnikÃ³w w rzeczywistych atakach na systemy AI.

## Sprawdzenie wiedzy

Jakie podejÅ›cie moÅ¼e byÄ‡ dobre do utrzymania integralnoÅ›ci danych i zapobiegania ich niewÅ‚aÅ›ciwemu wykorzystaniu?

1. Stosowanie silnych kontroli dostÄ™pu opartych na rolach oraz zarzÄ…dzanie danymi
1. WdraÅ¼anie i audytowanie etykietowania danych w celu zapobiegania ich niewÅ‚aÅ›ciwej interpretacji lub wykorzystaniu
1. Zapewnienie, Å¼e infrastruktura AI wspiera filtrowanie treÅ›ci

A:1, ChociaÅ¼ wszystkie trzy sÄ… Å›wietnymi rekomendacjami, zapewnienie odpowiednich uprawnieÅ„ dostÄ™pu do danych uÅ¼ytkownikom znacznie przyczyni siÄ™ do zapobiegania manipulacji i niewÅ‚aÅ›ciwej interpretacji danych wykorzystywanych przez LLM.

## ğŸš€ Wyzwanie

Dowiedz siÄ™ wiÄ™cej o tym, jak moÅ¼esz [zarzÄ…dzaÄ‡ i chroniÄ‡ wraÅ¼liwe informacje](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) w erze AI.

## Åšwietna robota, kontynuuj naukÄ™

Po ukoÅ„czeniu tej lekcji, sprawdÅº naszÄ… [kolekcjÄ™ nauki o generatywnym AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby dalej rozwijaÄ‡ swojÄ… wiedzÄ™ o generatywnym AI!

PrzejdÅº do Lekcji 14, gdzie przyjrzymy siÄ™ [cyklowi Å¼ycia aplikacji generatywnego AI](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ staramy siÄ™ zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za autorytatywne ÅºrÃ³dÅ‚o. W przypadku informacji krytycznych zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.