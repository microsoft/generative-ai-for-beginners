<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:25:16+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "pl"
}
-->
# Zabezpieczanie Twoich aplikacji generatywnej AI

[![Zabezpieczanie Twoich aplikacji generatywnej AI](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.pl.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Wprowadzenie

W tej lekcji omÃ³wimy:

- BezpieczeÅ„stwo w kontekÅ›cie systemÃ³w AI.
- Typowe zagroÅ¼enia i ryzyka dla systemÃ³w AI.
- Metody i kwestie zwiÄ…zane z zabezpieczaniem systemÃ³w AI.

## Cele nauki

Po ukoÅ„czeniu tej lekcji bÄ™dziesz rozumieÄ‡:

- ZagroÅ¼enia i ryzyka zwiÄ…zane z systemami AI.
- Powszechne metody i praktyki zabezpieczania systemÃ³w AI.
- Jak wdroÅ¼enie testÃ³w bezpieczeÅ„stwa moÅ¼e zapobiec nieoczekiwanym wynikom i utracie zaufania uÅ¼ytkownikÃ³w.

## Co oznacza bezpieczeÅ„stwo w kontekÅ›cie generatywnej AI?

W miarÄ™ jak technologie sztucznej inteligencji (AI) i uczenia maszynowego (ML) coraz bardziej wpÅ‚ywajÄ… na nasze Å¼ycie, kluczowe jest chronienie nie tylko danych klientÃ³w, ale takÅ¼e samych systemÃ³w AI. AI/ML jest coraz czÄ™Å›ciej wykorzystywane do wspierania procesÃ³w podejmowania decyzji o duÅ¼ej wartoÅ›ci w branÅ¼ach, gdzie bÅ‚Ä™dna decyzja moÅ¼e mieÄ‡ powaÅ¼ne konsekwencje.

Oto najwaÅ¼niejsze kwestie do rozwaÅ¼enia:

- **WpÅ‚yw AI/ML**: AI/ML majÄ… znaczÄ…cy wpÅ‚yw na codzienne Å¼ycie, dlatego ich ochrona staÅ‚a siÄ™ niezbÄ™dna.
- **Wyzwania bezpieczeÅ„stwa**: Ten wpÅ‚yw wymaga odpowiedniej uwagi, aby chroniÄ‡ produkty oparte na AI przed zaawansowanymi atakami, zarÃ³wno ze strony trolli, jak i zorganizowanych grup.
- **Problemy strategiczne**: BranÅ¼a technologiczna musi proaktywnie podejÅ›Ä‡ do wyzwaÅ„ strategicznych, aby zapewniÄ‡ dÅ‚ugoterminowe bezpieczeÅ„stwo klientÃ³w i ochronÄ™ danych.

Dodatkowo modele uczenia maszynowego w duÅ¼ej mierze nie potrafiÄ… odrÃ³Å¼niÄ‡ zÅ‚oÅ›liwych danych wejÅ›ciowych od nieszkodliwych anomalii. Znaczna czÄ™Å›Ä‡ danych treningowych pochodzi z niekuratorowanych, niemonitorowanych, publicznych zbiorÃ³w danych, ktÃ³re sÄ… otwarte na wkÅ‚ad osÃ³b trzecich. AtakujÄ…cy nie muszÄ… przejmowaÄ‡ kontroli nad zbiorami danych, gdy mogÄ… do nich swobodnie dodawaÄ‡ swoje dane. Z czasem dane zÅ‚oÅ›liwe o niskim zaufaniu stajÄ… siÄ™ danymi zaufanymi o wysokim zaufaniu, jeÅ›li struktura/format danych pozostaje poprawny.

Dlatego tak waÅ¼ne jest zapewnienie integralnoÅ›ci i ochrony magazynÃ³w danych, z ktÃ³rych korzystajÄ… Twoje modele do podejmowania decyzji.

## Zrozumienie zagroÅ¼eÅ„ i ryzyk zwiÄ…zanych z AI

W kontekÅ›cie AI i powiÄ…zanych systemÃ³w, zatrucie danych (data poisoning) jest obecnie najpowaÅ¼niejszym zagroÅ¼eniem bezpieczeÅ„stwa. Zatrucie danych polega na celowej zmianie informacji uÅ¼ywanych do trenowania AI, co powoduje bÅ‚Ä™dy w dziaÅ‚aniu modelu. Wynika to z braku ustandaryzowanych metod wykrywania i Å‚agodzenia takich atakÃ³w oraz z polegania na niezweryfikowanych, publicznych zbiorach danych do treningu. Aby utrzymaÄ‡ integralnoÅ›Ä‡ danych i zapobiec wadliwemu procesowi treningowemu, kluczowe jest Å›ledzenie pochodzenia i historii danych. W przeciwnym razie sprawdza siÄ™ stare powiedzenie â€Å›mieci wchodzÄ…, Å›mieci wychodzÄ…â€, co prowadzi do obniÅ¼enia jakoÅ›ci dziaÅ‚ania modelu.

Oto przykÅ‚ady, jak zatrucie danych moÅ¼e wpÅ‚ynÄ…Ä‡ na Twoje modele:

1. **Zmiana etykiet (Label Flipping)**: W zadaniu klasyfikacji binarnej przeciwnik celowo zmienia etykiety niewielkiej czÄ™Å›ci danych treningowych. Na przykÅ‚ad prÃ³bki nieszkodliwe sÄ… oznaczane jako zÅ‚oÅ›liwe, co powoduje, Å¼e model uczy siÄ™ bÅ‚Ä™dnych powiÄ…zaÅ„.\
   **PrzykÅ‚ad**: Filtr antyspamowy bÅ‚Ä™dnie klasyfikuje prawidÅ‚owe e-maile jako spam z powodu zmanipulowanych etykiet.
2. **Zatrucie cech (Feature Poisoning)**: AtakujÄ…cy subtelnie modyfikuje cechy w danych treningowych, aby wprowadziÄ‡ uprzedzenia lub zmyliÄ‡ model.\
   **PrzykÅ‚ad**: Dodawanie nieistotnych sÅ‚Ã³w kluczowych do opisÃ³w produktÃ³w, aby zmanipulowaÄ‡ systemy rekomendacji.
3. **WstrzykniÄ™cie danych (Data Injection)**: Wprowadzanie zÅ‚oÅ›liwych danych do zbioru treningowego, aby wpÅ‚ynÄ…Ä‡ na zachowanie modelu.\
   **PrzykÅ‚ad**: Dodawanie faÅ‚szywych recenzji uÅ¼ytkownikÃ³w, aby znieksztaÅ‚ciÄ‡ wyniki analizy sentymentu.
4. **Ataki tylnego wejÅ›cia (Backdoor Attacks)**: Przeciwnik wprowadza ukryty wzorzec (tylne wejÅ›cie) do danych treningowych. Model uczy siÄ™ rozpoznawaÄ‡ ten wzorzec i zachowuje siÄ™ zÅ‚oÅ›liwie po jego wykryciu.\
   **PrzykÅ‚ad**: System rozpoznawania twarzy trenowany na obrazach z tylnym wejÅ›ciem, ktÃ³ry bÅ‚Ä™dnie identyfikuje konkretnÄ… osobÄ™.

MITRE Corporation stworzyÅ‚a [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), bazÄ™ wiedzy o taktykach i technikach stosowanych przez przeciwnikÃ³w w rzeczywistych atakach na systemy AI.

> W systemach z AI pojawia siÄ™ coraz wiÄ™cej luk, poniewaÅ¼ wÅ‚Ä…czenie AI zwiÄ™ksza powierzchniÄ™ ataku istniejÄ…cych systemÃ³w poza tradycyjne cyberataki. StworzyliÅ›my ATLAS, aby zwiÄ™kszyÄ‡ Å›wiadomoÅ›Ä‡ tych unikalnych i rozwijajÄ…cych siÄ™ luk, gdy globalna spoÅ‚ecznoÅ›Ä‡ coraz czÄ™Å›ciej integruje AI w rÃ³Å¼nych systemach. ATLAS jest wzorowany na frameworku MITRE ATT&CKÂ® i jego taktyki, techniki oraz procedury (TTP) uzupeÅ‚niajÄ… te z ATT&CK.

Podobnie jak framework MITRE ATT&CKÂ®, szeroko stosowany w tradycyjnym cyberbezpieczeÅ„stwie do planowania zaawansowanych scenariuszy emulacji zagroÅ¼eÅ„, ATLAS oferuje Å‚atwo przeszukiwany zestaw TTP, ktÃ³ry pomaga lepiej zrozumieÄ‡ i przygotowaÄ‡ siÄ™ do obrony przed nowymi atakami.

Dodatkowo Open Web Application Security Project (OWASP) stworzyÅ‚ "[Top 10 listÄ™](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" najpowaÅ¼niejszych luk w aplikacjach wykorzystujÄ…cych LLM. Lista podkreÅ›la ryzyka zagroÅ¼eÅ„ takich jak wspomniane zatrucie danych oraz inne, na przykÅ‚ad:

- **Prompt Injection**: technika, w ktÃ³rej atakujÄ…cy manipulujÄ… duÅ¼ym modelem jÄ™zykowym (LLM) za pomocÄ… starannie przygotowanych danych wejÅ›ciowych, powodujÄ…c, Å¼e model zachowuje siÄ™ poza swoim zamierzonym dziaÅ‚aniem.
- **Luki w Å‚aÅ„cuchu dostaw (Supply Chain Vulnerabilities)**: komponenty i oprogramowanie tworzÄ…ce aplikacje uÅ¼ywane przez LLM, takie jak moduÅ‚y Pythona czy zewnÄ™trzne zbiory danych, mogÄ… same zostaÄ‡ przejÄ™te, co prowadzi do nieoczekiwanych wynikÃ³w, wprowadzenia uprzedzeÅ„, a nawet luk w infrastrukturze.
- **Nadmierne poleganie (Overreliance)**: LLM sÄ… podatne na bÅ‚Ä™dy i majÄ… tendencjÄ™ do â€halucynacjiâ€, czyli generowania nieprecyzyjnych lub niebezpiecznych wynikÃ³w. W kilku udokumentowanych przypadkach ludzie traktowali wyniki dosÅ‚ownie, co prowadziÅ‚o do niezamierzonych negatywnych konsekwencji w rzeczywistoÅ›ci.

Microsoft Cloud Advocate Rod Trent napisaÅ‚ darmowego ebooka, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), ktÃ³ry dogÅ‚Ä™bnie omawia te i inne pojawiajÄ…ce siÄ™ zagroÅ¼enia AI oraz dostarcza obszerne wskazÃ³wki, jak najlepiej radziÄ‡ sobie z tymi scenariuszami.

## Testowanie bezpieczeÅ„stwa systemÃ³w AI i LLM

Sztuczna inteligencja (AI) zmienia rÃ³Å¼ne dziedziny i branÅ¼e, oferujÄ…c nowe moÅ¼liwoÅ›ci i korzyÅ›ci dla spoÅ‚eczeÅ„stwa. Jednak AI niesie teÅ¼ powaÅ¼ne wyzwania i ryzyka, takie jak prywatnoÅ›Ä‡ danych, uprzedzenia, brak wyjaÅ›nialnoÅ›ci oraz potencjalne naduÅ¼ycia. Dlatego kluczowe jest zapewnienie, Å¼e systemy AI sÄ… bezpieczne i odpowiedzialne, czyli speÅ‚niajÄ… standardy etyczne i prawne oraz mogÄ… byÄ‡ zaufane przez uÅ¼ytkownikÃ³w i interesariuszy.

Testowanie bezpieczeÅ„stwa to proces oceny bezpieczeÅ„stwa systemu AI lub LLM poprzez identyfikacjÄ™ i wykorzystywanie ich luk. MoÅ¼e byÄ‡ przeprowadzane przez deweloperÃ³w, uÅ¼ytkownikÃ³w lub zewnÄ™trznych audytorÃ³w, w zaleÅ¼noÅ›ci od celu i zakresu testÃ³w. Do najczÄ™Å›ciej stosowanych metod testowania bezpieczeÅ„stwa systemÃ³w AI i LLM naleÅ¼Ä…:

- **Oczyszczanie danych (Data sanitization)**: Proces usuwania lub anonimizacji wraÅ¼liwych lub prywatnych informacji z danych treningowych lub wejÅ›ciowych systemu AI lub LLM. Oczyszczanie danych pomaga zapobiegaÄ‡ wyciekom danych i zÅ‚oÅ›liwej manipulacji, ograniczajÄ…c dostÄ™p do poufnych lub osobistych informacji.
- **Testy adwersarialne (Adversarial testing)**: Proces generowania i stosowania przykÅ‚adÃ³w adwersarialnych na wejÅ›ciu lub wyjÅ›ciu systemu AI lub LLM, aby oceniÄ‡ jego odpornoÅ›Ä‡ na ataki adwersarialne. Testy te pomagajÄ… wykryÄ‡ i zÅ‚agodziÄ‡ luki i sÅ‚aboÅ›ci, ktÃ³re mogÄ… byÄ‡ wykorzystane przez atakujÄ…cych.
- **Weryfikacja modelu (Model verification)**: Proces sprawdzania poprawnoÅ›ci i kompletnoÅ›ci parametrÃ³w lub architektury modelu AI lub LLM. Weryfikacja modelu pomaga wykryÄ‡ i zapobiec kradzieÅ¼y modelu, zapewniajÄ…c jego ochronÄ™ i uwierzytelnienie.
- **Walidacja wynikÃ³w (Output validation)**: Proces sprawdzania jakoÅ›ci i wiarygodnoÅ›ci wynikÃ³w generowanych przez system AI lub LLM. Walidacja wynikÃ³w pomaga wykryÄ‡ i skorygowaÄ‡ zÅ‚oÅ›liwÄ… manipulacjÄ™, zapewniajÄ…c spÃ³jnoÅ›Ä‡ i dokÅ‚adnoÅ›Ä‡ wyjÅ›cia.

OpenAI, lider w systemach AI, uruchomiÅ‚o seriÄ™ _ocen bezpieczeÅ„stwa_ w ramach inicjatywy red teaming, majÄ…cej na celu testowanie wynikÃ³w systemÃ³w AI w nadziei na przyczynienie siÄ™ do bezpieczeÅ„stwa AI.

> Oceny mogÄ… obejmowaÄ‡ proste testy pytanie-odpowiedÅº, jak i bardziej zÅ‚oÅ¼one symulacje. Oto przykÅ‚adowe oceny opracowane przez OpenAI do badania zachowaÅ„ AI z rÃ³Å¼nych perspektyw:

#### Perswazja

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi nakÅ‚oniÄ‡ inny system AI do wypowiedzenia sekretnego sÅ‚owa?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi przekonaÄ‡ inny system AI do przekazania pieniÄ™dzy?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi wpÅ‚ynÄ…Ä‡ na poparcie innego systemu AI dla propozycji politycznej?

#### Steganografia (ukryte wiadomoÅ›ci)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi przekazywaÄ‡ tajne wiadomoÅ›ci, nie dajÄ…c siÄ™ wykryÄ‡ innemu systemowi AI?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi kompresowaÄ‡ i dekompresowaÄ‡ wiadomoÅ›ci, umoÅ¼liwiajÄ…c ukrywanie tajnych informacji?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Jak dobrze system AI potrafi koordynowaÄ‡ dziaÅ‚ania z innym systemem AI bez bezpoÅ›redniej komunikacji?

### BezpieczeÅ„stwo AI

Niezwykle waÅ¼ne jest, aby chroniÄ‡ systemy AI przed zÅ‚oÅ›liwymi atakami, naduÅ¼yciami lub niezamierzonymi konsekwencjami. Obejmuje to dziaÅ‚ania majÄ…ce na celu zapewnienie bezpieczeÅ„stwa, niezawodnoÅ›ci i zaufania do systemÃ³w AI, takie jak:

- Zabezpieczanie danych i algorytmÃ³w uÅ¼ywanych do trenowania i dziaÅ‚ania modeli AI
- Zapobieganie nieautoryzowanemu dostÄ™powi, manipulacji lub sabotaÅ¼owi systemÃ³w AI
- Wykrywanie i Å‚agodzenie uprzedzeÅ„, dyskryminacji lub problemÃ³w etycznych w systemach AI
- Zapewnienie odpowiedzialnoÅ›ci, przejrzystoÅ›ci i wyjaÅ›nialnoÅ›ci decyzji i dziaÅ‚aÅ„ AI
- Dopasowanie celÃ³w i wartoÅ›ci systemÃ³w AI do celÃ³w ludzi i spoÅ‚eczeÅ„stwa

BezpieczeÅ„stwo AI jest kluczowe dla zapewnienia integralnoÅ›ci, dostÄ™pnoÅ›ci i poufnoÅ›ci systemÃ³w AI oraz danych. Do wyzwaÅ„ i moÅ¼liwoÅ›ci bezpieczeÅ„stwa AI naleÅ¼Ä…:

- MoÅ¼liwoÅ›Ä‡: WÅ‚Ä…czenie AI do strategii cyberbezpieczeÅ„stwa, poniewaÅ¼ AI moÅ¼e odgrywaÄ‡ kluczowÄ… rolÄ™ w identyfikacji zagroÅ¼eÅ„ i poprawie czasu reakcji. AI moÅ¼e pomÃ³c w automatyzacji i wspomaganiu wykrywania oraz Å‚agodzenia cyberatakÃ³w, takich jak phishing, malware czy ransomware.
- Wyzwanie: AI moÅ¼e byÄ‡ rÃ³wnieÅ¼ wykorzystywane przez przeciwnikÃ³w do przeprowadzania zaawansowanych atakÃ³w, takich jak generowanie faÅ‚szywych lub wprowadzajÄ…cych w bÅ‚Ä…d treÅ›ci, podszywanie siÄ™ pod uÅ¼ytkownikÃ³w czy wykorzystywanie luk w systemach AI. Dlatego twÃ³rcy AI majÄ… wyjÄ…tkowÄ… odpowiedzialnoÅ›Ä‡ za projektowanie systemÃ³w odpornych i wytrzymaÅ‚ych na naduÅ¼ycia.

### Ochrona danych

LLM mogÄ… stanowiÄ‡ zagroÅ¼enie dla prywatnoÅ›ci i bezpieczeÅ„stwa danych, z ktÃ³rych korzystajÄ…. Na przykÅ‚ad LLM mogÄ… zapamiÄ™tywaÄ‡ i ujawniaÄ‡ wraÅ¼liwe informacje ze swoich danych treningowych, takie jak imiona, adresy, hasÅ‚a czy numery kart kredytowych. MogÄ… teÅ¼ byÄ‡ manipulowane lub atakowane przez zÅ‚oÅ›liwych aktorÃ³w, ktÃ³rzy chcÄ… wykorzystaÄ‡ ich luki lub uprzedzenia. Dlatego waÅ¼ne jest, aby byÄ‡ Å›wiadomym tych zagroÅ¼eÅ„ i podejmowaÄ‡ odpowiednie Å›rodki ochrony danych uÅ¼ywanych z LLM. MoÅ¼esz podjÄ…Ä‡ nastÄ™pujÄ…ce kroki, aby chroniÄ‡ dane wykorzystywane z LLM:

- **Ograniczanie iloÅ›ci i rodzaju danych udostÄ™pnianych LLM**: UdostÄ™pniaj tylko dane niezbÄ™dne i istotne dla zamierzonych celÃ³w, unikajÄ…c dzielenia siÄ™ danymi wraÅ¼liwymi, poufnymi lub osobistymi. UÅ¼ytkownicy powinni takÅ¼e anonimizowaÄ‡ lub szyfrowaÄ‡ dane udostÄ™pniane LLM, na przykÅ‚ad usuwajÄ…c lub maskujÄ…c informacje identyfikujÄ…ce lub korzystajÄ…c z bezpiecznych kanaÅ‚Ã³w komunikacji.
- **Weryfikacja danych generowanych przez LLM**: Zawsze sprawdzaj dokÅ‚adnoÅ›Ä‡ i jakoÅ›Ä‡ wynikÃ³w generowanych przez LLM, aby upewniÄ‡ siÄ™, Å¼e nie zawierajÄ… niepoÅ¼Ä…danych lub nieodpowiednich informacji.
- **ZgÅ‚aszanie i alarmowanie o naruszeniach danych lub incydentach**: BÄ…dÅº czujny na podejrzane lub nietypowe zachowania LLM, takie jak generowanie tekstÃ³w nieistotnych, nieprecyzyjnych, obraÅºliwych lub szkodliwych. MoÅ¼e to wskazywaÄ‡ na naruszenie danych lub incydent bezpieczeÅ„stwa.

BezpieczeÅ„stwo danych, zarzÄ…dzanie nimi i zgodnoÅ›Ä‡ z przepisami sÄ… kluczowe dla kaÅ¼dej organizacji, ktÃ³ra chce wykorzystaÄ‡ moc danych i AI w Å›rodowisku multi-cloud. Zabezpieczenie i zarzÄ…dzanie wszystkimi danymi to zÅ‚oÅ¼one i wieloaspektowe zadanie. Musisz chroniÄ‡ i zarzÄ…dzaÄ‡ rÃ³Å¼nymi typami danych (ustrukturyzowanymi, nieustrukturyzowanymi oraz generowanymi przez AI) w rÃ³Å¼nych lokalizacjach w wielu chmurach, a takÅ¼e uwzglÄ™dniaÄ‡ istniejÄ…ce i przyszÅ‚e regulacje dotyczÄ…ce bezpieczeÅ„stwa danych, zarzÄ…dzania i AI. Aby chroniÄ‡ swoje dane, warto stosowaÄ‡ najlepsze praktyki i Å›rodki ostroÅ¼noÅ›ci, takie jak:

- Korzystanie z usÅ‚ug chmurowych lub platform oferujÄ…cych funkcje ochrony danych i prywatnoÅ›ci.
- UÅ¼ywanie narzÄ™dzi do kontroli jakoÅ›ci i walidacji danych, aby wykrywaÄ‡ bÅ‚Ä™dy, nies
> Praktyka AI red teamingu rozwinÄ™Å‚a siÄ™, zyskujÄ…c szersze znaczenie: obejmuje nie tylko wykrywanie luk w zabezpieczeniach, ale takÅ¼e badanie innych awarii systemu, takich jak generowanie potencjalnie szkodliwych treÅ›ci. Systemy AI niosÄ… ze sobÄ… nowe ryzyka, a red teaming jest kluczowy dla zrozumienia tych nowych zagroÅ¼eÅ„, takich jak wstrzykiwanie poleceÅ„ (prompt injection) czy tworzenie niezweryfikowanych treÅ›ci. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.pl.png)]()

PoniÅ¼ej znajdujÄ… siÄ™ kluczowe spostrzeÅ¼enia, ktÃ³re uksztaÅ‚towaÅ‚y program AI Red Team firmy Microsoft.

1. **Szeroki zakres AI Red Teamingu:**  
   AI red teaming obejmuje obecnie zarÃ³wno kwestie bezpieczeÅ„stwa, jak i odpowiedzialnej sztucznej inteligencji (RAI). Tradycyjnie red teaming skupiaÅ‚ siÄ™ na aspektach bezpieczeÅ„stwa, traktujÄ…c model jako wektor ataku (np. kradzieÅ¼ modelu bazowego). Jednak systemy AI wprowadzajÄ… nowe podatnoÅ›ci (np. wstrzykiwanie promptÃ³w, zatrucie danych), ktÃ³re wymagajÄ… szczegÃ³lnej uwagi. Poza bezpieczeÅ„stwem, AI red teaming bada takÅ¼e kwestie sprawiedliwoÅ›ci (np. stereotypy) oraz szkodliwe treÅ›ci (np. gloryfikacja przemocy). Wczesne wykrycie tych problemÃ³w pozwala na priorytetyzacjÄ™ inwestycji w obronÄ™.  
2. **ZÅ‚oÅ›liwe i niezamierzone bÅ‚Ä™dy:**  
   AI red teaming uwzglÄ™dnia bÅ‚Ä™dy zarÃ³wno z perspektywy zÅ‚oÅ›liwych atakÃ³w, jak i przypadkowych problemÃ³w. Na przykÅ‚ad podczas testÃ³w nowego Binga badamy nie tylko, jak zÅ‚oÅ›liwi przeciwnicy mogÄ… podwaÅ¼yÄ‡ system, ale takÅ¼e jak zwykli uÅ¼ytkownicy mogÄ… natknÄ…Ä‡ siÄ™ na problematyczne lub szkodliwe treÅ›ci. W przeciwieÅ„stwie do tradycyjnego red teamingu bezpieczeÅ„stwa, ktÃ³ry skupia siÄ™ gÅ‚Ã³wnie na zÅ‚oÅ›liwych aktorach, AI red teaming bierze pod uwagÄ™ szersze spektrum uÅ¼ytkownikÃ³w i potencjalnych bÅ‚Ä™dÃ³w.  
3. **Dynamiczny charakter systemÃ³w AI:**  
   Aplikacje AI nieustannie siÄ™ rozwijajÄ…. W przypadku duÅ¼ych modeli jÄ™zykowych deweloperzy dostosowujÄ… siÄ™ do zmieniajÄ…cych siÄ™ wymagaÅ„. CiÄ…gÅ‚y red teaming zapewnia staÅ‚Ä… czujnoÅ›Ä‡ i adaptacjÄ™ do ewoluujÄ…cych zagroÅ¼eÅ„.

AI red teaming nie jest rozwiÄ…zaniem kompleksowym i powinien byÄ‡ traktowany jako uzupeÅ‚nienie innych mechanizmÃ³w kontroli, takich jak [role-based access control (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) oraz kompleksowe rozwiÄ…zania zarzÄ…dzania danymi. Ma na celu wsparcie strategii bezpieczeÅ„stwa, ktÃ³ra koncentruje siÄ™ na stosowaniu bezpiecznych i odpowiedzialnych rozwiÄ…zaÅ„ AI, uwzglÄ™dniajÄ…cych prywatnoÅ›Ä‡ i bezpieczeÅ„stwo, a takÅ¼e dÄ…Å¼Ä…cych do minimalizacji uprzedzeÅ„, szkodliwych treÅ›ci i dezinformacji, ktÃ³re mogÄ… podwaÅ¼aÄ‡ zaufanie uÅ¼ytkownikÃ³w.

Oto lista dodatkowych materiaÅ‚Ã³w, ktÃ³re pomogÄ… lepiej zrozumieÄ‡, jak red teaming moÅ¼e pomÃ³c w identyfikacji i Å‚agodzeniu ryzyk w systemach AI:

- [Planowanie red teamingu dla duÅ¼ych modeli jÄ™zykowych (LLM) i ich zastosowaÅ„](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [Czym jest OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming â€“ kluczowa praktyka budowania bezpieczniejszych i bardziej odpowiedzialnych rozwiÄ…zaÅ„ AI](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), baza wiedzy o taktykach i technikach stosowanych przez przeciwnikÃ³w w rzeczywistych atakach na systemy AI.

## Sprawdzenie wiedzy

Jaki moÅ¼e byÄ‡ dobry sposÃ³b na utrzymanie integralnoÅ›ci danych i zapobieganie ich niewÅ‚aÅ›ciwemu wykorzystaniu?

1. Stosowanie silnych kontroli dostÄ™pu do danych opartych na rolach oraz zarzÄ…dzania danymi  
1. WdraÅ¼anie i audytowanie etykietowania danych, aby zapobiec ich bÅ‚Ä™dnej reprezentacji lub naduÅ¼yciom  
1. Zapewnienie, Å¼e infrastruktura AI wspiera filtrowanie treÅ›ci  

OdpowiedÅº: 1. ChoÄ‡ wszystkie trzy zalecenia sÄ… wartoÅ›ciowe, przypisanie odpowiednich uprawnieÅ„ dostÄ™pu do danych uÅ¼ytkownikom w duÅ¼ym stopniu zapobiega manipulacjom i bÅ‚Ä™dnej reprezentacji danych wykorzystywanych przez LLM.

## ğŸš€ Wyzwanie

Dowiedz siÄ™ wiÄ™cej o tym, jak moÅ¼esz [zarzÄ…dzaÄ‡ i chroniÄ‡ wraÅ¼liwe informacje](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) w erze AI.

## Åšwietna robota, kontynuuj naukÄ™

Po ukoÅ„czeniu tej lekcji sprawdÅº naszÄ… [kolekcjÄ™ materiaÅ‚Ã³w do nauki Generative AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby dalej rozwijaÄ‡ swojÄ… wiedzÄ™ na temat Generative AI!

PrzejdÅº do Lekcji 14, gdzie przyjrzymy siÄ™ [cyklowi Å¼ycia aplikacji Generative AI](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**ZastrzeÅ¼enie**:  
Niniejszy dokument zostaÅ‚ przetÅ‚umaczony przy uÅ¼yciu usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo Å¼e dÄ…Å¼ymy do dokÅ‚adnoÅ›ci, prosimy mieÄ‡ na uwadze, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jÄ™zyku ÅºrÃ³dÅ‚owym powinien byÄ‡ uznawany za ÅºrÃ³dÅ‚o autorytatywne. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnego tÅ‚umaczenia wykonanego przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z korzystania z tego tÅ‚umaczenia.