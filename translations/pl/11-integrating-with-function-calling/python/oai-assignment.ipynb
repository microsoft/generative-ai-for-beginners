{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Ta lekcja obejmuje:\n",
    "- Czym jest wywoływanie funkcji i do czego się je stosuje\n",
    "- Jak utworzyć wywołanie funkcji za pomocą OpenAI\n",
    "- Jak zintegrować wywołanie funkcji z aplikacją\n",
    "\n",
    "## Cele nauki\n",
    "\n",
    "Po ukończeniu tej lekcji będziesz wiedzieć i rozumieć:\n",
    "\n",
    "- Cel stosowania wywoływania funkcji\n",
    "- Konfigurowanie wywołania funkcji przy użyciu usługi OpenAI\n",
    "- Projektowanie skutecznych wywołań funkcji dla zastosowań w Twojej aplikacji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zrozumienie wywołań funkcji\n",
    "\n",
    "W tej lekcji chcemy stworzyć funkcję dla naszego startupu edukacyjnego, która pozwoli użytkownikom korzystać z chatbota do wyszukiwania kursów technicznych. Będziemy polecać kursy dopasowane do poziomu umiejętności użytkownika, jego obecnej roli oraz interesującej go technologii.\n",
    "\n",
    "Aby to zrealizować, użyjemy połączenia:\n",
    " - `OpenAI` do stworzenia doświadczenia czatu dla użytkownika\n",
    " - `Microsoft Learn Catalog API`, aby pomóc użytkownikom znaleźć kursy na podstawie ich zapytań\n",
    " - `Function Calling`, aby pobrać zapytanie użytkownika i przekazać je do funkcji, która wykona żądanie do API\n",
    "\n",
    "Na początek przyjrzyjmy się, dlaczego w ogóle warto korzystać z wywołań funkcji:\n",
    "\n",
    "print(\"Wiadomości w następnym żądaniu:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # uzyskaj nową odpowiedź od GPT, która może zobaczyć odpowiedź funkcji\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dlaczego wywoływanie funkcji\n",
    "\n",
    "Jeśli ukończyłeś już którąkolwiek z innych lekcji w tym kursie, prawdopodobnie rozumiesz, jak potężne są duże modele językowe (LLM). Mam nadzieję, że dostrzegasz też ich pewne ograniczenia.\n",
    "\n",
    "Wywoływanie funkcji to funkcja w usłudze OpenAI, która została stworzona, by rozwiązać następujące problemy:\n",
    "\n",
    "Niespójny format odpowiedzi:\n",
    "- Przed wprowadzeniem wywoływania funkcji odpowiedzi generowane przez duże modele językowe były nieustrukturyzowane i niespójne. Programiści musieli pisać skomplikowany kod walidujący, by obsłużyć każdą możliwą wariację odpowiedzi.\n",
    "\n",
    "Ograniczona integracja z danymi zewnętrznymi:\n",
    "- Wcześniej trudno było włączyć dane z innych części aplikacji do kontekstu rozmowy.\n",
    "\n",
    "Dzięki standaryzacji formatów odpowiedzi i łatwej integracji z danymi zewnętrznymi, wywoływanie funkcji upraszcza proces tworzenia aplikacji i zmniejsza potrzebę pisania dodatkowej logiki walidującej.\n",
    "\n",
    "Użytkownicy nie mogli uzyskać odpowiedzi na pytania typu „Jaka jest aktualna pogoda w Sztokholmie?”. Wynikało to z faktu, że modele były ograniczone do czasu, w którym zostały wytrenowane.\n",
    "\n",
    "Przyjrzyjmy się poniższemu przykładowi, który ilustruje ten problem:\n",
    "\n",
    "Załóżmy, że chcemy stworzyć bazę danych studentów, aby móc proponować im odpowiednie kursy. Poniżej znajdują się dwa opisy studentów, które zawierają bardzo podobne dane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chcemy wysłać to do LLM, aby przeanalizować dane. Później można to wykorzystać w naszej aplikacji do wysyłania do API lub przechowywania w bazie danych.\n",
    "\n",
    "Stwórzmy dwa identyczne prompt’y, w których poinstruujemy LLM, jakie informacje nas interesują:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chcemy wysłać to do LLM, aby przeanalizował części istotne dla naszego produktu. Dzięki temu możemy stworzyć dwa identyczne prompt'y, aby poinstruować LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po utworzeniu tych dwóch promptów, wyślemy je do LLM za pomocą `openai.ChatCompletion`. Przechowujemy prompt w zmiennej `messages` i przypisujemy rolę `user`. Ma to na celu naśladowanie wiadomości od użytkownika pisanej do chatbota.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nawet jeśli polecenia są takie same, a opisy podobne, możemy otrzymać różne formaty właściwości `Grades`.\n",
    "\n",
    "Jeśli uruchomisz powyższą komórkę kilka razy, format może być `3.7` lub `3.7 GPA`.\n",
    "\n",
    "Dzieje się tak, ponieważ LLM przyjmuje nieustrukturyzowane dane w postaci napisanego polecenia i zwraca również dane nieustrukturyzowane. Potrzebujemy mieć ustrukturyzowany format, aby wiedzieć, czego się spodziewać podczas przechowywania lub używania tych danych.\n",
    "\n",
    "Korzystając z wywołań funkcji, możemy mieć pewność, że otrzymamy dane w ustrukturyzowanej formie. Podczas korzystania z wywołań funkcji LLM faktycznie nie wywołuje ani nie uruchamia żadnych funkcji. Zamiast tego tworzymy strukturę, której LLM ma się trzymać w swoich odpowiedziach. Następnie używamy tych ustrukturyzowanych odpowiedzi, aby wiedzieć, jaką funkcję uruchomić w naszych aplikacjach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Schemat przepływu wywołań funkcji](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.pl.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykłady użycia wywołań funkcji\n",
    "\n",
    "**Wywoływanie narzędzi zewnętrznych**  \n",
    "Chatboty świetnie odpowiadają na pytania użytkowników. Dzięki wywołaniom funkcji chatboty mogą wykorzystywać wiadomości od użytkowników do realizacji określonych zadań. Na przykład student może poprosić chatbota: \"Wyślij e-mail do mojego wykładowcy z informacją, że potrzebuję więcej pomocy z tym tematem\". Wtedy może zostać wywołana funkcja `send_email(to: string, body: string)`\n",
    "\n",
    "**Tworzenie zapytań do API lub bazy danych**  \n",
    "Użytkownicy mogą wyszukiwać informacje za pomocą języka naturalnego, który zostaje przekształcony w odpowiednie zapytanie lub żądanie do API. Przykładem może być nauczyciel, który pyta: \"Którzy uczniowie ukończyli ostatnie zadanie?\", co może wywołać funkcję o nazwie `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Tworzenie danych strukturalnych**  \n",
    "Użytkownicy mogą wziąć fragment tekstu lub plik CSV i wykorzystać LLM do wyodrębnienia z niego najważniejszych informacji. Na przykład student może przekształcić artykuł z Wikipedii o porozumieniach pokojowych, aby stworzyć fiszki AI. Można to zrobić za pomocą funkcji `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tworzenie pierwszego wywołania funkcji\n",
    "\n",
    "Proces tworzenia wywołania funkcji obejmuje 3 główne kroki:\n",
    "1. Wywołanie Chat Completions API z listą Twoich funkcji oraz wiadomością od użytkownika\n",
    "2. Odczytanie odpowiedzi modelu, aby wykonać jakąś akcję, np. wywołać funkcję lub API\n",
    "3. Ponowne wywołanie Chat Completions API z odpowiedzią z Twojej funkcji, aby wykorzystać te informacje do stworzenia odpowiedzi dla użytkownika.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Przepływ wywołania funkcji](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.pl.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementy wywołania funkcji\n",
    "\n",
    "#### Wprowadzenie przez użytkownika\n",
    "\n",
    "Pierwszym krokiem jest utworzenie wiadomości od użytkownika. Można to zrobić dynamicznie, pobierając wartość z pola tekstowego, lub przypisać wartość bezpośrednio tutaj. Jeśli to Twój pierwszy kontakt z API Chat Completions, musimy określić `role` oraz `content` wiadomości.\n",
    "\n",
    "`role` może przyjmować wartości: `system` (tworzenie zasad), `assistant` (model) lub `user` (użytkownik końcowy). W przypadku wywoływania funkcji przypiszemy rolę `user` oraz przykładowe pytanie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie funkcji.\n",
    "\n",
    "Teraz zdefiniujemy funkcję oraz jej parametry. W tym przykładzie użyjemy tylko jednej funkcji o nazwie `search_courses`, ale możesz stworzyć ich więcej.\n",
    "\n",
    "**Ważne**: Funkcje są dołączane do wiadomości systemowej dla LLM i będą wliczane do dostępnej liczby tokenów.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definicje**\n",
    "\n",
    "Struktura definicji funkcji ma kilka poziomów, z których każdy posiada własne właściwości. Oto szczegółowy opis tej zagnieżdżonej struktury:\n",
    "\n",
    "**Właściwości funkcji na najwyższym poziomie:**\n",
    "\n",
    "`name` - Nazwa funkcji, którą chcemy wywołać.\n",
    "\n",
    "`description` - Opis działania funkcji. Ważne, aby był precyzyjny i jasny.\n",
    "\n",
    "`parameters` - Lista wartości oraz format, które model ma wygenerować w odpowiedzi.\n",
    "\n",
    "**Właściwości obiektu Parameters:**\n",
    "\n",
    "`type` - Typ danych obiektu parameters (zazwyczaj \"object\")\n",
    "\n",
    "`properties` - Lista konkretnych wartości, które model wykorzysta w odpowiedzi\n",
    "\n",
    "**Właściwości pojedynczego parametru:**\n",
    "\n",
    "`name` - Domyślnie określone przez klucz właściwości (np. \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Typ danych tego konkretnego parametru (np. \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - Opis konkretnego parametru\n",
    "\n",
    "**Właściwości opcjonalne:**\n",
    "\n",
    "`required` - Tablica z nazwami parametrów, które są wymagane do poprawnego wywołania funkcji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wywoływanie funkcji\n",
    "Po zdefiniowaniu funkcji musimy teraz uwzględnić ją w wywołaniu do Chat Completion API. Robimy to, dodając `functions` do zapytania. W tym przypadku `functions=functions`.\n",
    "\n",
    "Jest także opcja ustawienia `function_call` na `auto`. Oznacza to, że pozwolimy LLM zdecydować, która funkcja powinna zostać wywołana na podstawie wiadomości użytkownika, zamiast przypisywać ją samodzielnie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz przyjrzyjmy się odpowiedzi i zobaczmy, jak jest sformatowana:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Widzisz, że wywoływana jest nazwa funkcji, a na podstawie wiadomości użytkownika LLM był w stanie znaleźć dane pasujące do argumentów funkcji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integracja wywołań funkcji z aplikacją.\n",
    "\n",
    "Po przetestowaniu sformatowanej odpowiedzi z LLM, możemy teraz zintegrować ją z aplikacją.\n",
    "\n",
    "### Zarządzanie przepływem\n",
    "\n",
    "Aby zintegrować to z naszą aplikacją, wykonajmy następujące kroki:\n",
    "\n",
    "Najpierw wywołajmy usługi OpenAI i zapiszmy wiadomość w zmiennej o nazwie `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz zdefiniujemy funkcję, która wywoła API Microsoft Learn, aby pobrać listę kursów:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako dobra praktyka, sprawdzimy, czy model chce wywołać jakąś funkcję. Następnie utworzymy jedną z dostępnych funkcji i dopasujemy ją do tej, która jest wywoływana. \n",
    "Następnie weźmiemy argumenty funkcji i przypiszemy je do argumentów pochodzących z LLM.\n",
    "\n",
    "Na końcu dołączymy wiadomość o wywołaniu funkcji oraz wartości zwrócone przez wiadomość `search_courses`. Dzięki temu LLM będzie miał wszystkie potrzebne informacje,\n",
    "aby odpowiedzieć użytkownikowi w naturalny sposób.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyzwanie programistyczne\n",
    "\n",
    "Świetna robota! Aby dalej rozwijać swoją wiedzę na temat wywoływania funkcji OpenAI, możesz zbudować: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Więcej parametrów funkcji, które mogą pomóc użytkownikom znaleźć więcej kursów. Dostępne parametry API znajdziesz tutaj:\n",
    " - Stwórz kolejne wywołanie funkcji, które pobiera więcej informacji od użytkownika, na przykład jego język ojczysty\n",
    " - Dodaj obsługę błędów na wypadek, gdyby wywołanie funkcji i/lub API nie zwróciło żadnych odpowiednich kursów\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Zastrzeżenie**:  \nTen dokument został przetłumaczony przy użyciu usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było poprawne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być traktowany jako źródło nadrzędne. W przypadku informacji krytycznych zalecane jest skorzystanie z profesjonalnych usług tłumacza. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T21:01:42+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "pl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}