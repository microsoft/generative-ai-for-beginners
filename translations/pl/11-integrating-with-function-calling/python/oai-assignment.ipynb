{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Ta lekcja obejmie:\n",
    "- Czym jest wywoływanie funkcji i do czego służy\n",
    "- Jak utworzyć wywołanie funkcji za pomocą OpenAI\n",
    "- Jak zintegrować wywołanie funkcji z aplikacją\n",
    "\n",
    "## Cele nauki\n",
    "\n",
    "Po ukończeniu tej lekcji będziesz wiedzieć jak i rozumieć:\n",
    "\n",
    "- Cel używania wywoływania funkcji\n",
    "- Konfigurację wywołania funkcji za pomocą usługi OpenAI\n",
    "- Projektowanie skutecznych wywołań funkcji dla zastosowań Twojej aplikacji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zrozumienie wywołań funkcji\n",
    "\n",
    "Na tę lekcję chcemy zbudować funkcję dla naszego startupu edukacyjnego, która pozwoli użytkownikom korzystać z chatbota do wyszukiwania kursów technicznych. Będziemy polecać kursy dopasowane do ich poziomu umiejętności, obecnej roli oraz interesującej technologii.\n",
    "\n",
    "Do realizacji tego celu użyjemy kombinacji:\n",
    " - `OpenAI` do stworzenia doświadczenia czatu dla użytkownika\n",
    " - `Microsoft Learn Catalog API` do pomocy użytkownikom w znajdowaniu kursów na podstawie ich zapytań\n",
    " - `Function Calling` do przechwycenia zapytania użytkownika i przesłania go do funkcji wykonującej zapytanie do API.\n",
    "\n",
    "Aby zacząć, przyjrzyjmy się, dlaczego w ogóle chcielibyśmy używać wywołań funkcji:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # uzyskaj nową odpowiedź od GPT, która może zobaczyć odpowiedź funkcji\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dlaczego wywoływanie funkcji\n",
    "\n",
    "Jeśli ukończyłeś już jakąkolwiek inną lekcję w tym kursie, prawdopodobnie rozumiesz moc korzystania z dużych modeli językowych (LLM). Mamy nadzieję, że dostrzegasz również niektóre z ich ograniczeń.\n",
    "\n",
    "Wywoływanie funkcji to funkcja usługi OpenAI zaprojektowana, aby rozwiązać następujące wyzwania:\n",
    "\n",
    "Niespójne formatowanie odpowiedzi:\n",
    "- Przed wprowadzeniem wywoływania funkcji odpowiedzi z dużego modelu językowego były nieustrukturyzowane i niespójne. Programiści musieli pisać skomplikowany kod walidacyjny, aby obsłużyć każdą wariację w wyjściu.\n",
    "\n",
    "Ograniczona integracja z danymi zewnętrznymi:\n",
    "- Przed tą funkcją trudno było włączyć dane z innych części aplikacji do kontekstu czatu.\n",
    "\n",
    "Poprzez standaryzację formatów odpowiedzi i umożliwienie płynnej integracji z danymi zewnętrznymi, wywoływanie funkcji upraszcza rozwój i zmniejsza potrzebę dodatkowej logiki walidacyjnej.\n",
    "\n",
    "Użytkownicy nie mogli uzyskać odpowiedzi na pytania takie jak „Jaka jest aktualna pogoda w Sztokholmie?”. Dzieje się tak, ponieważ modele były ograniczone do czasu, na którym dane były trenowane.\n",
    "\n",
    "Spójrzmy na poniższy przykład ilustrujący ten problem:\n",
    "\n",
    "Załóżmy, że chcemy stworzyć bazę danych danych studentów, aby móc zasugerować im odpowiedni kurs. Poniżej mamy dwa opisy studentów, które są bardzo podobne pod względem zawartych danych.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chcemy wysłać to do LLM, aby przetworzyć dane. Może to być później użyte w naszej aplikacji do wysłania tego do API lub zapisania w bazie danych.\n",
    "\n",
    "Stwórzmy dwa identyczne polecenia, w których poinstruujemy LLM, jakich informacji jesteśmy zainteresowani:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chcemy wysłać to do LLM, aby przeanalizował części ważne dla naszego produktu. Możemy więc stworzyć dwa identyczne polecenia, aby instruować LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po utworzeniu tych dwóch promptów, wyślemy je do LLM za pomocą `openai.ChatCompletion`. Przechowujemy prompt w zmiennej `messages` i przypisujemy rolę `user`. Ma to na celu naśladowanie wiadomości od użytkownika wysyłanej do chatbota.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możemy wysłać oba zapytania do LLM i przeanalizować otrzymaną odpowiedź.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mimo że podpowiedzi są takie same, a opisy podobne, możemy otrzymać różne formaty właściwości `Grades`.\n",
    "\n",
    "Jeśli uruchomisz powyższą komórkę wielokrotnie, format może być `3.7` lub `3.7 GPA`.\n",
    "\n",
    "Wynika to z faktu, że LLM przyjmuje dane niestrukturalne w formie napisanego promptu i zwraca również dane niestrukturalne. Potrzebujemy mieć format strukturalny, aby wiedzieć, czego się spodziewać podczas przechowywania lub używania tych danych.\n",
    "\n",
    "Korzystając z wywołań funkcji, możemy mieć pewność, że otrzymamy dane w formacie strukturalnym. Podczas używania wywołań funkcji LLM faktycznie nie wywołuje ani nie uruchamia żadnych funkcji. Zamiast tego tworzymy strukturę, której LLM ma się trzymać w swoich odpowiedziach. Następnie używamy tych strukturalnych odpowiedzi, aby wiedzieć, jaką funkcję uruchomić w naszych aplikacjach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram przepływu wywołania funkcji](../../../../translated_images/pl/Function-Flow.083875364af4f4bb.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie możemy wziąć to, co zwraca funkcja, i odesłać to z powrotem do LLM. LLM odpowie wtedy używając języka naturalnego, aby odpowiedzieć na zapytanie użytkownika.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przypadki użycia wywołań funkcji\n",
    "\n",
    "**Wywoływanie narzędzi zewnętrznych**  \n",
    "Chatboty świetnie nadają się do udzielania odpowiedzi na pytania użytkowników. Dzięki wywoływaniu funkcji chatboty mogą wykorzystywać wiadomości od użytkowników do wykonywania określonych zadań. Na przykład student może poprosić chatbota o „Wysłanie e-maila do mojego wykładowcy z informacją, że potrzebuję więcej pomocy z tym przedmiotem”. Może to wywołać funkcję `send_email(to: string, body: string)`\n",
    "\n",
    "**Tworzenie zapytań do API lub bazy danych**  \n",
    "Użytkownicy mogą wyszukiwać informacje za pomocą języka naturalnego, który jest konwertowany na sformatowane zapytanie lub żądanie API. Przykładem może być nauczyciel, który pyta „Którzy uczniowie ukończyli ostatnie zadanie”, co może wywołać funkcję o nazwie `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Tworzenie danych strukturalnych**  \n",
    "Użytkownicy mogą wziąć blok tekstu lub plik CSV i użyć LLM do wyodrębnienia z niego ważnych informacji. Na przykład student może przekształcić artykuł z Wikipedii o umowach pokojowych, aby stworzyć karty do nauki AI. Można to zrobić za pomocą funkcji `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tworzenie pierwszego wywołania funkcji\n",
    "\n",
    "Proces tworzenia wywołania funkcji obejmuje 3 główne kroki:\n",
    "1. Wywołanie API Chat Completions z listą Twoich funkcji i wiadomością od użytkownika\n",
    "2. Odczytanie odpowiedzi modelu w celu wykonania akcji, np. wywołania funkcji lub API\n",
    "3. Ponowne wywołanie API Chat Completions z odpowiedzią z Twojej funkcji, aby wykorzystać te informacje do stworzenia odpowiedzi dla użytkownika.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Przepływ wywołania funkcji](../../../../translated_images/pl/LLM-Flow.3285ed8caf4796d7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementy wywołania funkcji \n",
    "\n",
    "#### Dane wejściowe użytkownika \n",
    "\n",
    "Pierwszym krokiem jest utworzenie wiadomości użytkownika. Może to być dynamicznie przypisana wartość pobrana z pola tekstowego lub możesz przypisać wartość tutaj. Jeśli to Twój pierwszy raz z API Chat Completions, musimy zdefiniować `role` oraz `content` wiadomości. \n",
    "\n",
    "`role` może być `system` (tworzenie reguł), `assistant` (model) lub `user` (użytkownik końcowy). Dla wywołania funkcji przypiszemy to jako `user` oraz przykładowe pytanie. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie funkcji.\n",
    "\n",
    "Następnie zdefiniujemy funkcję oraz parametry tej funkcji. Użyjemy tutaj tylko jednej funkcji o nazwie `search_courses`, ale możesz stworzyć wiele funkcji.\n",
    "\n",
    "**Ważne**: Funkcje są dołączane do komunikatu systemowego do LLM i będą wliczane do dostępnej liczby tokenów, które masz do dyspozycji.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definicje** \n",
    "\n",
    "Struktura definicji funkcji ma wiele poziomów, z których każdy ma własne właściwości. Oto podział zagnieżdżonej struktury:\n",
    "\n",
    "**Właściwości funkcji na najwyższym poziomie:**\n",
    "\n",
    "`name` - Nazwa funkcji, którą chcemy wywołać. \n",
    "\n",
    "`description` - Opis działania funkcji. Tutaj ważne jest, aby być precyzyjnym i jasnym. \n",
    "\n",
    "`parameters` - Lista wartości i formatu, które model ma wygenerować w swojej odpowiedzi. \n",
    "\n",
    "**Właściwości obiektu parametrów:**\n",
    "\n",
    "`type` - Typ danych obiektu parametrów (zazwyczaj \"object\")\n",
    "\n",
    "`properties` - Lista konkretnych wartości, które model wykorzysta w swojej odpowiedzi. \n",
    "\n",
    "**Właściwości pojedynczych parametrów:**\n",
    "\n",
    "`name` - Implicitnie zdefiniowane przez klucz właściwości (np. \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Typ danych tego konkretnego parametru (np. \"string\", \"number\", \"boolean\") \n",
    "\n",
    "`description` - Opis konkretnego parametru. \n",
    "\n",
    "**Właściwości opcjonalne:**\n",
    "\n",
    "`required` - Tablica wymieniająca, które parametry są wymagane do wykonania wywołania funkcji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wywołanie funkcji  \n",
    "Po zdefiniowaniu funkcji, musimy teraz uwzględnić ją w wywołaniu API Chat Completion. Robimy to, dodając `functions` do żądania. W tym przypadku `functions=functions`.  \n",
    "\n",
    "Istnieje również opcja ustawienia `function_call` na `auto`. Oznacza to, że pozwolimy LLM zdecydować, która funkcja powinna zostać wywołana na podstawie wiadomości użytkownika, zamiast przypisywać ją samodzielnie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się teraz odpowiedzi i zobaczmy, jak jest sformatowana:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Widać, że nazwa funkcji została wywołana, a na podstawie wiadomości użytkownika LLM był w stanie znaleźć dane pasujące do argumentów funkcji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Integrowanie wywołań funkcji z aplikacją. \n",
    "\n",
    "\n",
    "Po przetestowaniu sformatowanej odpowiedzi z LLM, możemy teraz zintegrować ją z aplikacją. \n",
    "\n",
    "### Zarządzanie przepływem \n",
    "\n",
    "Aby zintegrować to z naszą aplikacją, wykonajmy następujące kroki: \n",
    "\n",
    "Najpierw wykonajmy wywołanie do usług OpenAI i zapiszmy wiadomość w zmiennej o nazwie `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz zdefiniujemy funkcję, która wywoła API Microsoft Learn, aby uzyskać listę kursów:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako najlepszą praktykę, sprawdzimy następnie, czy model chce wywołać funkcję. Następnie utworzymy jedną z dostępnych funkcji i dopasujemy ją do wywoływanej funkcji.  \n",
    "Następnie weźmiemy argumenty funkcji i odwzorujemy je na argumenty z LLM.\n",
    "\n",
    "Na koniec dołączymy wiadomość wywołania funkcji oraz wartości zwrócone przez wiadomość `search_courses`. Dzięki temu LLM otrzyma wszystkie informacje potrzebne do  \n",
    "odpowiedzi użytkownikowi w języku naturalnym.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz wyślemy zaktualizowaną wiadomość do LLM, abyśmy mogli otrzymać odpowiedź w języku naturalnym zamiast odpowiedzi w formacie JSON API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyzwanie kodowania\n",
    "\n",
    "Świetna robota! Aby kontynuować naukę wywoływania funkcji OpenAI, możesz zbudować: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    " - Więcej parametrów funkcji, które mogą pomóc uczącym się znaleźć więcej kursów. Dostępne parametry API znajdziesz tutaj:  \n",
    " - Utwórz kolejne wywołanie funkcji, które pobierze więcej informacji od uczącego się, na przykład jego język ojczysty  \n",
    " - Utwórz obsługę błędów, gdy wywołanie funkcji i/lub wywołanie API nie zwróci żadnych odpowiednich kursów\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Zastrzeżenie**:  \nNiniejszy dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo że dokładamy starań, aby tłumaczenie było jak najbardziej precyzyjne, prosimy mieć na uwadze, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w języku źródłowym powinien być uznawany za źródło autorytatywne. W przypadku informacji krytycznych zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T10:12:20+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "pl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}