<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:45:59+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "pl"
}
-->
# Wprowadzenie do sieci neuronowych. Wielowarstwowy perceptron

W poprzedniej czÄ™Å›ci poznaliÅ›cie najprostszy model sieci neuronowej â€“ jednowarstwowy perceptron, liniowy model klasyfikacji dwuklasowej.

W tej sekcji rozszerzymy ten model do bardziej elastycznego frameworku, ktÃ³ry pozwoli nam:

* wykonywaÄ‡ **klasyfikacjÄ™ wieloklasowÄ…** oprÃ³cz dwuklasowej
* rozwiÄ…zywaÄ‡ **problemy regresji** oprÃ³cz klasyfikacji
* rozdzielaÄ‡ klasy, ktÃ³re nie sÄ… liniowo separowalne

Stworzymy rÃ³wnieÅ¼ wÅ‚asny moduÅ‚owy framework w Pythonie, ktÃ³ry umoÅ¼liwi budowanie rÃ³Å¼nych architektur sieci neuronowych.

## Formalizacja uczenia maszynowego

Zacznijmy od formalizacji problemu uczenia maszynowego. ZaÅ‚Ã³Å¼my, Å¼e mamy zbiÃ³r treningowy **X** z etykietami **Y** i musimy zbudowaÄ‡ model *f*, ktÃ³ry bÄ™dzie dawaÅ‚ jak najdokÅ‚adniejsze przewidywania. JakoÅ›Ä‡ przewidywaÅ„ mierzymy za pomocÄ… **funkcji straty** â„’. CzÄ™sto stosowane funkcje straty to:

* Dla problemu regresji, gdy przewidujemy liczbÄ™, moÅ¼emy uÅ¼yÄ‡ **bÅ‚Ä™du bezwzglÄ™dnego** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| lub **bÅ‚Ä™du kwadratowego** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Dla klasyfikacji stosujemy **0-1 loss** (co w praktyce odpowiada **dokÅ‚adnoÅ›ci** modelu) lub **logistycznÄ… funkcjÄ™ straty**.

Dla jednowarstwowego perceptronu funkcja *f* byÅ‚a zdefiniowana jako funkcja liniowa *f(x)=wx+b* (gdzie *w* to macierz wag, *x* to wektor cech wejÅ›ciowych, a *b* to wektor biasÃ³w). W przypadku rÃ³Å¼nych architektur sieci neuronowych funkcja ta moÅ¼e przyjmowaÄ‡ bardziej zÅ‚oÅ¼onÄ… formÄ™.

> W przypadku klasyfikacji czÄ™sto chcemy, aby wyjÅ›ciem sieci byÅ‚y prawdopodobieÅ„stwa odpowiadajÄ…cych klas. Aby przeksztaÅ‚ciÄ‡ dowolne liczby w prawdopodobieÅ„stwa (np. znormalizowaÄ‡ wyjÅ›cie), czÄ™sto stosujemy funkcjÄ™ **softmax** Ïƒ, a funkcja *f* przyjmuje postaÄ‡ *f(x)=Ïƒ(wx+b)*

W powyÅ¼szej definicji *f*, *w* i *b* nazywamy **parametrami** Î¸=âŸ¨*w,b*âŸ©. MajÄ…c zbiÃ³r danych âŸ¨**X**,**Y**âŸ©, moÅ¼emy obliczyÄ‡ caÅ‚kowity bÅ‚Ä…d na caÅ‚ym zbiorze jako funkcjÄ™ parametrÃ³w Î¸.

> âœ… **Celem trenowania sieci neuronowej jest minimalizacja bÅ‚Ä™du przez zmianÄ™ parametrÃ³w Î¸**

## Optymalizacja metodÄ… spadku gradientu

Istnieje dobrze znana metoda optymalizacji funkcji zwana **spadkiem gradientu**. Polega ona na tym, Å¼e moÅ¼emy obliczyÄ‡ pochodnÄ… (w przypadku wielowymiarowym nazywanÄ… **gradientem**) funkcji straty wzglÄ™dem parametrÃ³w i zmieniaÄ‡ parametry tak, aby bÅ‚Ä…d malaÅ‚. MoÅ¼na to sformalizowaÄ‡ nastÄ™pujÄ…co:

* Inicjalizujemy parametry losowymi wartoÅ›ciami w<sup>(0)</sup>, b<sup>(0)</sup>
* Powtarzamy wielokrotnie nastÄ™pujÄ…cy krok:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup> - Î· âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup> - Î· âˆ‚â„’/âˆ‚b

Podczas treningu kroki optymalizacji powinny byÄ‡ liczone na podstawie caÅ‚ego zbioru danych (pamiÄ™taj, Å¼e strata jest sumÄ… po wszystkich prÃ³bkach treningowych). Jednak w praktyce bierzemy maÅ‚e porcje danych zwane **minibatchami** i obliczamy gradienty na podstawie podzbioru danych. PoniewaÅ¼ podzbiÃ³r jest wybierany losowo za kaÅ¼dym razem, taka metoda nazywa siÄ™ **stochastycznym spadkiem gradientu** (SGD).

## Wielowarstwowe perceptrony i propagacja wsteczna

Jednowarstwowa sieÄ‡, jak widzieliÅ›my powyÅ¼ej, potrafi klasyfikowaÄ‡ klasy liniowo separowalne. Aby zbudowaÄ‡ bogatszy model, moÅ¼emy poÅ‚Ä…czyÄ‡ kilka warstw sieci. Matematycznie oznacza to, Å¼e funkcja *f* przyjmie bardziej zÅ‚oÅ¼onÄ… formÄ™ i bÄ™dzie obliczana w kilku krokach:
* z<sub>1</sub> = w<sub>1</sub>x + b<sub>1</sub>
* z<sub>2</sub> = w<sub>2</sub>Î±(z<sub>1</sub>) + b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Tutaj Î± to **nieliniowa funkcja aktywacji**, Ïƒ to funkcja softmax, a parametry Î¸ = âŸ¨*w<sub>1</sub>, b<sub>1</sub>, w<sub>2</sub>, b<sub>2</sub>*âŸ©.

Algorytm spadku gradientu pozostaje ten sam, ale obliczanie gradientÃ³w staje siÄ™ trudniejsze. Z wykorzystaniem reguÅ‚y Å‚aÅ„cuchowej rÃ³Å¼niczkowania moÅ¼emy obliczyÄ‡ pochodne jako:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… ReguÅ‚a Å‚aÅ„cuchowa jest uÅ¼ywana do obliczania pochodnych funkcji straty wzglÄ™dem parametrÃ³w.

ZauwaÅ¼, Å¼e lewostronna czÄ™Å›Ä‡ wszystkich tych wyraÅ¼eÅ„ jest taka sama, dziÄ™ki czemu moÅ¼emy efektywnie obliczaÄ‡ pochodne zaczynajÄ…c od funkcji straty i idÄ…c â€wsteczâ€ przez graf obliczeniowy. Metoda trenowania wielowarstwowego perceptronu nazywa siÄ™ **propagacjÄ… wstecznÄ…**, lub po prostu 'backprop'.

> TODO: cytowanie obrazka

> âœ… OmÃ³wimy backpropagation znacznie dokÅ‚adniej w naszym przykÅ‚adzie w notatniku.

## Podsumowanie

W tej lekcji stworzyliÅ›my wÅ‚asnÄ… bibliotekÄ™ sieci neuronowych i wykorzystaliÅ›my jÄ… do prostego zadania klasyfikacji dwuwymiarowej.

## ğŸš€ Wyzwanie

W doÅ‚Ä…czonym notatniku zaimplementujesz wÅ‚asny framework do budowy i trenowania wielowarstwowych perceptronÃ³w. BÄ™dziesz mÃ³gÅ‚ dokÅ‚adnie zobaczyÄ‡, jak dziaÅ‚ajÄ… nowoczesne sieci neuronowe.

PrzejdÅº do notatnika OwnFramework i przepracuj go.

## PowtÃ³rka i samodzielna nauka

Propagacja wsteczna to powszechny algorytm stosowany w AI i ML, warto zgÅ‚Ä™biÄ‡ go bardziej szczegÃ³Å‚owo.

## Zadanie

W tym laboratorium masz za zadanie wykorzystaÄ‡ framework, ktÃ³ry stworzyÅ‚eÅ› w tej lekcji, do rozwiÄ…zania problemu klasyfikacji rÄ™cznie pisanych cyfr MNIST.

* Instrukcje
* Notatnik

**ZastrzeÅ¼enie**:  
Niniejszy dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo Å¼e dÄ…Å¼ymy do jak najwiÄ™kszej dokÅ‚adnoÅ›ci, prosimy mieÄ‡ na uwadze, Å¼e tÅ‚umaczenia automatyczne mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jÄ™zyku ÅºrÃ³dÅ‚owym powinien byÄ‡ uznawany za ÅºrÃ³dÅ‚o autorytatywne. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnego tÅ‚umaczenia wykonanego przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z korzystania z tego tÅ‚umaczenia.