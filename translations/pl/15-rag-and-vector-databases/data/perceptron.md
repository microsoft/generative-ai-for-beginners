<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-07-09T16:57:24+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "pl"
}
-->
# Wprowadzenie do sieci neuronowych: Perceptron

Jedna z pierwszych prÃ³b implementacji czegoÅ› podobnego do wspÃ³Å‚czesnej sieci neuronowej zostaÅ‚a podjÄ™ta przez Franka Rosenblatta z Cornell Aeronautical Laboratory w 1957 roku. ByÅ‚a to implementacja sprzÄ™towa nazwana â€Mark-1â€, zaprojektowana do rozpoznawania prymitywnych figur geometrycznych, takich jak trÃ³jkÄ…ty, kwadraty i koÅ‚a.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> Obrazy z Wikipedii

Obraz wejÅ›ciowy byÅ‚ reprezentowany przez tablicÄ™ 20x20 fotokomÃ³rek, wiÄ™c sieÄ‡ neuronowa miaÅ‚a 400 wejÅ›Ä‡ i jedno wyjÅ›cie binarne. Prosta sieÄ‡ zawieraÅ‚a jeden neuron, zwany rÃ³wnieÅ¼ **jednostkÄ… logiki progowej**. Wagi sieci neuronowej dziaÅ‚aÅ‚y jak potencjometry, ktÃ³re wymagaÅ‚y rÄ™cznej regulacji podczas fazy treningu.

> âœ… Potencjometr to urzÄ…dzenie pozwalajÄ…ce uÅ¼ytkownikowi na regulacjÄ™ oporu w obwodzie.

> The New York Times pisaÅ‚ wtedy o perceptronie: *zarodek elektronicznego komputera, ktÃ³ry [Marynarka Wojenna] spodziewa siÄ™, Å¼e bÄ™dzie potrafiÅ‚ chodziÄ‡, mÃ³wiÄ‡, widzieÄ‡, pisaÄ‡, rozmnaÅ¼aÄ‡ siÄ™ i byÄ‡ Å›wiadomy swojego istnienia.*

## Model perceptronu

ZaÅ‚Ã³Å¼my, Å¼e mamy N cech w naszym modelu, w takim przypadku wektor wejÅ›ciowy bÄ™dzie wektorem o rozmiarze N. Perceptron to model **klasyfikacji binarnej**, czyli potrafi rozrÃ³Å¼niÄ‡ dwie klasy danych wejÅ›ciowych. ZaÅ‚oÅ¼ymy, Å¼e dla kaÅ¼dego wektora wejÅ›ciowego x wyjÅ›cie naszego perceptronu bÄ™dzie albo +1, albo -1, w zaleÅ¼noÅ›ci od klasy. WyjÅ›cie jest obliczane wedÅ‚ug wzoru:

y(x) = f(w<sup>T</sup>x)

gdzie f to funkcja aktywacji skokowej

## Trenowanie perceptronu

Aby wytrenowaÄ‡ perceptron, musimy znaleÅºÄ‡ wektor wag w, ktÃ³ry sklasyfikuje wiÄ™kszoÅ›Ä‡ wartoÅ›ci poprawnie, czyli da najmniejszy **bÅ‚Ä…d**. Ten bÅ‚Ä…d jest definiowany przez **kryterium perceptronu** w nastÄ™pujÄ…cy sposÃ³b:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

gdzie:

* suma jest liczona dla tych punktÃ³w treningowych i, ktÃ³re skutkujÄ… bÅ‚Ä™dnÄ… klasyfikacjÄ…
* x<sub>i</sub> to dane wejÅ›ciowe, a t<sub>i</sub> to -1 lub +1 dla przykÅ‚adÃ³w negatywnych i pozytywnych odpowiednio.

To kryterium traktujemy jako funkcjÄ™ wag w, ktÃ³rÄ… musimy zminimalizowaÄ‡. CzÄ™sto stosuje siÄ™ metodÄ™ zwanÄ… **spadkiem gradientu**, w ktÃ³rej zaczynamy od pewnych poczÄ…tkowych wag w<sup>(0)</sup>, a nastÄ™pnie na kaÅ¼dym kroku aktualizujemy wagi wedÅ‚ug wzoru:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

Tutaj Î· to tzw. **wspÃ³Å‚czynnik uczenia**, a âˆ‡E(w) oznacza **gradient** funkcji E. Po obliczeniu gradientu otrzymujemy

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Algorytm w Pythonie wyglÄ…da tak:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Podsumowanie

W tej lekcji dowiedziaÅ‚eÅ› siÄ™, czym jest perceptron, model klasyfikacji binarnej, oraz jak go wytrenowaÄ‡, uÅ¼ywajÄ…c wektora wag.

## ğŸš€ Wyzwanie

JeÅ›li chcesz sprÃ³bowaÄ‡ zbudowaÄ‡ wÅ‚asny perceptron, wyprÃ³buj to laboratorium na Microsoft Learn, ktÃ³re korzysta z Azure ML designer


## PrzeglÄ…d i samodzielna nauka

Aby zobaczyÄ‡, jak moÅ¼na uÅ¼yÄ‡ perceptronu do rozwiÄ…zania prostego problemu oraz problemÃ³w z Å¼ycia codziennego, i kontynuowaÄ‡ naukÄ™ â€“ przejdÅº do notatnika Perceptron.

Oto rÃ³wnieÅ¼ ciekawy artykuÅ‚ o perceptronach.

## Zadanie

W tej lekcji zaimplementowaliÅ›my perceptron do zadania klasyfikacji binarnej i uÅ¼yliÅ›my go do rozrÃ³Å¼nienia dwÃ³ch rÄ™cznie pisanych cyfr. W tym laboratorium masz za zadanie rozwiÄ…zaÄ‡ problem klasyfikacji cyfr w caÅ‚oÅ›ci, czyli okreÅ›liÄ‡, ktÃ³ra cyfra najprawdopodobniej odpowiada danemu obrazowi.

* Instrukcje
* Notatnik

**ZastrzeÅ¼enie**:  
Niniejszy dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo Å¼e dÄ…Å¼ymy do jak najwiÄ™kszej dokÅ‚adnoÅ›ci, prosimy mieÄ‡ na uwadze, Å¼e tÅ‚umaczenia automatyczne mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jÄ™zyku ÅºrÃ³dÅ‚owym powinien byÄ‡ uznawany za ÅºrÃ³dÅ‚o autorytatywne. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnego tÅ‚umaczenia wykonanego przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z korzystania z tego tÅ‚umaczenia.