<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2210a0466c812d9defc4df2d9a709ff9",
  "translation_date": "2026-01-18T18:13:17+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "pl"
}
-->
# Retrieval Augmented Generation (RAG) i bazy danych wektorowych

[![Retrieval Augmented Generation (RAG) i bazy danych wektorowych](../../../../../translated_images/pl/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

W lekcji o zastosowaniach wyszukiwania krÃ³tko omÃ³wiliÅ›my, jak zintegrowaÄ‡ wÅ‚asne dane z duÅ¼ymi modelami jÄ™zykowymi (LLM). W tej lekcji zagÅ‚Ä™bimy siÄ™ w koncepcje osadzania danych w aplikacji LLM, mechanikÄ™ procesu oraz metody przechowywania danych, obejmujÄ…ce zarÃ³wno osadzenia (embeddings), jak i tekst.

> **Wideo wkrÃ³tce**

## Wprowadzenie

W tej lekcji omÃ³wimy:

- Wprowadzenie do RAG â€” czym jest i dlaczego jest stosowane w sztucznej inteligencji (AI).

- Zrozumienie, czym sÄ… bazy danych wektorowych i stworzenie takiej dla naszej aplikacji.

- Praktyczny przykÅ‚ad integracji RAG w aplikacji.

## Cele nauki

Po ukoÅ„czeniu tej lekcji bÄ™dziesz w stanie:

- WyjaÅ›niÄ‡ znaczenie RAG w wyszukiwaniu i przetwarzaniu danych.

- SkonfigurowaÄ‡ aplikacjÄ™ RAG i osadziÄ‡ dane w LLM.

- Skutecznie zintegrowaÄ‡ RAG oraz bazy danych wektorowych w aplikacjach LLM.

## Nasz scenariusz: wzbogacenie LLM o wÅ‚asne dane

W tej lekcji chcemy dodaÄ‡ wÅ‚asne notatki do startupu edukacyjnego, co pozwoli chatbotowi uzyskaÄ‡ wiÄ™cej informacji na rÃ³Å¼ne tematy. KorzystajÄ…c z posiadanych notatek, uczniowie bÄ™dÄ… mogli lepiej siÄ™ uczyÄ‡ i rozumieÄ‡ rÃ³Å¼ne zagadnienia, co uÅ‚atwi im powtÃ³rki do egzaminÃ³w. Do stworzenia naszego scenariusza wykorzystamy:

- `Azure OpenAI:` LLM, ktÃ³rego uÅ¼yjemy do stworzenia chatbota

- `LekcjÄ™ "AI dla poczÄ…tkujÄ…cych" o sieciach neuronowych:` to bÄ™dÄ… dane, na ktÃ³rych osadzimy nasz LLM

- `Azure AI Search` i `Azure Cosmos DB:` baza danych wektorowych do przechowywania danych oraz tworzenia indeksu wyszukiwawczego

UÅ¼ytkownicy bÄ™dÄ… mogli tworzyÄ‡ quizy na podstawie notatek, fiszki do powtÃ³rek i streszczenia w zwiÄ™zÅ‚ej formie. Zanim zaczniemy, przyjrzyjmy siÄ™, czym jest RAG i jak dziaÅ‚a:

## Retrieval Augmented Generation (RAG)

Chatbot oparty o LLM przetwarza teksty uÅ¼ytkownikÃ³w, by generowaÄ‡ odpowiedzi. Jest zaprojektowany do interakcji i prowadzenia rozmÃ³w na szeroki zakres tematÃ³w. Jednak jego odpowiedzi sÄ… ograniczone do dostarczonego kontekstu i danych, na ktÃ³rych zostaÅ‚ wytrenowany. Na przykÅ‚ad, GPT-4 ma cutoff wiedzy na wrzesieÅ„ 2021, co oznacza, Å¼e nie zna wydarzeÅ„ po tej dacie. Ponadto dane uÅ¼ywane do trenowania LLM wykluczajÄ… poufne informacje jak notatki osobiste czy instrukcje obsÅ‚ugi firmy.

### Jak dziaÅ‚ajÄ… RAG (Retrieval Augmented Generation)

![rysunek przedstawiajÄ…cy dziaÅ‚anie RAG](../../../../../translated_images/pl/how-rag-works.f5d0ff63942bd3a6.webp)

ZaÅ‚Ã³Å¼my, Å¼e chcesz wdroÅ¼yÄ‡ chatbota tworzÄ…cego quizy na podstawie twoich notatek; potrzebujesz poÅ‚Ä…czenia z bazÄ… wiedzy. Tu z pomocÄ… przychodzi RAG. RAG dziaÅ‚a nastÄ™pujÄ…co:

- **Baza wiedzy:** Przed pobraniem, dokumenty muszÄ… zostaÄ‡ wczytane i przetworzone, zwykle dzielÄ…c duÅ¼e dokumenty na mniejsze fragmenty, konwertujÄ…c je do osadzeÅ„ tekstowych i przechowujÄ…c w bazie danych.

- **Zapytanie uÅ¼ytkownika:** uÅ¼ytkownik zadaje pytanie

- **Pobieranie (Retrieval):** W momencie zapytania, model osadzajÄ…cy pobiera istotne informacje z bazy wiedzy, aby dostarczyÄ‡ kontekst, ktÃ³ry zostanie doÅ‚Ä…czony do promptu.

- **Generowanie z uzupeÅ‚nieniem (Augmented Generation):** LLM ulepsza swojÄ… odpowiedÅº na podstawie pobranych danych. Pozwala to generowaÄ‡ odpowiedzi nie tylko oparte na danych treningowych, ale teÅ¼ na dodanym kontekÅ›cie. Pobierane dane wzbogacajÄ… odpowiedzi LLM, ktÃ³ry nastÄ™pnie zwraca odpowiedÅº na pytanie uÅ¼ytkownika.

![rysunek przedstawiajÄ…cy architekturÄ™ RAG](../../../../../translated_images/pl/encoder-decode.f2658c25d0eadee2.webp)

Architektura RAG jest realizowana za pomocÄ… transformatorÃ³w, skÅ‚adajÄ…cych siÄ™ z dwÃ³ch czÄ™Å›ci: enkodera i dekodera. Na przykÅ‚ad, gdy uÅ¼ytkownik zadaje pytanie, tekst wejÅ›ciowy jest "zakodowany" na wektory oddajÄ…ce znaczenie sÅ‚Ã³w, a nastÄ™pnie te wektory sÄ… "dekodowane" w indeks dokumentÃ³w i generujÄ… nowy tekst na podstawie zapytania uÅ¼ytkownika. LLM wykorzystuje model enkoder-dekoder do wygenerowania odpowiedzi.

Dwa podejÅ›cia do implementacji RAG wedÅ‚ug zaproponowanego artykuÅ‚u: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) to:

- **_RAG-Sequence_** uÅ¼ywajÄ…cy pobranych dokumentÃ³w do przewidzenia najlepszej moÅ¼liwej odpowiedzi na zapytanie uÅ¼ytkownika

- **RAG-Token** uÅ¼ywajÄ…cy dokumentÃ³w do generowania nastÄ™pnego tokenu, po czym ponownie pobierajÄ…cy dokumenty do odpowiedzi na zapytanie uÅ¼ytkownika

### Dlaczego warto uÅ¼ywaÄ‡ RAG?Â 

- **Bogactwo informacji:** zapewnia, Å¼e odpowiedzi tekstowe sÄ… aktualne i na bieÅ¼Ä…co. Poprawia wydajnoÅ›Ä‡ w zadaniach domenowych, uzyskujÄ…c dostÄ™p do wewnÄ™trznej bazy wiedzy.

- Ogranicza wymyÅ›lanie informacji dziÄ™ki wykorzystaniu **weryfikowalnych danych** w bazie wiedzy, by dostarczyÄ‡ kontekst do zapytaÅ„ uÅ¼ytkownika.

- Jest **opÅ‚acalne**, gdyÅ¼ jest taÅ„sze niÅ¼ dostrajanie (fine-tuning) LLM.

## Tworzenie bazy wiedzy

Nasza aplikacja bazuje na naszych osobistych danych, tj. lekcji o sieciach neuronowych z kursu AI dla poczÄ…tkujÄ…cych.

### Bazy danych wektorowych

Baza danych wektorowych, w przeciwieÅ„stwie do tradycyjnych baz, jest specjalistycznÄ… bazÄ… zaprojektowanÄ… do przechowywania, zarzÄ…dzania i wyszukiwania osadzonych wektorÃ³w. Przechowuje numeryczne reprezentacje dokumentÃ³w. RozÅ‚oÅ¼enie danych na osadzenia numeryczne uÅ‚atwia systemowi AI zrozumienie i przetwarzanie danych.

Przechowujemy nasze osadzenia w bazach danych wektorowych, poniewaÅ¼ LLM majÄ… limit tokenÃ³w przyjmowanych jako wejÅ›cie. Nie moÅ¼na przekazaÄ‡ caÅ‚ych osadzeÅ„ do LLM, wiÄ™c trzeba je podzieliÄ‡ na fragmenty, a gdy uÅ¼ytkownik zada pytanie, zwrÃ³cone zostanÄ… osadzenia najbardziej odpowiadajÄ…ce pytaniu, razem z promptem. PodziaÅ‚ na fragmenty (chunking) takÅ¼e redukuje koszty zwiÄ…zane z liczbÄ… tokenÃ³w przetwarzanych przez LLM.

Popularne bazy danych wektorowych to Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant i DeepLake. Model Azure Cosmos DB moÅ¼na utworzyÄ‡ za pomocÄ… Azure CLI poleceniem:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od tekstu do osadzeÅ„

Zanim przechowamy dane, musimy je konwertowaÄ‡ do osadzeÅ„ wektorowych przed zapisem w bazie. PracujÄ…c z duÅ¼ymi dokumentami lub dÅ‚ugimi tekstami, moÅ¼na je dzieliÄ‡ na fragmenty wedÅ‚ug przewidywanych zapytaÅ„. PodziaÅ‚ moÅ¼na robiÄ‡ na poziomie zdania lub akapitu. PoniewaÅ¼ podziaÅ‚ opiera siÄ™ na znaczeniu sÅ‚Ã³w w ich otoczeniu, moÅ¼na dodaÄ‡ dodatkowy kontekst do fragmentu, np. tytuÅ‚ dokumentu lub trochÄ™ tekstu przed i po fragmencie. MoÅ¼na dzieliÄ‡ dane w ten sposÃ³b:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # JeÅ›li ostatnia czÄ™Å›Ä‡ nie osiÄ…gnÄ™Å‚a minimalnej dÅ‚ugoÅ›ci, dodaj jÄ… mimo to
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Po podziale moÅ¼emy osadziÄ‡ tekst korzystajÄ…c z rÃ³Å¼nych modeli osadzajÄ…cych. Do wyboru sÄ… modele takie jak: word2vec, ada-002 od OpenAI, Azure Computer Vision i wiele innych. WybÃ³r modelu zaleÅ¼y od jÄ™zykÃ³w, typu kodowanej treÅ›ci (tekst/obraz/audio), rozmiaru wejÅ›cia, ktÃ³re moÅ¼e zakodowaÄ‡ oraz dÅ‚ugoÅ›ci osadzenia na wyjÅ›ciu.

PrzykÅ‚ad osadzonego tekstu za pomocÄ… modelu OpenAI `text-embedding-ada-002`:

![osadzenie sÅ‚owa cat](../../../../../translated_images/pl/cat.74cbd7946bc9ca38.webp)

## Wyszukiwanie i wyszukiwanie wektorowe

Gdy uÅ¼ytkownik zada pytanie, pobieracz (retriever) konwertuje je na wektor za pomocÄ… enkodera zapytania, a nastÄ™pnie przeszukuje nasz indeks dokumentÃ³w, znajdujÄ…c wektory powiÄ…zane z zapytaniem. Po ukoÅ„czeniu konwertuje zarÃ³wno wektor zapytania, jak i wektory dokumentÃ³w na tekst i przesyÅ‚a go do LLM.

### Pobieranie (Retrieval)

Pobieranie ma miejsce, gdy system szybko prÃ³buje znaleÅºÄ‡ dokumenty z indeksu speÅ‚niajÄ…ce kryteria wyszukiwania. Celem pobieracza jest uzyskanie dokumentÃ³w, ktÃ³re posÅ‚uÅ¼Ä… za kontekst i osadzenie LLM na twoich danych.

Istnieje kilka sposobÃ³w wyszukiwania w bazie danych, m.in.:

- **Wyszukiwanie sÅ‚Ã³w kluczowych** â€“ uÅ¼ywane do wyszukiwaÅ„ tekstowych

- **Wyszukiwanie wektorowe** â€“ konwertuje dokumenty z tekstu na reprezentacje wektorowe przy pomocy modeli osadzajÄ…cych, umoÅ¼liwiajÄ…c **wyszukiwanie semantyczne** na bazie znaczenia sÅ‚Ã³w. Pobieranie polega na zapytaniu dokumentÃ³w, ktÃ³rych wektorowe reprezentacje sÄ… najbliÅ¼sze pytaniu uÅ¼ytkownika.

- **Hybrydowe** â€“ poÅ‚Ä…czenie obu metod: wyszukiwania sÅ‚Ã³w kluczowych i wektorowego.

Problem pojawia siÄ™, gdy w bazie nie ma podobnej odpowiedzi na zapytanie â€” system wtedy zwraca najlepszÄ… dostÄ™pnÄ… informacjÄ™. MoÅ¼na jednak zastosowaÄ‡ taktyki jak ustawienie maksymalnej odlegÅ‚oÅ›ci dla relewantnoÅ›ci czy uÅ¼ycie wyszukiwania hybrydowego Å‚Ä…czÄ…cego sÅ‚owa kluczowe i wektorowe. W tej lekcji uÅ¼yjemy wyszukiwania hybrydowego, Å‚Ä…czÄ…cego obie metody. Dane bÄ™dziemy przechowywaÄ‡ w dataframe z kolumnami zawierajÄ…cymi fragmenty oraz osadzenia.

### PodobieÅ„stwo wektorÃ³w

Pobieracz przeszukuje bazÄ™ wiedzy w poszukiwaniu osadzeÅ„ blisko poÅ‚oÅ¼onych, czyli najbliÅ¼szych sÄ…siadÃ³w, poniewaÅ¼ majÄ… one podobny tekst. W scenariuszu, gdy uÅ¼ytkownik zadaje zapytanie, najpierw jest ono osadzone, a potem dopasowywane do podobnych osadzeÅ„. NajczÄ™Å›ciej stosowanÄ… miarÄ… podobieÅ„stwa jest podobieÅ„stwo cosinusowe, oparte na kÄ…cie miÄ™dzy dwoma wektorami.

Alternatywnie moÅ¼na zastosowaÄ‡ odlegÅ‚oÅ›Ä‡ euklidesowÄ…, czyli liniÄ™ prostÄ… miÄ™dzy koÅ„cami wektorÃ³w, lub iloczyn skalarny, mierzÄ…cy sumÄ™ iloczynÃ³w odpowiadajÄ…cych sobie elementÃ³w obu wektorÃ³w.

### Indeks wyszukiwania

Przed wykonaniem wyszukiwania musimy zbudowaÄ‡ indeks wyszukiwania dla naszej bazy wiedzy. Indeks przechowuje osadzenia i pozwala szybko odnajdowaÄ‡ najbardziej zbliÅ¼one fragmenty nawet w duÅ¼ej bazie danych. Indeks lokalnie moÅ¼na stworzyÄ‡ za pomocÄ…:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# UtwÃ³rz indeks wyszukiwania
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Aby zapytaÄ‡ indeks, moÅ¼esz uÅ¼yÄ‡ metody kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```

### Ponowne sortowanie wynikÃ³w (re-ranking)

Po zapytaniu bazy moÅ¼esz chcieÄ‡ posortowaÄ‡ wyniki od najbardziej relewantnych. Model rerankujÄ…cy LLM wykorzystuje uczenie maszynowe, by poprawiÄ‡ relewantnoÅ›Ä‡ wynikÃ³w, ukÅ‚adajÄ…c je od najlepiej pasujÄ…cych. W Azure AI Search reranking wykonywany jest automatycznie przez semantyczny re-ranker. PrzykÅ‚ad dziaÅ‚ania rerankingu z uÅ¼yciem najbliÅ¼szych sÄ…siadÃ³w:

```python
# ZnajdÅº najbardziej podobne dokumenty
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Wydrukuj najbardziej podobne dokumenty
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## PoÅ‚Ä…czenie wszystkiego razem

Ostatnim krokiem jest dodanie naszego LLM, aby uzyskaÄ‡ odpowiedzi osadzone w danych. MoÅ¼emy to zaimplementowaÄ‡ nastÄ™pujÄ…co:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # ZamieÅ„ pytanie na wektor zapytania
    query_vector = create_embeddings(user_input)

    # ZnajdÅº najbardziej podobne dokumenty
    distances, indices = nbrs.kneighbors([query_vector])

    # dodaj dokumenty do zapytania, aby zapewniÄ‡ kontekst
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # poÅ‚Ä…cz historiÄ™ z danymi od uÅ¼ytkownika
    history.append(user_input)

    # utwÃ³rz obiekt wiadomoÅ›ci
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # uÅ¼yj uzupeÅ‚niania czatu, aby wygenerowaÄ‡ odpowiedÅº
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Ocena naszej aplikacji

### Metryki oceny

- JakoÅ›Ä‡ odpowiedzi â€” brzmi naturalnie, pÅ‚ynnie i jak od czÅ‚owieka

- Osadzenie w danych: ocena, czy odpowiedÅº pochodzi z dostarczonych dokumentÃ³w

- TrafnoÅ›Ä‡: ocena, czy odpowiedÅº odpowiada i jest zwiÄ…zana z zadanym pytaniem

- PÅ‚ynnoÅ›Ä‡ â€” czy odpowiedÅº jest poprawna gramatycznie i sensowna

## PrzykÅ‚ady uÅ¼ycia RAG i baz danych wektorowych

Istnieje wiele przypadkÃ³w, gdzie wywoÅ‚ania funkcji mogÄ… ulepszyÄ‡ aplikacjÄ™, np.:

- Pytania i odpowiedzi: osadzenie danych firmowych w czacie, z ktÃ³rego pracownicy mogÄ… korzystaÄ‡ pytajÄ…c o informacje.

- Systemy rekomendacji: tworzenie systemÃ³w dopasowujÄ…cych najbardziej podobne wartoÅ›ci, np. filmy, restauracje i inne.

- UsÅ‚ugi chatbotÃ³w: przechowywanie historii czatÃ³w i personalizacja rozmowy na podstawie danych uÅ¼ytkownika.

- Wyszukiwanie obrazÃ³w na bazie osadzeÅ„ wektorowych, przydatne w rozpoznawaniu obrazÃ³w i wykrywaniu anomalii.

## Podsumowanie

OmÃ³wiliÅ›my podstawowe aspekty RAG: dodanie danych do aplikacji, zapytanie uÅ¼ytkownika oraz generowanie odpowiedzi. Aby uproÅ›ciÄ‡ tworzenie RAG, moÅ¼na uÅ¼ywaÄ‡ frameworkÃ³w jak Semantic Kernel, Langchain czy Autogen.

## Zadanie

Aby kontynuowaÄ‡ naukÄ™ Retrieval Augmented Generation (RAG) moÅ¼esz:

- ZbudowaÄ‡ front-end aplikacji uÅ¼ywajÄ…c wybranego frameworka

- WykorzystaÄ‡ framework, np. LangChain lub Semantic Kernel, i odtworzyÄ‡ aplikacjÄ™.

Gratulacje za ukoÅ„czenie lekcji ğŸ‘.

## Nauka siÄ™ nie koÅ„czy, kontynuuj podrÃ³Å¼

Po ukoÅ„czeniu tej lekcji zapoznaj siÄ™ z naszÄ… [kolekcjÄ… Generative AI Learning](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby dalej rozwijaÄ‡ swojÄ… wiedzÄ™ z zakresu generatywnej AI!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ZastrzeÅ¼enie**:  
Niniejszy dokument zostaÅ‚ przetÅ‚umaczony przy uÅ¼yciu automatycznej usÅ‚ugi tÅ‚umaczeniowej AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ dokÅ‚adamy staraÅ„, aby tÅ‚umaczenie byÅ‚o precyzyjne, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego jÄ™zyku ÅºrÃ³dÅ‚owym powinien byÄ‡ uznawany za ÅºrÃ³dÅ‚o nadrzÄ™dne. W przypadku informacji o znaczeniu krytycznym zaleca siÄ™ skorzystanie z profesjonalnego, ludzkiego tÅ‚umaczenia. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z korzystania z tego tÅ‚umaczenia.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->