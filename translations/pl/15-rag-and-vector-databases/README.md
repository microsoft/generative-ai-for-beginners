<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-07-09T16:11:46+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "pl"
}
-->
# Retrieval Augmented Generation (RAG) i bazy danych wektorowych

[![Retrieval Augmented Generation (RAG) i bazy danych wektorowych](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.pl.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

W lekcji o aplikacjach wyszukiwania krÃ³tko omÃ³wiliÅ›my, jak zintegrowaÄ‡ wÅ‚asne dane z duÅ¼ymi modelami jÄ™zykowymi (LLM). W tej lekcji zagÅ‚Ä™bimy siÄ™ w koncepcjÄ™ osadzania danych w aplikacji LLM, mechanikÄ™ tego procesu oraz metody przechowywania danych, obejmujÄ…ce zarÃ³wno embeddingi, jak i tekst.

> **Wideo wkrÃ³tce**

## Wprowadzenie

W tej lekcji omÃ³wimy:

- Wprowadzenie do RAG, czym jest i dlaczego jest wykorzystywane w AI (sztucznej inteligencji).

- Zrozumienie, czym sÄ… bazy danych wektorowych i jak stworzyÄ‡ jednÄ… dla naszej aplikacji.

- Praktyczny przykÅ‚ad integracji RAG w aplikacji.

## Cele nauki

Po ukoÅ„czeniu tej lekcji bÄ™dziesz potrafiÅ‚:

- WyjaÅ›niÄ‡ znaczenie RAG w wyszukiwaniu i przetwarzaniu danych.

- SkonfigurowaÄ‡ aplikacjÄ™ RAG i osadziÄ‡ swoje dane w LLM.

- Skutecznie integrowaÄ‡ RAG i bazy danych wektorowych w aplikacjach LLM.

## Nasz scenariusz: wzbogacanie naszych LLM o wÅ‚asne dane

W tej lekcji chcemy dodaÄ‡ wÅ‚asne notatki do startupu edukacyjnego, co pozwoli chatbotowi uzyskaÄ‡ wiÄ™cej informacji na rÃ³Å¼ne tematy. KorzystajÄ…c z naszych notatek, uczniowie bÄ™dÄ… mogli lepiej siÄ™ uczyÄ‡ i rozumieÄ‡ rÃ³Å¼ne zagadnienia, co uÅ‚atwi im powtÃ³rki przed egzaminami. Do stworzenia naszego scenariusza wykorzystamy:

- `Azure OpenAI:` LLM, ktÃ³rego uÅ¼yjemy do stworzenia chatbota

- `LekcjÄ™ AI dla poczÄ…tkujÄ…cych o sieciach neuronowych:` to bÄ™dÄ… dane, na ktÃ³rych osadzimy nasz LLM

- `Azure AI Search` i `Azure Cosmos DB:` baza danych wektorowych do przechowywania danych i tworzenia indeksu wyszukiwania

UÅ¼ytkownicy bÄ™dÄ… mogli tworzyÄ‡ quizy na podstawie swoich notatek, fiszki do powtÃ³rek oraz podsumowania w formie zwiÄ™zÅ‚ych przeglÄ…dÃ³w. Zacznijmy od wyjaÅ›nienia, czym jest RAG i jak dziaÅ‚a:

## Retrieval Augmented Generation (RAG)

Chatbot oparty na LLM przetwarza zapytania uÅ¼ytkownika, aby generowaÄ‡ odpowiedzi. Jest zaprojektowany tak, by byÄ‡ interaktywny i angaÅ¼owaÄ‡ siÄ™ w rozmowy na rÃ³Å¼ne tematy. Jednak jego odpowiedzi sÄ… ograniczone do kontekstu dostarczonego oraz danych, na ktÃ³rych byÅ‚ trenowany. Na przykÅ‚ad, GPT-4 ma cutoff wiedzy na wrzesieÅ„ 2021, co oznacza, Å¼e nie zna wydarzeÅ„ po tej dacie. Ponadto dane uÅ¼yte do trenowania LLM nie zawierajÄ… poufnych informacji, takich jak prywatne notatki czy instrukcje produktÃ³w firmy.

### Jak dziaÅ‚ajÄ… RAG (Retrieval Augmented Generation)

![rysunek pokazujÄ…cy, jak dziaÅ‚ajÄ… RAG](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.pl.png)

ZaÅ‚Ã³Å¼my, Å¼e chcesz wdroÅ¼yÄ‡ chatbota, ktÃ³ry tworzy quizy na podstawie twoich notatek â€” potrzebujesz wtedy poÅ‚Ä…czenia z bazÄ… wiedzy. Tu z pomocÄ… przychodzi RAG. RAG dziaÅ‚a w nastÄ™pujÄ…cy sposÃ³b:

- **Baza wiedzy:** Przed wyszukiwaniem dokumenty muszÄ… zostaÄ‡ zaÅ‚adowane i przetworzone, zwykle dzielÄ…c duÅ¼e dokumenty na mniejsze fragmenty, przeksztaÅ‚cajÄ…c je w embeddingi tekstowe i przechowujÄ…c w bazie danych.

- **Zapytanie uÅ¼ytkownika:** uÅ¼ytkownik zadaje pytanie

- **Wyszukiwanie:** Gdy uÅ¼ytkownik zada pytanie, model embeddingowy wyszukuje odpowiednie informacje w bazie wiedzy, aby dostarczyÄ‡ wiÄ™cej kontekstu, ktÃ³ry zostanie doÅ‚Ä…czony do promptu.

- **Generowanie z rozszerzeniem:** LLM wzbogaca swojÄ… odpowiedÅº na podstawie pobranych danych. Pozwala to na generowanie odpowiedzi nie tylko na podstawie danych wytrenowanych, ale takÅ¼e na podstawie istotnych informacji z dodanego kontekstu. Pobranie danych sÅ‚uÅ¼y do rozszerzenia odpowiedzi LLM. NastÄ™pnie LLM zwraca odpowiedÅº na pytanie uÅ¼ytkownika.

![rysunek pokazujÄ…cy architekturÄ™ RAG](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.pl.png)

Architektura RAG jest realizowana za pomocÄ… transformatorÃ³w skÅ‚adajÄ…cych siÄ™ z dwÃ³ch czÄ™Å›ci: enkodera i dekodera. Na przykÅ‚ad, gdy uÅ¼ytkownik zada pytanie, tekst wejÅ›ciowy jest â€zakodowanyâ€ do wektorÃ³w, ktÃ³re uchwytujÄ… znaczenie sÅ‚Ã³w, a wektory sÄ… â€dekodowaneâ€ do naszego indeksu dokumentÃ³w i generujÄ… nowy tekst na podstawie zapytania uÅ¼ytkownika. LLM uÅ¼ywa modelu enkoder-dekoder do generowania odpowiedzi.

Dwa podejÅ›cia do implementacji RAG wedÅ‚ug proponowanego artykuÅ‚u: [Retrieval-Augmented Generation for Knowledge intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) to:

- **_RAG-Sequence_** wykorzystujÄ…cy pobrane dokumenty do przewidzenia najlepszej moÅ¼liwej odpowiedzi na zapytanie uÅ¼ytkownika

- **RAG-Token** wykorzystujÄ…cy dokumenty do generowania kolejnego tokena, a nastÄ™pnie pobierajÄ…cy je, by odpowiedzieÄ‡ na zapytanie uÅ¼ytkownika

### Dlaczego warto uÅ¼ywaÄ‡ RAG?

- **Bogactwo informacji:** zapewnia, Å¼e odpowiedzi tekstowe sÄ… aktualne i zgodne z najnowszymi danymi. DziÄ™ki temu poprawia wydajnoÅ›Ä‡ w zadaniach specyficznych dla danej dziedziny, uzyskujÄ…c dostÄ™p do wewnÄ™trznej bazy wiedzy.

- Redukuje wymyÅ›lanie informacji, wykorzystujÄ…c **weryfikowalne dane** z bazy wiedzy, ktÃ³re dostarczajÄ… kontekst do zapytaÅ„ uÅ¼ytkownikÃ³w.

- Jest **opÅ‚acalne**, poniewaÅ¼ jest taÅ„sze niÅ¼ dostrajanie (fine-tuning) LLM.

## Tworzenie bazy wiedzy

Nasza aplikacja opiera siÄ™ na naszych osobistych danych, tj. lekcji o sieciach neuronowych z kursu AI dla poczÄ…tkujÄ…cych.

### Bazy danych wektorowych

Baza danych wektorowych, w przeciwieÅ„stwie do tradycyjnych baz, to specjalistyczna baza zaprojektowana do przechowywania, zarzÄ…dzania i wyszukiwania osadzonych wektorÃ³w. Przechowuje numeryczne reprezentacje dokumentÃ³w. Rozbicie danych na embeddingi numeryczne uÅ‚atwia naszemu systemowi AI zrozumienie i przetwarzanie danych.

Przechowujemy embeddingi w bazach danych wektorowych, poniewaÅ¼ LLM majÄ… limit liczby tokenÃ³w, ktÃ³re mogÄ… przyjÄ…Ä‡ jako wejÅ›cie. Nie moÅ¼na przekazaÄ‡ caÅ‚ych embeddingÃ³w do LLM, dlatego musimy je podzieliÄ‡ na fragmenty, a gdy uÅ¼ytkownik zada pytanie, zwracane sÄ… embeddingi najbardziej pasujÄ…ce do pytania wraz z promptem. Dzielenie na fragmenty rÃ³wnieÅ¼ zmniejsza koszty zwiÄ…zane z liczbÄ… tokenÃ³w przekazywanych do LLM.

Popularne bazy danych wektorowych to Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant i DeepLake. MoÅ¼esz stworzyÄ‡ model Azure Cosmos DB za pomocÄ… Azure CLI, uÅ¼ywajÄ…c nastÄ™pujÄ…cego polecenia:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od tekstu do embeddingÃ³w

Przed przechowywaniem danych musimy przekonwertowaÄ‡ je na embeddingi wektorowe. JeÅ›li pracujesz z duÅ¼ymi dokumentami lub dÅ‚ugimi tekstami, moÅ¼esz je podzieliÄ‡ na fragmenty w oparciu o spodziewane zapytania. Dzielenie moÅ¼na wykonaÄ‡ na poziomie zdaÅ„ lub akapitÃ³w. PoniewaÅ¼ fragmenty czerpiÄ… znaczenie z otaczajÄ…cych sÅ‚Ã³w, moÅ¼esz dodaÄ‡ do nich dodatkowy kontekst, np. tytuÅ‚ dokumentu lub tekst przed lub po fragmencie. MoÅ¼esz podzieliÄ‡ dane w nastÄ™pujÄ…cy sposÃ³b:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Po podzieleniu moÅ¼emy osadziÄ‡ tekst, korzystajÄ…c z rÃ³Å¼nych modeli embeddingowych. NiektÃ³re modele, ktÃ³re moÅ¼esz wykorzystaÄ‡, to: word2vec, ada-002 od OpenAI, Azure Computer Vision i wiele innych. WybÃ³r modelu zaleÅ¼y od jÄ™zykÃ³w, ktÃ³rych uÅ¼ywasz, rodzaju kodowanej zawartoÅ›ci (tekst/obraz/audio), rozmiaru wejÅ›cia, ktÃ³re moÅ¼e zakodowaÄ‡, oraz dÅ‚ugoÅ›ci wyjÅ›cia embeddingu.

PrzykÅ‚ad embeddingu tekstu z uÅ¼yciem modelu OpenAI `text-embedding-ada-002`:

![embedding sÅ‚owa cat](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.pl.png)

## Wyszukiwanie i wyszukiwanie wektorowe

Gdy uÅ¼ytkownik zada pytanie, retriever przeksztaÅ‚ca je w wektor za pomocÄ… enkodera zapytaÅ„, a nastÄ™pnie przeszukuje indeks dokumentÃ³w w poszukiwaniu wektorÃ³w powiÄ…zanych z zapytaniem. Po znalezieniu konwertuje zarÃ³wno wektor zapytania, jak i wektory dokumentÃ³w na tekst i przekazuje je do LLM.

### Wyszukiwanie

Wyszukiwanie odbywa siÄ™, gdy system prÃ³buje szybko znaleÅºÄ‡ dokumenty w indeksie speÅ‚niajÄ…ce kryteria wyszukiwania. Celem retrievera jest pobranie dokumentÃ³w, ktÃ³re posÅ‚uÅ¼Ä… do dostarczenia kontekstu i osadzenia LLM na twoich danych.

Istnieje kilka sposobÃ³w wyszukiwania w bazie danych, takich jak:

- **Wyszukiwanie sÅ‚Ã³w kluczowych** â€“ uÅ¼ywane do wyszukiwania tekstowego

- **Wyszukiwanie semantyczne** â€“ wykorzystuje znaczenie semantyczne sÅ‚Ã³w

- **Wyszukiwanie wektorowe** â€“ konwertuje dokumenty z tekstu na reprezentacje wektorowe za pomocÄ… modeli embeddingowych. Wyszukiwanie odbywa siÄ™ przez zapytanie o dokumenty, ktÃ³rych reprezentacje wektorowe sÄ… najbliÅ¼sze pytaniu uÅ¼ytkownika.

- **Hybrydowe** â€“ poÅ‚Ä…czenie wyszukiwania sÅ‚Ã³w kluczowych i wektorowego.

Problem z wyszukiwaniem pojawia siÄ™, gdy w bazie nie ma podobnej odpowiedzi do zapytania â€” system wtedy zwraca najlepsze dostÄ™pne informacje. MoÅ¼esz jednak zastosowaÄ‡ takie metody jak ustawienie maksymalnej odlegÅ‚oÅ›ci dla trafnoÅ›ci lub uÅ¼yÄ‡ wyszukiwania hybrydowego Å‚Ä…czÄ…cego sÅ‚owa kluczowe i wyszukiwanie wektorowe. W tej lekcji uÅ¼yjemy wyszukiwania hybrydowego, czyli poÅ‚Ä…czenia obu metod. Dane przechowamy w dataframe z kolumnami zawierajÄ…cymi fragmenty oraz embeddingi.

### PodobieÅ„stwo wektorÃ³w

Retriever bÄ™dzie przeszukiwaÅ‚ bazÄ™ wiedzy w poszukiwaniu embeddingÃ³w, ktÃ³re sÄ… blisko siebie, czyli najbliÅ¼szych sÄ…siadÃ³w, poniewaÅ¼ sÄ… to teksty podobne. W scenariuszu, gdy uÅ¼ytkownik zada pytanie, jest ono najpierw osadzone, a nastÄ™pnie dopasowane do podobnych embeddingÃ³w. NajczÄ™Å›ciej stosowanÄ… miarÄ… podobieÅ„stwa wektorÃ³w jest podobieÅ„stwo cosinusowe, oparte na kÄ…cie miÄ™dzy dwoma wektorami.

MoÅ¼emy teÅ¼ mierzyÄ‡ podobieÅ„stwo innymi metodami, takimi jak odlegÅ‚oÅ›Ä‡ euklidesowa (prosta linia miÄ™dzy koÅ„cami wektorÃ³w) oraz iloczyn skalarny (suma iloczynÃ³w odpowiadajÄ…cych sobie elementÃ³w dwÃ³ch wektorÃ³w).

### Indeks wyszukiwania

Przed wyszukiwaniem musimy zbudowaÄ‡ indeks wyszukiwania dla naszej bazy wiedzy. Indeks przechowuje embeddingi i pozwala szybko odnaleÅºÄ‡ najbardziej podobne fragmenty nawet w duÅ¼ej bazie danych. MoÅ¼emy stworzyÄ‡ indeks lokalnie, uÅ¼ywajÄ…c:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Ponowne sortowanie (re-ranking)

Po wykonaniu zapytania do bazy danych moÅ¼e byÄ‡ konieczne posortowanie wynikÃ³w od najbardziej trafnych. Re-ranking LLM wykorzystuje uczenie maszynowe, aby poprawiÄ‡ trafnoÅ›Ä‡ wynikÃ³w, ukÅ‚adajÄ…c je od najbardziej do najmniej istotnych. KorzystajÄ…c z Azure AI Search, re-ranking jest wykonywany automatycznie za pomocÄ… semantycznego rerankera. PrzykÅ‚ad dziaÅ‚ania re-rankingu z uÅ¼yciem najbliÅ¼szych sÄ…siadÃ³w:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## ÅÄ…czenie wszystkiego w caÅ‚oÅ›Ä‡

Ostatnim krokiem jest dodanie naszego LLM, aby uzyskaÄ‡ odpowiedzi oparte na naszych danych. MoÅ¼emy to zaimplementowaÄ‡ w nastÄ™pujÄ…cy sposÃ³b:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Ocena naszej aplikacji

### Metryki oceny

- JakoÅ›Ä‡ dostarczanych odpowiedzi â€“ czy brzmiÄ… naturalnie, pÅ‚ynnie i jak od czÅ‚owieka

- Osadzenie danych: ocena, czy odpowiedÅº pochodzi z dostarczonych dokumentÃ³w

- TrafnoÅ›Ä‡: ocena, czy odpowiedÅº pasuje i jest powiÄ…zana z zadanym pytaniem

- PÅ‚ynnoÅ›Ä‡ â€“ czy odpowiedÅº jest poprawna gramatycznie i zrozumiaÅ‚a

## Przypadki uÅ¼ycia RAG i baz danych wektorowych

Istnieje wiele zastosowaÅ„, gdzie wywoÅ‚ania funkcji mogÄ… usprawniÄ‡ twojÄ… aplikacjÄ™, np.:

- Pytania i odpowiedzi: osadzenie danych firmy w czacie, z ktÃ³rego mogÄ… korzystaÄ‡ pracownicy, zadajÄ…c pytania.

- Systemy rekomendacyjne: tworzenie systemÃ³w dopasowujÄ…cych najbardziej podobne wartoÅ›ci, np. filmy, restauracje i inne.

- UsÅ‚ugi chatbotÃ³w: przechowywanie historii rozmÃ³w i personalizacja konwersacji na podstawie danych uÅ¼ytkownika.

- Wyszukiwanie obrazÃ³w na podstawie embeddingÃ³w wektorowych, przydatne w rozpoznawaniu obrazÃ³w i wykrywaniu anomalii.

## Podsumowanie

OmÃ³wiliÅ›my podstawowe zagadnienia RAG, od dodawania danych do aplikacji, przez zapytania uÅ¼ytkownika, aÅ¼ po generowanie odpowiedzi. Aby uproÅ›ciÄ‡ tworzenie RAG, moÅ¼esz skorzystaÄ‡ z frameworkÃ³w takich jak Semantic Kernel, Langchain czy Autogen.

## Zadanie

Aby kontynuowaÄ‡ naukÄ™ Retrieval Augmented Generation (RAG), moÅ¼esz zbudowaÄ‡:

- Front-end aplikacji, korzystajÄ…c z wybranego przez siebie frameworka

- WykorzystaÄ‡ framework, np. LangChain lub Semantic Kernel, i odtworzyÄ‡ swojÄ… aplikacjÄ™.

Gratulacje za ukoÅ„czenie lekcji ğŸ‘.

## Nauka siÄ™ tutaj nie koÅ„czy, kontynuuj swojÄ… podrÃ³Å¼

Po ukoÅ„czeniu tej lekcji sprawdÅº naszÄ… [kolekcjÄ™ Generative AI Learning](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby dalej rozwijaÄ‡ swojÄ… wiedzÄ™ o Generative AI!

**ZastrzeÅ¼enie**:  
Niniejszy dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo Å¼e dÄ…Å¼ymy do dokÅ‚adnoÅ›ci, prosimy mieÄ‡ na uwadze, Å¼e tÅ‚umaczenia automatyczne mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jÄ™zyku ÅºrÃ³dÅ‚owym powinien byÄ‡ uznawany za ÅºrÃ³dÅ‚o autorytatywne. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnego tÅ‚umaczenia wykonanego przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z korzystania z tego tÅ‚umaczenia.