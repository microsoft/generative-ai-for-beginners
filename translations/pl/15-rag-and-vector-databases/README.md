<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-05-20T01:16:20+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "pl"
}
-->
# Generowanie wspomagane wyszukiwaniem (RAG) i bazy danych wektorowych

W lekcji dotyczcej aplikacji wyszukiwania kr贸tko om贸wilimy, jak zintegrowa wasne dane z du偶ymi modelami jzykowymi (LLM). W tej lekcji zagbimy si w pojcia zwizane z ugruntowaniem danych w aplikacji LLM, mechanik procesu oraz metody przechowywania danych, w tym zar贸wno osadze, jak i tekstu.

> **Wideo wkr贸tce**

## Wprowadzenie

W tej lekcji om贸wimy:

- Wprowadzenie do RAG, czym jest i dlaczego jest u偶ywane w AI (sztucznej inteligencji).

- Zrozumienie, czym s bazy danych wektorowych i tworzenie jednej dla naszej aplikacji.

- Praktyczny przykad integracji RAG z aplikacj.

## Cele nauki

Po ukoczeniu tej lekcji bdziesz w stanie:

- Wyjani znaczenie RAG w odnajdywaniu i przetwarzaniu danych.

- Skonfigurowa aplikacj RAG i ugruntowa swoje dane w LLM.

- Skuteczna integracja RAG i baz danych wektorowych w aplikacjach LLM.

## Nasz scenariusz: wzbogacanie naszych LLM o wasne dane

W tej lekcji chcemy doda wasne notatki do startupu edukacyjnego, co pozwala chatbotowi uzyska wicej informacji na r贸偶ne tematy. Dziki notatkom, kt贸re posiadamy, uczniowie bd mogli lepiej si uczy i zrozumie r贸偶ne tematy, co uatwi im przygotowanie si do egzamin贸w. Aby stworzy nasz scenariusz, u偶yjemy:

- `Azure OpenAI:` LLM, kt贸rego u偶yjemy do stworzenia naszego chatbota

- `AI for beginners' lesson on Neural Networks`: to bd dane, na kt贸rych ugruntujemy nasze LLM

- `Azure AI Search` i `Azure Cosmos DB:` baza danych wektorowych do przechowywania naszych danych i tworzenia indeksu wyszukiwania

U偶ytkownicy bd mogli tworzy quizy wiczeniowe z notatek, fiszki do powt贸rek i podsumowywa je do zwizych przegld贸w. Aby rozpocz, przyjrzyjmy si, czym jest RAG i jak dziaa:

## Generowanie wspomagane wyszukiwaniem (RAG)

Chatbot oparty na LLM przetwarza zapytania u偶ytkownika, aby generowa odpowiedzi. Jest zaprojektowany do interakcji i anga偶uje u偶ytkownik贸w w szerok gam temat贸w. Jednak jego odpowiedzi s ograniczone do dostarczonego kontekstu i danych treningowych. Na przykad, GPT-4 ma ograniczenie wiedzy do wrzenia 2021 roku, co oznacza, 偶e nie zna wydarze, kt贸re miay miejsce po tym okresie. Ponadto, dane u偶ywane do trenowania LLM wykluczaj poufne informacje, takie jak osobiste notatki czy podrcznik produktowy firmy.

### Jak dziaaj RAG (Generowanie wspomagane wyszukiwaniem)

Za贸偶my, 偶e chcesz wdro偶y chatbota, kt贸ry tworzy quizy z twoich notatek, bdziesz potrzebowa poczenia z baz wiedzy. Tutaj RAG przychodzi z pomoc. RAG dziaa w nastpujcy spos贸b:

- **Baza wiedzy:** Przed wyszukiwaniem dokumenty te musz zosta wczytane i przetworzone, zazwyczaj poprzez podzia du偶ych dokument贸w na mniejsze fragmenty, przeksztacenie ich w osadzenia tekstowe i przechowywanie w bazie danych.

- **Zapytanie u偶ytkownika:** u偶ytkownik zadaje pytanie

- **Wyszukiwanie:** Gdy u偶ytkownik zadaje pytanie, model osadze pobiera odpowiednie informacje z naszej bazy wiedzy, aby dostarczy wicej kontekstu, kt贸ry zostanie wczony do zapytania.

- **Generowanie wspomagane:** LLM wzbogaca swoj odpowied藕 na podstawie pobranych danych. Pozwala to na generowanie odpowiedzi nie tylko na podstawie wstpnie wytrenowanych danych, ale tak偶e na podstawie istotnych informacji z dodanego kontekstu. Pobierane dane s u偶ywane do wzbogacania odpowiedzi LLM. LLM nastpnie zwraca odpowied藕 na pytanie u偶ytkownika.

Architektura RAG jest implementowana przy u偶yciu transformator贸w skadajcych si z dw贸ch czci: enkodera i dekodera. Na przykad, gdy u偶ytkownik zadaje pytanie, tekst wejciowy jest 'kodowany' na wektory uchwytujce znaczenie s贸w, a wektory s 'dekodowane' w nasz indeks dokument贸w i generuj nowy tekst na podstawie zapytania u偶ytkownika. LLM wykorzystuje model enkoder-dekoder do generowania wyjcia.

Dwa podejcia przy implementacji RAG zgodnie z proponowanym artykuem: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) to:

- **_RAG-Sequence_** u偶ywanie pobranych dokument贸w do przewidywania najlepszej mo偶liwej odpowiedzi na zapytanie u偶ytkownika

- **RAG-Token** u偶ywanie dokument贸w do generowania nastpnego tokena, a nastpnie pobieranie ich w celu odpowiedzi na zapytanie u偶ytkownika

### Dlaczego warto u偶ywa RAG?

- **Bogactwo informacji:** zapewnia, 偶e odpowiedzi tekstowe s aktualne i bie偶ce. Zatem poprawia wydajno w zadaniach specyficznych dla domeny, uzyskujc dostp do wewntrznej bazy wiedzy.

- Redukuje faszowanie poprzez wykorzystanie **weryfikowalnych danych** w bazie wiedzy do dostarczenia kontekstu do zapyta u偶ytkownik贸w.

- Jest **kosztowo efektywne**, poniewa偶 s bardziej ekonomiczne w por贸wnaniu do dostrajania LLM.

## Tworzenie bazy wiedzy

Nasza aplikacja opiera si na naszych danych osobistych, tj. lekcji o sieciach neuronowych w programie AI For Beginners.

### Bazy danych wektorowych

Baza danych wektorowych, w przeciwiestwie do tradycyjnych baz danych, to specjalistyczna baza danych zaprojektowana do przechowywania, zarzdzania i wyszukiwania osadzonych wektor贸w. Przechowuje numeryczne reprezentacje dokument贸w. Rozbicie danych na numeryczne osadzenia uatwia naszemu systemowi AI zrozumienie i przetwarzanie danych.

Przechowujemy nasze osadzenia w bazach danych wektorowych, poniewa偶 LLM maj ograniczenie liczby token贸w, kt贸re akceptuj jako dane wejciowe. Poniewa偶 nie mo偶na przekaza caych osadze do LLM, musimy je podzieli na fragmenty, a gdy u偶ytkownik zadaje pytanie, osadzenia najbardziej podobne do pytania zostan zwr贸cone razem z zapytaniem. Podzia na fragmenty r贸wnie偶 redukuje koszty zwizane z liczb token贸w przekazywanych przez LLM.

Niekt贸re popularne bazy danych wektorowych to Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant i DeepLake. Mo偶esz stworzy model Azure Cosmos DB za pomoc Azure CLI z nastpujcym poleceniem:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od tekstu do osadze

Zanim przechowamy nasze dane, musimy je przekonwertowa na osadzenia wektorowe przed ich przechowaniem w bazie danych. Jeli pracujesz z du偶ymi dokumentami lub dugimi tekstami, mo偶esz je podzieli na fragmenty na podstawie oczekiwanych zapyta. Podzia mo偶na przeprowadzi na poziomie zdania lub akapitu. Poniewa偶 podzia wywodzi znaczenia z otaczajcych s贸w, mo偶esz doda dodatkowy kontekst do fragmentu, na przykad dodajc tytu dokumentu lub wstawiajc tekst przed lub po fragmencie. Mo偶esz podzieli dane w nastpujcy spos贸b:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Po podzieleniu mo偶emy nastpnie osadzi nasz tekst za pomoc r贸偶nych modeli osadze. Niekt贸re modele, kt贸re mo偶na u偶y, to: word2vec, ada-002 przez OpenAI, Azure Computer Vision i wiele innych. Wyb贸r modelu zale偶y od u偶ywanych jzyk贸w, rodzaju kodowanej treci (tekst/obrazy/d藕wik), wielkoci wejcia, kt贸re mo偶na zakodowa, oraz dugoci wyjcia osadzenia.

Przykad osadzonego tekstu przy u偶yciu modelu `text-embedding-ada-002` OpenAI to:

## Wyszukiwanie i wyszukiwanie wektorowe

Kiedy u偶ytkownik zadaje pytanie, retriever przeksztaca je w wektor za pomoc enkodera zapyta, a nastpnie przeszukuje nasz indeks wyszukiwania dokument贸w w poszukiwaniu odpowiednich wektor贸w w dokumencie, kt贸re s zwizane z danymi wejciowymi. Po zakoczeniu konwertuje zar贸wno wektor wejciowy, jak i wektory dokumentu na tekst i przekazuje je przez LLM.

### Wyszukiwanie

Wyszukiwanie ma miejsce, gdy system pr贸buje szybko znale藕 dokumenty z indeksu, kt贸re speniaj kryteria wyszukiwania. Celem retrievera jest uzyskanie dokument贸w, kt贸re bd u偶ywane do dostarczenia kontekstu i ugruntowania LLM na twoich danych.

Istnieje kilka sposob贸w na przeprowadzenie wyszukiwania w naszej bazie danych, takich jak:

- **Wyszukiwanie s贸w kluczowych** - u偶ywane do wyszukiwania tekstu

- **Wyszukiwanie semantyczne** - u偶ywa semantycznego znaczenia s贸w

- **Wyszukiwanie wektorowe** - konwertuje dokumenty z tekstu na reprezentacje wektorowe za pomoc modeli osadze. Wyszukiwanie odbywa si poprzez zapytanie dokument贸w, kt贸rych reprezentacje wektorowe s najbli偶sze pytaniu u偶ytkownika.

- **Hybrydowe** - poczenie zar贸wno wyszukiwania s贸w kluczowych, jak i wyszukiwania wektorowego.

Problem z wyszukiwaniem pojawia si, gdy w bazie danych nie ma podobnej odpowiedzi na zapytanie, wtedy system zwr贸ci najlepsze informacje, jakie mo偶e uzyska, jednak mo偶na u偶y taktyk, takich jak ustawienie maksymalnej odlegoci dla istotnoci lub u偶ycie wyszukiwania hybrydowego, kt贸re czy zar贸wno sowa kluczowe, jak i wyszukiwanie wektorowe. W tej lekcji u偶yjemy wyszukiwania hybrydowego, poczenia zar贸wno wyszukiwania wektorowego, jak i s贸w kluczowych. Przechowamy nasze dane w ramce danych z kolumnami zawierajcymi fragmenty oraz osadzenia.

### Podobiestwo wektorowe

Retriever przeszuka baz wiedzy w poszukiwaniu osadze, kt贸re s blisko siebie, najbli偶szego ssiada, poniewa偶 s to teksty, kt贸re s podobne. W scenariuszu, gdy u偶ytkownik zadaje zapytanie, jest ono najpierw osadzane, a nastpnie dopasowywane do podobnych osadze. Powszechn miar u偶ywan do okrelenia, jak podobne s r贸偶ne wektory, jest podobiestwo kosinusowe, kt贸re opiera si na kcie midzy dwoma wektorami.

Mo偶emy mierzy podobiestwo za pomoc innych alternatyw, kt贸re mo偶emy u偶y, takich jak odlego euklidesowa, kt贸ra jest lini prost midzy kocami wektor贸w, oraz iloczyn skalarny, kt贸ry mierzy sum iloczyn贸w odpowiadajcych sobie element贸w dw贸ch wektor贸w.

### Indeks wyszukiwania

Podczas wyszukiwania bdziemy musieli zbudowa indeks wyszukiwania dla naszej bazy wiedzy, zanim przeprowadzimy wyszukiwanie. Indeks przechowa nasze osadzenia i mo偶e szybko odnale藕 najpodobniejsze fragmenty nawet w du偶ej bazie danych. Mo偶emy stworzy nasz indeks lokalnie, u偶ywajc:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Re-ranking

Po zapytaniu bazy danych mo偶e by konieczne posortowanie wynik贸w od najbardziej istotnych. LLM do rerankingu wykorzystuje uczenie maszynowe do poprawy trafnoci wynik贸w wyszukiwania, porzdkujc je od najbardziej istotnych. Korzystajc z Azure AI Search, reranking jest wykonywany automatycznie za pomoc semantycznego rerankera. Przykad dziaania rerankingu przy u偶yciu najbli偶szych ssiad贸w:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Poczenie wszystkiego w cao

Ostatnim krokiem jest dodanie naszego LLM do caoci, aby m贸c uzyska odpowiedzi oparte na naszych danych. Mo偶emy to zaimplementowa w nastpujcy spos贸b:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Ocena naszej aplikacji

### Metryki oceny

- Jako dostarczanych odpowiedzi, zapewniajca, 偶e brzmi naturalnie, pynnie i ludzko

- Ugruntowanie danych: ocena, czy odpowied藕 pochodzi z dostarczonych dokument贸w

- Trafno: ocena, czy odpowied藕 pasuje i jest zwizana z zadanym pytaniem

- Pynno - czy odpowied藕 jest poprawna gramatycznie

## Przypadki u偶ycia RAG (Generowanie wspomagane wyszukiwaniem) i baz danych wektorowych

Istnieje wiele r贸偶nych przypadk贸w u偶ycia, w kt贸rych wywoania funkcji mog poprawi twoj aplikacj, takich jak:

- Pytania i odpowiedzi: ugruntowanie danych firmy do czatu, z kt贸rego mog korzysta pracownicy, aby zadawa pytania.

- Systemy rekomendacji: gdzie mo偶na stworzy system, kt贸ry dopasowuje najbardziej podobne wartoci, np. filmy, restauracje i wiele innych.

- Usugi chatbot贸w: mo偶esz przechowywa histori czatu i personalizowa rozmow na podstawie danych u偶ytkownika.

- Wyszukiwanie obraz贸w na podstawie osadze wektorowych, przydatne podczas rozpoznawania obraz贸w i wykrywania anomalii.

## Podsumowanie

Om贸wilimy podstawowe obszary RAG od dodawania naszych danych do aplikacji, zapytania u偶ytkownika i wyjcia. Aby uproci tworzenie RAG, mo偶na u偶y takich framework贸w jak Semanti Kernel, Langchain lub Autogen.

## Zadanie

Aby kontynuowa nauk na temat Generowania wspomaganego wyszukiwaniem (RAG), mo偶esz zbudowa:

- Stw贸rz interfejs front-end dla aplikacji, korzystajc z wybranego frameworka

- Wykorzystaj framework, LangChain lub Semantic Kernel, i odtw贸rz swoj aplikacj.

Gratulacje z ukoczenia lekcji .

## Nauka nie koczy si tutaj, kontynuuj podr贸偶

Po ukoczeniu tej lekcji, sprawd藕 nasz [Kolekcj nauki o AI generatywnej](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby kontynuowa pogbianie swojej wiedzy na temat AI generatywnej!

**Zastrze偶enie**:  
Ten dokument zosta przetumaczony przy u偶yciu usugi tumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chocia偶 staramy si zapewni dokadno, prosimy pamita, 偶e automatyczne tumaczenia mog zawiera bdy lub niecisoci. Oryginalny dokument w jego rodzimym jzyku powinien by uznawany za autorytatywne 藕r贸do. W przypadku informacji krytycznych zaleca si profesjonalne tumaczenie przez czowieka. Nie ponosimy odpowiedzialnoci za wszelkie nieporozumienia lub bdne interpretacje wynikajce z u偶ycia tego tumaczenia.