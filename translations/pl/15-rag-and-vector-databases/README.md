<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b4b0266fbadbba7ded891b6485adc66d",
  "translation_date": "2025-10-18T00:53:29+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "pl"
}
-->
# Generowanie wspomagane wyszukiwaniem (RAG) i bazy danych wektorowych

[![Generowanie wspomagane wyszukiwaniem (RAG) i bazy danych wektorowych](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.pl.png)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

W lekcji dotyczcej aplikacji wyszukiwania kr贸tko om贸wilimy, jak zintegrowa wasne dane z du偶ymi modelami jzykowymi (LLM). W tej lekcji zagbimy si w koncepcj osadzania danych w aplikacji LLM, mechanik tego procesu oraz metody przechowywania danych, w tym osadze i tekstu.

> **Wideo wkr贸tce**

## Wprowadzenie

W tej lekcji om贸wimy nastpujce zagadnienia:

- Wprowadzenie do RAG, czym jest i dlaczego jest wykorzystywane w sztucznej inteligencji (AI).

- Zrozumienie, czym s bazy danych wektorowych i stworzenie jednej dla naszej aplikacji.

- Praktyczny przykad integracji RAG z aplikacj.

## Cele nauki

Po ukoczeniu tej lekcji bdziesz w stanie:

- Wyjani znaczenie RAG w wyszukiwaniu i przetwarzaniu danych.

- Skonfigurowa aplikacj RAG i osadzi swoje dane w LLM.

- Skutecznie zintegrowa RAG i bazy danych wektorowych w aplikacjach LLM.

## Nasz scenariusz: ulepszanie LLM za pomoc wasnych danych

W tej lekcji chcemy doda nasze wasne notatki do startupu edukacyjnego, co pozwoli chatbotowi uzyska wicej informacji na r贸偶ne tematy. Dziki naszym notatkom uczniowie bd mogli lepiej si uczy i zrozumie r贸偶ne zagadnienia, co uatwi im przygotowanie si do egzamin贸w. Aby stworzy nasz scenariusz, u偶yjemy:

- `Azure OpenAI:` LLM, kt贸rego u偶yjemy do stworzenia naszego chatbota

- `Lekcja dla pocztkujcych na temat sieci neuronowych:` dane, na kt贸rych osadzimy nasz LLM

- `Azure AI Search` i `Azure Cosmos DB:` baza danych wektorowych do przechowywania naszych danych i tworzenia indeksu wyszukiwania

U偶ytkownicy bd mogli tworzy quizy na podstawie swoich notatek, fiszki do powt贸rek oraz podsumowania w formie zwizych przegld贸w. Aby rozpocz, przyjrzyjmy si, czym jest RAG i jak dziaa:

## Generowanie wspomagane wyszukiwaniem (RAG)

Chatbot oparty na LLM przetwarza zapytania u偶ytkownika, aby generowa odpowiedzi. Jest zaprojektowany tak, aby by interaktywny i anga偶owa u偶ytkownik贸w w szerokim zakresie temat贸w. Jednak jego odpowiedzi s ograniczone do kontekstu dostarczonego w danych treningowych. Na przykad, wiedza GPT-4 koczy si na wrzeniu 2021 roku, co oznacza, 偶e nie zna wydarze, kt贸re miay miejsce po tym okresie. Ponadto dane u偶ywane do trenowania LLM nie zawieraj poufnych informacji, takich jak osobiste notatki czy podrczniki produkt贸w firmy.

### Jak dziaaj RAG (Generowanie wspomagane wyszukiwaniem)

![schemat dziaania RAG](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.pl.png)

Za贸偶my, 偶e chcesz wdro偶y chatbota, kt贸ry tworzy quizy na podstawie Twoich notatek. Wymaga to poczenia z baz wiedzy. Tutaj z pomoc przychodzi RAG. RAG dziaa w nastpujcy spos贸b:

- **Baza wiedzy:** Przed wyszukiwaniem dokumenty musz zosta zaimportowane i wstpnie przetworzone, zazwyczaj poprzez podzielenie du偶ych dokument贸w na mniejsze fragmenty, przeksztacenie ich w osadzenia tekstowe i przechowywanie w bazie danych.

- **Zapytanie u偶ytkownika:** u偶ytkownik zadaje pytanie.

- **Wyszukiwanie:** Gdy u偶ytkownik zadaje pytanie, model osadze wyszukuje odpowiednie informacje w naszej bazie wiedzy, aby dostarczy wicej kontekstu, kt贸ry zostanie wczony do zapytania.

- **Generowanie wspomagane:** LLM ulepsza swoj odpowied藕 na podstawie wyszukanych danych. Pozwala to na generowanie odpowiedzi nie tylko na podstawie danych wstpnie przeszkolonych, ale tak偶e na podstawie istotnych informacji z dodanego kontekstu. Wyszukane dane s u偶ywane do wzbogacenia odpowiedzi LLM. Nastpnie LLM zwraca odpowied藕 na pytanie u偶ytkownika.

![schemat architektury RAG](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.pl.png)

Architektura RAG jest implementowana za pomoc transformator贸w skadajcych si z dw贸ch czci: kodera i dekodera. Na przykad, gdy u偶ytkownik zadaje pytanie, tekst wejciowy jest "kodowany" na wektory, kt贸re przechwytuj znaczenie s贸w, a nastpnie wektory s "dekodowane" w naszym indeksie dokument贸w, generujc nowy tekst na podstawie zapytania u偶ytkownika. LLM wykorzystuje zar贸wno model kodera, jak i dekodera do generowania odpowiedzi.

Dwa podejcia do implementacji RAG, zgodnie z proponowanym artykuem: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst), to:

- **_RAG-Sequence_** wykorzystujcy wyszukane dokumenty do przewidywania najlepszego mo偶liwego rozwizania zapytania u偶ytkownika.

- **RAG-Token** wykorzystujcy dokumenty do generowania kolejnego tokenu, a nastpnie wyszukiwania ich w celu odpowiedzi na zapytanie u偶ytkownika.

### Dlaczego warto korzysta z RAG?

- **Bogactwo informacji:** zapewnia, 偶e odpowiedzi tekstowe s aktualne i zgodne z rzeczywistoci. Dziki temu poprawia wydajno w zadaniach specyficznych dla danej dziedziny, uzyskujc dostp do wewntrznej bazy wiedzy.

- Redukuje fabrykowanie informacji, wykorzystujc **weryfikowalne dane** z bazy wiedzy, aby dostarczy kontekst do zapyta u偶ytkownika.

- Jest **kosztowo efektywne**, poniewa偶 jest bardziej ekonomiczne w por贸wnaniu do dostrajania LLM.

## Tworzenie bazy wiedzy

Nasza aplikacja opiera si na naszych osobistych danych, tj. lekcji o sieciach neuronowych z programu nauczania AI dla pocztkujcych.

### Bazy danych wektorowych

Baza danych wektorowych, w przeciwiestwie do tradycyjnych baz danych, jest specjalistyczn baz danych zaprojektowan do przechowywania, zarzdzania i wyszukiwania osadzonych wektor贸w. Przechowuje numeryczne reprezentacje dokument贸w. Rozbijanie danych na numeryczne osadzenia uatwia naszemu systemowi AI zrozumienie i przetwarzanie danych.

Przechowujemy nasze osadzenia w bazach danych wektorowych, poniewa偶 LLM maj ograniczenie liczby token贸w, kt贸re akceptuj jako dane wejciowe. Poniewa偶 nie mo偶na przekaza caych osadze do LLM, musimy je podzieli na fragmenty, a gdy u偶ytkownik zada pytanie, osadzenia najbardziej podobne do pytania zostan zwr贸cone razem z zapytaniem. Podzia na fragmenty r贸wnie偶 zmniejsza koszty zwizane z liczb token贸w przekazywanych przez LLM.

Niekt贸re popularne bazy danych wektorowych to Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant i DeepLake. Mo偶esz stworzy model Azure Cosmos DB za pomoc Azure CLI, u偶ywajc nastpujcego polecenia:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od tekstu do osadze

Przed przechowywaniem naszych danych musimy je przeksztaci w osadzenia wektorowe, zanim zostan zapisane w bazie danych. Jeli pracujesz z du偶ymi dokumentami lub dugimi tekstami, mo偶esz je podzieli na fragmenty na podstawie zapyta, kt贸rych si spodziewasz. Podzia na fragmenty mo偶na przeprowadzi na poziomie zdania lub akapitu. Poniewa偶 podzia na fragmenty wywodzi znaczenia z otaczajcych s贸w, mo偶esz doda dodatkowy kontekst do fragmentu, na przykad dodajc tytu dokumentu lub wczajc tekst przed lub po fragmencie. Mo偶esz podzieli dane w nastpujcy spos贸b:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Po podzieleniu na fragmenty mo偶emy osadzi nasz tekst, korzystajc z r贸偶nych modeli osadze. Niekt贸re modele, kt贸re mo偶na wykorzysta, to: word2vec, ada-002 od OpenAI, Azure Computer Vision i wiele innych. Wyb贸r modelu zale偶y od u偶ywanych jzyk贸w, rodzaju kodowanej treci (tekst/obrazy/d藕wik), rozmiaru danych wejciowych, kt贸re mo偶e zakodowa, oraz dugoci wyjciowego osadzenia.

Przykad osadzonego tekstu przy u偶yciu modelu OpenAI `text-embedding-ada-002`:
![osadzenie sowa kot](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.pl.png)

## Wyszukiwanie i wyszukiwanie wektorowe

Gdy u偶ytkownik zadaje pytanie, wyszukiwarka przeksztaca je w wektor za pomoc kodera zapyta, a nastpnie przeszukuje nasz indeks dokument贸w w poszukiwaniu odpowiednich wektor贸w w dokumencie, kt贸re s zwizane z danymi wejciowymi. Po zakoczeniu przeksztaca zar贸wno wektor wejciowy, jak i wektory dokument贸w w tekst i przekazuje je przez LLM.

### Wyszukiwanie

Wyszukiwanie odbywa si, gdy system pr贸buje szybko znale藕 dokumenty z indeksu, kt贸re speniaj kryteria wyszukiwania. Celem wyszukiwarki jest uzyskanie dokument贸w, kt贸re bd u偶ywane do dostarczenia kontekstu i osadzenia LLM w Twoich danych.

Istnieje kilka sposob贸w wyszukiwania w naszej bazie danych, takich jak:

- **Wyszukiwanie s贸w kluczowych** - u偶ywane do wyszukiwania tekst贸w.

- **Wyszukiwanie semantyczne** - wykorzystuje semantyczne znaczenie s贸w.

- **Wyszukiwanie wektorowe** - przeksztaca dokumenty z tekstu na reprezentacje wektorowe za pomoc modeli osadze. Wyszukiwanie odbywa si poprzez zapytania do dokument贸w, kt贸rych reprezentacje wektorowe s najbli偶sze pytaniu u偶ytkownika.

- **Hybrydowe** - poczenie zar贸wno wyszukiwania s贸w kluczowych, jak i wyszukiwania wektorowego.

Problem z wyszukiwaniem pojawia si, gdy w bazie danych nie ma odpowiedzi podobnej do zapytania, system zwr贸ci wtedy najlepsze dostpne informacje. Mo偶na jednak zastosowa takie taktyki, jak ustawienie maksymalnej odlegoci dla trafnoci lub u偶ycie wyszukiwania hybrydowego, kt贸re czy zar贸wno sowa kluczowe, jak i wyszukiwanie wektorowe. W tej lekcji u偶yjemy wyszukiwania hybrydowego, kt贸re czy wyszukiwanie wektorowe i s贸w kluczowych. Przechowamy nasze dane w ramce danych z kolumnami zawierajcymi fragmenty oraz osadzenia.

### Podobiestwo wektorowe

Wyszukiwarka przeszuka baz wiedzy w poszukiwaniu osadze, kt贸re s blisko siebie, najbli偶szych ssiad贸w, poniewa偶 s to teksty podobne. W scenariuszu, gdy u偶ytkownik zadaje zapytanie, jest ono najpierw osadzane, a nastpnie dopasowywane do podobnych osadze. Powszechn miar u偶ywan do okrelenia, jak bardzo podobne s r贸偶ne wektory, jest podobiestwo cosinusowe, kt贸re opiera si na kcie midzy dwoma wektorami.

Mo偶emy mierzy podobiestwo za pomoc innych alternatyw, takich jak odlego euklidesowa, kt贸ra jest prost lini midzy kocami wektor贸w, oraz iloczyn skalarny, kt贸ry mierzy sum iloczyn贸w odpowiadajcych sobie element贸w dw贸ch wektor贸w.

### Indeks wyszukiwania

Podczas wyszukiwania bdziemy musieli zbudowa indeks wyszukiwania dla naszej bazy wiedzy, zanim przeprowadzimy wyszukiwanie. Indeks przechowuje nasze osadzenia i mo偶e szybko wyszukiwa najbardziej podobne fragmenty nawet w du偶ej bazie danych. Mo偶emy stworzy nasz indeks lokalnie, u偶ywajc:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Ponowne rangowanie

Po zapytaniu bazy danych mo偶e by konieczne posortowanie wynik贸w od najbardziej trafnych. LLM do ponownego rangowania wykorzystuje uczenie maszynowe, aby poprawi trafno wynik贸w wyszukiwania, porzdkujc je od najbardziej trafnych. Korzystajc z Azure AI Search, ponowne rangowanie odbywa si automatycznie za pomoc semantycznego ponownego rangera. Przykad dziaania ponownego rangowania przy u偶yciu najbli偶szych ssiad贸w:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## czenie wszystkiego w cao

Ostatnim krokiem jest dodanie naszego LLM do procesu, aby m贸c uzyska odpowiedzi osadzone w naszych danych. Mo偶emy to zaimplementowa w nastpujcy spos贸b:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Ocena naszej aplikacji

### Metryki oceny

- Jako dostarczanych odpowiedzi, zapewniajc, 偶e brzmi naturalnie, pynnie i jak odpowiedzi czowieka.

- Osadzenie danych: ocena, czy odpowied藕 pochodzi z dostarczonych dokument贸w.

- Trafno: ocena, czy odpowied藕 pasuje i jest zwizana z zadanym pytaniem.

- Pynno - czy odpowied藕 jest poprawna gramatycznie.

## Przykady zastosowania RAG (Generowanie wspomagane wyszukiwaniem) i baz danych wektorowych

Istnieje wiele r贸偶nych zastosowa, w kt贸rych wywoania funkcji mog poprawi Twoj aplikacj, takich jak:

- Odpowiadanie na pytania: osadzenie danych firmy w czacie, kt贸ry mo偶e by u偶ywany przez pracownik贸w do zadawania pyta.

- Systemy rekomendacji: gdzie mo偶na stworzy system dopasowujcy najbardziej podobne wartoci, np. filmy, restauracje i wiele innych.

- Usugi chatbot贸w: mo偶na przechowywa histori czatu i personalizowa rozmow na podstawie danych u偶ytkownika.

- Wyszukiwanie obraz贸w na podstawie osadze wektorowych, przydatne przy rozpoznawaniu obraz贸w i wykrywaniu anomalii.

## Podsumowanie

Om贸wilimy podstawowe aspekty RAG, od dodawania danych do aplikacji, przez zapytanie u偶ytkownika, a偶 po wynik. Aby uproci tworzenie RAG, mo偶esz u偶y takich framework贸w jak Semantic Kernel, Langchain czy Autogen.

## Zadanie

Aby kontynuowa nauk o Generowaniu wspomaganym wyszukiwaniem (RAG), mo偶esz:

- Zbudowa interfejs u偶ytkownika dla aplikacji, korzystajc z wybranego frameworka.

- Wykorzysta framework, taki jak LangChain lub Semantic Kernel, i odtworzy swoj aplikacj.

Gratulacje z ukoczenia lekcji .

## Nauka nie koczy si tutaj, kontynuuj swoj podr贸偶

Po ukoczeniu tej lekcji zapoznaj si z nasz [kolekcj nauki o generatywnej AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby dalej rozwija swoj wiedz na temat generatywnej AI!

---

**Zastrze偶enie**:  
Ten dokument zosta przetumaczony za pomoc usugi tumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chocia偶 staramy si zapewni dokadno, prosimy pamita, 偶e automatyczne tumaczenia mog zawiera bdy lub niecisoci. Oryginalny dokument w jego rodzimym jzyku powinien by uznawany za autorytatywne 藕r贸do. W przypadku informacji krytycznych zaleca si skorzystanie z profesjonalnego tumaczenia przez czowieka. Nie ponosimy odpowiedzialnoci za jakiekolwiek nieporozumienia lub bdne interpretacje wynikajce z u偶ycia tego tumaczenia.