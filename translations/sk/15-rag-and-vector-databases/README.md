<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-07-09T16:19:33+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "sk"
}
-->
# Retrieval Augmented Generation (RAG) a vektorovÃ© databÃ¡zy

[![Retrieval Augmented Generation (RAG) a vektorovÃ© databÃ¡zy](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.sk.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

V lekcii o vyhÄ¾adÃ¡vacÃ­ch aplikÃ¡ciÃ¡ch sme si struÄne ukÃ¡zali, ako integrovaÅ¥ vlastnÃ© dÃ¡ta do veÄ¾kÃ½ch jazykovÃ½ch modelov (LLM). V tejto lekcii sa podrobnejÅ¡ie pozrieme na koncept zakotvenia vaÅ¡ich dÃ¡t v aplikÃ¡cii LLM, mechanizmy tohto procesu a metÃ³dy ukladania dÃ¡t, vrÃ¡tane embeddingov aj textu.

> **Video Äoskoro**

## Ãšvod

V tejto lekcii sa budeme venovaÅ¥:

- Ãšvodu do RAG, Äo to je a preÄo sa pouÅ¾Ã­va v AI (umelej inteligencii).

- Pochopeniu, Äo sÃº vektorovÃ© databÃ¡zy a vytvoreniu takej pre naÅ¡u aplikÃ¡ciu.

- PraktickÃ©mu prÃ­kladu, ako integrovaÅ¥ RAG do aplikÃ¡cie.

## Ciele uÄenia

Po dokonÄenÃ­ tejto lekcie budete schopnÃ­:

- VysvetliÅ¥ vÃ½znam RAG pri vyhÄ¾adÃ¡vanÃ­ a spracovanÃ­ dÃ¡t.

- NastaviÅ¥ RAG aplikÃ¡ciu a zakotviÅ¥ svoje dÃ¡ta v LLM.

- EfektÃ­vne integrovaÅ¥ RAG a vektorovÃ© databÃ¡zy v LLM aplikÃ¡ciÃ¡ch.

## NÃ¡Å¡ scenÃ¡r: vylepÅ¡enie naÅ¡ich LLM vlastnÃ½mi dÃ¡tami

Pre tÃºto lekciu chceme pridaÅ¥ vlastnÃ© poznÃ¡mky do vzdelÃ¡vacieho startupu, ktorÃ½ umoÅ¾nÃ­ chatbotovi zÃ­skaÅ¥ viac informÃ¡ciÃ­ o rÃ´znych predmetoch. VÄaka poznÃ¡mkam, ktorÃ© mÃ¡me, budÃº Å¡tudenti schopnÃ­ lepÅ¡ie Å¡tudovaÅ¥ a pochopiÅ¥ rÃ´zne tÃ©my, Äo im uÄ¾ahÄÃ­ prÃ­pravu na skÃºÅ¡ky. Na vytvorenie nÃ¡Å¡ho scenÃ¡ra pouÅ¾ijeme:

- `Azure OpenAI:` LLM, ktorÃ½ pouÅ¾ijeme na vytvorenie nÃ¡Å¡ho chatbota

- `Lekcia AI pre zaÄiatoÄnÃ­kov o neurÃ³novÃ½ch sieÅ¥ach:` toto budÃº dÃ¡ta, na ktorÃ½ch zakotvÃ­me nÃ¡Å¡ LLM

- `Azure AI Search` a `Azure Cosmos DB:` vektorovÃ¡ databÃ¡za na uloÅ¾enie naÅ¡ich dÃ¡t a vytvorenie vyhÄ¾adÃ¡vacieho indexu

PouÅ¾Ã­vatelia budÃº mÃ´cÅ¥ vytvÃ¡raÅ¥ cviÄnÃ© kvÃ­zy zo svojich poznÃ¡mok, opakovacie kartiÄky a zhrnutia do struÄnÃ½ch prehÄ¾adov. Na zaÄiatok si pozrime, Äo je RAG a ako funguje:

## Retrieval Augmented Generation (RAG)

Chatbot pohÃ¡ÅˆanÃ½ LLM spracovÃ¡va pouÅ¾Ã­vateÄ¾skÃ© poÅ¾iadavky na generovanie odpovedÃ­. Je navrhnutÃ½ tak, aby bol interaktÃ­vny a komunikoval s pouÅ¾Ã­vateÄ¾mi na Å¡irokÃº Å¡kÃ¡lu tÃ©m. Jeho odpovede sÃº vÅ¡ak obmedzenÃ© na kontext, ktorÃ½ mu je poskytnutÃ½, a na zÃ¡kladnÃ© trÃ©ningovÃ© dÃ¡ta. NaprÃ­klad, GPT-4 mÃ¡ znalosti len do septembra 2021, Äo znamenÃ¡, Å¾e nepoznÃ¡ udalosti, ktorÃ© sa stali po tomto dÃ¡tume. Okrem toho trÃ©ningovÃ© dÃ¡ta LLM nezahÅ•ÅˆajÃº dÃ´vernÃ© informÃ¡cie, ako sÃº osobnÃ© poznÃ¡mky alebo manuÃ¡ly spoloÄnosti.

### Ako fungujÃº RAG (Retrieval Augmented Generation)

![kresba znÃ¡zorÅˆujÃºca, ako fungujÃº RAG](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.sk.png)

Predstavte si, Å¾e chcete nasadiÅ¥ chatbota, ktorÃ½ vytvÃ¡ra kvÃ­zy z vaÅ¡ich poznÃ¡mok, budete potrebovaÅ¥ pripojenie k databÃ¡ze znalostÃ­. Tu prichÃ¡dza na pomoc RAG. RAG funguje nasledovne:

- **DatabÃ¡za znalostÃ­:** Pred vyhÄ¾adÃ¡vanÃ­m je potrebnÃ© tieto dokumenty naÄÃ­taÅ¥ a predspracovaÅ¥, zvyÄajne rozdelenÃ­m veÄ¾kÃ½ch dokumentov na menÅ¡ie Äasti, ich transformÃ¡ciou na textovÃ© embeddingy a uloÅ¾enÃ­m do databÃ¡zy.

- **PouÅ¾Ã­vateÄ¾skÃ½ dopyt:** pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ otÃ¡zku

- **VyhÄ¾adÃ¡vanie:** KeÄ pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ otÃ¡zku, embeddingovÃ½ model vyhÄ¾adÃ¡ relevantnÃ© informÃ¡cie v databÃ¡ze znalostÃ­, aby poskytol viac kontextu, ktorÃ½ sa zapracuje do promptu.

- **RozÅ¡Ã­renÃ¡ generÃ¡cia:** LLM vylepÅ¡Ã­ svoju odpoveÄ na zÃ¡klade zÃ­skanÃ½ch dÃ¡t. UmoÅ¾Åˆuje, aby odpoveÄ nebola zaloÅ¾enÃ¡ len na predtrÃ©novanÃ½ch dÃ¡tach, ale aj na relevantnÃ½ch informÃ¡ciÃ¡ch z pridanÃ©ho kontextu. ZÃ­skanÃ© dÃ¡ta sa pouÅ¾Ã­vajÃº na rozÅ¡Ã­renie odpovedÃ­ LLM. LLM potom vrÃ¡ti odpoveÄ na otÃ¡zku pouÅ¾Ã­vateÄ¾a.

![kresba znÃ¡zorÅˆujÃºca architektÃºru RAG](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.sk.png)

ArchitektÃºra RAG je implementovanÃ¡ pomocou transformerov pozostÃ¡vajÃºcich z dvoch ÄastÃ­: enkodÃ©ra a dekodÃ©ra. NaprÃ­klad, keÄ pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ otÃ¡zku, vstupnÃ½ text sa â€zakÃ³dujeâ€œ do vektorov zachytÃ¡vajÃºcich vÃ½znam slov a tieto vektory sa â€dekÃ³dujÃºâ€œ do nÃ¡Å¡ho dokumentovÃ©ho indexu a generujÃº novÃ½ text na zÃ¡klade pouÅ¾Ã­vateÄ¾skÃ©ho dopytu. LLM pouÅ¾Ã­va model enkodÃ©r-dekodÃ©r na generovanie vÃ½stupu.

Dva prÃ­stupy pri implementÃ¡cii RAG podÄ¾a navrhovanÃ©ho ÄlÃ¡nku: [Retrieval-Augmented Generation for Knowledge intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sÃº:

- **_RAG-Sequence_** pouÅ¾Ã­va zÃ­skanÃ© dokumenty na predpovedanie najlepÅ¡ej moÅ¾nej odpovede na pouÅ¾Ã­vateÄ¾skÃ½ dopyt

- **RAG-Token** pouÅ¾Ã­va dokumenty na generovanie ÄalÅ¡ieho tokenu, potom ich zÃ­skava na odpoveÄ na otÃ¡zku pouÅ¾Ã­vateÄ¾a

### PreÄo pouÅ¾Ã­vaÅ¥ RAG?

- **BohatosÅ¥ informÃ¡ciÃ­:** zabezpeÄuje, Å¾e textovÃ© odpovede sÃº aktuÃ¡lne a relevantnÃ©. TÃ½m zlepÅ¡uje vÃ½kon na ÃºlohÃ¡ch Å¡pecifickÃ½ch pre danÃº domÃ©nu prÃ­stupom k internej databÃ¡ze znalostÃ­.

- ZniÅ¾uje vymÃ½Å¡Ä¾anie odpovedÃ­ vyuÅ¾itÃ­m **overiteÄ¾nÃ½ch dÃ¡t** v databÃ¡ze znalostÃ­ na poskytnutie kontextu k pouÅ¾Ã­vateÄ¾skÃ½m otÃ¡zkam.

- Je **nÃ¡kladovo efektÃ­vny**, pretoÅ¾e je ekonomickejÅ¡Ã­ ako doladenie LLM.

## Vytvorenie databÃ¡zy znalostÃ­

NaÅ¡a aplikÃ¡cia je zaloÅ¾enÃ¡ na naÅ¡ich osobnÃ½ch dÃ¡tach, konkrÃ©tne na lekcii o neurÃ³novÃ½ch sieÅ¥ach z kurikula AI pre zaÄiatoÄnÃ­kov.

### VektorovÃ© databÃ¡zy

VektorovÃ¡ databÃ¡za, na rozdiel od tradiÄnÃ½ch databÃ¡z, je Å¡pecializovanÃ¡ databÃ¡za navrhnutÃ¡ na ukladanie, sprÃ¡vu a vyhÄ¾adÃ¡vanie vektorovÃ½ch embeddingov. UkladÃ¡ ÄÃ­selnÃ© reprezentÃ¡cie dokumentov. Rozkladanie dÃ¡t na ÄÃ­selnÃ© embeddingy uÄ¾ahÄuje nÃ¡Å¡mu AI systÃ©mu pochopenie a spracovanie dÃ¡t.

Embeddingy ukladÃ¡me do vektorovÃ½ch databÃ¡z, pretoÅ¾e LLM majÃº limit na poÄet tokenov, ktorÃ© prijÃ­majÃº ako vstup. KeÄÅ¾e nemÃ´Å¾ete poslaÅ¥ celÃ© embeddingy do LLM naraz, musÃ­me ich rozdeliÅ¥ na Äasti a keÄ pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ otÃ¡zku, vrÃ¡tia sa embeddingy najviac zodpovedajÃºce otÃ¡zke spolu s promptom. Rozdelenie tieÅ¾ zniÅ¾uje nÃ¡klady na poÄet tokenov prechÃ¡dzajÃºcich LLM.

Medzi populÃ¡rne vektorovÃ© databÃ¡zy patria Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant a DeepLake. Model Azure Cosmos DB mÃ´Å¾ete vytvoriÅ¥ pomocou Azure CLI prÃ­kazom:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od textu k embeddingom

Pred uloÅ¾enÃ­m dÃ¡t ich musÃ­me previesÅ¥ na vektorovÃ© embeddingy. Ak pracujete s veÄ¾kÃ½mi dokumentmi alebo dlhÃ½mi textami, mÃ´Å¾ete ich rozdeliÅ¥ podÄ¾a oÄakÃ¡vanÃ½ch dopytov. Rozdelenie mÃ´Å¾e byÅ¥ na Ãºrovni viet alebo odstavcov. KeÄÅ¾e rozdelenie vychÃ¡dza zo slov okolo, mÃ´Å¾ete ku kaÅ¾dej Äasti pridaÅ¥ ÄalÅ¡Ã­ kontext, naprÃ­klad nÃ¡zov dokumentu alebo nejakÃ½ text pred alebo za ÄasÅ¥ou. DÃ¡ta mÃ´Å¾ete rozdeliÅ¥ takto:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Po rozdelenÃ­ mÃ´Å¾eme text zakÃ³dovaÅ¥ pomocou rÃ´znych embedding modelov. NiektorÃ© modely, ktorÃ© mÃ´Å¾ete pouÅ¾iÅ¥, sÃº word2vec, ada-002 od OpenAI, Azure Computer Vision a mnoho ÄalÅ¡Ã­ch. VÃ½ber modelu zÃ¡visÃ­ od pouÅ¾Ã­vanÃ½ch jazykov, typu obsahu (text/obrÃ¡zky/audio), veÄ¾kosti vstupu, ktorÃ½ dokÃ¡Å¾e zakÃ³dovaÅ¥, a dÄºÅ¾ky vÃ½stupu embeddingu.

PrÃ­klad embeddingu textu pomocou modelu OpenAI `text-embedding-ada-002` je:
![embedding slova maÄka](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.sk.png)

## VyhÄ¾adÃ¡vanie a vektorovÃ© vyhÄ¾adÃ¡vanie

KeÄ pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ otÃ¡zku, retriever ju prevedie na vektor pomocou query enkodÃ©ra, potom vyhÄ¾adÃ¡ v naÅ¡om indexe dokumentov relevantnÃ© vektory sÃºvisiace so vstupom. Po dokonÄenÃ­ konvertuje vstupnÃ½ vektor aj vektory dokumentov spÃ¤Å¥ na text a posiela ich do LLM.

### VyhÄ¾adÃ¡vanie

VyhÄ¾adÃ¡vanie nastÃ¡va, keÄ systÃ©m rÃ½chlo hÄ¾adÃ¡ dokumenty v indexe, ktorÃ© spÄºÅˆajÃº kritÃ©riÃ¡ vyhÄ¾adÃ¡vania. CieÄ¾om retrievera je zÃ­skaÅ¥ dokumenty, ktorÃ© sa pouÅ¾ijÃº na poskytnutie kontextu a zakotvenie LLM vo vaÅ¡ich dÃ¡tach.

Existuje niekoÄ¾ko spÃ´sobov, ako vyhÄ¾adÃ¡vaÅ¥ v databÃ¡ze, naprÃ­klad:

- **VyhÄ¾adÃ¡vanie podÄ¾a kÄ¾ÃºÄovÃ½ch slov** â€“ pouÅ¾Ã­va sa na textovÃ© vyhÄ¾adÃ¡vanie

- **SÃ©mantickÃ© vyhÄ¾adÃ¡vanie** â€“ vyuÅ¾Ã­va sÃ©mantickÃ½ vÃ½znam slov

- **VektorovÃ© vyhÄ¾adÃ¡vanie** â€“ prevÃ¡dza dokumenty z textu na vektorovÃ© reprezentÃ¡cie pomocou embedding modelov. VyhÄ¾adÃ¡vanie prebieha dotazovanÃ­m dokumentov, ktorÃ½ch vektorovÃ© reprezentÃ¡cie sÃº najbliÅ¾Å¡ie k otÃ¡zke pouÅ¾Ã­vateÄ¾a.

- **HybridnÃ©** â€“ kombinÃ¡cia vyhÄ¾adÃ¡vania podÄ¾a kÄ¾ÃºÄovÃ½ch slov a vektorovÃ©ho vyhÄ¾adÃ¡vania.

ProblÃ©m pri vyhÄ¾adÃ¡vanÃ­ nastÃ¡va, keÄ v databÃ¡ze nie je podobnÃ¡ odpoveÄ na dopyt, systÃ©m potom vrÃ¡ti najlepÅ¡ie dostupnÃ© informÃ¡cie. MÃ´Å¾ete vÅ¡ak pouÅ¾iÅ¥ taktiky ako nastavenie maximÃ¡lnej vzdialenosti pre relevantnosÅ¥ alebo pouÅ¾iÅ¥ hybridnÃ© vyhÄ¾adÃ¡vanie kombinujÃºce kÄ¾ÃºÄovÃ© slovÃ¡ a vektorovÃ© vyhÄ¾adÃ¡vanie. V tejto lekcii pouÅ¾ijeme hybridnÃ© vyhÄ¾adÃ¡vanie, teda kombinÃ¡ciu vektorovÃ©ho a kÄ¾ÃºÄovÃ©ho vyhÄ¾adÃ¡vania. DÃ¡ta uloÅ¾Ã­me do dataframe s stÄºpcami obsahujÃºcimi Äasti textu aj embeddingy.

### VektorovÃ¡ podobnosÅ¥

Retriever vyhÄ¾adÃ¡va v databÃ¡ze embeddingy, ktorÃ© sÃº blÃ­zko seba, teda najbliÅ¾Å¡Ã­ch susedov, pretoÅ¾e ide o podobnÃ© texty. V scenÃ¡ri, keÄ pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ otÃ¡zku, tÃ¡ sa najprv zakÃ³duje a potom sa porovnÃ¡ s podobnÃ½mi embeddingami. BeÅ¾nou metrikou na meranie podobnosti vektorov je kosÃ­nusovÃ¡ podobnosÅ¥, ktorÃ¡ vychÃ¡dza z uhla medzi dvoma vektormi.

PodobnosÅ¥ mÃ´Å¾eme meraÅ¥ aj inÃ½mi metÃ³dami, naprÃ­klad euklidovskou vzdialenosÅ¥ou, ktorÃ¡ je priamkou medzi koncovÃ½mi bodmi vektorov, alebo skalÃ¡rnym sÃºÄinom, ktorÃ½ meria sÃºÄet sÃºÄinov zodpovedajÃºcich prvkov dvoch vektorov.

### VyhÄ¾adÃ¡vacÃ­ index

Pri vyhÄ¾adÃ¡vanÃ­ je potrebnÃ© najprv vytvoriÅ¥ vyhÄ¾adÃ¡vacÃ­ index pre naÅ¡u databÃ¡zu znalostÃ­. Index ukladÃ¡ embeddingy a umoÅ¾Åˆuje rÃ½chlo nÃ¡jsÅ¥ najpodobnejÅ¡ie Äasti aj vo veÄ¾kej databÃ¡ze. Index mÃ´Å¾eme vytvoriÅ¥ lokÃ¡lne pomocou:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Pretriedenie vÃ½sledkov (re-ranking)

Po dotazovanÃ­ databÃ¡zy mÃ´Å¾e byÅ¥ potrebnÃ© zoradiÅ¥ vÃ½sledky od najrelevantnejÅ¡Ã­ch. Re-ranking LLM vyuÅ¾Ã­va strojovÃ© uÄenie na zlepÅ¡enie relevantnosti vÃ½sledkov vyhÄ¾adÃ¡vania ich zoradenÃ­m od najrelevantnejÅ¡Ã­ch. Pri pouÅ¾itÃ­ Azure AI Search sa pretriedenie vykonÃ¡va automaticky pomocou sÃ©mantickÃ©ho pretriediaceho modelu. PrÃ­klad, ako funguje pretriedenie pomocou najbliÅ¾Å¡Ã­ch susedov:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Spojenie vÅ¡etkÃ©ho dohromady

PoslednÃ½m krokom je pridaÅ¥ nÃ¡Å¡ LLM, aby sme mohli zÃ­skaÅ¥ odpovede zakotvenÃ© v naÅ¡ich dÃ¡tach. ImplementovaÅ¥ to mÃ´Å¾eme takto:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Hodnotenie naÅ¡ej aplikÃ¡cie

### MetÃ³dy hodnotenia

- Kvalita odpovedÃ­ â€“ zabezpeÄiÅ¥, aby zneli prirodzene, plynulo a Ä¾udsky

- Zakotvenie dÃ¡t â€“ hodnotiÅ¥, Äi odpoveÄ vychÃ¡dza z poskytnutÃ½ch dokumentov

- RelevantnosÅ¥ â€“ hodnotiÅ¥, Äi odpoveÄ zodpovedÃ¡ a sÃºvisÃ­ s poloÅ¾enou otÃ¡zkou

- PlynulosÅ¥ â€“ Äi odpoveÄ dÃ¡va gramatickÃ½ zmysel

## PrÃ­klady pouÅ¾itia RAG a vektorovÃ½ch databÃ¡z

Existuje mnoho rÃ´znych prÃ­padov pouÅ¾itia, kde mÃ´Å¾u funkÄnÃ© volania zlepÅ¡iÅ¥ vaÅ¡u aplikÃ¡ciu, naprÃ­klad:

- OtÃ¡zky a odpovede: zakotvenie firemnÃ½ch dÃ¡t do chatu, ktorÃ½ mÃ´Å¾u pouÅ¾Ã­vaÅ¥ zamestnanci na kladenie otÃ¡zok.

- OdporÃºÄacie systÃ©my: vytvorenie systÃ©mu, ktorÃ½ nÃ¡jde najpodobnejÅ¡ie hodnoty, naprÃ­klad filmy, reÅ¡taurÃ¡cie a podobne.

- Chatbot sluÅ¾by: ukladanie histÃ³rie chatu a personalizÃ¡cia konverzÃ¡cie na zÃ¡klade pouÅ¾Ã­vateÄ¾skÃ½ch dÃ¡t.

- VyhÄ¾adÃ¡vanie obrÃ¡zkov na zÃ¡klade vektorovÃ½ch embeddingov, uÅ¾itoÄnÃ© pri rozpoznÃ¡vanÃ­ obrÃ¡zkov a detekcii anomÃ¡liÃ­.

## Zhrnutie

PreÅ¡li sme zÃ¡kladnÃ© oblasti RAG od pridania dÃ¡t do aplikÃ¡cie, cez pouÅ¾Ã­vateÄ¾skÃ½ dopyt aÅ¾ po vÃ½stup. Na zjednoduÅ¡enie tvorby RAG mÃ´Å¾ete pouÅ¾iÅ¥ frameworky ako Semantic Kernel, Langchain alebo Autogen.

## Zadanie

Na pokraÄovanie vo vzdelÃ¡vanÃ­ o Retrieval Augmented Generation (RAG) mÃ´Å¾ete vytvoriÅ¥:

- Front-end aplikÃ¡cie pomocou frameworku podÄ¾a vlastnÃ©ho vÃ½beru

- VyuÅ¾iÅ¥ framework, buÄ LangChain alebo Semantic Kernel, a znovu vytvoriÅ¥ svoju aplikÃ¡ciu.

Gratulujeme k dokonÄeniu lekcie ğŸ‘.

## UÄenie tu nekonÄÃ­, pokraÄujte v ceste

Po dokonÄenÃ­ tejto lekcie si pozrite naÅ¡u [kolekciu GeneratÃ­vneho AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby ste naÄalej rozvÃ­jali svoje znalosti v oblasti GeneratÃ­vnej AI!

**VyhlÃ¡senie o zodpovednosti**:  
Tento dokument bol preloÅ¾enÃ½ pomocou AI prekladateÄ¾skej sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keÄ sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, majte na pamÃ¤ti, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. OriginÃ¡lny dokument v jeho pÃ´vodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.