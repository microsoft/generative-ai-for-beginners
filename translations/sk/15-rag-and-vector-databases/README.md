<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2210a0466c812d9defc4df2d9a709ff9",
  "translation_date": "2026-01-18T19:02:22+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "sk"
}
-->
# Retrieval Augmented Generation (RAG) a vektorovÃ© databÃ¡zy

[![Retrieval Augmented Generation (RAG) and Vector Databases](../../../../../translated_images/sk/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

V lekcii o vyhÄ¾adÃ¡vacÃ­ch aplikÃ¡ciÃ¡ch sme struÄne spoznali, ako integrovaÅ¥ vlastnÃ© dÃ¡ta do veÄ¾kÃ½ch jazykovÃ½ch modelov (LLM). V tejto lekcii sa podrobnejÅ¡ie pozrieme na koncepty zakladajÃºce vaÅ¡e dÃ¡ta vo vaÅ¡ej LLM aplikÃ¡cii, mechanizmus tohto procesu a metÃ³dy ukladania dÃ¡t, vrÃ¡tane vektorovÃ½ch reprezentÃ¡ciÃ­ aj textu.

> **Video Äoskoro**

## Ãšvod

V tejto lekcii pokryjeme nasledovnÃ©:

- Ãšvod do RAG, Äo to je a preÄo sa pouÅ¾Ã­va v umelej inteligencii (AI).

- Pochopenie, Äo sÃº to vektorovÃ© databÃ¡zy a vytvorenie jednej pre naÅ¡u aplikÃ¡ciu.

- PraktickÃ½ prÃ­klad, ako integrovaÅ¥ RAG do aplikÃ¡cie.

## Ciele uÄenia

Po absolvovanÃ­ tejto lekcie budete schopnÃ­:

- VysvetliÅ¥ vÃ½znam RAG v zÃ­skavanÃ­ a spracovanÃ­ dÃ¡t.

- NastaviÅ¥ RAG aplikÃ¡ciu a zakladaÅ¥ svoje dÃ¡ta do LLM.

- EfektÃ­vne integrovaÅ¥ RAG a vektorovÃ© databÃ¡zy v LLM aplikÃ¡ciÃ¡ch.

## NÃ¡Å¡ scenÃ¡r: vylepÅ¡enie naÅ¡ich LLM vlastnÃ½mi dÃ¡tami

Pre tÃºto lekciu chceme pridaÅ¥ svoje vlastnÃ© poznÃ¡mky do vzdelÃ¡vacieho startupu, Äo umoÅ¾nÃ­ chatbotovi zÃ­skaÅ¥ viac informÃ¡ciÃ­ o rÃ´znych tÃ©mach. PouÅ¾itÃ­m poznÃ¡mok, ktorÃ© mÃ¡me, budÃº Å¡tudenti schopnÃ­ lepÅ¡ie Å¡tudovaÅ¥ a porozumieÅ¥ rÃ´znym tÃ©mam, Äo im uÄ¾ahÄÃ­ prÃ­pravu na skÃºÅ¡ky. Pre vytvorenie nÃ¡Å¡ho scenÃ¡ra pouÅ¾ijeme:

- `Azure OpenAI:` LLM, ktorÃ½ pouÅ¾ijeme na vytvorenie nÃ¡Å¡ho chatbota

- `Lekcia AI pre zaÄiatoÄnÃ­kov o neurÃ³novÃ½ch sieÅ¥ach`: to budÃº dÃ¡ta, na ktorÃ½ch zakladÃ¡me nÃ¡Å¡ LLM

- `Azure AI Search` a `Azure Cosmos DB:` vektorovÃ¡ databÃ¡za na uloÅ¾enie naÅ¡ich dÃ¡t a vytvorenie vyhÄ¾adÃ¡vacieho indexu

PouÅ¾Ã­vatelia budÃº mÃ´cÅ¥ vytvÃ¡raÅ¥ cviÄnÃ© kvÃ­zy z ich poznÃ¡mok, opakovacie kartiÄky a zhrnÃºÅ¥ ich do struÄnÃ½ch prehÄ¾adov. Aby sme zaÄali, pozrime sa, Äo je RAG a ako funguje:

## Retrieval Augmented Generation (RAG)

Chatbot zaloÅ¾enÃ½ na LLM spracuje pouÅ¾Ã­vateÄ¾skÃ© poÅ¾iadavky na generovanie odpovedÃ­. Je navrhnutÃ½ byÅ¥ interaktÃ­vny a komunikovaÅ¥ s pouÅ¾Ã­vateÄ¾mi na rÃ´zne tÃ©my. Jeho odpovede sÃº vÅ¡ak limitovanÃ© kontextom, ktorÃ½ mÃ¡ k dispozÃ­cii, a zÃ¡kladnÃ½mi trÃ©novacÃ­mi dÃ¡tami. NaprÃ­klad, dÃ¡tum ukonÄenia znalostÃ­ GPT-4 je september 2021, Äo znamenÃ¡, Å¾e nevie o udalostiach po tomto dÃ¡tume. Okrem toho dÃ¡ta pouÅ¾itÃ© na trÃ©novanie LLM nezahÅ•ÅˆajÃº dÃ´vernÃ© informÃ¡cie, ako sÃº osobnÃ© poznÃ¡mky alebo prÃ­ruÄky produktov spoloÄnosti.

### Ako fungujÃº RAG (Retrieval Augmented Generation)

![drawing showing how RAGs work](../../../../../translated_images/sk/how-rag-works.f5d0ff63942bd3a6.webp)

Povedzme, Å¾e chcete nasadiÅ¥ chatbota, ktorÃ½ vytvÃ¡ra kvÃ­zy z vaÅ¡ich poznÃ¡mok, budete potrebovaÅ¥ pripojenie k databÃ¡ze znalostÃ­. Tu prichÃ¡dza RAG na pomoc. RAG funguje nasledovne:

- **DatabÃ¡za znalostÃ­:** Pred samotnÃ½m vyhÄ¾adÃ¡vanÃ­m musia byÅ¥ dokumenty ingestovanÃ© a predspracovanÃ©, zvyÄajne rozdelenÃ­m veÄ¾kÃ½ch dokumentov na menÅ¡ie Äasti, transformÃ¡ciou na textovÃ© embeddingy a uloÅ¾enÃ­m do databÃ¡zy.

- **PouÅ¾Ã­vateÄ¾skÃ½ dopyt:** pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ otÃ¡zku

- **VyhÄ¾adÃ¡vanie:** KeÄ pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ otÃ¡zku, model embeddingu vyhÄ¾adÃ¡ relevantnÃ© informÃ¡cie z databÃ¡zy znalostÃ­, aby poskytol viac kontextu, ktorÃ½ sa zapracuje do promptu.

- **AumentovanÃ¡ generÃ¡cia:** LLM vylepÅ¡Ã­ svoju odpoveÄ na zÃ¡klade zÃ­skanÃ½ch dÃ¡t. UmoÅ¾nÃ­, aby odpoveÄ nebola zaloÅ¾enÃ¡ len na predtrÃ©novanÃ½ch dÃ¡tach, ale aj na relevantnÃ½ch informÃ¡ciÃ¡ch z pridanÃ©ho kontextu. ZÃ­skanÃ© dÃ¡ta sa pouÅ¾ijÃº na zvÃ½Å¡enie kvality odpovedÃ­ LLM. Potom LLM vrÃ¡ti odpoveÄ na pouÅ¾Ã­vateÄ¾ovu otÃ¡zku.

![drawing showing how RAGs architecture](../../../../../translated_images/sk/encoder-decode.f2658c25d0eadee2.webp)

ArchitektÃºra RAG sa implementuje pomocou transformerov pozostÃ¡vajÃºcich z dvoch ÄastÃ­: enkodÃ©ra a dekodÃ©ra. NaprÃ­klad, keÄ pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ otÃ¡zku, vstupnÃ½ text sa â€œenkÃ³dujeâ€ do vektorov zachytÃ¡vajÃºcich vÃ½znam slov a tieto vektory sa potom â€œdekÃ³dujÃºâ€ do nÃ¡Å¡ho indexu dokumentov a generuje sa novÃ½ text zaloÅ¾enÃ½ na pouÅ¾Ã­vateÄ¾skom dopyte. LLM pouÅ¾Ã­va model enkodÃ©r-dekodÃ©r na generovanie vÃ½stupu.

Dva prÃ­stupy pri implementÃ¡cii RAG podÄ¾a navrhovanÃ©ho ÄlÃ¡nku: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sÃº:

- **_RAG-Sequence_** vyuÅ¾Ã­va vyhÄ¾adanÃ© dokumenty na predpovedanie najlepÅ¡ej moÅ¾nej odpovede na pouÅ¾Ã­vateÄ¾ov dopyt

- **RAG-Token** pouÅ¾Ã­va dokumenty na generovanie ÄalÅ¡ieho tokenu, potom ich vyhÄ¾adÃ¡va, aby odpovedal na otÃ¡zku pouÅ¾Ã­vateÄ¾a

### PreÄo pouÅ¾Ã­vaÅ¥ RAG?Â 

- **BohatosÅ¥ informÃ¡ciÃ­:** zabezpeÄuje, Å¾e textovÃ© odpovede sÃº aktuÃ¡lne a sprÃ¡vne. TÃ½m zvÃ½Å¡i vÃ½kon v ÃºlohÃ¡ch Å¡pecifickÃ½ch pre danÃº oblasÅ¥ tÃ½m, Å¾e pristupuje k internej databÃ¡ze znalostÃ­.

- ZniÅ¾uje vymÃ½Å¡Ä¾anie odpovedÃ­ vyuÅ¾itÃ­m **overiteÄ¾nÃ½ch dÃ¡t** v databÃ¡ze znalostÃ­ na poskytnutie kontextu ku otÃ¡zkam pouÅ¾Ã­vateÄ¾a.

- Je **nÃ¡kladovo efektÃ­vny**, pretoÅ¾e je ekonomickejÅ¡Ã­ v porovnanÃ­ s dolaÄovanÃ­m (fine-tuningom) LLM.

## Vytvorenie databÃ¡zy znalostÃ­

NaÅ¡a aplikÃ¡cia je zaloÅ¾enÃ¡ na naÅ¡ich osobnÃ½ch dÃ¡tach, teda lekcii o neurÃ³novÃ½ch sieÅ¥ach z kurikula AI pre zaÄiatoÄnÃ­kov.

### VektorovÃ© databÃ¡zy

VektorovÃ¡ databÃ¡za, na rozdiel od tradiÄnÃ½ch databÃ¡z, je Å¡pecializovanÃ¡ databÃ¡za navrhnutÃ¡ na ukladanie, sprÃ¡vu a vyhÄ¾adÃ¡vanie vloÅ¾enÃ½ch vektorov. UkladÃ¡ ÄÃ­selnÃ© reprezentÃ¡cie dokumentov. PrevÃ¡dzanie dÃ¡t na ÄÃ­selnÃ© embeddingy uÄ¾ahÄuje nÃ¡Å¡mu AI systÃ©mu pochopenie a spracovanie dÃ¡t.

UkladÃ¡me naÅ¡e embeddingy do vektorovÃ½ch databÃ¡z, pretoÅ¾e LLM majÃº limit na poÄet tokenov, ktorÃ© prijÃ­majÃº ako vstup. KeÄÅ¾e nemÃ´Å¾ete odovzdaÅ¥ celÃ© embeddingy do LLM, je potrebnÃ© ich rozdeliÅ¥ na Äasti a keÄ pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ otÃ¡zku, vrÃ¡tia sa embeddingy, ktorÃ© sÃº k nej najbliÅ¾Å¡ie spolu s promptom. Chunkovanie tieÅ¾ zniÅ¾uje nÃ¡klady na poÄet tokenov prechÃ¡dzajÃºcich LLM.

NiektorÃ© populÃ¡rne vektorovÃ© databÃ¡zy zahÅ•ÅˆajÃº Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant a DeepLake. MÃ´Å¾ete vytvoriÅ¥ model Azure Cosmos DB pomocou Azure CLI s nasledujÃºcim prÃ­kazom:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od textu k embeddingom

Pred uloÅ¾enÃ­m dÃ¡t musÃ­me konvertovaÅ¥ text na vektorovÃ© embeddingy. Ak pracujete s veÄ¾kÃ½mi dokumentmi alebo dlhÃ½mi textami, mÃ´Å¾ete ich rozdeliÅ¥ na Äasti podÄ¾a oÄakÃ¡vanÃ½ch otÃ¡zok. Chunkovanie mÃ´Å¾e byÅ¥ na Ãºrovni viet alebo odstavcov. PretoÅ¾e chunkovanie vyvodzuje vÃ½znamy zo slov v okolÃ­, mÃ´Å¾ete ku chunkom pridaÅ¥ aj ÄalÅ¡Ã­ kontext, naprÃ­klad nÃ¡zov dokumentu alebo text pred a po chunke. DÃ¡ta mÃ´Å¾ete rozdeliÅ¥ nasledovne:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Ak poslednÃ½ blok nedosiahol minimÃ¡lnu dÄºÅ¾ku, pridajte ho aj tak
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

KeÄ sÃº dÃ¡ta rozdelenÃ©, mÃ´Å¾eme ich vloÅ¾iÅ¥ do embeddingov pomocou rÃ´znych modelov. NiektorÃ© modely, ktorÃ© mÃ´Å¾ete pouÅ¾iÅ¥, sÃº: word2vec, ada-002 od OpenAI, Azure Computer Vision a ÄalÅ¡ie. VÃ½ber modelu zÃ¡visÃ­ od pouÅ¾Ã­vanÃ½ch jazykov, typu kÃ³dovanÃ©ho obsahu (text/obrÃ¡zky/audio), veÄ¾kosti vstupu a dÄºÅ¾ky vÃ½stupnÃ©ho embeddingu.

PrÃ­klad vloÅ¾enÃ©ho textu pomocou modelu `text-embedding-ada-002` od OpenAI je:
![an embedding of the word cat](../../../../../translated_images/sk/cat.74cbd7946bc9ca38.webp)

## VyhÄ¾adÃ¡vanie a vektorovÃ© vyhÄ¾adÃ¡vanie

KeÄ pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ otÃ¡zku, vyhÄ¾adÃ¡vaÄ ju premenÃ­ na vektor pomocou enkodÃ©ra dotazu, potom prehÄ¾adÃ¡ nÃ¡Å¡ index dokumentov pre relevantnÃ© vektory, ktorÃ© sÃºvisia s danÃ½m vstupom. Po dokonÄenÃ­ konvertuje vstupnÃ½ vektor a vektory dokumentov spÃ¤Å¥ do textu a odovzdÃ¡ ich LLM.

### VyhÄ¾adÃ¡vanie

VyhÄ¾adÃ¡vanie nastÃ¡va, keÄ systÃ©m rÃ½chlo vyhÄ¾adÃ¡ dokumenty v indexe, ktorÃ© spÄºÅˆajÃº kritÃ©riÃ¡ hÄ¾adania. CieÄ¾om vyhÄ¾adÃ¡vaÄa je zÃ­skaÅ¥ dokumenty, ktorÃ© poskytnÃº kontext a zakotvia LLM vo vaÅ¡ich dÃ¡tach.

Existuje niekoÄ¾ko spÃ´sobov, ako vykonaÅ¥ vyhÄ¾adÃ¡vanie v databÃ¡ze, naprÃ­klad:

- **VyhÄ¾adÃ¡vanie podÄ¾a kÄ¾ÃºÄovÃ½ch slov** - pouÅ¾Ã­va sa na textovÃ© vyhÄ¾adÃ¡vanie

- **VektorovÃ© vyhÄ¾adÃ¡vanie** - prevÃ¡dza dokumenty z textu na vektorovÃ© reprezentÃ¡cie pomocou embedding modelov, Äo umoÅ¾Åˆuje **sÃ©mantickÃ© vyhÄ¾adÃ¡vanie** na zÃ¡klade vÃ½znamu slov. VyhÄ¾adÃ¡vanie sa uskutoÄnÃ­ vyhÄ¾adÃ¡vanÃ­m dokumentov, ktorÃ½ch vektorovÃ© reprezentÃ¡cie sÃº najbliÅ¾Å¡ie k otÃ¡zke pouÅ¾Ã­vateÄ¾a.

- **HybridnÃ©** - kombinÃ¡cia kÄ¾ÃºÄovÃ½ch slov a vektorovÃ©ho vyhÄ¾adÃ¡vania.

ProblÃ©m s vyhÄ¾adÃ¡vanÃ­m vznikÃ¡, keÄ v databÃ¡ze nie je odpoveÄ podobnÃ¡ dopytu, systÃ©m potom vrÃ¡ti najlepÅ¡ie dostupnÃ© informÃ¡cie, no mÃ´Å¾ete pouÅ¾iÅ¥ taktiky ako nastavenie maximÃ¡lnej vzdialenosti pre relevantnosÅ¥ alebo pouÅ¾iÅ¥ hybridnÃ© vyhÄ¾adÃ¡vanie, ktorÃ© kombinuje kÄ¾ÃºÄovÃ© slovÃ¡ a vektorovÃ© vyhÄ¾adÃ¡vanie. V tejto lekcii pouÅ¾ijeme hybridnÃ© vyhÄ¾adÃ¡vanie, teda kombinÃ¡ciu vektorovÃ©ho a kÄ¾ÃºÄovÃ©ho slovnÃ©ho vyhÄ¾adÃ¡vania. DÃ¡ta budeme ukladaÅ¥ do dÃ¡tovÃ©ho rÃ¡mca so stÄºpcami obsahujÃºcimi chunky aj embeddingy.

### VektorovÃ¡ podobnosÅ¥

VyhÄ¾adÃ¡vaÄ prehÄ¾adÃ¡ databÃ¡zu znalostÃ­ pre embeddingy, ktorÃ© sÃº k sebe navzÃ¡jom blÃ­zko, teda najbliÅ¾Å¡Ã­ susedia, pretoÅ¾e ide o podobnÃ© texty. V scenÃ¡ri, keÄ pouÅ¾Ã­vateÄ¾ poloÅ¾Ã­ dopyt, ten sa najskÃ´r premenÃ­ na embedding a potom sa porovnÃ¡ s podobnÃ½mi embeddingami. BeÅ¾nÃ¡ metrika pouÅ¾Ã­vanÃ¡ na zistenie podobnosti vektorov je kosÃ­nusovÃ¡ podobnosÅ¥, ktorÃ¡ sa zakladÃ¡ na uhle medzi dvoma vektormi.

PodobnosÅ¥ mÃ´Å¾eme meraÅ¥ aj inÃ½mi metÃ³dami, naprÃ­klad Euklidovskou vzdialenosÅ¥ou, ktorÃ¡ meria priamu lÃ­niu medzi koncovÃ½mi bodmi vektorov, alebo skalÃ¡rnym sÃºÄinom, ktorÃ½ meria sÃºÄet sÃºÄinov zodpovedajÃºcich prvkov dvoch vektorov.

### VyhÄ¾adÃ¡vacÃ­ index

Pred vykonanÃ­m vyhÄ¾adÃ¡vania musÃ­me vytvoriÅ¥ vyhÄ¾adÃ¡vacÃ­ index pre naÅ¡u databÃ¡zu znalostÃ­. Index ukladÃ¡ naÅ¡e embeddingy a umoÅ¾Åˆuje rÃ½chlo nÃ¡jsÅ¥ najpodobnejÅ¡ie chunky aj vo veÄ¾kej databÃ¡ze. Index mÃ´Å¾eme lokÃ¡lne vytvoriÅ¥ pomocou:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Vytvorte vyhÄ¾adÃ¡vacÃ­ index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Na dotazovanie indexu mÃ´Å¾ete pouÅ¾iÅ¥ metÃ³du kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```

### Pretriesenie (re-ranking)

KeÄ uÅ¾ mÃ¡te dotaz do databÃ¡zy, moÅ¾no budete chcieÅ¥ vÃ½sledky zoradiÅ¥ od najrelevantnejÅ¡Ã­ch po menej relevantnÃ©. Pretriesiaci LLM vyuÅ¾Ã­va strojovÃ© uÄenie na zlepÅ¡enie relevance vÃ½sledkov tÃ½m, Å¾e ich zoradÃ­ od najrelevantnejÅ¡Ã­ch. Pomocou Azure AI Search sa pretriesenie vykonÃ¡va automaticky pomocou sÃ©mantickÃ©ho pretriesiavaÄa. PrÃ­klad, ako funguje pretriesenie s pouÅ¾itÃ­m najbliÅ¾Å¡Ã­ch susedov:

```python
# NÃ¡jdite najpodobnejÅ¡ie dokumenty
distances, indices = nbrs.kneighbors([query_vector])

index = []
# VytlaÄte najpodobnejÅ¡ie dokumenty
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Zhrnutie dohromady

PoslednÃ½m krokom je pridaÅ¥ nÃ¡Å¡ LLM do celku, aby sme mohli zÃ­skaÅ¥ odpovede zakotvenÃ© v naÅ¡ich dÃ¡tach. Implementujeme to takto:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Konvertujte otÃ¡zku na dotazovÃ½ vektor
    query_vector = create_embeddings(user_input)

    # NÃ¡jdite najpodobnejÅ¡ie dokumenty
    distances, indices = nbrs.kneighbors([query_vector])

    # pridajte dokumenty k dotazu pre poskytnutie kontextu
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # skombinujte histÃ³riu a vstup pouÅ¾Ã­vateÄ¾a
    history.append(user_input)

    # vytvorte objekt sprÃ¡vy
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # pouÅ¾ite chatovÃ© dokonÄenie na generovanie odpovede
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Hodnotenie naÅ¡ej aplikÃ¡cie

### Metriky hodnotenia

- Kvalita poskytnutÃ½ch odpovedÃ­, aby zneli prirodzene, plynulo a Ä¾udsky

- ZakorenenosÅ¥ dÃ¡t: hodnotenie, Äi odpoveÄ vychÃ¡dza zo zdrojovÃ½ch dokumentov

- RelevantnosÅ¥: hodnotenie, Äi odpoveÄ zodpovedÃ¡ a sÃºvisÃ­ s poloÅ¾enou otÃ¡zkou

- PlynulosÅ¥ â€“ Äi odpoveÄ zodpovedÃ¡ gramatike

## PrÃ­klady pouÅ¾itia RAG (Retrieval Augmented Generation) a vektorovÃ½ch databÃ¡z

Existuje mnoho rÃ´znych prÃ­padov pouÅ¾itia, kde funkcia volanÃ­ mÃ´Å¾e zlepÅ¡iÅ¥ vaÅ¡u aplikÃ¡ciu, naprÃ­klad:

- OtÃ¡zky a odpovede: zakorenÃ­te firemnÃ© dÃ¡ta do chatu, ktorÃ½ mÃ´Å¾u pouÅ¾Ã­vaÅ¥ zamestnanci na kladenie otÃ¡zok.

- OdporÃºÄacie systÃ©my: kde mÃ´Å¾ete vytvoriÅ¥ systÃ©m, ktorÃ½ zodpovedÃ¡ najpodobnejÅ¡Ã­m hodnotÃ¡m, naprÃ­klad filmy, reÅ¡taurÃ¡cie a mnoho ÄalÅ¡Ã­ch.

- Chatbot sluÅ¾by: mÃ´Å¾ete ukladaÅ¥ histÃ³riu chatu a personalizovaÅ¥ konverzÃ¡ciu na zÃ¡klade pouÅ¾Ã­vateÄ¾skÃ½ch dÃ¡t.

- VyhÄ¾adÃ¡vanie obrÃ¡zkov na zÃ¡klade vektorovÃ½ch embeddingov, uÅ¾itoÄnÃ© pri rozpoznÃ¡vanÃ­ obrÃ¡zkov a detekcii anomÃ¡liÃ­.

## Zhrnutie

PreÅ¡li sme zÃ¡kladnÃ© oblasti RAG od pridÃ¡vania naÅ¡ich dÃ¡t do aplikÃ¡cie, pouÅ¾Ã­vateÄ¾skÃ©ho dopytu aÅ¾ po vÃ½stup. Pre zjednoduÅ¡enie tvorby RAG mÃ´Å¾ete pouÅ¾iÅ¥ rÃ¡mce ako Semantic Kernel, Langchain alebo Autogen.

## Zadanie

Pre pokraÄovanie vo vaÅ¡om Å¡tÃºdiu Retrieval Augmented Generation (RAG) mÃ´Å¾ete vytvoriÅ¥:

- Vytvorte front-end pre aplikÃ¡ciu pomocou frameworku podÄ¾a vÃ¡Å¡ho vÃ½beru

- VyuÅ¾ite framework, buÄ LangChain alebo Semantic Kernel, a zrekonÅ¡truujte svoju aplikÃ¡ciu.

Gratulujeme k absolvovaniu lekcie ğŸ‘.

## UÄenie nekonÄÃ­ tu, pokraÄujte v Ceste

Po dokonÄenÃ­ tejto lekcie si pozrite naÅ¡u [kolekciu GeneratÃ­vneho AI uÄenia](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby ste pokraÄovali vo zdokonaÄ¾ovanÃ­ svojich znalostÃ­ o generatÃ­vnej AI!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**VyhlÃ¡senie o zodpovednosti**:
Tento dokument bol preloÅ¾enÃ½ pomocou AI prekladateÄ¾skej sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keÄ sa snaÅ¾Ã­me o presnosÅ¥, vezmite prosÃ­m na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. OriginÃ¡lny dokument v jeho pÃ´vodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre dÃ´leÅ¾itÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. NezodpovedÃ¡me za Å¾iadne nedorozumenia alebo nesprÃ¡vne vÃ½klady vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->