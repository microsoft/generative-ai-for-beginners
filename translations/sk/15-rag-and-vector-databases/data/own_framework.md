<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:50:50+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "sk"
}
-->
# Ãšvod do neurÃ³novÃ½ch sietÃ­. ViacvrstvovÃ½ perceptrÃ³n

V predchÃ¡dzajÃºcej Äasti ste sa nauÄili o najjednoduchÅ¡om modeli neurÃ³novej siete â€“ jednovrstvovom perceptrÃ³ne, lineÃ¡rnom modeli pre binÃ¡rnu klasifikÃ¡ciu.

V tejto Äasti tento model rozÅ¡Ã­rime do flexibilnejÅ¡ieho rÃ¡mca, ktorÃ½ nÃ¡m umoÅ¾nÃ­:

* vykonÃ¡vaÅ¥ **viactriednu klasifikÃ¡ciu** okrem binÃ¡rnej
* rieÅ¡iÅ¥ **regresnÃ© Ãºlohy** okrem klasifikÃ¡cie
* separovaÅ¥ triedy, ktorÃ© nie sÃº lineÃ¡rne oddeliteÄ¾nÃ©

TieÅ¾ si vyvineme vlastnÃ½ modulÃ¡rny rÃ¡mec v Pythone, ktorÃ½ nÃ¡m umoÅ¾nÃ­ zostavovaÅ¥ rÃ´zne architektÃºry neurÃ³novÃ½ch sietÃ­.

## FormalizÃ¡cia strojovÃ©ho uÄenia

ZaÄnime formalizÃ¡ciou problÃ©mu strojovÃ©ho uÄenia. Predpokladajme, Å¾e mÃ¡me trÃ©novaciu mnoÅ¾inu dÃ¡t **X** s oznaÄeniami **Y** a potrebujeme vytvoriÅ¥ model *f*, ktorÃ½ bude robiÅ¥ Äo najpresnejÅ¡ie predikcie. Kvalita predikciÃ­ sa meria pomocou **funkcie straty** â„’. ÄŒasto pouÅ¾Ã­vanÃ© funkcie straty sÃº:

* Pre regresnÃ© Ãºlohy, kde potrebujeme predpovedaÅ¥ ÄÃ­slo, mÃ´Å¾eme pouÅ¾iÅ¥ **absolÃºtnu chybu** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| alebo **Å¡tvorcovÃº chybu** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pre klasifikÃ¡ciu pouÅ¾Ã­vame **0-1 stratu** (Äo je v podstate to istÃ© ako **presnosÅ¥** modelu) alebo **logistickÃº stratu**.

Pre jednovrstvovÃ½ perceptrÃ³n bola funkcia *f* definovanÃ¡ ako lineÃ¡rna funkcia *f(x)=wx+b* (kde *w* je matica vÃ¡h, *x* je vektor vstupnÃ½ch vlastnostÃ­ a *b* je vektor biasu). Pre rÃ´zne architektÃºry neurÃ³novÃ½ch sietÃ­ mÃ´Å¾e maÅ¥ tÃ¡to funkcia zloÅ¾itejÅ¡iu podobu.

> Pri klasifikÃ¡cii je Äasto Å¾iaduce zÃ­skaÅ¥ pravdepodobnosti prÃ­sluÅ¡nÃ½ch tried ako vÃ½stup siete. Na prevod Ä¾ubovoÄ¾nÃ½ch ÄÃ­sel na pravdepodobnosti (napr. na normalizÃ¡ciu vÃ½stupu) Äasto pouÅ¾Ã­vame **softmax** funkciu Ïƒ, a funkcia *f* sa stÃ¡va *f(x)=Ïƒ(wx+b)*

V definÃ­cii *f* vyÅ¡Å¡ie sa *w* a *b* nazÃ½vajÃº **parametre** Î¸=âŸ¨*w,b*âŸ©. Pre danÃº mnoÅ¾inu dÃ¡t âŸ¨**X**,**Y**âŸ© mÃ´Å¾eme vypoÄÃ­taÅ¥ celkovÃº chybu na celej mnoÅ¾ine ako funkciu parametrov Î¸.

> âœ… **CieÄ¾om trÃ©novania neurÃ³novej siete je minimalizovaÅ¥ chybu zmenou parametrov Î¸**

## OptimalizÃ¡cia pomocou gradientnÃ©ho zostupu

Existuje znÃ¡ma metÃ³da optimalizÃ¡cie funkciÃ­ nazÃ½vanÃ¡ **gradientnÃ½ zostup**. MyÅ¡lienka je, Å¾e mÃ´Å¾eme vypoÄÃ­taÅ¥ derivÃ¡ciu (v prÃ­pade viacrozmernom nazÃ½vanÃº **gradient**) funkcie straty vzhÄ¾adom na parametre a meniÅ¥ parametre tak, aby sa chyba zniÅ¾ovala. DÃ¡ sa to formalizovaÅ¥ takto:

* Inicializujeme parametre nÃ¡hodnÃ½mi hodnotami w<sup>(0)</sup>, b<sup>(0)</sup>
* Opakujeme nasledujÃºci krok mnohokrÃ¡t:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup> - Î· âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup> - Î· âˆ‚â„’/âˆ‚b

PoÄas trÃ©novania by sa optimalizaÄnÃ© kroky mali poÄÃ­taÅ¥ so zohÄ¾adnenÃ­m celej mnoÅ¾iny dÃ¡t (pamÃ¤tajte, Å¾e strata sa poÄÃ­ta ako sÃºÄet cez vÅ¡etky trÃ©novacie vzorky). V praxi vÅ¡ak berieme malÃ© Äasti dÃ¡t nazÃ½vanÃ© **minibatches** a poÄÃ­tame gradienty na zÃ¡klade podmnoÅ¾iny dÃ¡t. KeÄÅ¾e podmnoÅ¾ina sa vyberÃ¡ nÃ¡hodne pri kaÅ¾dom kroku, tÃ¡to metÃ³da sa nazÃ½va **stochastickÃ½ gradientnÃ½ zostup** (SGD).

## ViacvrstvovÃ© perceptrÃ³ny a spÃ¤tnÃ¡ propagÃ¡cia

JednovrstvovÃ¡ sieÅ¥, ako sme videli vyÅ¡Å¡ie, dokÃ¡Å¾e klasifikovaÅ¥ lineÃ¡rne oddeliteÄ¾nÃ© triedy. Na vytvorenie bohatÅ¡ieho modelu mÃ´Å¾eme spojiÅ¥ niekoÄ¾ko vrstiev siete. Matematicky to znamenÃ¡, Å¾e funkcia *f* bude maÅ¥ zloÅ¾itejÅ¡iu podobu a bude sa poÄÃ­taÅ¥ v niekoÄ¾kÃ½ch krokoch:
* z<sub>1</sub> = w<sub>1</sub>x + b<sub>1</sub>
* z<sub>2</sub> = w<sub>2</sub>Î±(z<sub>1</sub>) + b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Kde Î± je **nelineÃ¡rna aktivaÄnÃ¡ funkcia**, Ïƒ je softmax funkcia a parametre Î¸ = âŸ¨*w<sub>1</sub>, b<sub>1</sub>, w<sub>2</sub>, b<sub>2</sub>*âŸ©.

Algoritmus gradientnÃ©ho zostupu zostÃ¡va rovnakÃ½, ale vÃ½poÄet gradientov je zloÅ¾itejÅ¡Ã­. VÄaka pravidlu reÅ¥azovej derivÃ¡cie mÃ´Å¾eme derivÃ¡cie vypoÄÃ­taÅ¥ takto:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Pravidlo reÅ¥azovej derivÃ¡cie sa pouÅ¾Ã­va na vÃ½poÄet derivÃ¡ciÃ­ funkcie straty vzhÄ¾adom na parametre.

VÅ¡imnite si, Å¾e Ä¾avÃ¡ ÄasÅ¥ vÅ¡etkÃ½ch tÃ½chto vÃ½razov je rovnakÃ¡, a preto mÃ´Å¾eme efektÃ­vne poÄÃ­taÅ¥ derivÃ¡cie zaÄÃ­najÃºc od funkcie straty a postupovaÅ¥ â€dozaduâ€œ cez vÃ½poÄtovÃ½ graf. TÃ¡to metÃ³da trÃ©novania viacvrstvovÃ©ho perceptrÃ³nu sa nazÃ½va **spÃ¤tnÃ¡ propagÃ¡cia** alebo â€backpropâ€œ.

> TODO: citÃ¡cia obrÃ¡zka

> âœ… SpÃ¤tnÃº propagÃ¡ciu podrobnejÅ¡ie rozoberieme v naÅ¡om prÃ­klade v notebooku.

## ZÃ¡ver

V tejto lekcii sme si vytvorili vlastnÃº kniÅ¾nicu neurÃ³novÃ½ch sietÃ­ a pouÅ¾ili ju na jednoduchÃº dvojrozmernÃº klasifikaÄnÃº Ãºlohu.

## ğŸš€ VÃ½zva

V sprievodnom notebooku si implementujete vlastnÃ½ rÃ¡mec na tvorbu a trÃ©novanie viacvrstvovÃ½ch perceptrÃ³nov. Podrobne tak uvidÃ­te, ako modernÃ© neurÃ³novÃ© siete fungujÃº.

Prejdite do notebooku OwnFramework a prejdite si ho.

## PrehÄ¾ad a samostatnÃ© Å¡tÃºdium

SpÃ¤tnÃ¡ propagÃ¡cia je beÅ¾nÃ½ algoritmus pouÅ¾Ã­vanÃ½ v AI a ML, stojÃ­ za to sa mu venovaÅ¥ podrobnejÅ¡ie.

## Zadanie

V tomto laboratÃ³riu mÃ¡te pouÅ¾iÅ¥ rÃ¡mec, ktorÃ½ ste si vytvorili v tejto lekcii, na rieÅ¡enie klasifikÃ¡cie ruÄne pÃ­sanÃ½ch ÄÃ­slic MNIST.

* InÅ¡trukcie
* Notebook

**VyhlÃ¡senie o zodpovednosti**:  
Tento dokument bol preloÅ¾enÃ½ pomocou AI prekladateÄ¾skej sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, majte na pamÃ¤ti, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. OriginÃ¡lny dokument v jeho pÃ´vodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.