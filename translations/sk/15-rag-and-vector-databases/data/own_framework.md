<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:26:22+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "sk"
}
-->
# Ãšvod do NeurÃ³novÃ½ch SietÃ­. ViacvrstvovÃ½ Perceptron

V predchÃ¡dzajÃºcej Äasti ste sa nauÄili o najjednoduchÅ¡om modeli neurÃ³novej siete - jednovrstvovom perceptrone, lineÃ¡rnom modeli dvojtriednej klasifikÃ¡cie.

V tejto Äasti rozÅ¡Ã­rime tento model na flexibilnejÅ¡Ã­ rÃ¡mec, ktorÃ½ nÃ¡m umoÅ¾nÃ­:

* vykonÃ¡vaÅ¥ **viactriednu klasifikÃ¡ciu** okrem dvojtriednej
* rieÅ¡iÅ¥ **regresnÃ© problÃ©my** okrem klasifikÃ¡cie
* oddeliÅ¥ triedy, ktorÃ© nie sÃº lineÃ¡rne oddeliteÄ¾nÃ©

Vyvineme tieÅ¾ vlastnÃ½ modulÃ¡rny rÃ¡mec v Pythone, ktorÃ½ nÃ¡m umoÅ¾nÃ­ konÅ¡truovaÅ¥ rÃ´zne architektÃºry neurÃ³novÃ½ch sietÃ­.

## FormalizÃ¡cia StrojovÃ©ho UÄenia

ZaÄnime formalizÃ¡ciou problÃ©mu strojovÃ©ho uÄenia. Predpokladajme, Å¾e mÃ¡me trÃ©ningovÃº dÃ¡tovÃº sadu **X** s oznaÄeniami **Y**, a potrebujeme vytvoriÅ¥ model *f*, ktorÃ½ bude robiÅ¥ najpresnejÅ¡ie predpovede. Kvalita predpovedÃ­ sa meria pomocou **funkcie straty** â„’. NasledujÃºce funkcie straty sa Äasto pouÅ¾Ã­vajÃº:

* Pre regresnÃ½ problÃ©m, keÄ potrebujeme predpovedaÅ¥ ÄÃ­slo, mÃ´Å¾eme pouÅ¾iÅ¥ **absolÃºtnu chybu** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, alebo **kvadratickÃº chybu** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pre klasifikÃ¡ciu pouÅ¾Ã­vame **0-1 stratu** (Äo je v podstate to istÃ© ako **presnosÅ¥** modelu), alebo **logistickÃº stratu**.

Pre jednovrstvovÃ½ perceptron bola funkcia *f* definovanÃ¡ ako lineÃ¡rna funkcia *f(x)=wx+b* (tu *w* je matica vÃ¡h, *x* je vektor vstupnÃ½ch vlastnostÃ­, a *b* je vektor predpojatosti). Pre rÃ´zne architektÃºry neurÃ³novÃ½ch sietÃ­ mÃ´Å¾e tÃ¡to funkcia nadobudnÃºÅ¥ zloÅ¾itejÅ¡iu formu.

> V prÃ­pade klasifikÃ¡cie je Äasto Å¾iaduce zÃ­skaÅ¥ pravdepodobnosti zodpovedajÃºcich tried ako vÃ½stup siete. Na prevod Ä¾ubovoÄ¾nÃ½ch ÄÃ­sel na pravdepodobnosti (napr. na normalizÃ¡ciu vÃ½stupu) Äasto pouÅ¾Ã­vame funkciu **softmax** Ïƒ, a funkcia *f* sa stÃ¡va *f(x)=Ïƒ(wx+b)*

V definÃ­cii *f* vyÅ¡Å¡ie, *w* a *b* sa nazÃ½vajÃº **parametre** Î¸=âŸ¨*w,b*âŸ©. S danou dÃ¡tovou sadou âŸ¨**X**,**Y**âŸ© mÃ´Å¾eme vypoÄÃ­taÅ¥ celkovÃº chybu na celej dÃ¡tovej sade ako funkciu parametrov Î¸.

> âœ… **CieÄ¾om trÃ©ningu neurÃ³novej siete je minimalizovaÅ¥ chybu zmenou parametrov Î¸**

## OptimalizÃ¡cia Pomocou GradientnÃ©ho Zostupu

Existuje dobre znÃ¡ma metÃ³da optimalizÃ¡cie funkciÃ­ nazÃ½vanÃ¡ **gradientnÃ½ zostup**. Idea je, Å¾e mÃ´Å¾eme vypoÄÃ­taÅ¥ derivÃ¡ciu (v multi-dimenzionÃ¡lnom prÃ­pade nazÃ½vanÃº **gradient**) funkcie straty vzhÄ¾adom na parametre, a meniÅ¥ parametre tak, aby sa chyba zmenÅ¡ovala. To mÃ´Å¾eme formalizovaÅ¥ nasledovne:

* Inicializujte parametre nejakÃ½mi nÃ¡hodnÃ½mi hodnotami w<sup>(0)</sup>, b<sup>(0)</sup>
* Opakujte nasledujÃºci krok mnohokrÃ¡t:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

PoÄas trÃ©ningu by sa optimalizaÄnÃ© kroky mali vypoÄÃ­tavaÅ¥ s ohÄ¾adom na celÃº dÃ¡tovÃº sadu (pamÃ¤tajte, Å¾e strata sa poÄÃ­ta ako sÃºÄet cez vÅ¡etky trÃ©ningovÃ© vzorky). AvÅ¡ak v reÃ¡lnom Å¾ivote berieme malÃ© Äasti dÃ¡tovej sady nazÃ½vanÃ© **minibatch**, a poÄÃ­tame gradienty na zÃ¡klade podmnoÅ¾iny dÃ¡t. PretoÅ¾e podmnoÅ¾ina je vyberanÃ¡ nÃ¡hodne zakaÅ¾dÃ½m, takÃ¡to metÃ³da sa nazÃ½va **stochastickÃ½ gradientnÃ½ zostup** (SGD).

## ViacvrstvovÃ© Perceptrony a SpÃ¤tnÃ¡ PropagÃ¡cia

JednovrstvovÃ¡ sieÅ¥, ako sme videli vyÅ¡Å¡ie, je schopnÃ¡ klasifikovaÅ¥ lineÃ¡rne oddeliteÄ¾nÃ© triedy. Aby sme vytvorili bohatÅ¡Ã­ model, mÃ´Å¾eme kombinovaÅ¥ niekoÄ¾ko vrstiev siete. Matematicky by to znamenalo, Å¾e funkcia *f* by mala zloÅ¾itejÅ¡iu formu a bude sa poÄÃ­taÅ¥ v niekoÄ¾kÃ½ch krokoch:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Tu, Î± je **nelineÃ¡rna aktivaÄnÃ¡ funkcia**, Ïƒ je softmax funkcia, a parametre Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritmus gradientnÃ©ho zostupu by zostal rovnakÃ½, ale bolo by Å¥aÅ¾Å¡ie poÄÃ­taÅ¥ gradienty. VzhÄ¾adom na pravidlo reÅ¥azovej diferenciÃ¡cie mÃ´Å¾eme vypoÄÃ­taÅ¥ derivÃ¡cie ako:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Pravidlo reÅ¥azovej diferenciÃ¡cie sa pouÅ¾Ã­va na vÃ½poÄet derivÃ¡ciÃ­ funkcie straty vzhÄ¾adom na parametre.

VÅ¡imnite si, Å¾e Ä¾avÃ¡ ÄasÅ¥ vÅ¡etkÃ½ch tÃ½chto vÃ½razov je rovnakÃ¡, a preto mÃ´Å¾eme efektÃ­vne poÄÃ­taÅ¥ derivÃ¡cie poÄnÃºc funkciou straty a Ã­sÅ¥ "spÃ¤Å¥" cez vÃ½poÄtovÃ½ graf. TakÅ¾e metÃ³da trÃ©ningu viacvrstvovÃ©ho perceptronu sa nazÃ½va **spÃ¤tnÃ¡ propagÃ¡cia**, alebo 'backprop'.

> TODO: citÃ¡cia obrÃ¡zku

> âœ… SpÃ¤tnÃº propagÃ¡ciu pokryjeme oveÄ¾a podrobnejÅ¡ie v naÅ¡om prÃ­klade v notebooku.  

## ZÃ¡ver

V tejto lekcii sme vytvorili vlastnÃº kniÅ¾nicu neurÃ³novÃ½ch sietÃ­ a pouÅ¾ili sme ju na jednoduchÃº dvojrozmernÃº klasifikaÄnÃº Ãºlohu.

## ğŸš€ VÃ½zva

V sprievodnom notebooku implementujete vlastnÃ½ rÃ¡mec na vytvÃ¡ranie a trÃ©ning viacvrstvovÃ½ch perceptronov. Budete mÃ´cÅ¥ vidieÅ¥ podrobne, ako modernÃ© neurÃ³novÃ© siete fungujÃº.

PokraÄujte do notebooku OwnFramework a prejdite si ho.

## PrehÄ¾ad a SamoÅ¡tÃºdium

SpÃ¤tnÃ¡ propagÃ¡cia je beÅ¾nÃ½ algoritmus pouÅ¾Ã­vanÃ½ v AI a ML, stojÃ­ za to ho Å¡tudovaÅ¥ podrobnejÅ¡ie.

## Zadanie

V tomto laboratÃ³riu sa od vÃ¡s poÅ¾aduje pouÅ¾iÅ¥ rÃ¡mec, ktorÃ½ ste vytvorili v tejto lekcii, na rieÅ¡enie klasifikÃ¡cie ruÄne pÃ­sanÃ½ch ÄÃ­slic MNIST.

* Pokyny
* Notebook

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou AI prekladateÄ¾skej sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keÄ sa snaÅ¾Ã­me o presnosÅ¥, uvedomte si, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. NezodpovedÃ¡me za Å¾iadne nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.