<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T02:06:49+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "sk"
}
-->
# RÃ¡mce pre neurÃ³novÃ© siete

Ako sme sa uÅ¾ nauÄili, aby sme mohli efektÃ­vne trÃ©novaÅ¥ neurÃ³novÃ© siete, musÃ­me urobiÅ¥ dve veci:

* OperovaÅ¥ na tenzoroch, napr. nÃ¡sobiÅ¥, sÄÃ­taÅ¥ a poÄÃ­taÅ¥ niektorÃ© funkcie ako sigmoid alebo softmax
* PoÄÃ­taÅ¥ gradienty vÅ¡etkÃ½ch vÃ½razov, aby sme mohli vykonaÅ¥ optimalizÃ¡ciu pomocou gradientnÃ©ho zostupu

ZatiaÄ¾ Äo kniÅ¾nica `numpy` dokÃ¡Å¾e prvÃº ÄasÅ¥, potrebujeme nejakÃ½ mechanizmus na vÃ½poÄet gradientov. V naÅ¡om rÃ¡mci, ktorÃ½ sme vyvinuli v predchÃ¡dzajÃºcej sekcii, sme museli manuÃ¡lne programovaÅ¥ vÅ¡etky derivaÄnÃ© funkcie vo vnÃºtri metÃ³dy `backward`, ktorÃ¡ vykonÃ¡va spÃ¤tnÃº propagÃ¡ciu. IdeÃ¡lne by mal rÃ¡mec poskytnÃºÅ¥ moÅ¾nosÅ¥ poÄÃ­taÅ¥ gradienty *akÃ©hokoÄ¾vek vÃ½razu*, ktorÃ½ mÃ´Å¾eme definovaÅ¥.

ÄalÅ¡ou dÃ´leÅ¾itou vecou je schopnosÅ¥ vykonÃ¡vaÅ¥ vÃ½poÄty na GPU alebo inÃ½ch Å¡pecializovanÃ½ch vÃ½poÄtovÃ½ch jednotkÃ¡ch, ako je TPU. TrÃ©ning hlbokÃ½ch neurÃ³novÃ½ch sietÃ­ vyÅ¾aduje *veÄ¾a* vÃ½poÄtov a schopnosÅ¥ paralelizovaÅ¥ tieto vÃ½poÄty na GPU je veÄ¾mi dÃ´leÅ¾itÃ¡.

> âœ… TermÃ­n 'paralelizovaÅ¥' znamenÃ¡ rozloÅ¾iÅ¥ vÃ½poÄty na viacero zariadenÃ­.

V sÃºÄasnosti sÃº dva najpopulÃ¡rnejÅ¡ie rÃ¡mce pre neurÃ³novÃ© siete: TensorFlow a PyTorch. Oba poskytujÃº nÃ­zkoÃºrovÅˆovÃ© API na operovanie s tenzormi na CPU aj GPU. Na vrchole nÃ­zkoÃºrovÅˆovÃ©ho API je aj vysokoÃºrovÅˆovÃ© API, nazÃ½vanÃ© Keras a PyTorch Lightning.

NÃ­zkourovÅˆovÃ© API | TensorFlow| PyTorch
------------------|-------------------------------------|--------------------------------
VysokoÃºrovÅˆovÃ© API| Keras| Pytorch

**NÃ­zkoÃºrovÅˆovÃ© API** v oboch rÃ¡mcoch umoÅ¾ÅˆujÃº vytvÃ¡raÅ¥ tzv. **vÃ½poÄtovÃ© grafy**. Tento graf definuje, ako vypoÄÃ­taÅ¥ vÃ½stup (zvyÄajne funkciu straty) s danÃ½mi vstupnÃ½mi parametrami a mÃ´Å¾e byÅ¥ poslanÃ½ na vÃ½poÄet na GPU, ak je dostupnÃ½. ExistujÃº funkcie na diferenciÃ¡ciu tohto vÃ½poÄtovÃ©ho grafu a vÃ½poÄet gradientov, ktorÃ© mÃ´Å¾u byÅ¥ nÃ¡sledne pouÅ¾itÃ© na optimalizÃ¡ciu parametrov modelu.

**VysokoÃºrovÅˆovÃ© API** povaÅ¾ujÃº neurÃ³novÃ© siete za **sekvenciu vrstiev** a konÅ¡trukcia vÃ¤ÄÅ¡iny neurÃ³novÃ½ch sietÃ­ je tak oveÄ¾a jednoduchÅ¡ia. TrÃ©ning modelu zvyÄajne vyÅ¾aduje prÃ­pravu dÃ¡t a nÃ¡slednÃ© zavolanie funkcie `fit` na vykonanie prÃ¡ce.

VysokoÃºrovÅˆovÃ© API umoÅ¾ÅˆujÃº rÃ½chlu konÅ¡trukciu typickÃ½ch neurÃ³novÃ½ch sietÃ­ bez obÃ¡v o mnoÅ¾stvo detailov. ZÃ¡roveÅˆ nÃ­zkoÃºrovÅˆovÃ© API ponÃºkajÃº oveÄ¾a vÃ¤ÄÅ¡iu kontrolu nad trÃ©ningovÃ½m procesom, a preto sa Äasto pouÅ¾Ã­vajÃº v vÃ½skume, keÄ sa zaoberÃ¡te novÃ½mi architektÃºrami neurÃ³novÃ½ch sietÃ­.

Je tieÅ¾ dÃ´leÅ¾itÃ© pochopiÅ¥, Å¾e mÃ´Å¾ete pouÅ¾iÅ¥ obidve API spolu, napr. mÃ´Å¾ete vyvinÃºÅ¥ vlastnÃº architektÃºru vrstvy siete pomocou nÃ­zkoÃºrovÅˆovÃ©ho API a potom ju pouÅ¾iÅ¥ vo vÃ¤ÄÅ¡ej sieti konÅ¡truovanej a trÃ©novanej pomocou vysokoÃºrovÅˆovÃ©ho API. Alebo mÃ´Å¾ete definovaÅ¥ sieÅ¥ pomocou vysokoÃºrovÅˆovÃ©ho API ako sekvenciu vrstiev a potom pouÅ¾iÅ¥ vlastnÃ½ nÃ­zkoÃºrovÅˆovÃ½ trÃ©ningovÃ½ cyklus na vykonanie optimalizÃ¡cie. Obe API pouÅ¾Ã­vajÃº rovnakÃ© zÃ¡kladnÃ© koncepty a sÃº navrhnutÃ© tak, aby spolu dobre fungovali.

## UÄenie

V tomto kurze ponÃºkame vÃ¤ÄÅ¡inu obsahu pre PyTorch a TensorFlow. MÃ´Å¾ete si vybraÅ¥ preferovanÃ½ rÃ¡mec a prejsÅ¥ len prÃ­sluÅ¡nÃ© notebooky. Ak si nie ste istÃ­, ktorÃ½ rÃ¡mec zvoliÅ¥, preÄÃ­tajte si niektorÃ© diskusie na internete tÃ½kajÃºce sa **PyTorch vs. TensorFlow**. MÃ´Å¾ete sa tieÅ¾ pozrieÅ¥ na oba rÃ¡mce, aby ste zÃ­skali lepÅ¡ie pochopenie.

Kde je to moÅ¾nÃ©, pouÅ¾ijeme vysokoÃºrovÅˆovÃ© API pre jednoduchosÅ¥. VerÃ­me vÅ¡ak, Å¾e je dÃ´leÅ¾itÃ© pochopiÅ¥, ako neurÃ³novÃ© siete fungujÃº od zÃ¡kladu, preto na zaÄiatku zaÄÃ­name pracovaÅ¥ s nÃ­zkoÃºrovÅˆovÃ½m API a tenzormi. Ak vÅ¡ak chcete rÃ½chlo zaÄaÅ¥ a nechcete trÃ¡viÅ¥ veÄ¾a Äasu uÄenÃ­m sa tÃ½chto detailov, mÃ´Å¾ete ich preskoÄiÅ¥ a prejsÅ¥ priamo do notebookov s vysokoÃºrovÅˆovÃ½m API.

## âœï¸ CviÄenia: RÃ¡mce

PokraÄujte vo svojom uÄenÃ­ v nasledujÃºcich notebookoch:

NÃ­zkourovÅˆovÃ© API | TensorFlow+Keras Notebook | PyTorch
------------------|-------------------------------------|--------------------------------
VysokoÃºrovÅˆovÃ© API| Keras | *PyTorch Lightning*

Po zvlÃ¡dnutÃ­ rÃ¡mcov si zopakujeme pojem preuÄenia.

# PreuÄenie

PreuÄenie je mimoriadne dÃ´leÅ¾itÃ½ koncept v strojovom uÄenÃ­ a je veÄ¾mi dÃ´leÅ¾itÃ© ho pochopiÅ¥ sprÃ¡vne!

ZvÃ¡Å¾te nasledujÃºci problÃ©m aproximÃ¡cie 5 bodov (reprezentovanÃ½ch `x` na grafoch niÅ¾Å¡ie):

!lineÃ¡rny | preuÄenie
-------------------------|--------------------------
**LineÃ¡rny model, 2 parametre** | **NelineÃ¡rny model, 7 parametrov**
Chyba trÃ©ningu = 5.3 | Chyba trÃ©ningu = 0
Chyba validÃ¡cie = 5.1 | Chyba validÃ¡cie = 20

* VÄ¾avo vidÃ­me dobrÃº aproximÃ¡ciu priamkou. PretoÅ¾e poÄet parametrov je adekvÃ¡tny, model sprÃ¡vne pochopÃ­ rozloÅ¾enie bodov.
* Vpravo je model prÃ­liÅ¡ silnÃ½. PretoÅ¾e mÃ¡me len 5 bodov a model mÃ¡ 7 parametrov, mÃ´Å¾e sa prispÃ´sobiÅ¥ tak, aby preÅ¡iel cez vÅ¡etky body, ÄÃ­m sa chyba trÃ©ningu stane 0. AvÅ¡ak, to zabrÃ¡ni modelu pochopiÅ¥ sprÃ¡vny vzor za dÃ¡tami, preto je chyba validÃ¡cie veÄ¾mi vysokÃ¡.

Je veÄ¾mi dÃ´leÅ¾itÃ© nÃ¡jsÅ¥ sprÃ¡vnu rovnovÃ¡hu medzi bohatosÅ¥ou modelu (poÄet parametrov) a poÄtom trÃ©ningovÃ½ch vzoriek.

## PreÄo dochÃ¡dza k preuÄeniu

  * Nedostatok trÃ©ningovÃ½ch dÃ¡t
  * PrÃ­liÅ¡ silnÃ½ model
  * PrÃ­liÅ¡ veÄ¾a Å¡umu vo vstupnÃ½ch dÃ¡tach

## Ako detekovaÅ¥ preuÄenie

Ako mÃ´Å¾ete vidieÅ¥ z grafu vyÅ¡Å¡ie, preuÄenie mÃ´Å¾e byÅ¥ detekovanÃ© veÄ¾mi nÃ­zkou chybou trÃ©ningu a vysokou chybou validÃ¡cie. NormÃ¡lne poÄas trÃ©ningu vidÃ­me, ako chyby trÃ©ningu a validÃ¡cie zaÄÃ­najÃº klesaÅ¥, a potom v urÄitom bode chyba validÃ¡cie mÃ´Å¾e prestaÅ¥ klesaÅ¥ a zaÄaÅ¥ stÃºpaÅ¥. To bude znamenÃ­m preuÄenia a indikÃ¡torom, Å¾e by sme pravdepodobne mali zastaviÅ¥ trÃ©ning v tomto bode (alebo aspoÅˆ urobiÅ¥ snÃ­mku modelu).

## Ako predÃ­sÅ¥ preuÄeniu

Ak vidÃ­te, Å¾e dochÃ¡dza k preuÄeniu, mÃ´Å¾ete urobiÅ¥ jedno z nasledujÃºcich:

 * ZvÃ½Å¡iÅ¥ mnoÅ¾stvo trÃ©ningovÃ½ch dÃ¡t
 * ZnÃ­Å¾iÅ¥ komplexnosÅ¥ modelu
 * PouÅ¾iÅ¥ nejakÃº techniku regularizÃ¡cie, ako je Dropout, ktorÃº budeme neskÃ´r rozoberaÅ¥.

## PreuÄenie a kompromis medzi zaujatÃ­m a rozptylom

PreuÄenie je vlastne prÃ­pad vÅ¡eobecnejÅ¡ieho problÃ©mu v Å¡tatistike nazÃ½vanÃ©ho kompromis medzi zaujatÃ­m a rozptylom. Ak zvÃ¡Å¾ime moÅ¾nÃ© zdroje chyby v naÅ¡om modeli, mÃ´Å¾eme vidieÅ¥ dva typy chÃ½b:

* **Chyby zaujatia** sÃº spÃ´sobenÃ© tÃ½m, Å¾e nÃ¡Å¡ algoritmus nedokÃ¡Å¾e sprÃ¡vne zachytiÅ¥ vzÅ¥ah medzi trÃ©ningovÃ½mi dÃ¡tami. MÃ´Å¾e to byÅ¥ vÃ½sledok toho, Å¾e nÃ¡Å¡ model nie je dostatoÄne silnÃ½ (**poduÄenie**).
* **Chyby rozptylu**, ktorÃ© sÃº spÃ´sobenÃ© tÃ½m, Å¾e model aproximuje Å¡um vo vstupnÃ½ch dÃ¡tach namiesto zmysluplnÃ©ho vzÅ¥ahu (**preuÄenie**).

PoÄas trÃ©ningu sa chyba zaujatia zniÅ¾uje (ako sa nÃ¡Å¡ model uÄÃ­ aproximovaÅ¥ dÃ¡ta) a chyba rozptylu sa zvyÅ¡uje. Je dÃ´leÅ¾itÃ© zastaviÅ¥ trÃ©ning - buÄ manuÃ¡lne (keÄ detekujeme preuÄenie) alebo automaticky (zavedenÃ­m regularizÃ¡cie) - aby sme prediÅ¡li preuÄeniu.

## ZÃ¡ver

V tejto lekcii ste sa nauÄili o rozdieloch medzi rÃ´znymi API pre dva najpopulÃ¡rnejÅ¡ie AI rÃ¡mce, TensorFlow a PyTorch. Okrem toho ste sa nauÄili o veÄ¾mi dÃ´leÅ¾itej tÃ©me, preuÄenÃ­.

## ğŸš€ VÃ½zva

V priloÅ¾enÃ½ch notebookoch nÃ¡jdete 'Ãºlohy' na konci; prejdite si notebooky a splÅˆte Ãºlohy.

## PrehÄ¾ad & SamoÅ¡tÃºdium

Urobte si vÃ½skum na nasledujÃºce tÃ©my:

- TensorFlow
- PyTorch
- PreuÄenie

PoloÅ¾te si nasledujÃºce otÃ¡zky:

- AkÃ½ je rozdiel medzi TensorFlow a PyTorch?
- AkÃ½ je rozdiel medzi preuÄenÃ­m a poduÄenÃ­m?

## Zadanie

V tomto laboratÃ³riu sa od vÃ¡s poÅ¾aduje vyrieÅ¡iÅ¥ dva problÃ©my klasifikÃ¡cie pomocou jedno- a viacvrstvovÃ½ch plne prepojenÃ½ch sietÃ­ pomocou PyTorch alebo TensorFlow.

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, upozorÅˆujeme, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. NezodpovedÃ¡me za Å¾iadne nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.