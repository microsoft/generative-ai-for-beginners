<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:36:47+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "sk"
}
-->
# Frameworky neurÃ³novÃ½ch sietÃ­

Ako sme sa uÅ¾ nauÄili, na efektÃ­vne trÃ©novanie neurÃ³novÃ½ch sietÃ­ potrebujeme urobiÅ¥ dve veci:

* OperovaÅ¥ s tenzormi, naprÃ­klad nÃ¡sobiÅ¥, sÄÃ­taÅ¥ a poÄÃ­taÅ¥ niektorÃ© funkcie ako sigmoid alebo softmax
* VypoÄÃ­taÅ¥ gradienty vÅ¡etkÃ½ch vÃ½razov, aby sme mohli vykonaÅ¥ optimalizÃ¡ciu pomocou gradientnÃ©ho zostupu

KÃ½m kniÅ¾nica `numpy` zvlÃ¡dne prvÃº ÄasÅ¥, potrebujeme mechanizmus na vÃ½poÄet gradientov. V naÅ¡om frameworku, ktorÃ½ sme vyvinuli v predchÃ¡dzajÃºcej Äasti, sme museli manuÃ¡lne programovaÅ¥ vÅ¡etky derivÃ¡cie vo funkcii `backward`, ktorÃ¡ vykonÃ¡va spÃ¤tnÃº propagÃ¡ciu. IdeÃ¡lne by framework mal umoÅ¾niÅ¥ vÃ½poÄet gradientov *akÃ©hokoÄ¾vek vÃ½razu*, ktorÃ½ definujeme.

ÄalÅ¡ou dÃ´leÅ¾itou vecou je moÅ¾nosÅ¥ vykonÃ¡vaÅ¥ vÃ½poÄty na GPU alebo inÃ½ch Å¡pecializovanÃ½ch vÃ½poÄtovÃ½ch jednotkÃ¡ch, ako je TPU. TrÃ©ning hlbokÃ½ch neurÃ³novÃ½ch sietÃ­ vyÅ¾aduje *veÄ¾a* vÃ½poÄtov, a preto je veÄ¾mi dÃ´leÅ¾itÃ© vedieÅ¥ tieto vÃ½poÄty paralelizovaÅ¥ na GPU.

> âœ… TermÃ­n 'paralelizovaÅ¥' znamenÃ¡ rozloÅ¾iÅ¥ vÃ½poÄty na viacerÃ© zariadenia.

V sÃºÄasnosti sÃº dva najpopulÃ¡rnejÅ¡ie neurÃ³novÃ© frameworky: TensorFlow a PyTorch. Obe poskytujÃº nÃ­zkoÃºrovÅˆovÃ© API na prÃ¡cu s tenzormi na CPU aj GPU. Nad tÃ½mto nÃ­zkoÃºrovÅˆovÃ½m API existuje aj vysokoÃºrovÅˆovÃ© API, nazÃ½vanÃ© Keras a PyTorch Lightning.

NÃ­zkoÃºrovÅˆovÃ© API | TensorFlow | PyTorch
------------------|------------|---------
VysokoÃºrovÅˆovÃ© API| Keras      | PyTorch

**NÃ­zkoÃºrovÅˆovÃ© API** v oboch frameworkoch umoÅ¾Åˆuje vytvÃ¡raÅ¥ tzv. **vÃ½poÄtovÃ© grafy**. Tento graf definuje, ako vypoÄÃ­taÅ¥ vÃ½stup (zvyÄajne funkciu straty) pre danÃ© vstupnÃ© parametre a mÃ´Å¾e byÅ¥ odoslanÃ½ na vÃ½poÄet na GPU, ak je k dispozÃ­cii. ExistujÃº funkcie na diferenciÃ¡ciu tohto vÃ½poÄtovÃ©ho grafu a vÃ½poÄet gradientov, ktorÃ© sa potom pouÅ¾Ã­vajÃº na optimalizÃ¡ciu parametrov modelu.

**VysokoÃºrovÅˆovÃ© API** v podstate povaÅ¾uje neurÃ³novÃ© siete za **sekvenciu vrstiev** a vÃ½razne uÄ¾ahÄuje konÅ¡trukciu vÃ¤ÄÅ¡iny neurÃ³novÃ½ch sietÃ­. TrÃ©ning modelu zvyÄajne vyÅ¾aduje prÃ­pravu dÃ¡t a nÃ¡slednÃ© zavolanie funkcie `fit`, ktorÃ¡ vykonÃ¡ trÃ©ning.

VysokoÃºrovÅˆovÃ© API umoÅ¾Åˆuje veÄ¾mi rÃ½chlo zostaviÅ¥ typickÃ© neurÃ³novÃ© siete bez starostÃ­ o mnoÅ¾stvo detailov. Na druhej strane nÃ­zkoÃºrovÅˆovÃ© API ponÃºka oveÄ¾a vÃ¤ÄÅ¡iu kontrolu nad trÃ©ningovÃ½m procesom, a preto sa Äasto pouÅ¾Ã­va vo vÃ½skume, keÄ pracujete s novÃ½mi architektÃºrami neurÃ³novÃ½ch sietÃ­.

Je tieÅ¾ dÃ´leÅ¾itÃ© pochopiÅ¥, Å¾e mÃ´Å¾ete pouÅ¾Ã­vaÅ¥ obe API spoloÄne, naprÃ­klad mÃ´Å¾ete vyvinÃºÅ¥ vlastnÃº architektÃºru vrstvy pomocou nÃ­zkoÃºrovÅˆovÃ©ho API a potom ju pouÅ¾iÅ¥ v rÃ¡mci vÃ¤ÄÅ¡ej siete zostavenej a trÃ©novanej cez vysokoÃºrovÅˆovÃ© API. Alebo mÃ´Å¾ete definovaÅ¥ sieÅ¥ pomocou vysokoÃºrovÅˆovÃ©ho API ako sekvenciu vrstiev a potom pouÅ¾iÅ¥ vlastnÃº nÃ­zkoÃºrovÅˆovÃº trÃ©ningovÃº sluÄku na optimalizÃ¡ciu. Obe API pouÅ¾Ã­vajÃº rovnakÃ© zÃ¡kladnÃ© koncepty a sÃº navrhnutÃ© tak, aby dobre spolupracovali.

## UÄenie

V tomto kurze ponÃºkame vÃ¤ÄÅ¡inu obsahu pre PyTorch aj TensorFlow. MÃ´Å¾ete si vybraÅ¥ preferovanÃ½ framework a prejsÅ¥ si len prÃ­sluÅ¡nÃ© notebooky. Ak si nie ste istÃ­, ktorÃ½ framework zvoliÅ¥, preÄÃ­tajte si diskusie na internete o **PyTorch vs. TensorFlow**. MÃ´Å¾ete si tieÅ¾ pozrieÅ¥ oba frameworky, aby ste zÃ­skali lepÅ¡ie pochopenie.

Kde je to moÅ¾nÃ©, pouÅ¾ijeme vysokoÃºrovÅˆovÃ© API pre jednoduchosÅ¥. VerÃ­me vÅ¡ak, Å¾e je dÃ´leÅ¾itÃ© pochopiÅ¥, ako neurÃ³novÃ© siete fungujÃº od zÃ¡kladov, preto na zaÄiatku pracujeme s nÃ­zkoÃºrovÅˆovÃ½m API a tenzormi. Ak vÅ¡ak chcete zaÄaÅ¥ rÃ½chlo a nechcete trÃ¡viÅ¥ veÄ¾a Äasu Å¡tÃºdiom detailov, mÃ´Å¾ete tieto Äasti preskoÄiÅ¥ a Ã­sÅ¥ priamo do notebookov s vysokoÃºrovÅˆovÃ½m API.

## âœï¸ CviÄenia: Frameworky

PokraÄujte v uÄenÃ­ v nasledujÃºcich notebookoch:

NÃ­zkoÃºrovÅˆovÃ© API | TensorFlow+Keras Notebook | PyTorch
------------------|-----------------------------|---------
VysokoÃºrovÅˆovÃ© API| Keras                       | *PyTorch Lightning*

Po zvlÃ¡dnutÃ­ frameworkov si zrekapitulujme pojem overfitting.

# Overfitting

Overfitting je mimoriadne dÃ´leÅ¾itÃ½ pojem v strojovom uÄenÃ­ a je veÄ¾mi dÃ´leÅ¾itÃ© ho sprÃ¡vne pochopiÅ¥!

ZvÃ¡Å¾te nasledujÃºci problÃ©m aproximÃ¡cie 5 bodov (na grafoch niÅ¾Å¡ie oznaÄenÃ½ch `x`):

!linear | overfit
-------------------------|--------------------------
**LineÃ¡rny model, 2 parametre** | **NelineÃ¡rny model, 7 parametrov**
TrÃ©ningovÃ¡ chyba = 5.3 | TrÃ©ningovÃ¡ chyba = 0
ValidÃ¡cia chyba = 5.1 | ValidÃ¡cia chyba = 20

* VÄ¾avo vidÃ­me dobrÃº priamu aproximÃ¡ciu. PretoÅ¾e poÄet parametrov je primeranÃ½, model sprÃ¡vne zachytÃ¡va rozloÅ¾enie bodov.
* Vpravo je model prÃ­liÅ¡ silnÃ½. KeÄÅ¾e mÃ¡me len 5 bodov a model mÃ¡ 7 parametrov, mÃ´Å¾e sa prispÃ´sobiÅ¥ tak, Å¾e prejde vÅ¡etkÃ½mi bodmi, ÄÃ­m je trÃ©ningovÃ¡ chyba nulovÃ¡. To vÅ¡ak brÃ¡ni modelu pochopiÅ¥ sprÃ¡vny vzor za dÃ¡tami, a preto je validaÄnÃ¡ chyba veÄ¾mi vysokÃ¡.

Je veÄ¾mi dÃ´leÅ¾itÃ© nÃ¡jsÅ¥ sprÃ¡vnu rovnovÃ¡hu medzi komplexnosÅ¥ou modelu (poÄtom parametrov) a poÄtom trÃ©novacÃ­ch vzoriek.

## PreÄo dochÃ¡dza k overfittingu

  * Nedostatok trÃ©novacÃ­ch dÃ¡t
  * PrÃ­liÅ¡ silnÃ½ model
  * PrÃ­liÅ¡ veÄ¾a Å¡umu vo vstupnÃ½ch dÃ¡tach

## Ako odhaliÅ¥ overfitting

Ako vidÃ­te z grafu vyÅ¡Å¡ie, overfitting sa dÃ¡ odhaliÅ¥ podÄ¾a veÄ¾mi nÃ­zkej trÃ©ningovej chyby a vysokej validaÄnej chyby. PoÄas trÃ©ningu zvyÄajne vidÃ­me, Å¾e trÃ©ningovÃ¡ aj validaÄnÃ¡ chyba zaÄnÃº klesaÅ¥, no v urÄitom bode sa validaÄnÃ¡ chyba mÃ´Å¾e prestaÅ¥ zniÅ¾ovaÅ¥ a zaÄne rÃ¡sÅ¥. To je znak overfittingu a indikÃ¡cia, Å¾e by sme mali trÃ©ning pravdepodobne zastaviÅ¥ (alebo aspoÅˆ uloÅ¾iÅ¥ momentÃ¡lny stav modelu).

overfitting

## Ako predÃ­sÅ¥ overfittingu

Ak vidÃ­te, Å¾e dochÃ¡dza k overfittingu, mÃ´Å¾ete urobiÅ¥ niektorÃ© z nasledujÃºcich krokov:

 * ZvÃ½Å¡iÅ¥ mnoÅ¾stvo trÃ©novacÃ­ch dÃ¡t
 * ZnÃ­Å¾iÅ¥ zloÅ¾itosÅ¥ modelu
 * PouÅ¾iÅ¥ nejakÃº regularizaÄnÃº techniku, naprÃ­klad Dropout, ktorÃº si neskÃ´r rozoberieme.

## Overfitting a kompromis medzi Bias a Varianciou

Overfitting je vlastne Å¡pecifickÃ½ prÃ­pad vÅ¡eobecnejÅ¡ieho problÃ©mu v Å¡tatistike nazÃ½vanÃ©ho kompromis Bias-Variance. Ak zvÃ¡Å¾ime moÅ¾nÃ© zdroje chÃ½b v naÅ¡om modeli, mÃ´Å¾eme rozlÃ­Å¡iÅ¥ dva typy chÃ½b:

* **Bias chyby** sÃº spÃ´sobenÃ© tÃ½m, Å¾e nÃ¡Å¡ algoritmus nedokÃ¡Å¾e sprÃ¡vne zachytiÅ¥ vzÅ¥ah v trÃ©novacÃ­ch dÃ¡tach. MÃ´Å¾e to byÅ¥ spÃ´sobenÃ© tÃ½m, Å¾e model nie je dostatoÄne silnÃ½ (**underfitting**).
* **Variance chyby** sÃº spÃ´sobenÃ© tÃ½m, Å¾e model aproximuje Å¡um vo vstupnÃ½ch dÃ¡tach namiesto zmysluplnÃ©ho vzÅ¥ahu (**overfitting**).

PoÄas trÃ©ningu sa bias chyba zniÅ¾uje (model sa uÄÃ­ aproximovaÅ¥ dÃ¡ta) a variance chyba rastie. Je dÃ´leÅ¾itÃ© trÃ©ning zastaviÅ¥ â€“ buÄ manuÃ¡lne (keÄ zistÃ­me overfitting) alebo automaticky (zavedenÃ­m regularizÃ¡cie) â€“ aby sme prediÅ¡li overfittingu.

## Zhrnutie

V tejto lekcii ste sa dozvedeli o rozdieloch medzi rÃ´znymi API pre dva najpopulÃ¡rnejÅ¡ie AI frameworky, TensorFlow a PyTorch. Okrem toho ste sa nauÄili o veÄ¾mi dÃ´leÅ¾itej tÃ©me, overfittingu.

## ğŸš€ VÃ½zva

V sprievodnÃ½ch notebookoch nÃ¡jdete na konci â€Ãºlohyâ€œ; prejdite si notebooky a splÅˆte ich.

## PrehÄ¾ad a samostatnÃ© Å¡tÃºdium

Urobte si prieskum na nasledujÃºce tÃ©my:

- TensorFlow
- PyTorch
- Overfitting

PoloÅ¾te si nasledujÃºce otÃ¡zky:

- AkÃ½ je rozdiel medzi TensorFlow a PyTorch?
- AkÃ½ je rozdiel medzi overfittingom a underfittingom?

## Zadanie

V tomto laboratÃ³riu mÃ¡te vyrieÅ¡iÅ¥ dva klasifikaÄnÃ© problÃ©my pomocou jednovrstvovÃ½ch a viacvrstvovÃ½ch plne prepojenÃ½ch sietÃ­ s pouÅ¾itÃ­m PyTorch alebo TensorFlow.

**VyhlÃ¡senie o zodpovednosti**:  
Tento dokument bol preloÅ¾enÃ½ pomocou AI prekladateÄ¾skej sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keÄ sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, majte na pamÃ¤ti, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. OriginÃ¡lny dokument v jeho pÃ´vodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.