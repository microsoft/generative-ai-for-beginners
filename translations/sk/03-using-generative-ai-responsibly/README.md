<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7f8f4c11f8c1cb6e1794442dead414ea",
  "translation_date": "2025-07-09T09:02:35+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "sk"
}
-->
# PouÅ¾Ã­vanie generatÃ­vnej AI zodpovedne

[![PouÅ¾Ã­vanie generatÃ­vnej AI zodpovedne](../../../translated_images/03-lesson-banner.1ed56067a452d97709d51f6cc8b6953918b2287132f4909ade2008c936cd4af9.sk.png)](https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst)

> _Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre zobrazenie videa k tejto lekcii_

Je Ä¾ahkÃ© byÅ¥ fascinovanÃ½ AI, najmÃ¤ generatÃ­vnou AI, no je potrebnÃ© zvÃ¡Å¾iÅ¥, ako ju pouÅ¾Ã­vaÅ¥ zodpovedne. Treba braÅ¥ do Ãºvahy, ako zabezpeÄiÅ¥, aby vÃ½stupy boli spravodlivÃ©, neÅ¡kodnÃ© a ÄalÅ¡ie aspekty. TÃ¡to kapitola vÃ¡m poskytne potrebnÃ½ kontext, na Äo myslieÅ¥ a ako aktÃ­vne zlepÅ¡iÅ¥ svoje pouÅ¾Ã­vanie AI.

## Ãšvod

V tejto lekcii sa dozviete:

- PreÄo by ste mali uprednostniÅ¥ ZodpovednÃº AI pri tvorbe aplikÃ¡ciÃ­ s generatÃ­vnou AI.
- ZÃ¡kladnÃ© princÃ­py Zodpovednej AI a ich vzÅ¥ah k generatÃ­vnej AI.
- Ako tieto princÃ­py Zodpovednej AI uviesÅ¥ do praxe pomocou stratÃ©gie a nÃ¡strojov.

## Ciele uÄenia

Po dokonÄenÃ­ tejto lekcie budete vedieÅ¥:

- AkÃ½ vÃ½znam mÃ¡ ZodpovednÃ¡ AI pri tvorbe aplikÃ¡ciÃ­ s generatÃ­vnou AI.
- Kedy myslieÅ¥ na a aplikovaÅ¥ zÃ¡kladnÃ© princÃ­py Zodpovednej AI pri tvorbe generatÃ­vnych AI aplikÃ¡ciÃ­.
- AkÃ© nÃ¡stroje a stratÃ©gie mÃ¡te k dispozÃ­cii na praktickÃº implementÃ¡ciu konceptu Zodpovednej AI.

## PrincÃ­py Zodpovednej AI

NadÅ¡enie pre generatÃ­vnu AI nikdy nebolo vÃ¤ÄÅ¡ie. Toto nadÅ¡enie pritiahlo mnoÅ¾stvo novÃ½ch vÃ½vojÃ¡rov, pozornosÅ¥ a financovanie do tejto oblasti. Hoci je to veÄ¾mi pozitÃ­vne pre kaÅ¾dÃ©ho, kto chce budovaÅ¥ produkty a firmy vyuÅ¾Ã­vajÃºce generatÃ­vnu AI, je rovnako dÃ´leÅ¾itÃ© postupovaÅ¥ zodpovedne.

V priebehu tohto kurzu sa zameriavame na budovanie nÃ¡Å¡ho startupu a nÃ¡Å¡ho vzdelÃ¡vacieho AI produktu. PouÅ¾ijeme princÃ­py Zodpovednej AI: SpravodlivosÅ¥, InkluzÃ­vnosÅ¥, SpoÄ¾ahlivosÅ¥/BezpeÄnosÅ¥, BezpeÄnosÅ¥ a Ochrana sÃºkromia, TransparentnosÅ¥ a ZodpovednosÅ¥. Pomocou tÃ½chto princÃ­pov preskÃºmame, ako sÃºvisia s naÅ¡Ã­m vyuÅ¾itÃ­m generatÃ­vnej AI v naÅ¡ich produktoch.

## PreÄo by ste mali uprednostniÅ¥ ZodpovednÃº AI

Pri tvorbe produktu vedie k najlepÅ¡Ã­m vÃ½sledkom prÃ­stup zameranÃ½ na Äloveka, ktorÃ½ mÃ¡ na pamÃ¤ti najlepÅ¡Ã­ zÃ¡ujem pouÅ¾Ã­vateÄ¾a.

JedineÄnosÅ¥ generatÃ­vnej AI spoÄÃ­va v jej schopnosti vytvÃ¡raÅ¥ uÅ¾itoÄnÃ© odpovede, informÃ¡cie, usmernenia a obsah pre pouÅ¾Ã­vateÄ¾ov. To mÃ´Å¾e byÅ¥ dosiahnutÃ© bez mnohÃ½ch manuÃ¡lnych krokov, Äo vedie k veÄ¾mi pÃ´sobivÃ½m vÃ½sledkom. Bez sprÃ¡vneho plÃ¡novania a stratÃ©giÃ­ vÅ¡ak mÃ´Å¾e, Å¾iaÄ¾, viesÅ¥ aj k Å¡kodlivÃ½m dÃ´sledkom pre vaÅ¡ich pouÅ¾Ã­vateÄ¾ov, vÃ¡Å¡ produkt a spoloÄnosÅ¥ ako celok.

Pozrime sa na niektorÃ© (ale nie vÅ¡etky) z tÃ½chto potenciÃ¡lne Å¡kodlivÃ½ch dÃ´sledkov:

### HalucinÃ¡cie

HalucinÃ¡cie sÃº termÃ­n pouÅ¾Ã­vanÃ½ na opis situÃ¡cie, keÄ LLM vytvorÃ­ obsah, ktorÃ½ je buÄ Ãºplne nezmyselnÃ½, alebo vieme, Å¾e je fakticky nesprÃ¡vny na zÃ¡klade inÃ½ch zdrojov informÃ¡ciÃ­.

NaprÃ­klad, ak vytvorÃ­me funkciu pre nÃ¡Å¡ startup, ktorÃ¡ umoÅ¾Åˆuje Å¡tudentom klÃ¡sÅ¥ historickÃ© otÃ¡zky modelu. Å tudent sa opÃ½ta: `Kto bol jedinÃ½m preÅ¾ivÅ¡Ã­m Titanicu?`

Model vygeneruje odpoveÄ ako tÃ¡to:

![Prompt saying "Who was the sole survivor of the Titanic"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp)

> _(Zdroj: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

Ide o veÄ¾mi sebavedomÃº a dÃ´kladnÃº odpoveÄ. BohuÅ¾iaÄ¾, je nesprÃ¡vna. Aj pri minimÃ¡lnom vÃ½skume by sme zistili, Å¾e preÅ¾ilo viac ako jedna osoba z katastrofy Titanicu. Pre Å¡tudenta, ktorÃ½ prÃ¡ve zaÄÃ­na skÃºmaÅ¥ tÃºto tÃ©mu, mÃ´Å¾e byÅ¥ tÃ¡to odpoveÄ dostatoÄne presvedÄivÃ¡ na to, aby ju nepochyboval a povaÅ¾oval za fakt. DÃ´sledkom mÃ´Å¾e byÅ¥, Å¾e AI systÃ©m bude povaÅ¾ovanÃ½ za nespoÄ¾ahlivÃ½ a negatÃ­vne to ovplyvnÃ­ reputÃ¡ciu nÃ¡Å¡ho startupu.

S kaÅ¾dou novou verziou LLM sme zaznamenali zlepÅ¡enia v minimalizÃ¡cii halucinÃ¡ciÃ­. Napriek tomu ako tvorcovia aplikÃ¡ciÃ­ a pouÅ¾Ã­vatelia musÃ­me byÅ¥ stÃ¡le vedomÃ­ tÃ½chto obmedzenÃ­.

### Å kodlivÃ½ obsah

V predchÃ¡dzajÃºcej Äasti sme sa venovali situÃ¡ciÃ¡m, keÄ LLM vytvÃ¡ra nesprÃ¡vne alebo nezmyselnÃ© odpovede. ÄalÅ¡Ã­m rizikom, na ktorÃ© musÃ­me dÃ¡vaÅ¥ pozor, je, keÄ model odpovedÃ¡ Å¡kodlivÃ½m obsahom.

Å kodlivÃ½ obsah moÅ¾no definovaÅ¥ ako:

- Poskytovanie inÅ¡trukciÃ­ alebo povzbudzovanie k sebapoÅ¡kodzovaniu alebo poÅ¡kodeniu urÄitÃ½ch skupÃ­n.
- NenÃ¡vistnÃ½ alebo poniÅ¾ujÃºci obsah.
- UsmerÅˆovanie plÃ¡novania akÃ©hokoÄ¾vek Ãºtoku alebo nÃ¡silnÃ½ch Äinov.
- Poskytovanie nÃ¡vodov, ako nÃ¡jsÅ¥ nelegÃ¡lny obsah alebo spÃ¡chaÅ¥ nelegÃ¡lne Äiny.
- Zobrazovanie sexuÃ¡lne explicitnÃ©ho obsahu.

Pre nÃ¡Å¡ startup chceme zabezpeÄiÅ¥, aby sme mali sprÃ¡vne nÃ¡stroje a stratÃ©gie na zabrÃ¡nenie tomu, aby Å¡tudenti videli takÃ½to obsah.

### Nedostatok spravodlivosti

SpravodlivosÅ¥ znamenÃ¡ â€zabezpeÄiÅ¥, aby AI systÃ©m bol bez predsudkov a diskriminÃ¡cie a aby vÅ¡etkÃ½ch spravodlivo a rovnako zaobchÃ¡dzal.â€œ V oblasti generatÃ­vnej AI chceme zabezpeÄiÅ¥, aby model nevytvÃ¡ral vÃ½stupy, ktorÃ© by posilÅˆovali vyluÄujÃºce pohÄ¾ady na svet voÄi marginalizovanÃ½m skupinÃ¡m.

TakÃ©to vÃ½stupy nielenÅ¾e niÄia pozitÃ­vne pouÅ¾Ã­vateÄ¾skÃ© skÃºsenosti, ale spÃ´sobujÃº aj ÄalÅ¡iu spoloÄenskÃº ujmu. Ako tvorcovia aplikÃ¡ciÃ­ by sme mali vÅ¾dy myslieÅ¥ na Å¡irokÃº a rÃ´znorodÃº pouÅ¾Ã­vateÄ¾skÃº zÃ¡kladÅˆu pri budovanÃ­ rieÅ¡enÃ­ s generatÃ­vnou AI.

## Ako pouÅ¾Ã­vaÅ¥ generatÃ­vnu AI zodpovedne

KeÄÅ¾e sme si uvedomili vÃ½znam Zodpovednej generatÃ­vnej AI, pozrime sa na 4 kroky, ktorÃ© mÃ´Å¾eme podniknÃºÅ¥, aby sme naÅ¡e AI rieÅ¡enia budovali zodpovedne:

![Mitigate Cycle](../../../translated_images/mitigate-cycle.babcd5a5658e1775d5f2cb47f2ff305cca090400a72d98d0f9e57e9db5637c72.sk.png)

### Meranie potenciÃ¡lnych Å¡kÃ´d

Pri testovanÃ­ softvÃ©ru testujeme oÄakÃ¡vanÃ© akcie pouÅ¾Ã­vateÄ¾a v aplikÃ¡cii. Podobne je dobrÃ© otestovaÅ¥ rÃ´zne typy promptov, ktorÃ© pouÅ¾Ã­vatelia pravdepodobne pouÅ¾ijÃº, aby sme zmerali potenciÃ¡lne Å¡kody.

KeÄÅ¾e nÃ¡Å¡ startup buduje vzdelÃ¡vacÃ­ produkt, je vhodnÃ© pripraviÅ¥ zoznam promptov sÃºvisiacich so vzdelÃ¡vanÃ­m. MÃ´Å¾u pokrÃ½vaÅ¥ konkrÃ©tny predmet, historickÃ© fakty alebo otÃ¡zky o Å¡tudentskom Å¾ivote.

### Zmiernenie potenciÃ¡lnych Å¡kÃ´d

Teraz je Äas nÃ¡jsÅ¥ spÃ´soby, ako zabrÃ¡niÅ¥ alebo obmedziÅ¥ potenciÃ¡lne Å¡kody spÃ´sobenÃ© modelom a jeho odpoveÄami. MÃ´Å¾eme to posudzovaÅ¥ v 4 rÃ´znych vrstvÃ¡ch:

![Mitigation Layers](../../../translated_images/mitigation-layers.377215120b9a1159a8c3982c6bbcf41b6adf8c8fa04ce35cbaeeb13b4979cdfc.sk.png)

- **Model**. VybraÅ¥ sprÃ¡vny model pre konkrÃ©tny prÃ­pad pouÅ¾itia. VÃ¤ÄÅ¡ie a zloÅ¾itejÅ¡ie modely ako GPT-4 mÃ´Å¾u predstavovaÅ¥ vÃ¤ÄÅ¡ie riziko Å¡kodlivÃ©ho obsahu pri pouÅ¾itÃ­ v menÅ¡Ã­ch a Å¡pecifickejÅ¡Ã­ch prÃ­padoch. PouÅ¾itie vlastnÃ½ch trÃ©ningovÃ½ch dÃ¡t na doladenie modelu tieÅ¾ zniÅ¾uje riziko Å¡kodlivÃ©ho obsahu.

- **BezpeÄnostnÃ½ systÃ©m**. BezpeÄnostnÃ½ systÃ©m je sÃºbor nÃ¡strojov a nastavenÃ­ na platforme, ktorÃ¡ model poskytuje, a pomÃ¡ha zmierniÅ¥ Å¡kody. PrÃ­kladom je systÃ©m filtrovania obsahu v sluÅ¾be Azure OpenAI. SystÃ©my by mali tieÅ¾ detegovaÅ¥ Ãºtoky typu jailbreak a neÅ¾iaducu aktivitu, naprÃ­klad poÅ¾iadavky od botov.

- **Metaprompt**. Metaprompt a zakotvenie sÃº spÃ´soby, ako mÃ´Å¾eme model usmerniÅ¥ alebo obmedziÅ¥ na zÃ¡klade urÄitÃ½ch sprÃ¡vanÃ­ a informÃ¡ciÃ­. MÃ´Å¾e to byÅ¥ pouÅ¾itie systÃ©movÃ½ch vstupov na definovanie urÄitÃ½ch limitov modelu. Okrem toho poskytovanie vÃ½stupov, ktorÃ© sÃº relevantnejÅ¡ie pre rozsah alebo domÃ©nu systÃ©mu.

MÃ´Å¾e to tieÅ¾ zahÅ•ÅˆaÅ¥ techniky ako Retrieval Augmented Generation (RAG), kde model ÄerpÃ¡ informÃ¡cie len z vybranÃ½ch dÃ´veryhodnÃ½ch zdrojov. V tomto kurze je neskÃ´r lekcia o [budovanÃ­ vyhÄ¾adÃ¡vacÃ­ch aplikÃ¡ciÃ­](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **PouÅ¾Ã­vateÄ¾skÃ¡ skÃºsenosÅ¥**. PoslednÃ¡ vrstva je, kde pouÅ¾Ã­vateÄ¾ priamo interaguje s modelom cez rozhranie naÅ¡ej aplikÃ¡cie. MÃ´Å¾eme navrhnÃºÅ¥ UI/UX tak, aby sme obmedzili typy vstupov, ktorÃ© mÃ´Å¾e pouÅ¾Ã­vateÄ¾ posielaÅ¥ modelu, ako aj text alebo obrÃ¡zky, ktorÃ© sa mu zobrazujÃº. Pri nasadzovanÃ­ AI aplikÃ¡cie musÃ­me byÅ¥ tieÅ¾ transparentnÃ­ o tom, Äo naÅ¡a generatÃ­vna AI aplikÃ¡cia dokÃ¡Å¾e a Äo nie.

MÃ¡me celÃº lekciu venovanÃº [navrhovaniu UX pre AI aplikÃ¡cie](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **Vyhodnotenie modelu**. PrÃ¡ca s LLM mÃ´Å¾e byÅ¥ nÃ¡roÄnÃ¡, pretoÅ¾e nemÃ¡me vÅ¾dy kontrolu nad dÃ¡tami, na ktorÃ½ch bol model trÃ©novanÃ½. Napriek tomu by sme mali vÅ¾dy vyhodnocovaÅ¥ vÃ½kon a vÃ½stupy modelu. Je dÃ´leÅ¾itÃ© meraÅ¥ presnosÅ¥ modelu, podobnosÅ¥, zakotvenosÅ¥ a relevantnosÅ¥ vÃ½stupu. To pomÃ¡ha zabezpeÄiÅ¥ transparentnosÅ¥ a dÃ´veru u zainteresovanÃ½ch strÃ¡n a pouÅ¾Ã­vateÄ¾ov.

### PrevÃ¡dzka zodpovednÃ©ho generatÃ­vneho AI rieÅ¡enia

Vybudovanie prevÃ¡dzkovej praxe okolo vaÅ¡ich AI aplikÃ¡ciÃ­ je zÃ¡vereÄnÃ½m krokom. ZahÅ•Åˆa spoluprÃ¡cu s ÄalÅ¡Ã­mi ÄasÅ¥ami nÃ¡Å¡ho startupu, ako sÃº prÃ¡vne a bezpeÄnostnÃ© oddelenia, aby sme zabezpeÄili sÃºlad so vÅ¡etkÃ½mi regulaÄnÃ½mi poÅ¾iadavkami. Pred spustenÃ­m chceme tieÅ¾ vytvoriÅ¥ plÃ¡ny na doruÄenie, rieÅ¡enie incidentov a nÃ¡vrat k predchÃ¡dzajÃºcej verzii, aby sme prediÅ¡li Å¡kodÃ¡m na pouÅ¾Ã­vateÄ¾och.

## NÃ¡stroje

Hoci sa vÃ½voj rieÅ¡enÃ­ Zodpovednej AI mÃ´Å¾e zdaÅ¥ nÃ¡roÄnÃ½, je to prÃ¡ca, ktorÃ¡ sa oplatÃ­. Ako oblasÅ¥ generatÃ­vnej AI rastie, vyvÃ­jajÃº sa aj nÃ¡stroje, ktorÃ© pomÃ¡hajÃº vÃ½vojÃ¡rom efektÃ­vne zaÄleniÅ¥ zodpovednosÅ¥ do ich pracovnÃ½ch postupov. NaprÃ­klad [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) dokÃ¡Å¾e cez API poÅ¾iadavku detegovaÅ¥ Å¡kodlivÃ½ obsah a obrÃ¡zky.

## Kontrola vedomostÃ­

Na Äo by ste mali dbaÅ¥, aby ste zabezpeÄili zodpovednÃ© pouÅ¾Ã­vanie AI?

1. Aby bola odpoveÄ sprÃ¡vna.
1. Aby sa AI nepouÅ¾Ã­vala na kriminÃ¡lne ÃºÄely.
1. Aby AI bola bez predsudkov a diskriminÃ¡cie.

OdpoveÄ: SprÃ¡vne sÃº body 2 a 3. ZodpovednÃ¡ AI vÃ¡m pomÃ¡ha zvÃ¡Å¾iÅ¥, ako zmierniÅ¥ Å¡kodlivÃ© ÃºÄinky, predsudky a ÄalÅ¡ie.

## ğŸš€ VÃ½zva

PreÄÃ­tajte si o [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) a zistite, Äo mÃ´Å¾ete vo svojom pouÅ¾Ã­vanÃ­ prijaÅ¥.

## SkvelÃ¡ prÃ¡ca, pokraÄujte v uÄenÃ­

Po dokonÄenÃ­ tejto lekcie si pozrite naÅ¡u [kolekciu GeneratÃ­vne AI uÄenia](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) a pokraÄujte v rozÅ¡irovanÃ­ svojich znalostÃ­ o generatÃ­vnej AI!

Prejdite na Lekciu 4, kde sa pozrieme na [ZÃ¡klady prompt engineeringu](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol preloÅ¾enÃ½ pomocou AI prekladateÄ¾skej sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keÄ sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, majte na pamÃ¤ti, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.