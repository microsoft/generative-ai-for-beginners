<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4d57fad773cbeb69c5dd62e65c34200d",
  "translation_date": "2025-10-17T21:55:24+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "sk"
}
-->
# PouÅ¾Ã­vanie generatÃ­vnej AI zodpovedne

[![PouÅ¾Ã­vanie generatÃ­vnej AI zodpovedne](../../../translated_images/03-lesson-banner.1ed56067a452d97709d51f6cc8b6953918b2287132f4909ade2008c936cd4af9.sk.png)](https://youtu.be/YOp-e1GjZdA?si=7Wv4wu3x44L1DCVj)

> _Kliknite na obrÃ¡zok vyÅ¡Å¡ie a pozrite si video k tejto lekcii_

Je Ä¾ahkÃ© byÅ¥ fascinovanÃ½ AI, najmÃ¤ generatÃ­vnou AI, ale je potrebnÃ© zvÃ¡Å¾iÅ¥, ako ju pouÅ¾Ã­vaÅ¥ zodpovedne. MusÃ­te premÃ½Å¡Ä¾aÅ¥ nad tÃ½m, ako zabezpeÄiÅ¥, aby vÃ½stupy boli spravodlivÃ©, neÅ¡kodnÃ© a podobne. TÃ¡to kapitola mÃ¡ za cieÄ¾ poskytnÃºÅ¥ vÃ¡m uvedenÃ½ kontext, Äo zvÃ¡Å¾iÅ¥ a ako podniknÃºÅ¥ aktÃ­vne kroky na zlepÅ¡enie vÃ¡Å¡ho pouÅ¾Ã­vania AI.

## Ãšvod

TÃ¡to lekcia pokryje:

- PreÄo by ste mali uprednostniÅ¥ zodpovednÃº AI pri budovanÃ­ aplikÃ¡ciÃ­ generatÃ­vnej AI.
- ZÃ¡kladnÃ© princÃ­py zodpovednej AI a ich vzÅ¥ah ku generatÃ­vnej AI.
- Ako uviesÅ¥ tieto princÃ­py zodpovednej AI do praxe prostrednÃ­ctvom stratÃ©gie a nÃ¡strojov.

## Ciele uÄenia

Po dokonÄenÃ­ tejto lekcie budete vedieÅ¥:

- DÃ´leÅ¾itosÅ¥ zodpovednej AI pri budovanÃ­ aplikÃ¡ciÃ­ generatÃ­vnej AI.
- Kedy premÃ½Å¡Ä¾aÅ¥ a aplikovaÅ¥ zÃ¡kladnÃ© princÃ­py zodpovednej AI pri budovanÃ­ aplikÃ¡ciÃ­ generatÃ­vnej AI.
- AkÃ© nÃ¡stroje a stratÃ©gie mÃ¡te k dispozÃ­cii na uvedenie konceptu zodpovednej AI do praxe.

## PrincÃ­py zodpovednej AI

NadÅ¡enie z generatÃ­vnej AI nikdy nebolo vyÅ¡Å¡ie. Toto nadÅ¡enie prinieslo do tejto oblasti veÄ¾a novÃ½ch vÃ½vojÃ¡rov, pozornosti a financovania. Hoci je to veÄ¾mi pozitÃ­vne pre kaÅ¾dÃ©ho, kto chce budovaÅ¥ produkty a firmy vyuÅ¾Ã­vajÃºce generatÃ­vnu AI, je tieÅ¾ dÃ´leÅ¾itÃ© postupovaÅ¥ zodpovedne.

V priebehu tohto kurzu sa zameriavame na budovanie nÃ¡Å¡ho startupu a nÃ¡Å¡ho vzdelÃ¡vacieho produktu AI. PouÅ¾ijeme princÃ­py zodpovednej AI: spravodlivosÅ¥, inkluzÃ­vnosÅ¥, spoÄ¾ahlivosÅ¥/bezpeÄnosÅ¥, zabezpeÄenie a sÃºkromie, transparentnosÅ¥ a zodpovednosÅ¥. S tÃ½mito princÃ­pmi preskÃºmame, ako sa vzÅ¥ahujÃº na naÅ¡e vyuÅ¾Ã­vanie generatÃ­vnej AI v naÅ¡ich produktoch.

## PreÄo by ste mali uprednostniÅ¥ zodpovednÃº AI

Pri budovanÃ­ produktu dosiahneme najlepÅ¡ie vÃ½sledky, ak zvolÃ­me prÃ­stup zameranÃ½ na Äloveka a budeme maÅ¥ na pamÃ¤ti najlepÅ¡ie zÃ¡ujmy naÅ¡ich pouÅ¾Ã­vateÄ¾ov.

JedineÄnosÅ¥ generatÃ­vnej AI spoÄÃ­va v jej schopnosti vytvÃ¡raÅ¥ uÅ¾itoÄnÃ© odpovede, informÃ¡cie, usmernenia a obsah pre pouÅ¾Ã­vateÄ¾ov. To sa dÃ¡ dosiahnuÅ¥ bez mnohÃ½ch manuÃ¡lnych krokov, Äo mÃ´Å¾e viesÅ¥ k veÄ¾mi pÃ´sobivÃ½m vÃ½sledkom. Bez sprÃ¡vneho plÃ¡novania a stratÃ©giÃ­ to vÅ¡ak mÃ´Å¾e, bohuÅ¾iaÄ¾, viesÅ¥ k Å¡kodlivÃ½m vÃ½sledkom pre vaÅ¡ich pouÅ¾Ã­vateÄ¾ov, vÃ¡Å¡ produkt a spoloÄnosÅ¥ ako celok.

Pozrime sa na niektorÃ© (ale nie vÅ¡etky) z tÃ½chto potenciÃ¡lne Å¡kodlivÃ½ch vÃ½sledkov:

### HalucinÃ¡cie

HalucinÃ¡cie sÃº termÃ­n pouÅ¾Ã­vanÃ½ na opis situÃ¡cie, keÄ LLM vytvÃ¡ra obsah, ktorÃ½ je buÄ Ãºplne nezmyselnÃ½, alebo nieÄo, Äo vieme, Å¾e je fakticky nesprÃ¡vne na zÃ¡klade inÃ½ch zdrojov informÃ¡ciÃ­.

Predstavme si, Å¾e vytvorÃ­me funkciu pre nÃ¡Å¡ startup, ktorÃ¡ umoÅ¾nÃ­ Å¡tudentom klÃ¡sÅ¥ historickÃ© otÃ¡zky modelu. Å tudent sa opÃ½ta otÃ¡zku `Kto bol jedinÃ½m preÅ¾ivÅ¡Ã­m Titanicu?`

Model vytvorÃ­ odpoveÄ, ako je tÃ¡ niÅ¾Å¡ie:

![VÃ½zva "Kto bol jedinÃ½m preÅ¾ivÅ¡Ã­m Titanicu"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp)

> _(Zdroj: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

Toto je veÄ¾mi sebavedomÃ¡ a dÃ´kladnÃ¡ odpoveÄ. BohuÅ¾iaÄ¾, je nesprÃ¡vna. Aj s minimÃ¡lnym mnoÅ¾stvom vÃ½skumu by Älovek zistil, Å¾e Titanic mal viac ako jednÃ©ho preÅ¾ivÅ¡ieho. Pre Å¡tudenta, ktorÃ½ prÃ¡ve zaÄÃ­na skÃºmaÅ¥ tÃºto tÃ©mu, mÃ´Å¾e byÅ¥ tÃ¡to odpoveÄ presvedÄivÃ¡ natoÄ¾ko, Å¾e ju nebude spochybÅˆovaÅ¥ a bude ju povaÅ¾ovaÅ¥ za fakt. DÃ´sledky toho mÃ´Å¾u viesÅ¥ k tomu, Å¾e AI systÃ©m bude nespoÄ¾ahlivÃ½ a negatÃ­vne ovplyvnÃ­ reputÃ¡ciu nÃ¡Å¡ho startupu.

S kaÅ¾dou iterÃ¡ciou danÃ©ho LLM sme zaznamenali zlepÅ¡enie vÃ½konu pri minimalizovanÃ­ halucinÃ¡ciÃ­. Aj s tÃ½mto zlepÅ¡enÃ­m vÅ¡ak musÃ­me ako tvorcovia aplikÃ¡ciÃ­ a pouÅ¾Ã­vatelia zostaÅ¥ obozretnÃ­ voÄi tÃ½mto obmedzeniam.

### Å kodlivÃ½ obsah

V predchÃ¡dzajÃºcej Äasti sme sa venovali situÃ¡ciÃ¡m, keÄ LLM vytvÃ¡ra nesprÃ¡vne alebo nezmyselnÃ© odpovede. ÄalÅ¡Ã­m rizikom, ktorÃ© musÃ­me maÅ¥ na pamÃ¤ti, je, keÄ model odpovedÃ¡ Å¡kodlivÃ½m obsahom.

Å kodlivÃ½ obsah mÃ´Å¾e byÅ¥ definovanÃ½ ako:

- Poskytovanie pokynov alebo podpora sebapoÅ¡kodzovania alebo poÅ¡kodzovania urÄitÃ½ch skupÃ­n.
- NenÃ¡vistnÃ½ alebo poniÅ¾ujÃºci obsah.
- UsmerÅˆovanie plÃ¡novania akÃ©hokoÄ¾vek typu Ãºtoku alebo nÃ¡silnÃ½ch Äinov.
- Poskytovanie pokynov, ako nÃ¡jsÅ¥ nelegÃ¡lny obsah alebo spÃ¡chaÅ¥ nelegÃ¡lne Äiny.
- Zobrazovanie sexuÃ¡lne explicitnÃ©ho obsahu.

Pre nÃ¡Å¡ startup chceme zabezpeÄiÅ¥, Å¾e mÃ¡me sprÃ¡vne nÃ¡stroje a stratÃ©gie na zabrÃ¡nenie tomu, aby Å¡tudenti videli tento typ obsahu.

### Nedostatok spravodlivosti

SpravodlivosÅ¥ je definovanÃ¡ ako â€zabezpeÄenie, Å¾e AI systÃ©m je bez predsudkov a diskriminÃ¡cie a Å¾e zaobchÃ¡dza so vÅ¡etkÃ½mi spravodlivo a rovnako.â€œ Vo svete generatÃ­vnej AI chceme zabezpeÄiÅ¥, aby vyluÄujÃºce svetonÃ¡zory marginalizovanÃ½ch skupÃ­n neboli posilÅˆovanÃ© vÃ½stupmi modelu.

Tieto typy vÃ½stupov nielenÅ¾e narÃºÅ¡ajÃº budovanie pozitÃ­vnych produktovÃ½ch skÃºsenostÃ­ pre naÅ¡ich pouÅ¾Ã­vateÄ¾ov, ale tieÅ¾ spÃ´sobujÃº ÄalÅ¡ie spoloÄenskÃ© Å¡kody. Ako tvorcovia aplikÃ¡ciÃ­ by sme mali vÅ¾dy myslieÅ¥ na Å¡irokÃº a rozmanitÃº pouÅ¾Ã­vateÄ¾skÃº zÃ¡kladÅˆu pri budovanÃ­ rieÅ¡enÃ­ s generatÃ­vnou AI.

## Ako pouÅ¾Ã­vaÅ¥ generatÃ­vnu AI zodpovedne

Teraz, keÄ sme identifikovali dÃ´leÅ¾itosÅ¥ zodpovednej generatÃ­vnej AI, pozrime sa na 4 kroky, ktorÃ© mÃ´Å¾eme podniknÃºÅ¥ na zodpovednÃ© budovanie naÅ¡ich AI rieÅ¡enÃ­:

![Cyklus zmierÅˆovania](../../../translated_images/mitigate-cycle.babcd5a5658e1775d5f2cb47f2ff305cca090400a72d98d0f9e57e9db5637c72.sk.png)

### Meranie potenciÃ¡lnych Å¡kÃ´d

Pri testovanÃ­ softvÃ©ru testujeme oÄakÃ¡vanÃ© akcie pouÅ¾Ã­vateÄ¾a v aplikÃ¡cii. Podobne je dobrÃ© testovaÅ¥ rÃ´znorodÃº sadu vÃ½ziev, ktorÃ© pouÅ¾Ã­vatelia pravdepodobne pouÅ¾ijÃº, aby sme mohli meraÅ¥ potenciÃ¡lne Å¡kody.

KeÄÅ¾e nÃ¡Å¡ startup buduje vzdelÃ¡vacÃ­ produkt, bolo by dobrÃ© pripraviÅ¥ zoznam vÃ½ziev sÃºvisiacich so vzdelÃ¡vanÃ­m. To by mohlo zahÅ•ÅˆaÅ¥ pokrytie urÄitÃ©ho predmetu, historickÃ½ch faktov a vÃ½ziev tÃ½kajÃºcich sa Å¡tudentskÃ©ho Å¾ivota.

### ZmierÅˆovanie potenciÃ¡lnych Å¡kÃ´d

Teraz je Äas nÃ¡jsÅ¥ spÃ´soby, ako mÃ´Å¾eme predchÃ¡dzaÅ¥ alebo obmedziÅ¥ potenciÃ¡lne Å¡kody spÃ´sobenÃ© modelom a jeho odpoveÄami. MÃ´Å¾eme sa na to pozrieÅ¥ v 4 rÃ´znych vrstvÃ¡ch:

![Vrstvy zmierÅˆovania](../../../translated_images/mitigation-layers.377215120b9a1159a8c3982c6bbcf41b6adf8c8fa04ce35cbaeeb13b4979cdfc.sk.png)

- **Model**. VÃ½ber sprÃ¡vneho modelu pre sprÃ¡vny prÃ­pad pouÅ¾itia. VÃ¤ÄÅ¡ie a zloÅ¾itejÅ¡ie modely, ako je GPT-4, mÃ´Å¾u predstavovaÅ¥ vÃ¤ÄÅ¡ie riziko Å¡kodlivÃ©ho obsahu, keÄ sa aplikujÃº na menÅ¡ie a Å¡pecifickejÅ¡ie prÃ­pady pouÅ¾itia. PouÅ¾itie vaÅ¡ich trÃ©ningovÃ½ch Ãºdajov na doladenie tieÅ¾ zniÅ¾uje riziko Å¡kodlivÃ©ho obsahu.

- **BezpeÄnostnÃ½ systÃ©m**. BezpeÄnostnÃ½ systÃ©m je sÃºbor nÃ¡strojov a konfigurÃ¡ciÃ­ na platforme, ktorÃ¡ slÃºÅ¾i modelu, a pomÃ¡ha zmierÅˆovaÅ¥ Å¡kody. PrÃ­kladom je systÃ©m filtrovania obsahu na Azure OpenAI sluÅ¾be. SystÃ©my by mali tieÅ¾ detekovaÅ¥ Ãºtoky na obÃ­denie ochrany a neÅ¾iaducu aktivitu, ako sÃº poÅ¾iadavky od botov.

- **Metaprompt**. Metaprompt a ukotvenie sÃº spÃ´soby, ako mÃ´Å¾eme model usmerniÅ¥ alebo obmedziÅ¥ na zÃ¡klade urÄitÃ½ch sprÃ¡vanÃ­ a informÃ¡ciÃ­. To by mohlo zahÅ•ÅˆaÅ¥ pouÅ¾itie systÃ©movÃ½ch vstupov na definovanie urÄitÃ½ch limitov modelu. Okrem toho poskytovanie vÃ½stupov, ktorÃ© sÃº relevantnejÅ¡ie pre rozsah alebo domÃ©nu systÃ©mu.

MÃ´Å¾e to zahÅ•ÅˆaÅ¥ aj pouÅ¾itie technÃ­k, ako je Retrieval Augmented Generation (RAG), aby model Äerpal informÃ¡cie iba z vÃ½beru dÃ´veryhodnÃ½ch zdrojov. NeskÃ´r v tomto kurze je lekcia o [budovanÃ­ vyhÄ¾adÃ¡vacÃ­ch aplikÃ¡ciÃ­](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **PouÅ¾Ã­vateÄ¾skÃ¡ skÃºsenosÅ¥**. PoslednÃ¡ vrstva je miesto, kde pouÅ¾Ã­vateÄ¾ priamo interaguje s modelom prostrednÃ­ctvom rozhrania naÅ¡ej aplikÃ¡cie. TÃ½mto spÃ´sobom mÃ´Å¾eme navrhnÃºÅ¥ UI/UX tak, aby obmedzilo pouÅ¾Ã­vateÄ¾a v typoch vstupov, ktorÃ© mÃ´Å¾e posielaÅ¥ modelu, ako aj text alebo obrÃ¡zky zobrazovanÃ© pouÅ¾Ã­vateÄ¾ovi. Pri nasadzovanÃ­ AI aplikÃ¡cie musÃ­me byÅ¥ tieÅ¾ transparentnÃ­ o tom, Äo naÅ¡a generatÃ­vna AI aplikÃ¡cia dokÃ¡Å¾e a Äo nie.

MÃ¡me celÃº lekciu venovanÃº [Navrhovaniu UX pre AI aplikÃ¡cie](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Hodnotenie modelu**. PrÃ¡ca s LLM mÃ´Å¾e byÅ¥ nÃ¡roÄnÃ¡, pretoÅ¾e nemÃ¡me vÅ¾dy kontrolu nad Ãºdajmi, na ktorÃ½ch bol model trÃ©novanÃ½. Napriek tomu by sme mali vÅ¾dy hodnotiÅ¥ vÃ½kon a vÃ½stupy modelu. StÃ¡le je dÃ´leÅ¾itÃ© meraÅ¥ presnosÅ¥, podobnosÅ¥, ukotvenosÅ¥ a relevantnosÅ¥ vÃ½stupu modelu. To pomÃ¡ha poskytovaÅ¥ transparentnosÅ¥ a dÃ´veru zainteresovanÃ½m stranÃ¡m a pouÅ¾Ã­vateÄ¾om.

### PrevÃ¡dzkovanie zodpovednÃ©ho rieÅ¡enia generatÃ­vnej AI

Budovanie operaÄnej praxe okolo vaÅ¡ich AI aplikÃ¡ciÃ­ je poslednÃ¡ fÃ¡za. To zahÅ•Åˆa spoluprÃ¡cu s inÃ½mi ÄasÅ¥ami nÃ¡Å¡ho startupu, ako je prÃ¡vne oddelenie a bezpeÄnosÅ¥, aby sme zabezpeÄili sÃºlad so vÅ¡etkÃ½mi regulaÄnÃ½mi politikami. Pred spustenÃ­m chceme tieÅ¾ vytvoriÅ¥ plÃ¡ny okolo dodÃ¡vky, rieÅ¡enia incidentov a nÃ¡vratu spÃ¤Å¥, aby sme zabrÃ¡nili akÃ©mukoÄ¾vek poÅ¡kodeniu naÅ¡ich pouÅ¾Ã­vateÄ¾ov.

## NÃ¡stroje

Hoci sa prÃ¡ca na vÃ½voji zodpovednÃ½ch AI rieÅ¡enÃ­ mÃ´Å¾e zdaÅ¥ nÃ¡roÄnÃ¡, je to prÃ¡ca, ktorÃ¡ stojÃ­ za nÃ¡mahu. Ako oblasÅ¥ generatÃ­vnej AI rastie, viac nÃ¡strojov na pomoc vÃ½vojÃ¡rom efektÃ­vne integrovaÅ¥ zodpovednosÅ¥ do ich pracovnÃ½ch postupov bude dozrievaÅ¥. NaprÃ­klad [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) mÃ´Å¾e pomÃ´cÅ¥ detekovaÅ¥ Å¡kodlivÃ½ obsah a obrÃ¡zky prostrednÃ­ctvom API poÅ¾iadavky.

## Kontrola vedomostÃ­

Na Äo by ste mali dbaÅ¥, aby ste zabezpeÄili zodpovednÃ© pouÅ¾Ã­vanie AI?

1. Å½e odpoveÄ je sprÃ¡vna.
1. Å kodlivÃ© pouÅ¾itie, Å¾e AI nie je pouÅ¾Ã­vanÃ¡ na kriminÃ¡lne ÃºÄely.
1. ZabezpeÄenie, Å¾e AI je bez predsudkov a diskriminÃ¡cie.

A: 2 a 3 sÃº sprÃ¡vne. ZodpovednÃ¡ AI vÃ¡m pomÃ¡ha zvÃ¡Å¾iÅ¥, ako zmierniÅ¥ Å¡kodlivÃ© ÃºÄinky a predsudky a viac.

## ğŸš€ VÃ½zva

PreÄÃ­tajte si o [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) a zistite, Äo mÃ´Å¾ete prijaÅ¥ pre svoje pouÅ¾itie.

## SkvelÃ¡ prÃ¡ca, pokraÄujte vo svojom uÄenÃ­

Po dokonÄenÃ­ tejto lekcie si pozrite naÅ¡u [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby ste pokraÄovali v rozÅ¡irovanÃ­ svojich vedomostÃ­ o generatÃ­vnej AI!

Prejdite na lekciu 4, kde sa pozrieme na [ZÃ¡klady inÅ¾inierstva vÃ½ziev](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nenesieme zodpovednosÅ¥ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.