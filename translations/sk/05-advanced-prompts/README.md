<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b2651fb16bcfbc62b8e518751ed90fdb",
  "translation_date": "2025-10-17T21:52:11+00:00",
  "source_file": "05-advanced-prompts/README.md",
  "language_code": "sk"
}
-->
# Vytv치ranie pokro캜il칳ch promptov

[![Vytv치ranie pokro캜il칳ch promptov](../../../translated_images/05-lesson-banner.522610fd4a2cd82dbed66bb7e6fe104ed6da172e085dbb4d9100b28dc73ed435.sk.png)](https://youtu.be/BAjzkaCdRok?si=NmUIyRf7-cDgbjtt)

Zhr켿me si niektor칠 poznatky z predch치dzaj칰cej kapitoly:

> Prompt _engineering_ je proces, ktor칳m **usmer켿ujeme model k relevantnej코칤m odpovediam** poskytovan칤m u쬴to캜nej코칤ch in코trukci칤 alebo kontextu.

Existuj칰 dva kroky pri p칤san칤 promptov: kon코trukcia promptu, kde poskytujeme relevantn칳 kontext, a _optimaliz치cia_, teda postupn칠 zlep코ovanie promptu.

V tomto bode u m치me z치kladn칠 pochopenie toho, ako p칤sa콘 prompty, ale potrebujeme 칤s콘 hlb코ie. V tejto kapitole prejdete od sk칰코ania r칪znych promptov k pochopeniu, pre캜o je jeden prompt lep코칤 ako druh칳. Nau캜칤te sa, ako kon코truova콘 prompty pod쬬 z치kladn칳ch techn칤k, ktor칠 sa daj칰 aplikova콘 na ak칳ko쭀ek LLM.

## 칔vod

V tejto kapitole sa budeme venova콘 nasleduj칰cim t칠mam:

- Roz코칤renie va코ich znalost칤 o prompt engineeringu aplikovan칤m r칪znych techn칤k na va코e prompty.
- Konfigur치cia va코ich promptov na zmenu v칳stupu.

## Ciele u캜enia

Po dokon캜en칤 tejto lekcie budete schopn칤:

- Aplikova콘 techniky prompt engineeringu, ktor칠 zlep코uj칰 v칳sledok va코ich promptov.
- Vykon치va콘 prompting, ktor칳 je bu캞 variabiln칳 alebo deterministick칳.

## Prompt engineering

Prompt engineering je proces vytv치rania promptov, ktor칠 produkuj칰 po쬬dovan칳 v칳sledok. Prompt engineering nie je in쬴nierska discipl칤na, ale sk칪r s칰bor techn칤k, ktor칠 m칪쬰te aplikova콘 na dosiahnutie po쬬dovan칠ho v칳sledku.

### Pr칤klad promptu

Pozrime sa na z치kladn칳 prompt, ako je tento:

> Vygeneruj 10 ot치zok o geografii.

V tomto prompte vlastne aplikujete s칰bor r칪znych techn칤k promptovania.

Rozlo쬸e si to.

- **Kontext**, 코pecifikujete, 쬰 by to malo by콘 o "geografii".
- **Obmedzenie v칳stupu**, chcete maxim치lne 10 ot치zok.

### Obmedzenia jednoduch칠ho promptovania

Mo쬹o dosiahnete po쬬dovan칳 v칳sledok, mo쬹o nie. Ot치zky sa vygeneruj칰, ale geografia je ve쬶치 t칠ma a mo쬹o nedosiahnete to, 캜o chcete, z nasleduj칰cich d칪vodov:

- **Ve쬶치 t칠ma**, neviete, 캜i to bude o krajin치ch, hlavn칳ch mest치ch, riekach a podobne.
- **Form치t**, 캜o ak chcete, aby ot치zky boli naform치tovan칠 ur캜it칳m sp칪sobom?

Ako vid칤te, pri vytv치ran칤 promptov je ve쬬 vec칤, ktor칠 treba zv치쬴콘.

Doteraz sme videli jednoduch칳 pr칤klad promptu, ale generat칤vna AI je schopn치 ove쬬 viac, aby pomohla 쬿캞om v r칪znych rol치ch a odvetviach. Po캞me presk칰ma콘 niektor칠 z치kladn칠 techniky.

### Techniky promptovania

Najprv mus칤me pochopi콘, 쬰 promptovanie je _emergentn치_ vlastnos콘 LLM, 캜o znamen치, 쬰 to nie je funkcia zabudovan치 do modelu, ale sk칪r nie캜o, 캜o objavujeme pri pou쮂셨an칤 modelu.

Existuj칰 niektor칠 z치kladn칠 techniky, ktor칠 m칪쬰me pou쬴콘 na promptovanie LLM. Po캞me ich presk칰ma콘.

- **Zero-shot prompting**, ide o najz치kladnej코iu formu promptovania. Je to jedin칳 prompt, ktor칳 쬴ada odpove캞 od LLM na z치klade jeho tr칠ningov칳ch d치t.
- **Few-shot prompting**, tento typ promptovania usmer켿uje LLM poskytovan칤m 1 alebo viacer칳ch pr칤kladov, na ktor칠 sa m칪쬰 spo쬬hn칰콘 pri generovan칤 odpovede.
- **Chain-of-thought**, tento typ promptovania u캜칤 LLM, ako rozlo쬴콘 probl칠m na kroky.
- **Generated knowledge**, na zlep코enie odpovede promptu m칪쬰te poskytn칰콘 generovan칠 fakty alebo znalosti navy코e k v치코mu promptu.
- **Least to most**, podobne ako chain-of-thought, t치to technika spo캜칤va v rozdelen칤 probl칠mu na s칠riu krokov a n치slednom po쬴adan칤 o vykonanie t칳chto krokov v porad칤.
- **Self-refine**, t치to technika spo캜칤va v kritike v칳stupu LLM a n치slednom po쬴adan칤 o jeho zlep코enie.
- **Maieutic prompting**, tu chcete zabezpe캜i콘, 쬰 odpove캞 LLM je spr치vna, a po쬴adate ho, aby vysvetlilo r칪zne 캜asti odpovede. Ide o formu self-refine.

### Zero-shot prompting

Tento 코t칳l promptovania je ve쬸i jednoduch칳, pozost치va z jedin칠ho promptu. T치to technika je pravdepodobne to, 캜o pou쮂셨ate, ke캞 za캜칤nate u캜i콘 sa o LLM. Tu je pr칤klad:

- Prompt: "캛o je algebra?"
- Odpove캞: "Algebra je odvetvie matematiky, ktor칠 sk칰ma matematick칠 symboly a pravidl치 na manipul치ciu s t칳mito symbolmi."

### Few-shot prompting

Tento 코t칳l promptovania pom치ha modelu poskytovan칤m nieko쬶칳ch pr칤kladov spolu s po쬴adavkou. Pozost치va z jedin칠ho promptu s dodato캜n칳mi d치tami 코pecifick칳mi pre 칰lohu. Tu je pr칤klad:

- Prompt: "Nap칤코 b치se켿 v 코t칳le Shakespeara. Tu je nieko쬶o pr칤kladov Shakespearov칳ch sonetov:
  Sonet 18: 'M치m 콘a prirovna콘 k letn칠mu d켿u? Si kraj코칤 a miernej코칤...'
  Sonet 116: 'Nech mi nie je prek치쬶ou spojenie prav칳ch mysl칤. L치ska nie je l치skou, ktor치 sa men칤, ke캞 sa men칤...'
  Sonet 132: 'Tvoje o캜i milujem, a ony, akoby ma 쬿tovali, poznaj칰c tvoje srdce, mu캜ia ma poh콋dan칤m,...'
  Teraz nap칤코 sonet o kr치se mesiaca."
- Odpove캞: "Na oblohe mesiac jemne 쬴ari, v striebristom svetle, ktor칠 vrh치 svoju jemn칰 milos콘,..."

Pr칤klady poskytuj칰 LLM kontext, form치t alebo 코t칳l po쬬dovan칠ho v칳stupu. Pom치haj칰 modelu pochopi콘 konkr칠tnu 칰lohu a generova콘 presnej코ie a relevantnej코ie odpovede.

### Chain-of-thought

Chain-of-thought je ve쬸i zauj칤mav치 technika, preto쬰 ide o vedenie LLM cez s칠riu krokov. My코lienka je in코truova콘 LLM tak칳m sp칪sobom, aby pochopilo, ako nie캜o urobi콘. Zv치쬾e nasleduj칰ci pr칤klad, s a bez chain-of-thought:

    - Prompt: "Alice m치 5 jab컄k, hod칤 3 jablk치, d치 2 Bobovi a Bob jej jedno vr치ti, ko쬶o jab컄k m치 Alice?"
    - Odpove캞: 5

LLM odpovie 5, 캜o je nespr치vne. Spr치vna odpove캞 je 1 jablko, pod쬬 v칳po캜tu (5 -3 -2 + 1 = 1).

Ako m칪쬰me nau캜i콘 LLM, aby to urobilo spr치vne?

Sk칰sme chain-of-thought. Aplik치cia chain-of-thought znamen치:

1. Dajte LLM podobn칳 pr칤klad.
1. Uk치쬾e v칳po캜et a ako ho spr치vne vypo캜칤ta콘.
1. Poskytnite p칪vodn칳 prompt.

Tu je postup:

- Prompt: "Lisa m치 7 jab컄k, hod칤 1 jablko, d치 4 jablk치 Bartovi a Bart jej jedno vr치ti:
  7 -1 = 6
  6 -4 = 2
  2 +1 = 3  
  Alice m치 5 jab컄k, hod칤 3 jablk치, d치 2 Bobovi a Bob jej jedno vr치ti, ko쬶o jab컄k m치 Alice?"
  Odpove캞: 1

V코imnite si, ako p칤코eme podstatne dlh코ie prompty s 캞al코칤m pr칤kladom, v칳po캜tom a potom p칪vodn칳m promptom, a dospejeme k spr치vnej odpovedi 1.

Ako vid칤te, chain-of-thought je ve쬸i siln치 technika.

### Generated knowledge

Mnohokr치t, ke캞 chcete zostavi콘 prompt, chcete to urobi콘 pomocou 칰dajov va코ej vlastnej spolo캜nosti. Chcete, aby 캜as콘 promptu poch치dzala od spolo캜nosti a druh치 캜as콘 by mala by콘 skuto캜n칳 prompt, ktor칳 v치s zauj칤ma.

Ako pr칤klad, takto m칪쬰 vyzera콘 v치코 prompt, ak ste v pois콘ovn칤ctve:

```text
{{company}}: {{company_name}}
{{products}}:
{{products_list}}
Please suggest an insurance given the following budget and requirements:
Budget: {{budget}}
Requirements: {{requirements}}
```

Vy코코ie vid칤te, ako je prompt zostaven칳 pomocou 코abl칩ny. V 코abl칩ne je nieko쬶o premenn칳ch, ozna캜en칳ch `{{variable}}`, ktor칠 bud칰 nahraden칠 skuto캜n칳mi hodnotami z API spolo캜nosti.

Tu je pr칤klad, ako by prompt mohol vyzera콘 po nahraden칤 premenn칳ch obsahom z va코ej spolo캜nosti:

```text
Insurance company: ACME Insurance
Insurance products (cost per month):
- Car, cheap, 500 USD
- Car, expensive, 1100 USD
- Home, cheap, 600 USD
- Home, expensive, 1200 USD
- Life, cheap, 100 USD

Please suggest an insurance given the following budget and requirements:
Budget: $1000
Requirements: Car, Home, and Life insurance
```

Spustenie tohto promptu cez LLM vyprodukuje odpove캞 ako t치to:

```output
Given the budget and requirements, we suggest the following insurance package from ACME Insurance:
- Car, cheap, 500 USD
- Home, cheap, 600 USD
- Life, cheap, 100 USD
Total cost: $1,200 USD
```

Ako vid칤te, navrhuje aj 쬴votn칠 poistenie, 캜o by nemalo. Tento v칳sledok nazna캜uje, 쬰 mus칤me optimalizova콘 prompt t칳m, 쬰 ho zmen칤me, aby bol jasnej코칤 v tom, 캜o m칪쬰 povoli콘. Po nieko쬶칳ch _pokusoch a omyloch_ dospejeme k nasleduj칰cemu promptu:

```text
Insurance company: ACME Insurance
Insurance products (cost per month):
- type: Car, cheap, cost: 500 USD
- type: Car, expensive, cost: 1100 USD
- type: Home, cheap, cost: 600 USD
- type: Home, expensive, cost: 1200 USD
- type: Life, cheap, cost: 100 USD

Please suggest an insurance given the following budget and requirements:
Budget: $1000 restrict choice to types: Car, Home
```

V코imnite si, ako pridanie _type_ a _cost_ a tie pou쬴tie k쮂줷꼂v칠ho slova _restrict_ pom치ha LLM pochopi콘, 캜o chceme.

Teraz dostaneme nasleduj칰cu odpove캞:

```output
Given the budget and requirements, we suggest the Car, Cheap insurance product which costs 500 USD per month.
```

Cie쬺m tohto pr칤kladu bolo uk치za콘, 쬰 aj ke캞 pou쮂셨ame z치kladn칰 techniku ako _generated knowledge_, st치le mus칤me vo v칛캜코ine pr칤padov optimalizova콘 prompt, aby sme dosiahli po쬬dovan칳 v칳sledok.

### Least-to-most

My코lienka Least-to-most promptovania je rozdelenie v칛캜코ieho probl칠mu na podprobl칠my. T칳mto sp칪sobom pom치hate usmerni콘 LLM, ako "zdola콘" v칛캜코칤 probl칠m. Dobr칳m pr칤kladom m칪쬰 by콘 d치tov치 veda, kde m칪쬰te po쬴ada콘 LLM, aby rozdelil probl칠m takto:

> Prompt: Ako vykona콘 d치tov칰 anal칳zu v 5 krokoch?

Va코a AI asistentka odpovie:

1. Zber d치t
1. 캛istenie d치t
1. Anal칳za d치t
1. Vizualiz치cia d치t
1. Prezent치cia d치t

### Self-refine, kritika v칳sledkov

Pri generat칤vnych AI a LLM nem칪쬰te d칪verova콘 v칳stupu. Mus칤te ho overi콘. Napokon, LLM v치m len prezentuje, 캜o je najpravdepodobnej코ia vec, ktor칰 poveda콘, nie 캜o je spr치vne. Preto je dobr칳 n치pad po쬴ada콘 LLM, aby samo seba kritizovalo, 캜o n치s vedie k technike self-refine.

Ako to funguje, je nasledovn칠:

1. Po캜iato캜n칳 prompt, ktor칳 쬴ada LLM vyrie코i콘 probl칠m
1. LLM odpovie
1. Kritizujete odpove캞 a po쬴adate AI o zlep코enie
1. LLM odpovie znova, tentoraz zoh쬬d켿uj칰c kritiku a navrhne rie코enia, ktor칠 vymyslelo

Tento proces m칪쬰te opakova콘 to쬶okr치t, ko쬶o chcete.

Tu je pr칤klad pou쬴tia tejto techniky:

> Prompt: "Vytvor Python Web API s routami products a customers"

AI odpove캞:

```python
import flask
app = Flask(__name__)

@app.route('/products')
def products():
    return 'Products'

@app.route('/customers')
def customers():
    return 'Customers'
```

> Prompt: navrhni 3 vylep코enia vy코코ie uveden칠ho k칩du

AI odpove캞:

```text
Sure, here are three suggestions to improve the code:

Import the correct module: The first line should import Flask instead of flask. The correct import statement should be from flask import Flask.

Add a main function: Add a if __name__ == '__main__': block to the end of the file to run the application when the file is executed.

Use f-strings: Use f-strings to format the return statements in the route functions. For example, return 'Products' can be changed to return f'This is the {request.path} page'.
```

```python
from flask import Flask, request

app = Flask(__name__)

@app.route('/products')
def products():
    return f'This is the {request.path} page'

@app.route('/customers')
def customers():
    return f'This is the {request.path} page'

if __name__ == '__main__':
    app.run()
```

Ako vid칤te, vy코코ie uveden치 odpove캞 AI zlep코uje prv칳 navrhnut칳 k칩d v캞aka kritike prvej odpovede.

### Maieutic prompting

Maieutic prompting je technika podobn치 self-refine, ale ide sk칪r o to, aby LLM vysvetlilo samo seba. Cie쬺m je zn칤쬴콘 nekonzistencie vo v칳stupe LLM, aby sa zabezpe캜ilo, 쬰 dosiahne spr치vnu odpove캞. Postup, ktor칳 treba dodr쬴ava콘, je:

1. Po쬴adajte LLM, aby odpovedalo na ot치zku
1. Pre ka쬯칰 캜as콘 odpovede po쬴adajte LLM, aby ju vysvetlilo podrobnej코ie.
1. Ak s칰 nekonzistencie, vyra캞te 캜asti, ktor칠 s칰 nekonzistentn칠.

Opakujte kroky 2 a 3, k칳m neprejdete v코etky 캜asti a nebudete spokojn칤 s odpove캞ou.

Tu je pr칤klad promptu:

> prompt: Ako m칪쬰m vytvori콘 kr칤zov칳 pl치n na zmiernenie pand칠mie v 5 krokoch?
> LLM odpove캞:

```text
1. Identify the risks
1. Identify the stakeholders
1. Identify the resources
1. Identify the actions
1. Identify the communication channels
```

Identifikovalo 5 krokov, ale m칪쬰me ur캜i콘, 캜i je to spr치vne? Po캞me po쬴ada콘 LLM, aby vysvetlilo ka쬯칳 krok podrobnej코ie:

> prompt: Vysvetli prv칳 krok podrobnej코ie, ak칠 s칰 rizik치 pand칠mie podrobne?

```text
The risks are the following:

- Loss of life
- Loss of business
- Loss of reputation
- Loss of customers
- Loss of employees
```

V tomto bode chceme zabezpe캜i콘, 쬰 LLM je spr치vne, tak쬰 ho po쬴adame, aby vysvetlilo prv칠 riziko podrobnej코ie a d칰fame, 쬰 je konzistentn칠 s vy코코ie uvedenou odpove캞ou:

> prompt: V pand칠mii, ktor칠 je najv칛캜코ie riziko a pre캜o?

```text
The biggest risk is loss of life because it's the most important thing to protect.
```

> Ak칠 s칰 dve najv칛캜코ie rizik치 v pand칠mii?

```text
The two biggest risks are loss of life and loss of business.
```

V tomto bode je LLM konzistentn칠 a spom칤na "쬴vot" a "biznis" ako dve najv칛캜코ie rizik치. Teraz m칪쬰me pokra캜ova콘 na 캞al코칤 krok a c칤ti콘 sa pomerne ist칤. Av코ak, nemali by sme slepo d칪verova콘 LLM, v쬯y by sme mali overi콘 v칳stup.

## Variabilita v칳stupu

LLM s칰 zo svojej podstaty nedeterministick칠, 캜o znamen치, 쬰 dostanete r칪zne v칳sledky zaka쬯칳m, ke캞 spust칤te ten ist칳 prompt. Sk칰ste napr칤klad nasleduj칰ci prompt:

> "Vygeneruj k칩d pre Python Web API"

```python
# Import necessary modules
import flask
from flask import request, jsonify

# Create a Flask app
app = flask.Flask(__name__)

# Create a route for the API
@app.route('/api', methods=['GET'])
def api():
    # Get the data from the request
    data = request.args

    # Process the data
    result = process_data(data)

    # Return the result as JSON
    return jsonify(result)

# Function to process the data
def process_data(data):
    # Do something with the data
    result = data + 1

    # Return the result
    return result

# Run the app
if __name__ == '__main__':
    app.run()

```

Spustenie toho ist칠ho promptu znova generuje mierne odli코n칰 odpove캞:

```python
#import necessary packages
import flask
from flask import request, jsonify

#create the Flask application
app = flask.Flask(__name__)
app.config["DEBUG"] = True

#create a list of books
books = [
    {'id': 0,
     'title': 'A Fire Upon the Deep',
     'author': 'Vernor Vinge',
     'first_sentence': 'The coldsleep itself was dreamless.',
     'year_published': '1992'},
    {'id': 1,
     'title': 'The Ones Who Walk Away From Omelas',
     'author': 'Ursula K. Le Guin',
     'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.',
     'published': '1973'},
    {'id': 2,
     'title': 'Dhalgren',
     'author': 'Samuel R. Delany',
     'first_sentence': 'to wound the autumnal city.',
     'published': '1975'}
]

#create an endpoint for the API
@app.route('/', methods=['GET'])
def home():
    return '''<h1>Books API</h1>
<p>A prototype API for retrieving books.</p>'''

#create an endpoint to return all books
@app.route('/api/v1/resources/books/all', methods=['GET'])
def api_all():
    return jsonify(books)

#create an endpoint to return a single book
@app.route('/api/v1/resources/books', methods=['GET'])
def api_id():
    #check if an ID was provided as part of the URL
    #if ID is provided, assign it to a variable
    #if no ID is provided, display an error
    if 'id' in request.args:
        id = int(request.args['id'])
    else:
        return "Error: No id field provided. Please specify an id."

    #create an empty list for our results
    results = []

    #loop through the data and match results that fit the requested ID
    #IDs are unique, but other fields might return many results
    for book in books:
        if book['id'] == id:
            results.append(book)

    #use the jsonify function from Flask to convert our list of
    #Python dictionaries to the JSON format
    return jsonify(results)

app.run()
```

> Je variabiln칳 v칳stup probl칠m?

Z치le쮂 na tom, 캜o sa sna쮂셦e dosiahnu콘. Ak chcete konkr칠tnu odpove캞, potom je to probl칠m. Ak v치m nevad칤 variabiln칳 v칳stup, ako napr칤klad "Vygeneruj ak칠ko쭀ek 3 ot치zky o geografii", potom to nie je probl칠m.

### Pou쬴tie teploty na variabilitu v칳stupu

Dobre, rozhodli sme sa, 쬰 chceme obmedzi콘 v칳stup, aby bol predv칤date쬹ej코칤, teda viac deterministick칳. Ako to urobi콘?

Teplota je hodnota medzi 0 a 1, kde 0 je najviac deterministick치 a 1 je najviac variabiln치. Predvolen치 hodnota je 0.7. Pozrime sa, 캜o sa stane pri dvoch spusteniach toho ist칠ho promptu s teplotou nastavenou na 0.1:

> "Vygeneruj k칩d pre Python Web API"

```python
#import necessary libraries
import flask
from flask import request, jsonify

#create an instance of the Flask class
app = flask.Flask(__name__)

#create an endpoint for the API
@app.route('/api/v1/endpoint', methods=['GET'])
def api_endpoint():
    #get the request data
    data = request.get_json()

    #process the data
    result = process_data(data)

    #return the result
    return jsonify(result)

#function to process the data
def process_data(data):
    #process the data
    result = {'result': 'success'}

    #return the result
    return result

#run the Flask app
if __name__ == '__main__':
    app.run()

```

Spustenie promptu znova n치m d치va tento v칳sledok:

```python
#import necessary libraries
import flask
from flask import request, jsonify

#create a Flask app
app = flask.Flask(__name__)

#create a route for the API
@app.route('/api', methods=['GET'])
def api_call():
    #get the data from the request
    data = request.get_json()

    #process the data
    result = process_data(data)

    #return the result
    return jsonify(result)

#define the process_data function
def process_data(data):
    #do something with the data
    result = data + 1

    #return the result
    return result

#run the app
if __name__ == '__main__':
    app.run()

```

Medzi t칳mito dvoma v칳stupmi je len mal칳 rozdiel. Tentoraz urobme opak, nastavme teplotu na 0.9:

```python
# Import necessary libraries
import flask
from flask import request, jsonify

# Create a Flask app
app = flask.Flask(__name__)

# Create a route for the API
@app.route('/api', methods=['GET'])
def api_call():
    # Get the data from the request
    data = request.args

    # Process the data
    result = process_data(data)

    # Return the result
    return jsonify(result)

# Function to process the data
def process_data(data):
    # Do something with the data
    result = data + 1

    # Return the result
    return result

# Run the app
if __name__ == '__main__':
    app.run()

```

a druh칳 pokus s hodnotou teploty 0.9:

```python
import flask
from flask import request, jsonify

# create the Flask app
app = flask.Flask(__name__)
app.config['DEBUG'] = True

# create some test data
books = [
    {'id': 0, 'title': 'A Fire Upon The Deep', 'author': 'Vernor Vinge', 'first_sentence': 'The coldsleep itself was dreamless.', 'year_published': '1992'},
    {'id': 1, 'title': 'The Ones Who Walk Away From Omelas', 'author': 'Ursula K. Le Guin', 'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.', 'published': '1973'},
    {'id': 2, 'title': 'Dhalgren', 'author': 'Samuel R. Delany', 'first_sentence': 'to wound the autumnal city.', 'published': '1975'}
]

# create an endpoint
@app.route('/', methods=['GET'])
def home():
    return '''<h1>Welcome to our book API!</h1>'''

@app.route('/api/v1/resources/books

```

Ako vid칤te, v칳sledky nemohli by콘 rozmanitej코ie.

> V코imnite si, 쬰 existuje viac parametrov, ktor칠 m칪쬰te zmeni콘 na ovplyvnenie v칳stupu, ako napr칤klad top-k, top-p, penaliz치cia opakovania, penaliz치cia d컄쬶y a penaliz치cia rozmanitosti, ale tieto s칰 mimo rozsah tejto u캜ebnej osnovy.

## Dobr칠 praktiky

Existuje mnoho prakt칤k, ktor칠 m칪쬰te pou쬴콘 na dosiahnutie po쬬dovan칠ho v칳sledku. Ako budete pou쮂셨a콘 prompting 캜astej코ie, n치jdete svoj vlastn칳 코t칳l.

Okrem techn칤k, ktor칠 sme prebrali, existuj칰 niektor칠 dobr칠 praktiky, ktor칠 je vhodn칠 zv치쬴콘 pri pr치ci s LLM.

Tu s칰 niektor칠 dobr칠 praktiky, ktor칠 je vhodn칠 zv치쬴콘:

- **맗ecifikujte kontext**. Kontext je d칪le쬴t칳, 캜칤m viac m칪쬰te 코pecifikova콘, ako napr칤klad oblas콘, t칠mu at캞., t칳m lep코ie.
- Obmedzte v칳stup. Ak chcete konkr칠tny po캜et polo쬴ek alebo konkr칠tnu d컄쬶u, 코pecifikujte to.
- **맗ecifikujte 캜o a ako**. Nezabudnite uvies콘, 캜o chcete a ako to chcete, napr칤klad "Vytvorte Python Web API s trasami products a customers, rozde쬾e ho do 3 s칰borov".
- **Pou쮂셨ajte 코abl칩ny**. 캛asto budete chcie콘 obohati콘 svoje prompty 칰dajmi z va코ej spolo캜nosti. Pou쮂셨ajte 코abl칩ny na tento 칰캜el. 마bl칩ny m칪쬿 obsahova콘 premenn칠, ktor칠 nahrad칤te skuto캜n칳mi 칰dajmi.
- **P칤코te spr치vne**. LLM v치m m칪쬰 poskytn칰콘 spr치vnu odpove캞, ale ak p칤코ete spr치vne, dostanete lep코iu odpove캞.

## Zadanie

Tu je k칩d v Pythone, ktor칳 ukazuje, ako vytvori콘 jednoduch칠 API pomocou Flask:

```python
from flask import Flask, request

app = Flask(__name__)

@app.route('/')
def hello():
    name = request.args.get('name', 'World')
    return f'Hello, {name}!'

if __name__ == '__main__':
    app.run()
```

Pou쬴te AI asistenta, ako je GitHub Copilot alebo ChatGPT, a aplikujte techniku "self-refine" na zlep코enie k칩du.

## Rie코enie

Pok칰ste sa vyrie코i콘 zadanie pridan칤m vhodn칳ch promptov do k칩du.

> [!TIP]
> Formulujte prompt na po쬴adanie o zlep코enie, je dobr칠 obmedzi콘 po캜et zlep코en칤. M칪쬰te tie po쬴ada콘 o zlep코enie konkr칠tnym sp칪sobom, napr칤klad architekt칰ra, v칳kon, bezpe캜nos콘 at캞.

[Rie코enie](../../../05-advanced-prompts/python/aoai-solution.py)

## Kontrola vedomost칤

Pre캜o by som mal pou쬴콘 chain-of-thought prompting? Uk치쬾e mi 1 spr치vnu odpove캞 a 2 nespr치vne odpovede.

1. Nau캜i콘 LLM, ako vyrie코i콘 probl칠m.
1. B, Nau캜i콘 LLM n치js콘 chyby v k칩de.
1. C, In코truova콘 LLM, aby pri코lo s r칪znymi rie코eniami.

A: 1, preto쬰 chain-of-thought je o tom, ako uk치za콘 LLM, ako vyrie코i콘 probl칠m poskytnut칤m s칠rie krokov, podobn칳ch probl칠mov a sp칪sobov, ak칳mi boli vyrie코en칠.

## 游 V칳zva

Pr치ve ste pou쬴li techniku self-refine v zadan칤. Vezmite ak칳ko쭀ek program, ktor칳 ste vytvorili, a zv치쬾e, ak칠 zlep코enia by ste chceli aplikova콘. Teraz pou쬴te techniku self-refine na aplik치ciu navrhovan칳ch zmien. 캛o si mysl칤te o v칳sledku, je lep코칤 alebo hor코칤?

## Skvel치 pr치ca! Pokra캜ujte vo svojom vzdel치van칤

Po dokon캜en칤 tejto lekcie si pozrite na코u [kolekciu Generative AI Learning](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby ste pokra캜ovali v roz코irovan칤 svojich znalost칤 o generat칤vnej AI!

Prejdite na Lekciu 6, kde aplikujeme na코e znalosti o Prompt Engineering [vytvoren칤m aplik치ci칤 na generovanie textu](../06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst).

---

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol prelo쬰n칳 pomocou slu쬭y AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa sna쮂셠e o presnos콘, pros칤m, berte na vedomie, 쬰 automatizovan칠 preklady m칪쬿 obsahova콘 chyby alebo nepresnosti. P칪vodn칳 dokument v jeho rodnom jazyku by mal by콘 pova쬺van칳 za autoritat칤vny zdroj. Pre kritick칠 inform치cie sa odpor칰캜a profesion치lny 쬿dsk칳 preklad. Nenesieme zodpovednos콘 za ak칠ko쭀ek nedorozumenia alebo nespr치vne interpret치cie vypl칳vaj칰ce z pou쬴tia tohto prekladu.