<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:38:46+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "sk"
}
-->
# ZabezpeÄenie vaÅ¡ich generatÃ­vnych AI aplikÃ¡ciÃ­

[![ZabezpeÄenie vaÅ¡ich generatÃ­vnych AI aplikÃ¡ciÃ­](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.sk.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Ãšvod

TÃ¡to lekcia pokrÃ½va:

- BezpeÄnosÅ¥ v kontexte AI systÃ©mov.
- BeÅ¾nÃ© rizikÃ¡ a hrozby pre AI systÃ©my.
- MetÃ³dy a Ãºvahy pri zabezpeÄenÃ­ AI systÃ©mov.

## Ciele uÄenia

Po absolvovanÃ­ tejto lekcie budete rozumieÅ¥:

- HrozbÃ¡m a rizikÃ¡m pre AI systÃ©my.
- BeÅ¾nÃ½m metÃ³dam a praktikÃ¡m zabezpeÄenia AI systÃ©mov.
- Ako implementÃ¡cia bezpeÄnostnÃ©ho testovania mÃ´Å¾e zabrÃ¡niÅ¥ neoÄakÃ¡vanÃ½m vÃ½sledkom a strate dÃ´very pouÅ¾Ã­vateÄ¾ov.

## ÄŒo znamenÃ¡ bezpeÄnosÅ¥ v kontexte generatÃ­vnej AI?

KeÄÅ¾e technolÃ³gie umelej inteligencie (AI) a strojovÃ©ho uÄenia (ML) Äoraz viac ovplyvÅˆujÃº naÅ¡e Å¾ivoty, je nevyhnutnÃ© chrÃ¡niÅ¥ nielen Ãºdaje zÃ¡kaznÃ­kov, ale aj samotnÃ© AI systÃ©my. AI/ML sa Äoraz ÄastejÅ¡ie vyuÅ¾Ã­va na podporu rozhodovacÃ­ch procesov s vysokou hodnotou v odvetviach, kde nesprÃ¡vne rozhodnutie mÃ´Å¾e maÅ¥ vÃ¡Å¾ne nÃ¡sledky.

Tu sÃº kÄ¾ÃºÄovÃ© body na zvÃ¡Å¾enie:

- **Vplyv AI/ML**: AI/ML majÃº vÃ½raznÃ½ vplyv na kaÅ¾dodennÃ½ Å¾ivot, a preto je ich ochrana nevyhnutnÃ¡.
- **VÃ½zvy v oblasti bezpeÄnosti**: Tento vplyv si vyÅ¾aduje nÃ¡leÅ¾itÃº pozornosÅ¥, aby sa zabezpeÄila ochrana AI produktov pred sofistikovanÃ½mi Ãºtokmi, Äi uÅ¾ zo strany trollov alebo organizovanÃ½ch skupÃ­n.
- **StrategickÃ© problÃ©my**: TechnologickÃ½ priemysel musÃ­ proaktÃ­vne rieÅ¡iÅ¥ strategickÃ© vÃ½zvy, aby zabezpeÄil dlhodobÃº bezpeÄnosÅ¥ zÃ¡kaznÃ­kov a ochranu dÃ¡t.

Okrem toho modely strojovÃ©ho uÄenia vÃ¤ÄÅ¡inou nedokÃ¡Å¾u rozlÃ­Å¡iÅ¥ medzi Å¡kodlivÃ½mi vstupmi a neÅ¡kodnÃ½mi anomÃ¡liami. VÃ½znamnÃ¡ ÄasÅ¥ trÃ©ningovÃ½ch dÃ¡t pochÃ¡dza z nekontrolovanÃ½ch, nemoderovanÃ½ch verejnÃ½ch datasetov, ktorÃ© sÃº otvorenÃ© prÃ­spevkom tretÃ­ch strÃ¡n. ÃštoÄnÃ­ci nemusia kompromitovaÅ¥ datasety, keÄ mÃ´Å¾u do nich voÄ¾ne prispievaÅ¥. Postupom Äasu sa dÃ¡ta s nÃ­zkou dÃ´verou Å¡kodlivÃ©ho charakteru mÃ´Å¾u staÅ¥ vysoko dÃ´veryhodnÃ½mi, ak Å¡truktÃºra/formÃ¡t dÃ¡t zostane sprÃ¡vny.

Preto je kÄ¾ÃºÄovÃ© zabezpeÄiÅ¥ integritu a ochranu dÃ¡tovÃ½ch ÃºloÅ¾Ã­sk, ktorÃ© vaÅ¡e modely pouÅ¾Ã­vajÃº na rozhodovanie.

## Pochopenie hrozieb a rizÃ­k AI

PokiaÄ¾ ide o AI a sÃºvisiace systÃ©my, otrava dÃ¡t (data poisoning) je dnes najvÃ½znamnejÅ¡ou bezpeÄnostnou hrozbou. Otrava dÃ¡t nastÃ¡va, keÄ niekto Ãºmyselne menÃ­ informÃ¡cie pouÅ¾Ã­vanÃ© na trÃ©novanie AI, Äo spÃ´sobuje chyby v jej rozhodovanÃ­. Je to spÃ´sobenÃ© absenciou Å¡tandardizovanÃ½ch metÃ³d detekcie a zmierÅˆovania, spolu s naÅ¡ou zÃ¡vislosÅ¥ou na nedÃ´veryhodnÃ½ch alebo nekontrolovanÃ½ch verejnÃ½ch datasetoch. Na udrÅ¾anie integrity dÃ¡t a zabrÃ¡nenie chybnÃ©ho trÃ©ningu je nevyhnutnÃ© sledovaÅ¥ pÃ´vod a lÃ­niu vaÅ¡ich dÃ¡t. Inak platÃ­ starÃ© prÃ­slovie â€garbage in, garbage outâ€œ (smieÅ¥Ã¡cke dÃ¡ta vedÃº k smieÅ¥Ã¡ckym vÃ½sledkom), Äo vedie k zhorÅ¡eniu vÃ½konu modelu.

Tu sÃº prÃ­klady, ako otrava dÃ¡t mÃ´Å¾e ovplyvniÅ¥ vaÅ¡e modely:

1. **PrevrÃ¡tenie oznaÄenÃ­ (Label Flipping)**: Pri binÃ¡rnej klasifikÃ¡cii ÃºtoÄnÃ­k Ãºmyselne zmenÃ­ oznaÄenia malej Äasti trÃ©ningovÃ½ch dÃ¡t. NaprÃ­klad neÅ¡kodnÃ© vzorky sÃº oznaÄenÃ© ako Å¡kodlivÃ©, Äo vedie model k nesprÃ¡vnym zÃ¡verom.\
   **PrÃ­klad**: SpamovÃ½ filter nesprÃ¡vne klasifikuje legitÃ­mne e-maily ako spam kvÃ´li manipulovanÃ½m oznaÄeniam.
2. **Otrava vlastnostÃ­ (Feature Poisoning)**: ÃštoÄnÃ­k jemne upravÃ­ vlastnosti v trÃ©ningovÃ½ch dÃ¡tach, aby zaviedol zaujatosti alebo zmÃ½lil model.\
   **PrÃ­klad**: Pridanie irelevantnÃ½ch kÄ¾ÃºÄovÃ½ch slov do popisov produktov na manipulÃ¡ciu odporÃºÄacÃ­ch systÃ©mov.
3. **Vkladanie dÃ¡t (Data Injection)**: Vkladanie Å¡kodlivÃ½ch dÃ¡t do trÃ©ningovej mnoÅ¾iny s cieÄ¾om ovplyvniÅ¥ sprÃ¡vanie modelu.\
   **PrÃ­klad**: ZavÃ¡dzanie faloÅ¡nÃ½ch recenziÃ­ pouÅ¾Ã­vateÄ¾ov na skreslenie vÃ½sledkov analÃ½zy sentimentu.
4. **Ãštoky cez zadnÃ© vrÃ¡tka (Backdoor Attacks)**: ÃštoÄnÃ­k vloÅ¾Ã­ skrytÃ½ vzor (zadnÃ© vrÃ¡tka) do trÃ©ningovÃ½ch dÃ¡t. Model sa nauÄÃ­ tento vzor rozpoznÃ¡vaÅ¥ a sprÃ¡va sa Å¡kodlivo, keÄ je vzor aktivovanÃ½.\
   **PrÃ­klad**: SystÃ©m rozpoznÃ¡vania tvÃ¡rÃ­ trÃ©novanÃ½ na obrÃ¡zkoch so zadnÃ½mi vrÃ¡tkami, ktorÃ½ nesprÃ¡vne identifikuje konkrÃ©tnu osobu.

SpoloÄnosÅ¥ MITRE vytvorila [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), databÃ¡zu taktÃ­k a technÃ­k pouÅ¾Ã­vanÃ½ch ÃºtoÄnÃ­kmi pri reÃ¡lnych Ãºtokoch na AI systÃ©my.

> PoÄet zraniteÄ¾nostÃ­ v systÃ©moch s AI rastie, pretoÅ¾e zaÄlenenie AI rozÅ¡iruje povrch Ãºtoku existujÃºcich systÃ©mov nad rÃ¡mec tradiÄnÃ½ch kybernetickÃ½ch Ãºtokov. ATLAS sme vyvinuli, aby sme zvÃ½Å¡ili povedomie o tÃ½chto jedineÄnÃ½ch a vyvÃ­jajÃºcich sa zraniteÄ¾nostiach, keÄÅ¾e globÃ¡lna komunita Äoraz viac integruje AI do rÃ´znych systÃ©mov. ATLAS je modelovanÃ½ podÄ¾a rÃ¡mca MITRE ATT&CKÂ® a jeho taktiky, techniky a postupy (TTP) dopÄºÅˆajÃº tie v ATT&CK.

Podobne ako rÃ¡mec MITRE ATT&CKÂ®, ktorÃ½ sa Å¡iroko pouÅ¾Ã­va v tradiÄnej kybernetickej bezpeÄnosti na plÃ¡novanie scenÃ¡rov emulÃ¡cie pokroÄilÃ½ch hrozieb, ATLAS poskytuje Ä¾ahko vyhÄ¾adateÄ¾nÃº sadu TTP, ktorÃ© pomÃ¡hajÃº lepÅ¡ie pochopiÅ¥ a pripraviÅ¥ sa na obranu proti novÃ½m Ãºtokom.

Okrem toho Open Web Application Security Project (OWASP) vytvoril "[Top 10 zoznam](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" najkritickejÅ¡Ã­ch zraniteÄ¾nostÃ­ v aplikÃ¡ciÃ¡ch vyuÅ¾Ã­vajÃºcich LLM. Zoznam zdÃ´razÅˆuje rizikÃ¡ hrozieb ako spomÃ­nanÃ¡ otrava dÃ¡t, ale aj ÄalÅ¡ie, naprÃ­klad:

- **Prompt Injection**: technika, pri ktorej ÃºtoÄnÃ­ci manipulujÃº veÄ¾kÃ½ jazykovÃ½ model (LLM) pomocou starostlivo vytvorenÃ½ch vstupov, Äo spÃ´sobuje, Å¾e model sa sprÃ¡va mimo svojho zamÃ½Å¡Ä¾anÃ©ho sprÃ¡vania.
- **ZraniteÄ¾nosti dodÃ¡vateÄ¾skÃ©ho reÅ¥azca**: Komponenty a softvÃ©r, ktorÃ© tvoria aplikÃ¡cie pouÅ¾Ã­vanÃ© LLM, ako naprÃ­klad Python moduly alebo externÃ© datasety, mÃ´Å¾u byÅ¥ kompromitovanÃ©, Äo vedie k neoÄakÃ¡vanÃ½m vÃ½sledkom, zavedenÃ½m zaujatostiam a dokonca zraniteÄ¾nostiam v zÃ¡kladnej infraÅ¡truktÃºre.
- **NadmernÃ¡ dÃ´vera**: LLM sÃº omylnÃ© a majÃº tendenciu halucinovaÅ¥, poskytujÃºc nepresnÃ© alebo nebezpeÄnÃ© vÃ½sledky. V niekoÄ¾kÃ½ch zdokumentovanÃ½ch prÃ­padoch Ä¾udia brali vÃ½sledky doslovne, Äo viedlo k neÃºmyselnÃ½m negatÃ­vnym dÃ´sledkom v reÃ¡lnom svete.

Microsoft Cloud Advocate Rod Trent napÃ­sal bezplatnÃº elektronickÃº knihu, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), ktorÃ¡ sa hlboko venuje tÃ½mto a ÄalÅ¡Ã­m novÃ½m hrozbÃ¡m AI a poskytuje rozsiahle odporÃºÄania, ako najlepÅ¡ie rieÅ¡iÅ¥ tieto scenÃ¡re.

## BezpeÄnostnÃ© testovanie AI systÃ©mov a LLM

UmelÃ¡ inteligencia (AI) menÃ­ rÃ´zne oblasti a odvetvia, prinÃ¡Å¡ajÃºc novÃ© moÅ¾nosti a vÃ½hody pre spoloÄnosÅ¥. AI vÅ¡ak prinÃ¡Å¡a aj vÃ½znamnÃ© vÃ½zvy a rizikÃ¡, ako sÃº ochrana sÃºkromia dÃ¡t, zaujatosti, nedostatok vysvetliteÄ¾nosti a potenciÃ¡lne zneuÅ¾itie. Preto je nevyhnutnÃ© zabezpeÄiÅ¥, aby AI systÃ©my boli bezpeÄnÃ© a zodpovednÃ©, Äo znamenÃ¡, Å¾e dodrÅ¾iavajÃº etickÃ© a prÃ¡vne normy a mÃ´Å¾u byÅ¥ dÃ´veryhodnÃ© pre pouÅ¾Ã­vateÄ¾ov a zainteresovanÃ© strany.

BezpeÄnostnÃ© testovanie je proces hodnotenia bezpeÄnosti AI systÃ©mu alebo LLM identifikovanÃ­m a vyuÅ¾Ã­vanÃ­m ich zraniteÄ¾nostÃ­. MÃ´Å¾e ho vykonÃ¡vaÅ¥ vÃ½vojÃ¡r, pouÅ¾Ã­vateÄ¾ alebo externÃ½ audÃ­tor, v zÃ¡vislosti od ÃºÄelu a rozsahu testovania. NiektorÃ© z najbeÅ¾nejÅ¡Ã­ch metÃ³d bezpeÄnostnÃ©ho testovania AI systÃ©mov a LLM sÃº:

- **SanitizÃ¡cia dÃ¡t**: Proces odstraÅˆovania alebo anonymizÃ¡cie citlivÃ½ch alebo sÃºkromnÃ½ch informÃ¡ciÃ­ z trÃ©ningovÃ½ch dÃ¡t alebo vstupov AI systÃ©mu Äi LLM. SanitizÃ¡cia dÃ¡t pomÃ¡ha predchÃ¡dzaÅ¥ Ãºniku dÃ¡t a Å¡kodlivej manipulÃ¡cii tÃ½m, Å¾e zniÅ¾uje expozÃ­ciu dÃ´vernÃ½ch alebo osobnÃ½ch Ãºdajov.
- **AdversariÃ¡lne testovanie**: Proces generovania a aplikÃ¡cie adversariÃ¡lnych prÃ­kladov na vstup alebo vÃ½stup AI systÃ©mu Äi LLM na hodnotenie jeho odolnosti voÄi adversariÃ¡lnym Ãºtokom. PomÃ¡ha identifikovaÅ¥ a zmierniÅ¥ zraniteÄ¾nosti a slabiny, ktorÃ© by mohli ÃºtoÄnÃ­ci vyuÅ¾iÅ¥.
- **VerifikÃ¡cia modelu**: Proces overovania sprÃ¡vnosti a Ãºplnosti parametrov alebo architektÃºry modelu AI systÃ©mu Äi LLM. PomÃ¡ha odhaliÅ¥ a zabrÃ¡niÅ¥ krÃ¡deÅ¾i modelu tÃ½m, Å¾e zabezpeÄuje ochranu a autentifikÃ¡ciu modelu.
- **ValidÃ¡cia vÃ½stupu**: Proces overovania kvality a spoÄ¾ahlivosti vÃ½stupu AI systÃ©mu Äi LLM. PomÃ¡ha odhaliÅ¥ a opraviÅ¥ Å¡kodlivÃº manipulÃ¡ciu tÃ½m, Å¾e zabezpeÄuje konzistentnosÅ¥ a presnosÅ¥ vÃ½stupu.

OpenAI, lÃ­der v oblasti AI systÃ©mov, zriadil sÃ©riu _bezpeÄnostnÃ½ch hodnotenÃ­_ v rÃ¡mci svojej iniciatÃ­vy red teaming, zameranÃº na testovanie vÃ½stupov AI systÃ©mov s cieÄ¾om prispieÅ¥ k bezpeÄnosti AI.

> Hodnotenia mÃ´Å¾u byÅ¥ od jednoduchÃ½ch otÃ¡zok a odpovedÃ­ aÅ¾ po zloÅ¾itejÅ¡ie simulÃ¡cie. Tu sÃº konkrÃ©tne prÃ­klady hodnotenÃ­ vyvinutÃ½ch OpenAI na posÃºdenie sprÃ¡vania AI z rÃ´znych uhlov pohÄ¾adu:

#### PresviedÄanie

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m oklamaÅ¥ inÃ½ AI systÃ©m, aby povedal tajnÃ© slovo?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m presvedÄiÅ¥ inÃ½ AI systÃ©m, aby daroval peniaze?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m ovplyvniÅ¥ podporu politickÃ©ho nÃ¡vrhu inÃ½m AI systÃ©mom?

#### Steganografia (skrytÃ© sprÃ¡vy)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m prenÃ¡Å¡aÅ¥ tajnÃ© sprÃ¡vy bez odhalenia inÃ½m AI systÃ©mom?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m komprimovaÅ¥ a dekomprimovaÅ¥ sprÃ¡vy, aby umoÅ¾nil skrytÃ© sprÃ¡vy?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m koordinovaÅ¥ s inÃ½m AI systÃ©mom bez priamej komunikÃ¡cie?

### BezpeÄnosÅ¥ AI

Je nevyhnutnÃ© chrÃ¡niÅ¥ AI systÃ©my pred Å¡kodlivÃ½mi Ãºtokmi, zneuÅ¾itÃ­m alebo neÃºmyselnÃ½mi dÃ´sledkami. To zahÅ•Åˆa kroky na zabezpeÄenie bezpeÄnosti, spoÄ¾ahlivosti a dÃ´veryhodnosti AI systÃ©mov, ako naprÃ­klad:

- ZabezpeÄenie dÃ¡t a algoritmov pouÅ¾Ã­vanÃ½ch na trÃ©novanie a prevÃ¡dzku AI modelov
- Prevencia neoprÃ¡vnenÃ©ho prÃ­stupu, manipulÃ¡cie alebo sabotÃ¡Å¾e AI systÃ©mov
- Detekcia a zmierÅˆovanie zaujatosti, diskriminÃ¡cie alebo etickÃ½ch problÃ©mov v AI systÃ©moch
- ZabezpeÄenie zodpovednosti, transparentnosti a vysvetliteÄ¾nosti rozhodnutÃ­ a akciÃ­ AI
- Zladenie cieÄ¾ov a hodnÃ´t AI systÃ©mov s hodnotami Ä¾udÃ­ a spoloÄnosti

BezpeÄnosÅ¥ AI je dÃ´leÅ¾itÃ¡ pre zabezpeÄenie integrity, dostupnosti a dÃ´vernosti AI systÃ©mov a dÃ¡t. NiektorÃ© vÃ½zvy a prÃ­leÅ¾itosti v oblasti bezpeÄnosti AI sÃº:

- PrÃ­leÅ¾itosÅ¥: ZaÄlenenie AI do kybernetickej bezpeÄnosti, keÄÅ¾e mÃ´Å¾e zohrÃ¡vaÅ¥ kÄ¾ÃºÄovÃº Ãºlohu pri identifikÃ¡cii hrozieb a zlepÅ¡ovanÃ­ reakÄnÃ½ch Äasov. AI mÃ´Å¾e pomÃ´cÅ¥ automatizovaÅ¥ a rozÅ¡irovaÅ¥ detekciu a zmierÅˆovanie kybernetickÃ½ch Ãºtokov, ako sÃº phishing, malware alebo ransomware.
- VÃ½zva: AI mÃ´Å¾u vyuÅ¾iÅ¥ aj ÃºtoÄnÃ­ci na spustenie sofistikovanÃ½ch Ãºtokov, ako je generovanie faloÅ¡nÃ©ho alebo zavÃ¡dzajÃºceho obsahu, vydÃ¡vanie sa za pouÅ¾Ã­vateÄ¾ov alebo zneuÅ¾Ã­vanie zraniteÄ¾nostÃ­ AI systÃ©mov. Preto majÃº vÃ½vojÃ¡ri AI jedineÄnÃº zodpovednosÅ¥ navrhovaÅ¥ systÃ©my, ktorÃ© sÃº odolnÃ© a robustnÃ© proti zneuÅ¾itiu.

### Ochrana dÃ¡t

LLM mÃ´Å¾u predstavovaÅ¥ rizikÃ¡ pre sÃºkromie a bezpeÄnosÅ¥ dÃ¡t, ktorÃ© pouÅ¾Ã­vajÃº. NaprÃ­klad LLM mÃ´Å¾u potenciÃ¡lne zapamÃ¤taÅ¥ si a uniknÃºÅ¥ citlivÃ© informÃ¡cie zo svojich trÃ©ningovÃ½ch dÃ¡t, ako sÃº osobnÃ© menÃ¡, adresy, heslÃ¡ alebo ÄÃ­sla kreditnÃ½ch kariet. MÃ´Å¾u byÅ¥ tieÅ¾ manipulovanÃ© alebo napadnutÃ© Å¡kodlivÃ½mi aktÃ©rmi, ktorÃ­ chcÃº vyuÅ¾iÅ¥ ich zraniteÄ¾nosti alebo zaujatosti. Preto je dÃ´leÅ¾itÃ© byÅ¥ si vedomÃ½ tÃ½chto rizÃ­k a prijaÅ¥ vhodnÃ© opatrenia na ochranu dÃ¡t pouÅ¾Ã­vanÃ½ch s LLM. Medzi kroky na ochranu dÃ¡t patria:

- **Obmedzenie mnoÅ¾stva a typu dÃ¡t, ktorÃ© zdieÄ¾ate s LLM**: ZdieÄ¾ajte len dÃ¡ta, ktorÃ© sÃº nevyhnutnÃ© a relevantnÃ© pre zamÃ½Å¡Ä¾anÃ© ÃºÄely, a vyhnite sa zdieÄ¾aniu citlivÃ½ch, dÃ´vernÃ½ch alebo osobnÃ½ch Ãºdajov. PouÅ¾Ã­vatelia by mali tieÅ¾ anonymizovaÅ¥ alebo Å¡ifrovaÅ¥ dÃ¡ta, ktorÃ© zdieÄ¾ajÃº s LLM, naprÃ­klad odstrÃ¡nenÃ­m alebo zakrytÃ­m identifikaÄnÃ½ch informÃ¡ciÃ­ alebo pouÅ¾itÃ­m zabezpeÄenÃ½ch komunikaÄnÃ½ch kanÃ¡lov.
- **Overovanie dÃ¡t generovanÃ½ch LLM**: VÅ¾dy kontrolujte presnosÅ¥ a kvalitu vÃ½stupu generovanÃ©ho LLM, aby ste sa uistili, Å¾e neobsahuje neÅ¾iaduce alebo nevhodnÃ© informÃ¡cie.
- **HlÃ¡senie a upozorÅˆovanie na akÃ©koÄ¾vek Ãºniky dÃ¡t alebo incidenty**: BuÄte ostraÅ¾itÃ­ voÄi podozrivÃ½m alebo abnormÃ¡lnym aktivitÃ¡m alebo sprÃ¡vaniu LLM, ako je generovanie nerelevantnÃ½ch, nepresnÃ½ch, urÃ¡Å¾livÃ½ch alebo Å¡kodlivÃ½ch textov. To mÃ´Å¾e byÅ¥ indikÃ¡torom Ãºniku dÃ¡t alebo bezpeÄnostnÃ©ho incidentu.

BezpeÄnosÅ¥ dÃ¡t, sprÃ¡va a sÃºlad s predpismi sÃº kÄ¾ÃºÄovÃ© pre kaÅ¾dÃº organizÃ¡ciu, ktorÃ¡ chce vyuÅ¾iÅ¥ silu dÃ¡t a AI v multi-cloud prostredÃ­. ZabezpeÄenie a sprÃ¡va vÅ¡etkÃ½ch vaÅ¡ich dÃ¡t je komplexnÃ¡ a mnohostrannÃ¡ Ãºloha. MusÃ­te zabezpeÄiÅ¥ a spravovaÅ¥ rÃ´zne typy dÃ¡t (Å¡truktÃºrovanÃ©, neÅ¡truktÃºrovanÃ© a dÃ¡ta generovanÃ© AI) na rÃ´znych
> Praktika AI red teamingu sa vyvinula a nadobudla Å¡irÅ¡Ã­ vÃ½znam: nezahÅ•Åˆa len hÄ¾adanie bezpeÄnostnÃ½ch zraniteÄ¾nostÃ­, ale aj zisÅ¥ovanie inÃ½ch systÃ©movÃ½ch chÃ½b, ako je generovanie potenciÃ¡lne Å¡kodlivÃ©ho obsahu. AI systÃ©my prinÃ¡Å¡ajÃº novÃ© rizikÃ¡ a red teaming je kÄ¾ÃºÄovÃ½ pre pochopenie tÃ½chto novÃ½ch hrozieb, ako sÃº prompt injection a tvorba nepodloÅ¾enÃ©ho obsahu. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.sk.png)]()

NiÅ¾Å¡ie sÃº uvedenÃ© kÄ¾ÃºÄovÃ© poznatky, ktorÃ© formovali program AI Red Team spoloÄnosti Microsoft.

1. **Å irokÃ½ rozsah AI red teamingu:**  
   AI red teaming teraz zahÅ•Åˆa nielen bezpeÄnostnÃ©, ale aj vÃ½sledky v oblasti zodpovednej AI (RAI). TradiÄne sa red teaming zameriaval na bezpeÄnostnÃ© aspekty, kde sa model povaÅ¾oval za vektor Ãºtoku (napr. krÃ¡deÅ¾ zÃ¡kladnÃ©ho modelu). AI systÃ©my vÅ¡ak prinÃ¡Å¡ajÃº novÃ© bezpeÄnostnÃ© zraniteÄ¾nosti (napr. prompt injection, poisoning), ktorÃ© si vyÅ¾adujÃº osobitnÃº pozornosÅ¥. Okrem bezpeÄnosti AI red teaming skÃºma aj otÃ¡zky spravodlivosti (napr. stereotypizÃ¡ciu) a Å¡kodlivÃ½ obsah (napr. glorifikÃ¡ciu nÃ¡silia). VÄasnÃ© odhalenie tÃ½chto problÃ©mov umoÅ¾Åˆuje lepÅ¡ie prioritizovaÅ¥ investÃ­cie do obrany.
2. **Zlyhania z malÃ­gneho aj neÅ¡kodnÃ©ho pohÄ¾adu:**  
   AI red teaming zohÄ¾adÅˆuje zlyhania z pohÄ¾adu malÃ­gneho aj neÅ¡kodnÃ©ho sprÃ¡vania. NaprÃ­klad pri red teamingu novÃ©ho Bingu skÃºmame nielen to, ako mÃ´Å¾u Å¡kodlivÃ­ ÃºtoÄnÃ­ci systÃ©m zneuÅ¾iÅ¥, ale aj to, ako beÅ¾nÃ­ pouÅ¾Ã­vatelia mÃ´Å¾u naraziÅ¥ na problematickÃ½ alebo Å¡kodlivÃ½ obsah. Na rozdiel od tradiÄnÃ©ho bezpeÄnostnÃ©ho red teamingu, ktorÃ½ sa sÃºstreÄuje hlavne na malÃ­gnych aktÃ©rov, AI red teaming zohÄ¾adÅˆuje Å¡irÅ¡ie spektrum pouÅ¾Ã­vateÄ¾skÃ½ch typov a moÅ¾nÃ½ch zlyhanÃ­.
3. **DynamickÃ¡ povaha AI systÃ©mov:**  
   AI aplikÃ¡cie sa neustÃ¡le vyvÃ­jajÃº. Pri aplikÃ¡ciÃ¡ch zaloÅ¾enÃ½ch na veÄ¾kÃ½ch jazykovÃ½ch modeloch sa vÃ½vojÃ¡ri prispÃ´sobujÃº meniacim sa poÅ¾iadavkÃ¡m. NeustÃ¡ly red teaming zabezpeÄuje trvalÃº ostraÅ¾itosÅ¥ a prispÃ´sobenie sa novÃ½m rizikÃ¡m.

AI red teaming nie je vÅ¡eliekom a mal by byÅ¥ povaÅ¾ovanÃ½ za doplnkovÃ½ nÃ¡stroj k ÄalÅ¡Ã­m kontrolÃ¡m, ako je [role-based access control (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) a komplexnÃ© rieÅ¡enia sprÃ¡vy dÃ¡t. Je urÄenÃ½ na doplnenie bezpeÄnostnej stratÃ©gie, ktorÃ¡ sa zameriava na pouÅ¾Ã­vanie bezpeÄnÃ½ch a zodpovednÃ½ch AI rieÅ¡enÃ­, ktorÃ© berÃº do Ãºvahy ochranu sÃºkromia a bezpeÄnosÅ¥, priÄom sa snaÅ¾ia minimalizovaÅ¥ predsudky, Å¡kodlivÃ½ obsah a dezinformÃ¡cie, ktorÃ© mÃ´Å¾u podkopaÅ¥ dÃ´veru pouÅ¾Ã­vateÄ¾ov.

Tu je zoznam ÄalÅ¡Ã­ch zdrojov, ktorÃ© vÃ¡m pomÃ´Å¾u lepÅ¡ie pochopiÅ¥, ako red teaming mÃ´Å¾e pomÃ´cÅ¥ identifikovaÅ¥ a zmierniÅ¥ rizikÃ¡ vo vaÅ¡ich AI systÃ©moch:

- [PlÃ¡novanie red teamingu pre veÄ¾kÃ© jazykovÃ© modely (LLM) a ich aplikÃ¡cie](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [ÄŒo je OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming â€“ kÄ¾ÃºÄovÃ¡ prax pre budovanie bezpeÄnejÅ¡Ã­ch a zodpovednejÅ¡Ã­ch AI rieÅ¡enÃ­](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), databÃ¡za taktÃ­k a technÃ­k pouÅ¾Ã­vanÃ½ch ÃºtoÄnÃ­kmi pri reÃ¡lnych Ãºtokoch na AI systÃ©my.

## Overenie vedomostÃ­

AkÃ½ by mohol byÅ¥ dobrÃ½ prÃ­stup na udrÅ¾anie integrity dÃ¡t a zabrÃ¡nenie ich zneuÅ¾itiu?

1. MaÅ¥ silnÃ© riadenie prÃ­stupu k dÃ¡tam na zÃ¡klade rolÃ­ a sprÃ¡vu dÃ¡t  
1. ImplementovaÅ¥ a auditovaÅ¥ oznaÄovanie dÃ¡t, aby sa zabrÃ¡nilo nesprÃ¡vnemu zobrazeniu alebo zneuÅ¾itiu dÃ¡t  
1. ZabezpeÄiÅ¥, aby vaÅ¡a AI infraÅ¡truktÃºra podporovala filtrovanie obsahu

OdpoveÄ: 1, Hoci vÅ¡etky tri odporÃºÄania sÃº vÃ½bornÃ©, sprÃ¡vne prideÄ¾ovanie prÃ­stupovÃ½ch prÃ¡v k dÃ¡tam pouÅ¾Ã­vateÄ¾om vÃ½razne pomÃ¡ha predchÃ¡dzaÅ¥ manipulÃ¡cii a nesprÃ¡vnemu zobrazeniu dÃ¡t pouÅ¾Ã­vanÃ½ch veÄ¾kÃ½mi jazykovÃ½mi modelmi.

## ğŸš€ VÃ½zva

PreÄÃ­tajte si viac o tom, ako mÃ´Å¾ete [spravovaÅ¥ a chrÃ¡niÅ¥ citlivÃ© informÃ¡cie](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) v Ã©re AI.

## SkvelÃ¡ prÃ¡ca, pokraÄujte v uÄenÃ­

Po dokonÄenÃ­ tejto lekcie si pozrite naÅ¡u [kolekciu Generative AI Learning](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby ste si Äalej rozÅ¡irovali vedomosti o generatÃ­vnej AI!

Prejdite na Lekciu 14, kde sa pozrieme na [cyklus Å¾ivotnosti aplikÃ¡ciÃ­ generatÃ­vnej AI](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**VyhlÃ¡senie o zodpovednosti**:  
Tento dokument bol preloÅ¾enÃ½ pomocou AI prekladateÄ¾skej sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, majte na pamÃ¤ti, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. OriginÃ¡lny dokument v jeho pÃ´vodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.