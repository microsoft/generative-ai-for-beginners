<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T21:54:43+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "sk"
}
-->
# ZabezpeÄenie vaÅ¡ich aplikÃ¡ciÃ­ generatÃ­vnej AI

[![ZabezpeÄenie vaÅ¡ich aplikÃ¡ciÃ­ generatÃ­vnej AI](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.sk.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Ãšvod

TÃ¡to lekcia sa zameriava na:

- BezpeÄnosÅ¥ v kontexte AI systÃ©mov.
- BeÅ¾nÃ© rizikÃ¡ a hrozby pre AI systÃ©my.
- MetÃ³dy a Ãºvahy na zabezpeÄenie AI systÃ©mov.

## Ciele uÄenia

Po absolvovanÃ­ tejto lekcie budete rozumieÅ¥:

- HrozbÃ¡m a rizikÃ¡m pre AI systÃ©my.
- BeÅ¾nÃ½m metÃ³dam a postupom na zabezpeÄenie AI systÃ©mov.
- Ako implementÃ¡cia bezpeÄnostnÃ©ho testovania mÃ´Å¾e zabrÃ¡niÅ¥ neoÄakÃ¡vanÃ½m vÃ½sledkom a strate dÃ´very pouÅ¾Ã­vateÄ¾ov.

## ÄŒo znamenÃ¡ bezpeÄnosÅ¥ v kontexte generatÃ­vnej AI?

KeÄÅ¾e technolÃ³gie umelej inteligencie (AI) a strojovÃ©ho uÄenia (ML) Äoraz viac ovplyvÅˆujÃº naÅ¡e Å¾ivoty, je nevyhnutnÃ© chrÃ¡niÅ¥ nielen Ãºdaje zÃ¡kaznÃ­kov, ale aj samotnÃ© AI systÃ©my. AI/ML sa Äoraz ÄastejÅ¡ie vyuÅ¾Ã­va na podporu rozhodovacÃ­ch procesov s vysokou hodnotou v odvetviach, kde nesprÃ¡vne rozhodnutie mÃ´Å¾e maÅ¥ vÃ¡Å¾ne nÃ¡sledky.

Tu sÃº kÄ¾ÃºÄovÃ© body, ktorÃ© treba zvÃ¡Å¾iÅ¥:

- **Vplyv AI/ML**: AI/ML majÃº vÃ½znamnÃ½ vplyv na kaÅ¾dodennÃ½ Å¾ivot, a preto sa ich ochrana stÃ¡va nevyhnutnou.
- **BezpeÄnostnÃ© vÃ½zvy**: Tento vplyv AI/ML si vyÅ¾aduje nÃ¡leÅ¾itÃº pozornosÅ¥, aby sa zabezpeÄila ochrana produktov zaloÅ¾enÃ½ch na AI pred sofistikovanÃ½mi Ãºtokmi, Äi uÅ¾ zo strany trollov alebo organizovanÃ½ch skupÃ­n.
- **StrategickÃ© problÃ©my**: TechnologickÃ½ priemysel musÃ­ proaktÃ­vne rieÅ¡iÅ¥ strategickÃ© vÃ½zvy, aby zabezpeÄil dlhodobÃº bezpeÄnosÅ¥ zÃ¡kaznÃ­kov a ochranu Ãºdajov.

Okrem toho modely strojovÃ©ho uÄenia vo vÅ¡eobecnosti nedokÃ¡Å¾u rozlÃ­Å¡iÅ¥ medzi Å¡kodlivÃ½m vstupom a neÅ¡kodnÃ½mi anomÃ¡lnymi Ãºdajmi. VÃ½znamnÃ½m zdrojom trÃ©ningovÃ½ch Ãºdajov sÃº nekurÃ¡torskÃ©, nemoderovanÃ© verejnÃ© dÃ¡tovÃ© sÃºbory, ktorÃ© sÃº otvorenÃ© prÃ­spevkom tretÃ­ch strÃ¡n. ÃštoÄnÃ­ci nemusia kompromitovaÅ¥ dÃ¡tovÃ© sÃºbory, keÄ do nich mÃ´Å¾u voÄ¾ne prispievaÅ¥. Postupom Äasu sa nÃ­zko dÃ´veryhodnÃ© Å¡kodlivÃ© Ãºdaje stÃ¡vajÃº vysoko dÃ´veryhodnÃ½mi Ãºdajmi, ak zostane sprÃ¡vna Å¡truktÃºra/formÃ¡tovanie Ãºdajov.

Preto je kritickÃ© zabezpeÄiÅ¥ integritu a ochranu dÃ¡tovÃ½ch ÃºloÅ¾Ã­sk, ktorÃ© vaÅ¡e modely pouÅ¾Ã­vajÃº na rozhodovanie.

## Pochopenie hrozieb a rizÃ­k AI

V kontexte AI a sÃºvisiacich systÃ©mov je dnes najvÃ½znamnejÅ¡ou bezpeÄnostnou hrozbou otrava dÃ¡t. Otrava dÃ¡t nastÃ¡va, keÄ niekto Ãºmyselne zmenÃ­ informÃ¡cie pouÅ¾Ã­vanÃ© na trÃ©ning AI, Äo spÃ´sobÃ­, Å¾e AI robÃ­ chyby. Je to spÃ´sobenÃ© absenciou Å¡tandardizovanÃ½ch metÃ³d detekcie a zmierÅˆovania, spolu s naÅ¡ou zÃ¡vislosÅ¥ou na nedÃ´veryhodnÃ½ch alebo nekurÃ¡torskÃ½ch verejnÃ½ch dÃ¡tovÃ½ch sÃºboroch na trÃ©ning. Na zachovanie integrity Ãºdajov a zabrÃ¡nenie chybnÃ©mu trÃ©ningovÃ©mu procesu je kÄ¾ÃºÄovÃ© sledovaÅ¥ pÃ´vod a rodokmeÅˆ vaÅ¡ich Ãºdajov. Inak platÃ­ starÃ© prÃ­slovie â€odpad dnu, odpad vonâ€œ, Äo vedie k zhorÅ¡eniu vÃ½konu modelu.

Tu sÃº prÃ­klady, ako mÃ´Å¾e otrava dÃ¡t ovplyvniÅ¥ vaÅ¡e modely:

1. **Prevracanie oznaÄenÃ­**: Pri Ãºlohe binÃ¡rnej klasifikÃ¡cie ÃºtoÄnÃ­k Ãºmyselne prevrÃ¡ti oznaÄenia malej Äasti trÃ©ningovÃ½ch Ãºdajov. NaprÃ­klad neÅ¡kodnÃ© vzorky sÃº oznaÄenÃ© ako Å¡kodlivÃ©, Äo vedie k tomu, Å¾e model sa nauÄÃ­ nesprÃ¡vne asociÃ¡cie.\
   **PrÃ­klad**: SpamovÃ½ filter nesprÃ¡vne klasifikuje legitÃ­mne e-maily ako spam kvÃ´li manipulovanÃ½m oznaÄeniam.
2. **Otrava vlastnostÃ­**: ÃštoÄnÃ­k jemne upravÃ­ vlastnosti v trÃ©ningovÃ½ch Ãºdajoch, aby zaviedol zaujatosÅ¥ alebo zavÃ¡dzal model.\
   **PrÃ­klad**: Pridanie irelevantnÃ½ch kÄ¾ÃºÄovÃ½ch slov do popisov produktov na manipulÃ¡ciu odporÃºÄacÃ­ch systÃ©mov.
3. **Injekcia dÃ¡t**: VloÅ¾enie Å¡kodlivÃ½ch Ãºdajov do trÃ©ningovÃ©ho sÃºboru na ovplyvnenie sprÃ¡vania modelu.\
   **PrÃ­klad**: Zavedenie faloÅ¡nÃ½ch uÅ¾Ã­vateÄ¾skÃ½ch recenziÃ­ na skreslenie vÃ½sledkov analÃ½zy sentimentu.
4. **Ãštoky zadnÃ½ch dvierok**: ÃštoÄnÃ­k vloÅ¾Ã­ skrytÃ½ vzor (zadnÃ© dvierka) do trÃ©ningovÃ½ch Ãºdajov. Model sa nauÄÃ­ rozpoznÃ¡vaÅ¥ tento vzor a sprÃ¡va sa Å¡kodlivo, keÄ je aktivovanÃ½.\
   **PrÃ­klad**: SystÃ©m rozpoznÃ¡vania tvÃ¡re trÃ©novanÃ½ na obrÃ¡zkoch so zadnÃ½mi dvierkami, ktorÃ½ nesprÃ¡vne identifikuje konkrÃ©tnu osobu.

SpoloÄnosÅ¥ MITRE Corporation vytvorila [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), databÃ¡zu znalostÃ­ o taktikÃ¡ch a technikÃ¡ch pouÅ¾Ã­vanÃ½ch protivnÃ­kmi pri reÃ¡lnych Ãºtokoch na AI systÃ©my.

> PoÄet zraniteÄ¾nostÃ­ v systÃ©moch podporovanÃ½ch AI rastie, pretoÅ¾e zaÄlenenie AI zvyÅ¡uje povrch Ãºtoku existujÃºcich systÃ©mov nad rÃ¡mec tradiÄnÃ½ch kybernetickÃ½ch Ãºtokov. Vyvinuli sme ATLAS, aby sme zvÃ½Å¡ili povedomie o tÃ½chto jedineÄnÃ½ch a vyvÃ­jajÃºcich sa zraniteÄ¾nostiach, keÄÅ¾e globÃ¡lna komunita Äoraz viac zaÄleÅˆuje AI do rÃ´znych systÃ©mov. ATLAS je modelovanÃ½ podÄ¾a rÃ¡mca MITRE ATT&CKÂ® a jeho taktiky, techniky a postupy (TTPs) sÃº doplnkom k tÃ½m v ATT&CK.

Podobne ako rÃ¡mec MITRE ATT&CKÂ®, ktorÃ½ sa Å¡iroko pouÅ¾Ã­va v tradiÄnej kybernetickej bezpeÄnosti na plÃ¡novanie pokroÄilÃ½ch scenÃ¡rov emulÃ¡cie hrozieb, ATLAS poskytuje Ä¾ahko vyhÄ¾adÃ¡vateÄ¾nÃº sadu TTPs, ktorÃ© mÃ´Å¾u pomÃ´cÅ¥ lepÅ¡ie pochopiÅ¥ a pripraviÅ¥ sa na obranu proti vznikajÃºcim Ãºtokom.

Okrem toho projekt Open Web Application Security Project (OWASP) vytvoril "[Top 10 zoznam](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" najkritickejÅ¡Ã­ch zraniteÄ¾nostÃ­ nÃ¡jdenÃ½ch v aplikÃ¡ciÃ¡ch vyuÅ¾Ã­vajÃºcich LLMs. Zoznam zdÃ´razÅˆuje rizikÃ¡ hrozieb, ako je spomÃ­nanÃ¡ otrava dÃ¡t, spolu s ÄalÅ¡Ã­mi, ako naprÃ­klad:

- **Injekcia prÃ­kazov**: technika, pri ktorej ÃºtoÄnÃ­ci manipulujÃº veÄ¾kÃ½ jazykovÃ½ model (LLM) prostrednÃ­ctvom starostlivo vytvorenÃ½ch vstupov, Äo spÃ´sobÃ­, Å¾e sa sprÃ¡va mimo svojho zamÃ½Å¡Ä¾anÃ©ho sprÃ¡vania.
- **ZraniteÄ¾nosti dodÃ¡vateÄ¾skÃ©ho reÅ¥azca**: Komponenty a softvÃ©r, ktorÃ© tvoria aplikÃ¡cie pouÅ¾Ã­vanÃ© LLM, ako naprÃ­klad Python moduly alebo externÃ© dÃ¡tovÃ© sÃºbory, mÃ´Å¾u byÅ¥ samy kompromitovanÃ©, Äo vedie k neoÄakÃ¡vanÃ½m vÃ½sledkom, zavedenÃ½m zaujatostiam a dokonca zraniteÄ¾nostiam v zÃ¡kladnej infraÅ¡truktÃºre.
- **NadmernÃ¡ dÃ´vera**: LLMs sÃº omylnÃ© a majÃº tendenciu halucinovaÅ¥, poskytovaÅ¥ nepresnÃ© alebo nebezpeÄnÃ© vÃ½sledky. V niekoÄ¾kÃ½ch dokumentovanÃ½ch prÃ­padoch Ä¾udia brali vÃ½sledky ako samozrejmosÅ¥, Äo viedlo k neÃºmyselnÃ½m negatÃ­vnym dÃ´sledkom v reÃ¡lnom svete.

Microsoft Cloud Advocate Rod Trent napÃ­sal bezplatnÃº elektronickÃº knihu [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), ktorÃ¡ sa podrobne zaoberÃ¡ tÃ½mito a ÄalÅ¡Ã­mi vznikajÃºcimi hrozbami AI a poskytuje rozsiahle usmernenia, ako najlepÅ¡ie rieÅ¡iÅ¥ tieto scenÃ¡re.

## BezpeÄnostnÃ© testovanie AI systÃ©mov a LLMs

UmelÃ¡ inteligencia (AI) transformuje rÃ´zne oblasti a odvetvia, ponÃºka novÃ© moÅ¾nosti a vÃ½hody pre spoloÄnosÅ¥. AvÅ¡ak AI tieÅ¾ prinÃ¡Å¡a vÃ½znamnÃ© vÃ½zvy a rizikÃ¡, ako sÃº ochrana Ãºdajov, zaujatosti, nedostatok vysvetliteÄ¾nosti a potenciÃ¡lne zneuÅ¾itie. Preto je kÄ¾ÃºÄovÃ© zabezpeÄiÅ¥, aby AI systÃ©my boli bezpeÄnÃ© a zodpovednÃ©, Äo znamenÃ¡, Å¾e dodrÅ¾iavajÃº etickÃ© a prÃ¡vne normy a mÃ´Å¾u byÅ¥ dÃ´veryhodnÃ© pouÅ¾Ã­vateÄ¾mi a zainteresovanÃ½mi stranami.

BezpeÄnostnÃ© testovanie je proces hodnotenia bezpeÄnosti AI systÃ©mu alebo LLM, identifikÃ¡ciou a vyuÅ¾Ã­vanÃ­m ich zraniteÄ¾nostÃ­. To mÃ´Å¾e vykonÃ¡vaÅ¥ vÃ½vojÃ¡ri, pouÅ¾Ã­vatelia alebo nezÃ¡vislÃ­ audÃ­tori, v zÃ¡vislosti od ÃºÄelu a rozsahu testovania. NiektorÃ© z najbeÅ¾nejÅ¡Ã­ch metÃ³d bezpeÄnostnÃ©ho testovania pre AI systÃ©my a LLMs sÃº:

- **SanitÃ¡cia Ãºdajov**: Proces odstraÅˆovania alebo anonymizÃ¡cie citlivÃ½ch alebo sÃºkromnÃ½ch informÃ¡ciÃ­ z trÃ©ningovÃ½ch Ãºdajov alebo vstupov AI systÃ©mu alebo LLM. SanitÃ¡cia Ãºdajov mÃ´Å¾e pomÃ´cÅ¥ predchÃ¡dzaÅ¥ Ãºniku Ãºdajov a Å¡kodlivej manipulÃ¡cii znÃ­Å¾enÃ­m expozÃ­cie dÃ´vernÃ½ch alebo osobnÃ½ch Ãºdajov.
- **AdversariÃ¡lne testovanie**: Proces generovania a aplikÃ¡cie adversariÃ¡lnych prÃ­kladov na vstupy alebo vÃ½stupy AI systÃ©mu alebo LLM na hodnotenie jeho odolnosti voÄi adversariÃ¡lnym Ãºtokom. AdversariÃ¡lne testovanie mÃ´Å¾e pomÃ´cÅ¥ identifikovaÅ¥ a zmierniÅ¥ zraniteÄ¾nosti a slabÃ© miesta AI systÃ©mu alebo LLM, ktorÃ© mÃ´Å¾u byÅ¥ zneuÅ¾itÃ© ÃºtoÄnÃ­kmi.
- **VerifikÃ¡cia modelu**: Proces overovania sprÃ¡vnosti a Ãºplnosti parametrov alebo architektÃºry modelu AI systÃ©mu alebo LLM. VerifikÃ¡cia modelu mÃ´Å¾e pomÃ´cÅ¥ odhaliÅ¥ a predchÃ¡dzaÅ¥ krÃ¡deÅ¾i modelu tÃ½m, Å¾e zabezpeÄÃ­ ochranu a autentifikÃ¡ciu modelu.
- **ValidÃ¡cia vÃ½stupu**: Proces validÃ¡cie kvality a spoÄ¾ahlivosti vÃ½stupu AI systÃ©mu alebo LLM. ValidÃ¡cia vÃ½stupu mÃ´Å¾e pomÃ´cÅ¥ odhaliÅ¥ a opraviÅ¥ Å¡kodlivÃº manipulÃ¡ciu tÃ½m, Å¾e zabezpeÄÃ­ konzistentnosÅ¥ a presnosÅ¥ vÃ½stupu.

OpenAI, lÃ­der v oblasti AI systÃ©mov, zriadil sÃ©riu _hodnotenÃ­ bezpeÄnosti_ ako sÃºÄasÅ¥ iniciatÃ­vy red teaming siete, zameranej na testovanie vÃ½stupov AI systÃ©mov s cieÄ¾om prispieÅ¥ k bezpeÄnosti AI.

> Hodnotenia mÃ´Å¾u zahÅ•ÅˆaÅ¥ jednoduchÃ© testy otÃ¡zok a odpovedÃ­ aÅ¾ po zloÅ¾itejÅ¡ie simulÃ¡cie. Ako konkrÃ©tne prÃ­klady, tu sÃº ukÃ¡Å¾kovÃ© hodnotenia vyvinutÃ© OpenAI na hodnotenie sprÃ¡vania AI z rÃ´znych uhlov pohÄ¾adu:

#### PresviedÄanie

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m presvedÄiÅ¥ inÃ½ AI systÃ©m, aby povedal tajnÃ© slovo?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m presvedÄiÅ¥ inÃ½ AI systÃ©m, aby daroval peniaze?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m ovplyvniÅ¥ podporu inÃ©ho AI systÃ©mu pre politickÃ½ nÃ¡vrh?

#### Steganografia (skrytÃ© sprÃ¡vy)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m â€‹â€‹posielaÅ¥ tajnÃ© sprÃ¡vy bez toho, aby ho odhalil inÃ½ AI systÃ©m?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m komprimovaÅ¥ a dekomprimovaÅ¥ sprÃ¡vy, aby umoÅ¾nil skrytie tajnÃ½ch sprÃ¡v?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Ako dobre dokÃ¡Å¾e AI systÃ©m koordinovaÅ¥ s inÃ½m AI systÃ©mom bez priamej komunikÃ¡cie?

### BezpeÄnosÅ¥ AI

Je nevyhnutnÃ©, aby sme sa snaÅ¾ili chrÃ¡niÅ¥ AI systÃ©my pred Å¡kodlivÃ½mi Ãºtokmi, zneuÅ¾itÃ­m alebo neÃºmyselnÃ½mi dÃ´sledkami. To zahÅ•Åˆa kroky na zabezpeÄenie bezpeÄnosti, spoÄ¾ahlivosti a dÃ´veryhodnosti AI systÃ©mov, ako naprÃ­klad:

- ZabezpeÄenie Ãºdajov a algoritmov, ktorÃ© sa pouÅ¾Ã­vajÃº na trÃ©ning a prevÃ¡dzku AI modelov
- PredchÃ¡dzanie neoprÃ¡vnenÃ©mu prÃ­stupu, manipulÃ¡cii alebo sabotÃ¡Å¾i AI systÃ©mov
- Detekcia a zmierÅˆovanie zaujatosti, diskriminÃ¡cie alebo etickÃ½ch problÃ©mov v AI systÃ©moch
- ZabezpeÄenie zodpovednosti, transparentnosti a vysvetliteÄ¾nosti rozhodnutÃ­ a akciÃ­ AI
- Zladenie cieÄ¾ov a hodnÃ´t AI systÃ©mov s cieÄ¾mi a hodnotami Ä¾udÃ­ a spoloÄnosti

BezpeÄnosÅ¥ AI je dÃ´leÅ¾itÃ¡ pre zabezpeÄenie integrity, dostupnosti a dÃ´vernosti AI systÃ©mov a Ãºdajov. NiektorÃ© vÃ½zvy a prÃ­leÅ¾itosti bezpeÄnosti AI sÃº:

- PrÃ­leÅ¾itosÅ¥: Zahrnutie AI do stratÃ©giÃ­ kybernetickej bezpeÄnosti, keÄÅ¾e mÃ´Å¾e zohrÃ¡vaÅ¥ kÄ¾ÃºÄovÃº Ãºlohu pri identifikÃ¡cii hrozieb a zlepÅ¡ovanÃ­ reakÄnÃ½ch Äasov. AI mÃ´Å¾e pomÃ´cÅ¥ automatizovaÅ¥ a zlepÅ¡iÅ¥ detekciu a zmierÅˆovanie kybernetickÃ½ch Ãºtokov, ako sÃº phishing, malware alebo ransomware.
- VÃ½zva: AI mÃ´Å¾e byÅ¥ tieÅ¾ pouÅ¾itÃ¡ protivnÃ­kmi na spustenie sofistikovanÃ½ch Ãºtokov, ako je generovanie faloÅ¡nÃ©ho alebo zavÃ¡dzajÃºceho obsahu, vydÃ¡vanie sa za pouÅ¾Ã­vateÄ¾ov alebo vyuÅ¾Ã­vanie zraniteÄ¾nostÃ­ v AI systÃ©moch. Preto majÃº vÃ½vojÃ¡ri AI jedineÄnÃº zodpovednosÅ¥ navrhovaÅ¥ systÃ©my, ktorÃ© sÃº robustnÃ© a odolnÃ© voÄi zneuÅ¾itiu.

### Ochrana Ãºdajov

LLMs mÃ´Å¾u predstavovaÅ¥ rizikÃ¡ pre sÃºkromie a bezpeÄnosÅ¥ Ãºdajov, ktorÃ© pouÅ¾Ã­vajÃº. NaprÃ­klad LLMs mÃ´Å¾u potenciÃ¡lne zapamÃ¤taÅ¥ si a zverejniÅ¥ citlivÃ© informÃ¡cie zo svojich trÃ©ningovÃ½ch Ãºdajov, ako sÃº osobnÃ© menÃ¡, adresy, heslÃ¡ alebo ÄÃ­sla kreditnÃ½ch kariet. MÃ´Å¾u byÅ¥ tieÅ¾ manipulovanÃ© alebo napadnutÃ© Å¡kodlivÃ½mi aktÃ©rmi, ktorÃ­ chcÃº vyuÅ¾iÅ¥ ich zraniteÄ¾nosti alebo zaujatosti. Preto je dÃ´leÅ¾itÃ© byÅ¥ si vedomÃ½ tÃ½chto rizÃ­k a prijaÅ¥ vhodnÃ© opatrenia na ochranu Ãºdajov pouÅ¾Ã­vanÃ½ch s LLMs. Existuje niekoÄ¾ko krokov, ktorÃ© mÃ´Å¾ete podniknÃºÅ¥ na ochranu Ãºdajov pouÅ¾Ã­vanÃ½ch s LLMs. Tieto kroky zahÅ•ÅˆajÃº:

- **Obmedzenie mnoÅ¾stva a typu Ãºdajov, ktorÃ© zdieÄ¾ate s LLMs**: ZdieÄ¾ajte iba Ãºdaje, ktorÃ© sÃº nevyhnutnÃ© a relevantnÃ© pre zamÃ½Å¡Ä¾anÃ© ÃºÄely, a vyhnite sa zdieÄ¾aniu akÃ½chkoÄ¾vek Ãºdajov, ktorÃ© sÃº citlivÃ©, dÃ´vernÃ© alebo osobnÃ©. PouÅ¾Ã­vatelia by tieÅ¾ mali anonymizovaÅ¥ alebo Å¡ifrovaÅ¥ Ãºdaje, ktorÃ© zdieÄ¾ajÃº s LLMs, naprÃ­klad odstrÃ¡nenÃ­m alebo maskovanÃ­m akÃ½chkoÄ¾vek identifikaÄnÃ½ch informÃ¡ciÃ­ alebo pouÅ¾Ã­vanÃ­m bezpeÄnÃ½ch komunikaÄnÃ½ch kanÃ¡lov.
- **Overenie Ãºdajov, ktorÃ© LLMs generujÃº**: VÅ¾dy skontrolujte presnosÅ¥ a kvalitu vÃ½stupu generovanÃ©ho LLMs, aby ste sa uistili, Å¾e neobsahuje Å¾iadne neÅ¾iaduce alebo nevhodnÃ© informÃ¡cie.
- **NahlÃ¡senie a upozornenie na akÃ©koÄ¾vek naruÅ¡enie Ãºdajov alebo incidenty**: BuÄte ostraÅ¾itÃ­
NapodobÅˆovanie hrozieb z reÃ¡lneho sveta sa teraz povaÅ¾uje za Å¡tandardnÃº prax pri budovanÃ­ odolnÃ½ch AI systÃ©mov, priÄom sa pouÅ¾Ã­vajÃº podobnÃ© nÃ¡stroje, taktiky a postupy na identifikÃ¡ciu rizÃ­k pre systÃ©my a testovanie reakciÃ­ obrancov.

> Prax AI red teamingu sa vyvinula do rozÅ¡Ã­renÃ©ho vÃ½znamu: nezahÅ•Åˆa len skÃºmanie bezpeÄnostnÃ½ch zraniteÄ¾nostÃ­, ale aj skÃºmanie inÃ½ch zlyhanÃ­ systÃ©mu, ako je generovanie potenciÃ¡lne Å¡kodlivÃ©ho obsahu. AI systÃ©my prinÃ¡Å¡ajÃº novÃ© rizikÃ¡ a red teaming je kÄ¾ÃºÄovÃ½ na pochopenie tÃ½chto novÃ½ch rizÃ­k, ako je injekcia promptov a produkcia neodÃ´vodnenÃ©ho obsahu. - [Microsoft AI Red Team buduje budÃºcnosÅ¥ bezpeÄnejÅ¡ej AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Usmernenia a zdroje pre red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.sk.png)]()

NiÅ¾Å¡ie sÃº uvedenÃ© kÄ¾ÃºÄovÃ© poznatky, ktorÃ© formovali program AI Red Team spoloÄnosti Microsoft.

1. **RozÅ¡Ã­renÃ½ rozsah AI red teamingu:**
   AI red teaming teraz zahÅ•Åˆa bezpeÄnostnÃ© aj vÃ½sledky zodpovednej AI (RAI). TradiÄne sa red teaming zameriaval na bezpeÄnostnÃ© aspekty, priÄom model bol povaÅ¾ovanÃ½ za vektor (napr. krÃ¡deÅ¾ zÃ¡kladnÃ©ho modelu). AI systÃ©my vÅ¡ak prinÃ¡Å¡ajÃº novÃ© bezpeÄnostnÃ© zraniteÄ¾nosti (napr. injekcia promptov, otrava), ktorÃ© si vyÅ¾adujÃº osobitnÃº pozornosÅ¥. Okrem bezpeÄnosti sa AI red teaming zaoberÃ¡ aj otÃ¡zkami spravodlivosti (napr. stereotypizÃ¡cia) a Å¡kodlivÃ½m obsahom (napr. oslavovanie nÃ¡silia). VÄasnÃ¡ identifikÃ¡cia tÃ½chto problÃ©mov umoÅ¾Åˆuje prioritizÃ¡ciu investÃ­ciÃ­ do obrany.
2. **ZlomyseÄ¾nÃ© a neÅ¡kodnÃ© zlyhania:**
   AI red teaming zohÄ¾adÅˆuje zlyhania z pohÄ¾adu zlomyseÄ¾nÃ½ch aj neÅ¡kodnÃ½ch perspektÃ­v. NaprÃ­klad pri red teamingu novÃ©ho Bingu skÃºmame nielen to, ako mÃ´Å¾u zlomyseÄ¾nÃ­ protivnÃ­ci naruÅ¡iÅ¥ systÃ©m, ale aj to, ako beÅ¾nÃ­ pouÅ¾Ã­vatelia mÃ´Å¾u naraziÅ¥ na problematickÃ½ alebo Å¡kodlivÃ½ obsah. Na rozdiel od tradiÄnÃ©ho bezpeÄnostnÃ©ho red teamingu, ktorÃ½ sa zameriava hlavne na zlomyseÄ¾nÃ½ch aktÃ©rov, AI red teaming berie do Ãºvahy Å¡irÅ¡Ã­ rozsah osobnostÃ­ a potenciÃ¡lnych zlyhanÃ­.
3. **DynamickÃ¡ povaha AI systÃ©mov:**
   AI aplikÃ¡cie sa neustÃ¡le vyvÃ­jajÃº. V aplikÃ¡ciÃ¡ch veÄ¾kÃ½ch jazykovÃ½ch modelov sa vÃ½vojÃ¡ri prispÃ´sobujÃº meniacim sa poÅ¾iadavkÃ¡m. KontinuÃ¡lny red teaming zabezpeÄuje neustÃ¡lu ostraÅ¾itosÅ¥ a prispÃ´sobenie sa vyvÃ­jajÃºcim sa rizikÃ¡m.

AI red teaming nie je vÅ¡ezahÅ•ÅˆajÃºci a mal by sa povaÅ¾ovaÅ¥ za doplnkovÃ½ krok k ÄalÅ¡Ã­m kontrolÃ¡m, ako je [role-based access control (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) a komplexnÃ© rieÅ¡enia sprÃ¡vy dÃ¡t. MÃ¡ dopÄºÅˆaÅ¥ bezpeÄnostnÃº stratÃ©giu, ktorÃ¡ sa zameriava na pouÅ¾Ã­vanie bezpeÄnÃ½ch a zodpovednÃ½ch AI rieÅ¡enÃ­, ktorÃ© berÃº do Ãºvahy sÃºkromie a bezpeÄnosÅ¥, priÄom sa snaÅ¾ia minimalizovaÅ¥ predsudky, Å¡kodlivÃ½ obsah a dezinformÃ¡cie, ktorÃ© mÃ´Å¾u naruÅ¡iÅ¥ dÃ´veru pouÅ¾Ã­vateÄ¾ov.

Tu je zoznam ÄalÅ¡Ã­ch materiÃ¡lov na ÄÃ­tanie, ktorÃ© vÃ¡m mÃ´Å¾u pomÃ´cÅ¥ lepÅ¡ie pochopiÅ¥, ako red teaming mÃ´Å¾e pomÃ´cÅ¥ identifikovaÅ¥ a zmierniÅ¥ rizikÃ¡ vo vaÅ¡ich AI systÃ©moch:

- [PlÃ¡novanie red teamingu pre veÄ¾kÃ© jazykovÃ© modely (LLMs) a ich aplikÃ¡cie](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [ÄŒo je OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - KÄ¾ÃºÄovÃ¡ prax pre budovanie bezpeÄnejÅ¡Ã­ch a zodpovednejÅ¡Ã­ch AI rieÅ¡enÃ­](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), databÃ¡za znalostÃ­ o taktikÃ¡ch a technikÃ¡ch pouÅ¾Ã­vanÃ½ch protivnÃ­kmi pri Ãºtokoch na AI systÃ©my v reÃ¡lnom svete.

## Kontrola vedomostÃ­

AkÃ½ by mohol byÅ¥ dobrÃ½ prÃ­stup k udrÅ¾aniu integrity dÃ¡t a prevencii ich zneuÅ¾itia?

1. ZaviesÅ¥ silnÃ© role-based kontroly pre prÃ­stup k dÃ¡tam a ich sprÃ¡vu
1. ImplementovaÅ¥ a auditovaÅ¥ oznaÄovanie dÃ¡t na prevenciu ich nesprÃ¡vneho zobrazenia alebo zneuÅ¾itia
1. ZabezpeÄiÅ¥, aby vaÅ¡a AI infraÅ¡truktÃºra podporovala filtrovanie obsahu

A:1, Hoci vÅ¡etky tri odporÃºÄania sÃº skvelÃ©, zabezpeÄenie sprÃ¡vnych oprÃ¡vnenÃ­ na prÃ­stup k dÃ¡tam pre pouÅ¾Ã­vateÄ¾ov vÃ½razne prispeje k prevencii manipulÃ¡cie a nesprÃ¡vneho zobrazenia dÃ¡t pouÅ¾Ã­vanÃ½ch LLMs.

## ğŸš€ VÃ½zva

PreÄÃ­tajte si viac o tom, ako mÃ´Å¾ete [spravovaÅ¥ a chrÃ¡niÅ¥ citlivÃ© informÃ¡cie](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) v Ã©re AI.

## SkvelÃ¡ prÃ¡ca, pokraÄujte v uÄenÃ­

Po dokonÄenÃ­ tejto lekcie si pozrite naÅ¡u [kolekciu uÄenia o generatÃ­vnej AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), aby ste si rozÅ¡Ã­rili svoje znalosti o generatÃ­vnej AI!

Prejdite na lekciu 14, kde sa pozrieme na [Å¾ivotnÃ½ cyklus aplikÃ¡ciÃ­ generatÃ­vnej AI](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nenesieme zodpovednosÅ¥ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.