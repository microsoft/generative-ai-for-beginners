{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Χτίζοντας με τα μοντέλα της οικογένειας Meta\n",
    "\n",
    "## Εισαγωγή\n",
    "\n",
    "Σε αυτό το μάθημα θα καλύψουμε:\n",
    "\n",
    "- Εξερεύνηση των δύο βασικών μοντέλων της οικογένειας Meta - Llama 3.1 και Llama 3.2\n",
    "- Κατανόηση των περιπτώσεων χρήσης και των σεναρίων για κάθε μοντέλο\n",
    "- Παράδειγμα κώδικα που δείχνει τα μοναδικά χαρακτηριστικά κάθε μοντέλου\n",
    "\n",
    "## Η οικογένεια μοντέλων Meta\n",
    "\n",
    "Σε αυτό το μάθημα, θα εξερευνήσουμε 2 μοντέλα από την οικογένεια Meta ή το \"Llama Herd\" - Llama 3.1 και Llama 3.2\n",
    "\n",
    "Αυτά τα μοντέλα διατίθενται σε διάφορες παραλλαγές και είναι διαθέσιμα στο marketplace μοντέλων του Github. Εδώ θα βρείτε περισσότερες πληροφορίες για το πώς να χρησιμοποιήσετε τα Github Models για [πρωτοτυποποίηση με μοντέλα AI](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Παραλλαγές μοντέλων:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Σημείωση: Το Llama 3 είναι επίσης διαθέσιμο στα Github Models αλλά δεν θα καλυφθεί σε αυτό το μάθημα*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Με 405 δισεκατομμύρια παραμέτρους, το Llama 3.1 ανήκει στην κατηγορία των ανοιχτού κώδικα LLM.\n",
    "\n",
    "Η έκδοση αυτή αποτελεί αναβάθμιση σε σχέση με το προηγούμενο Llama 3 προσφέροντας:\n",
    "\n",
    "- Μεγαλύτερο παράθυρο συμφραζομένων - 128k tokens έναντι 8k tokens\n",
    "- Μεγαλύτερο μέγιστο αριθμό εξόδου tokens - 4096 έναντι 2048\n",
    "- Καλύτερη υποστήριξη πολλαπλών γλωσσών - λόγω της αύξησης των tokens εκπαίδευσης\n",
    "\n",
    "Αυτά επιτρέπουν στο Llama 3.1 να διαχειρίζεται πιο σύνθετες περιπτώσεις χρήσης κατά την ανάπτυξη εφαρμογών GenAI, όπως:\n",
    "\n",
    "- Ενσωματωμένη κλήση συναρτήσεων - η δυνατότητα να καλεί εξωτερικά εργαλεία και συναρτήσεις εκτός της ροής εργασίας του LLM\n",
    "- Καλύτερη απόδοση RAG - χάρη στο μεγαλύτερο παράθυρο συμφραζομένων\n",
    "- Δημιουργία συνθετικών δεδομένων - η δυνατότητα δημιουργίας αποτελεσματικών δεδομένων για εργασίες όπως το fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Κλήση Ενσωματωμένων Συναρτήσεων\n",
    "\n",
    "Το Llama 3.1 έχει βελτιωθεί ώστε να είναι πιο αποτελεσματικό στη χρήση συναρτήσεων ή εργαλείων. Διαθέτει επίσης δύο ενσωματωμένα εργαλεία που το μοντέλο μπορεί να αναγνωρίσει ότι χρειάζεται να χρησιμοποιηθούν, ανάλογα με το αίτημα του χρήστη. Αυτά τα εργαλεία είναι:\n",
    "\n",
    "- **Brave Search** - Μπορεί να χρησιμοποιηθεί για να λαμβάνετε ενημερωμένες πληροφορίες, όπως ο καιρός, μέσω αναζήτησης στο διαδίκτυο\n",
    "- **Wolfram Alpha** - Μπορεί να χρησιμοποιηθεί για πιο σύνθετους μαθηματικούς υπολογισμούς, ώστε να μην χρειάζεται να γράψετε δικές σας συναρτήσεις.\n",
    "\n",
    "Μπορείτε επίσης να δημιουργήσετε τα δικά σας προσαρμοσμένα εργαλεία που το LLM μπορεί να καλεί.\n",
    "\n",
    "Στο παρακάτω παράδειγμα κώδικα:\n",
    "\n",
    "- Ορίζουμε τα διαθέσιμα εργαλεία (brave_search, wolfram_alpha) στο σύστημα prompt.\n",
    "- Στέλνουμε ένα αίτημα χρήστη που ρωτά για τον καιρό σε μια συγκεκριμένη πόλη.\n",
    "- Το LLM θα απαντήσει με μια κλήση εργαλείου προς το Brave Search, η οποία θα μοιάζει κάπως έτσι: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Σημείωση: Το παράδειγμα αυτό κάνει μόνο την κλήση του εργαλείου. Αν θέλετε να λάβετε τα αποτελέσματα, θα χρειαστεί να δημιουργήσετε έναν δωρεάν λογαριασμό στη σελίδα του Brave API και να ορίσετε τη συνάρτηση μόνοι σας.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Παρόλο που είναι ένα LLM, ένας περιορισμός που έχει το Llama 3.1 είναι η πολυτροπικότητα. Δηλαδή, η δυνατότητα να χρησιμοποιεί διαφορετικούς τύπους εισόδου όπως εικόνες ως προτροπές και να παρέχει απαντήσεις. Αυτή η δυνατότητα είναι ένα από τα βασικά χαρακτηριστικά του Llama 3.2. Αυτά τα χαρακτηριστικά περιλαμβάνουν επίσης:\n",
    "\n",
    "- Πολυτροπικότητα - έχει τη δυνατότητα να αξιολογεί τόσο κείμενο όσο και εικόνες ως προτροπές\n",
    "- Παραλλαγές μικρού έως μεσαίου μεγέθους (11B και 90B) - αυτό προσφέρει ευέλικτες επιλογές ανάπτυξης,\n",
    "- Παραλλαγές μόνο κειμένου (1B και 3B) - αυτό επιτρέπει στο μοντέλο να αναπτυχθεί σε edge / κινητές συσκευές και προσφέρει χαμηλή καθυστέρηση\n",
    "\n",
    "Η υποστήριξη πολυτροπικότητας αποτελεί ένα σημαντικό βήμα στον κόσμο των μοντέλων ανοιχτού κώδικα. Το παρακάτω παράδειγμα κώδικα δέχεται τόσο μια εικόνα όσο και μια προτροπή κειμένου για να λάβει ανάλυση της εικόνας από το Llama 3.2 90B.\n",
    "\n",
    "### Υποστήριξη Πολυτροπικότητας με το Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Η μάθηση δεν σταματά εδώ, συνέχισε το ταξίδι\n",
    "\n",
    "Αφού ολοκληρώσεις αυτό το μάθημα, δες τη [συλλογή εκμάθησης Generative AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) για να συνεχίσεις να αναβαθμίζεις τις γνώσεις σου στη Generative AI!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Αποποίηση Ευθύνης**:  \nΑυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να γνωρίζετε ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρανοήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:43:15+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}