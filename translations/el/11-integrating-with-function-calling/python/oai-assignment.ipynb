{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Εισαγωγή\n",
    "\n",
    "Αυτό το μάθημα θα καλύψει:\n",
    "- Τι είναι το function calling και πού χρησιμοποιείται\n",
    "- Πώς να δημιουργήσετε ένα function call με τη χρήση του OpenAI\n",
    "- Πώς να ενσωματώσετε ένα function call σε μια εφαρμογή\n",
    "\n",
    "## Στόχοι Μάθησης\n",
    "\n",
    "Αφού ολοκληρώσετε αυτό το μάθημα, θα ξέρετε και θα κατανοείτε:\n",
    "\n",
    "- Τον σκοπό της χρήσης του function calling\n",
    "- Πώς να ρυθμίσετε ένα Function Call με τη χρήση της υπηρεσίας OpenAI\n",
    "- Πώς να σχεδιάζετε αποτελεσματικά function calls για τη δική σας περίπτωση χρήσης εφαρμογής\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Κατανόηση των Κλήσεων Συναρτήσεων\n",
    "\n",
    "Σε αυτό το μάθημα, θέλουμε να δημιουργήσουμε μια λειτουργία για τη startup εκπαίδευσής μας που επιτρέπει στους χρήστες να χρησιμοποιούν ένα chatbot για να βρουν τεχνικά μαθήματα. Θα προτείνουμε μαθήματα που ταιριάζουν στο επίπεδο δεξιοτήτων τους, τον τρέχοντα ρόλο τους και την τεχνολογία που τους ενδιαφέρει.\n",
    "\n",
    "Για να το πετύχουμε αυτό, θα χρησιμοποιήσουμε έναν συνδυασμό από:\n",
    " - `OpenAI` για να δημιουργήσουμε μια εμπειρία συνομιλίας για τον χρήστη\n",
    " - `Microsoft Learn Catalog API` για να βοηθήσουμε τους χρήστες να βρουν μαθήματα με βάση το αίτημά τους\n",
    " - `Function Calling` για να πάρουμε το ερώτημα του χρήστη και να το στείλουμε σε μια συνάρτηση ώστε να γίνει το αίτημα στο API.\n",
    "\n",
    "Για να ξεκινήσουμε, ας δούμε γιατί θα θέλαμε να χρησιμοποιήσουμε τις κλήσεις συναρτήσεων εξαρχής:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # παίρνουμε μια νέα απάντηση από το GPT όπου μπορεί να δει την απάντηση της συνάρτησης\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Γιατί Χρησιμοποιούμε Function Calling\n",
    "\n",
    "Αν έχετε ολοκληρώσει κάποιο άλλο μάθημα σε αυτό το σεμινάριο, πιθανότατα καταλαβαίνετε τη δύναμη της χρήσης των Μεγάλων Γλωσσικών Μοντέλων (LLMs). Ελπίζω επίσης να βλέπετε και κάποιους από τους περιορισμούς τους.\n",
    "\n",
    "Το Function Calling είναι μια δυνατότητα της υπηρεσίας OpenAI που έχει σχεδιαστεί για να αντιμετωπίζει τις παρακάτω προκλήσεις:\n",
    "\n",
    "Ασυνεπής Μορφοποίηση Απαντήσεων:\n",
    "- Πριν το function calling, οι απαντήσεις από ένα μεγάλο γλωσσικό μοντέλο ήταν ανοργάνωτες και ασυνεπείς. Οι προγραμματιστές έπρεπε να γράφουν πολύπλοκο κώδικα επικύρωσης για να διαχειριστούν κάθε παραλλαγή στην έξοδο.\n",
    "\n",
    "Περιορισμένη Ενσωμάτωση με Εξωτερικά Δεδομένα:\n",
    "- Πριν από αυτή τη δυνατότητα, ήταν δύσκολο να ενσωματωθούν δεδομένα από άλλα μέρη μιας εφαρμογής σε ένα περιβάλλον συνομιλίας.\n",
    "\n",
    "Με το να τυποποιεί τα φορμάτ των απαντήσεων και να επιτρέπει την εύκολη ενσωμάτωση με εξωτερικά δεδομένα, το function calling απλοποιεί την ανάπτυξη και μειώνει την ανάγκη για επιπλέον λογική επικύρωσης.\n",
    "\n",
    "Οι χρήστες δεν μπορούσαν να πάρουν απαντήσεις όπως \"Ποιος είναι ο τρέχων καιρός στη Στοκχόλμη;\". Αυτό συμβαίνει επειδή τα μοντέλα περιορίζονταν στη χρονική περίοδο που είχαν εκπαιδευτεί τα δεδομένα τους.\n",
    "\n",
    "Ας δούμε το παρακάτω παράδειγμα που δείχνει αυτό το πρόβλημα:\n",
    "\n",
    "Ας υποθέσουμε ότι θέλουμε να δημιουργήσουμε μια βάση δεδομένων με στοιχεία φοιτητών ώστε να μπορούμε να τους προτείνουμε το κατάλληλο μάθημα. Παρακάτω έχουμε δύο περιγραφές φοιτητών που είναι πολύ παρόμοιες ως προς τα δεδομένα που περιέχουν.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θέλουμε να στείλουμε αυτό σε ένα LLM για να αναλύσει τα δεδομένα. Αυτό μπορεί αργότερα να χρησιμοποιηθεί στην εφαρμογή μας για να σταλεί σε ένα API ή να αποθηκευτεί σε μια βάση δεδομένων.\n",
    "\n",
    "Ας δημιουργήσουμε δύο πανομοιότυπα prompts με τα οποία δίνουμε οδηγίες στο LLM για τις πληροφορίες που μας ενδιαφέρουν:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θέλουμε να στείλουμε αυτό σε ένα LLM για να αναλύσει τα μέρη που είναι σημαντικά για το προϊόν μας. Έτσι μπορούμε να δημιουργήσουμε δύο πανομοιότυπα prompts για να δώσουμε οδηγίες στο LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αφού δημιουργήσουμε αυτές τις δύο προτροπές, θα τις στείλουμε στο LLM χρησιμοποιώντας `openai.ChatCompletion`. Αποθηκεύουμε την προτροπή στη μεταβλητή `messages` και αναθέτουμε τον ρόλο σε `user`. Αυτό γίνεται για να μιμηθούμε ένα μήνυμα από έναν χρήστη που γράφεται σε ένα chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα μπορούμε να στείλουμε και τα δύο αιτήματα στο LLM και να εξετάσουμε την απάντηση που λαμβάνουμε.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αν και τα prompts είναι τα ίδια και οι περιγραφές παρόμοιες, μπορούμε να πάρουμε διαφορετικές μορφές της ιδιότητας `Grades`.\n",
    "\n",
    "Αν εκτελέσετε το παραπάνω κελί πολλές φορές, η μορφή μπορεί να είναι `3.7` ή `3.7 GPA`.\n",
    "\n",
    "Αυτό συμβαίνει επειδή το LLM λαμβάνει μη δομημένα δεδομένα με τη μορφή του γραπτού prompt και επιστρέφει επίσης μη δομημένα δεδομένα. Χρειαζόμαστε μια δομημένη μορφή ώστε να ξέρουμε τι να περιμένουμε όταν αποθηκεύουμε ή χρησιμοποιούμε αυτά τα δεδομένα.\n",
    "\n",
    "Χρησιμοποιώντας functional calling, μπορούμε να διασφαλίσουμε ότι λαμβάνουμε πίσω δομημένα δεδομένα. Όταν χρησιμοποιούμε function calling, το LLM δεν καλεί ή εκτελεί πραγματικά καμία συνάρτηση. Αντίθετα, δημιουργούμε μια δομή που το LLM ακολουθεί στις απαντήσεις του. Έπειτα, χρησιμοποιούμε αυτές τις δομημένες απαντήσεις για να ξέρουμε ποια συνάρτηση να εκτελέσουμε στις εφαρμογές μας.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Διάγραμμα Ροής Κλήσης Συνάρτησης](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.el.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Περιπτώσεις Χρήσης για τη χρήση κλήσεων συναρτήσεων\n",
    "\n",
    "**Κλήση Εξωτερικών Εργαλείων**  \n",
    "Τα chatbots είναι εξαιρετικά στο να παρέχουν απαντήσεις σε ερωτήσεις των χρηστών. Με τη χρήση κλήσεων συναρτήσεων, τα chatbots μπορούν να αξιοποιούν τα μηνύματα των χρηστών για να ολοκληρώσουν συγκεκριμένες εργασίες. Για παράδειγμα, ένας φοιτητής μπορεί να ζητήσει από το chatbot να \"Στείλε email στον καθηγητή μου λέγοντας ότι χρειάζομαι περισσότερη βοήθεια με αυτό το μάθημα\". Αυτό μπορεί να πραγματοποιηθεί με μια κλήση στη συνάρτηση `send_email(to: string, body: string)`\n",
    "\n",
    "**Δημιουργία Ερωτημάτων API ή Βάσης Δεδομένων**  \n",
    "Οι χρήστες μπορούν να βρουν πληροφορίες χρησιμοποιώντας φυσική γλώσσα, η οποία μετατρέπεται σε διαμορφωμένο ερώτημα ή αίτημα API. Ένα παράδειγμα είναι ένας καθηγητής που ζητάει \"Ποιοι είναι οι μαθητές που ολοκλήρωσαν την τελευταία εργασία\", το οποίο μπορεί να καλέσει μια συνάρτηση με όνομα `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Δημιουργία Δομημένων Δεδομένων**  \n",
    "Οι χρήστες μπορούν να πάρουν ένα κείμενο ή ένα αρχείο CSV και να χρησιμοποιήσουν το LLM για να εξάγουν σημαντικές πληροφορίες από αυτό. Για παράδειγμα, ένας φοιτητής μπορεί να μετατρέψει ένα άρθρο της Wikipedia για συμφωνίες ειρήνης ώστε να δημιουργήσει κάρτες επανάληψης με τεχνητή νοημοσύνη. Αυτό μπορεί να γίνει με τη χρήση μιας συνάρτησης όπως η `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Δημιουργία της Πρώτης σας Κλήσης Συνάρτησης\n",
    "\n",
    "Η διαδικασία δημιουργίας μιας κλήσης συνάρτησης περιλαμβάνει 3 βασικά βήματα:\n",
    "1. Κλήση του Chat Completions API με μια λίστα από τις συναρτήσεις σας και ένα μήνυμα χρήστη\n",
    "2. Διαβάζετε την απάντηση του μοντέλου για να εκτελέσετε μια ενέργεια, δηλαδή να εκτελέσετε μια συνάρτηση ή μια κλήση API\n",
    "3. Κάνετε μια ακόμη κλήση στο Chat Completions API με την απάντηση από τη συνάρτησή σας, ώστε να χρησιμοποιήσετε αυτές τις πληροφορίες για να δημιουργήσετε μια απάντηση προς τον χρήστη.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ροή μιας Κλήσης Συνάρτησης](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.el.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Στοιχεία μιας κλήσης συνάρτησης\n",
    "\n",
    "#### Εισαγωγή από τον χρήστη\n",
    "\n",
    "Το πρώτο βήμα είναι να δημιουργήσετε ένα μήνυμα χρήστη. Αυτό μπορεί να οριστεί δυναμικά λαμβάνοντας την τιμή από ένα πεδίο κειμένου ή μπορείτε να ορίσετε μια τιμή εδώ. Αν είναι η πρώτη σας φορά που δουλεύετε με το Chat Completions API, πρέπει να ορίσουμε το `role` και το `content` του μηνύματος.\n",
    "\n",
    "Το `role` μπορεί να είναι είτε `system` (δημιουργία κανόνων), `assistant` (το μοντέλο) ή `user` (ο τελικός χρήστης). Για την κλήση συνάρτησης, θα το ορίσουμε ως `user` και θα βάλουμε μια ενδεικτική ερώτηση.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Δημιουργία συναρτήσεων.\n",
    "\n",
    "Στη συνέχεια θα ορίσουμε μια συνάρτηση και τις παραμέτρους αυτής της συνάρτησης. Εδώ θα χρησιμοποιήσουμε μόνο μία συνάρτηση με όνομα `search_courses`, αλλά μπορείτε να δημιουργήσετε όσες συναρτήσεις θέλετε.\n",
    "\n",
    "**Σημαντικό**: Οι συναρτήσεις συμπεριλαμβάνονται στο μήνυμα συστήματος προς το LLM και θα υπολογίζονται στον συνολικό αριθμό διαθέσιμων tokens που έχετε.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ορισμοί**\n",
    "\n",
    "Η δομή ορισμού μιας συνάρτησης έχει πολλά επίπεδα, το καθένα με τις δικές του ιδιότητες. Ακολουθεί μια ανάλυση της δομής με εμφωλευμένα επίπεδα:\n",
    "\n",
    "**Ιδιότητες Συνάρτησης Ανώτατου Επιπέδου:**\n",
    "\n",
    "`name` - Το όνομα της συνάρτησης που θέλουμε να κληθεί.\n",
    "\n",
    "`description` - Αυτή είναι η περιγραφή του πώς λειτουργεί η συνάρτηση. Εδώ είναι σημαντικό να είμαστε συγκεκριμένοι και σαφείς.\n",
    "\n",
    "`parameters` - Μια λίστα με τιμές και μορφή που θέλετε το μοντέλο να παράγει στην απάντησή του.\n",
    "\n",
    "**Ιδιότητες Αντικειμένου Παραμέτρων:**\n",
    "\n",
    "`type` - Ο τύπος δεδομένων του αντικειμένου παραμέτρων (συνήθως \"object\")\n",
    "\n",
    "`properties` - Λίστα με τις συγκεκριμένες τιμές που θα χρησιμοποιήσει το μοντέλο για την απάντησή του\n",
    "\n",
    "**Ιδιότητες Μεμονωμένης Παραμέτρου:**\n",
    "\n",
    "`name` - Ορίζεται έμμεσα από το κλειδί της ιδιότητας (π.χ., \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Ο τύπος δεδομένων αυτής της συγκεκριμένης παραμέτρου (π.χ., \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - Περιγραφή της συγκεκριμένης παραμέτρου\n",
    "\n",
    "**Προαιρετικές Ιδιότητες:**\n",
    "\n",
    "`required` - Ένας πίνακας που αναφέρει ποιες παράμετροι είναι απαραίτητες για να ολοκληρωθεί η κλήση της συνάρτησης\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Κάνοντας την κλήση της συνάρτησης\n",
    "Αφού ορίσουμε μια συνάρτηση, τώρα πρέπει να την συμπεριλάβουμε στην κλήση προς το Chat Completion API. Αυτό το κάνουμε προσθέτοντας το `functions` στο αίτημα. Σε αυτή την περίπτωση `functions=functions`.\n",
    "\n",
    "Υπάρχει επίσης η επιλογή να ορίσουμε το `function_call` σε `auto`. Αυτό σημαίνει ότι θα αφήσουμε το LLM να αποφασίσει ποια συνάρτηση πρέπει να κληθεί με βάση το μήνυμα του χρήστη, αντί να το ορίσουμε εμείς οι ίδιοι.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα ας δούμε την απάντηση και πώς είναι διαμορφωμένη:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Μπορείς να δεις ότι το όνομα της συνάρτησης καλείται και από το μήνυμα του χρήστη, το LLM κατάφερε να βρει τα δεδομένα για να συμπληρώσει τα ορίσματα της συνάρτησης.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ενσωμάτωση Κλήσεων Συναρτήσεων σε μια Εφαρμογή.\n",
    "\n",
    "Αφού έχουμε δοκιμάσει την μορφοποιημένη απάντηση από το LLM, τώρα μπορούμε να την ενσωματώσουμε σε μια εφαρμογή.\n",
    "\n",
    "### Διαχείριση της ροής\n",
    "\n",
    "Για να το ενσωματώσουμε στην εφαρμογή μας, ας ακολουθήσουμε τα παρακάτω βήματα:\n",
    "\n",
    "Πρώτα, ας κάνουμε την κλήση στις υπηρεσίες της OpenAI και ας αποθηκεύσουμε το μήνυμα σε μια μεταβλητή που ονομάζεται `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα θα ορίσουμε τη συνάρτηση που θα καλέσει το Microsoft Learn API για να πάρει μια λίστα μαθημάτων:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ως καλή πρακτική, θα δούμε αν το μοντέλο θέλει να καλέσει μια συνάρτηση. Στη συνέχεια, θα δημιουργήσουμε μία από τις διαθέσιμες συναρτήσεις και θα την αντιστοιχίσουμε στη συνάρτηση που καλείται. \n",
    "Έπειτα, θα πάρουμε τα ορίσματα της συνάρτησης και θα τα αντιστοιχίσουμε στα ορίσματα από το LLM.\n",
    "\n",
    "Τέλος, θα προσθέσουμε το μήνυμα κλήσης της συνάρτησης και τις τιμές που επιστράφηκαν από το μήνυμα `search_courses`. Αυτό δίνει στο LLM όλες τις πληροφορίες που χρειάζεται για να\n",
    "απαντήσει στον χρήστη με φυσική γλώσσα.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Πρόκληση Κώδικα\n",
    "\n",
    "Μπράβο! Για να συνεχίσεις τη μάθησή σου σχετικά με το OpenAI Function Calling μπορείς να δημιουργήσεις: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Περισσότερες παραμέτρους στη συνάρτηση που θα βοηθήσουν τους εκπαιδευόμενους να βρουν περισσότερα μαθήματα. Μπορείς να βρεις τις διαθέσιμες παραμέτρους του API εδώ:\n",
    " - Δημιούργησε μια ακόμη κλήση συνάρτησης που θα λαμβάνει περισσότερες πληροφορίες από τον εκπαιδευόμενο, όπως τη μητρική του γλώσσα\n",
    " - Πρόσθεσε διαχείριση σφαλμάτων όταν η κλήση της συνάρτησης ή/και του API δεν επιστρέφει κατάλληλα μαθήματα\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Αποποίηση Ευθύνης**:  \nΑυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθεια για ακρίβεια, παρακαλούμε να γνωρίζετε ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρανοήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T21:03:59+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}