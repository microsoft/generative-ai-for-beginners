{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Κεφάλαιο 7: Δημιουργία Εφαρμογών Συνομιλίας\n",
    "## Γρήγορη Εκκίνηση με το Github Models API\n",
    "\n",
    "Αυτό το notebook είναι προσαρμοσμένο από το [Azure OpenAI Samples Repository](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst) που περιλαμβάνει notebooks με πρόσβαση σε υπηρεσίες [Azure OpenAI](notebook-azure-openai.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Επισκόπηση  \n",
    "«Τα μεγάλα γλωσσικά μοντέλα είναι συναρτήσεις που αντιστοιχούν κείμενο σε κείμενο. Δεδομένης μιας εισόδου κειμένου, ένα μεγάλο γλωσσικό μοντέλο προσπαθεί να προβλέψει το επόμενο κείμενο που θα ακολουθήσει»(1). Αυτό το σημειωματάριο \"γρήγορης εκκίνησης\" θα εισάγει τους χρήστες σε βασικές έννοιες των LLM, στις βασικές απαιτήσεις πακέτων για να ξεκινήσετε με το AML, σε μια ήπια εισαγωγή στον σχεδιασμό προτροπών, καθώς και σε μερικά σύντομα παραδείγματα διαφορετικών περιπτώσεων χρήσης.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Περιεχόμενα  \n",
    "\n",
    "[Επισκόπηση](../../../../07-building-chat-applications/python)  \n",
    "[Πώς να χρησιμοποιήσετε την υπηρεσία OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[1. Δημιουργία της υπηρεσίας OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[2. Εγκατάσταση](../../../../07-building-chat-applications/python)    \n",
    "[3. Διαπιστευτήρια](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Περιπτώσεις χρήσης](../../../../07-building-chat-applications/python)    \n",
    "[1. Περίληψη κειμένου](../../../../07-building-chat-applications/python)  \n",
    "[2. Κατηγοριοποίηση κειμένου](../../../../07-building-chat-applications/python)  \n",
    "[3. Δημιουργία νέων ονομάτων προϊόντων](../../../../07-building-chat-applications/python)  \n",
    "[4. Βελτιστοποίηση ταξινομητή](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Αναφορές](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Δημιουργήστε το πρώτο σας prompt  \n",
    "Αυτή η σύντομη άσκηση προσφέρει μια βασική εισαγωγή στην υποβολή prompts σε ένα μοντέλο στα Github Models για μια απλή εργασία \"περίληψης\".\n",
    "\n",
    "\n",
    "**Βήματα**:  \n",
    "1. Εγκαταστήστε τη βιβλιοθήκη `azure-ai-inference` στο περιβάλλον python σας, αν δεν το έχετε κάνει ήδη.  \n",
    "2. Φορτώστε τις βασικές βοηθητικές βιβλιοθήκες και ρυθμίστε τα διαπιστευτήρια για τα Github Models.  \n",
    "3. Επιλέξτε ένα μοντέλο για την εργασία σας  \n",
    "4. Δημιουργήστε ένα απλό prompt για το μοντέλο  \n",
    "5. Υποβάλετε το αίτημά σας στο API του μοντέλου!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Εγκαταστήστε το `azure-ai-inference`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 2. Εισαγωγή βοηθητικών βιβλιοθηκών και δημιουργία διαπιστευτηρίων\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Εύρεση του κατάλληλου μοντέλου  \n",
    "Τα μοντέλα GPT-3.5-turbo ή GPT-4 μπορούν να κατανοήσουν και να παράγουν φυσική γλώσσα.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Σχεδιασμός Προτροπών  \n",
    "\n",
    "«Η μαγεία των μεγάλων γλωσσικών μοντέλων είναι ότι, εκπαιδευόμενα να ελαχιστοποιούν το σφάλμα πρόβλεψης σε τεράστιες ποσότητες κειμένου, τα μοντέλα καταλήγουν να μαθαίνουν έννοιες που είναι χρήσιμες για αυτές τις προβλέψεις. Για παράδειγμα, μαθαίνουν έννοιες όπως»(1):\n",
    "\n",
    "* πώς να γράφουν σωστά\n",
    "* πώς λειτουργεί η γραμματική\n",
    "* πώς να παραφράζουν\n",
    "* πώς να απαντούν σε ερωτήσεις\n",
    "* πώς να διατηρούν μια συζήτηση\n",
    "* πώς να γράφουν σε πολλές γλώσσες\n",
    "* πώς να γράφουν κώδικα\n",
    "* κ.ά.\n",
    "\n",
    "#### Πώς να ελέγξετε ένα μεγάλο γλωσσικό μοντέλο  \n",
    "«Από όλες τις εισόδους σε ένα μεγάλο γλωσσικό μοντέλο, μακράν η πιο σημαντική είναι η προτροπή κειμένου»(1).\n",
    "\n",
    "Τα μεγάλα γλωσσικά μοντέλα μπορούν να παρακινηθούν να παράγουν έξοδο με διάφορους τρόπους:\n",
    "\n",
    "Οδηγία: Πείτε στο μοντέλο τι θέλετε\n",
    "Συμπλήρωση: Κάντε το μοντέλο να συμπληρώσει την αρχή αυτού που θέλετε\n",
    "Επίδειξη: Δείξτε στο μοντέλο τι θέλετε, είτε με:\n",
    "Λίγα παραδείγματα στην προτροπή\n",
    "Εκατοντάδες ή χιλιάδες παραδείγματα σε ένα σύνολο εκπαίδευσης για fine-tuning\n",
    "\n",
    "#### Υπάρχουν τρεις βασικές οδηγίες για τη δημιουργία προτροπών:\n",
    "\n",
    "**Δείξτε και εξηγήστε**. Κάντε ξεκάθαρο τι θέλετε είτε με οδηγίες, παραδείγματα ή συνδυασμό των δύο. Αν θέλετε το μοντέλο να ταξινομήσει μια λίστα αντικειμένων αλφαβητικά ή να κατηγοριοποιήσει μια παράγραφο με βάση το συναίσθημα, δείξτε του ότι αυτό θέλετε.\n",
    "\n",
    "**Παρέχετε ποιοτικά δεδομένα**. Αν προσπαθείτε να δημιουργήσετε έναν ταξινομητή ή να κάνετε το μοντέλο να ακολουθήσει ένα μοτίβο, βεβαιωθείτε ότι υπάρχουν αρκετά παραδείγματα. Ελέγξτε προσεκτικά τα παραδείγματά σας — το μοντέλο συνήθως είναι αρκετά έξυπνο για να καταλάβει βασικά ορθογραφικά λάθη και να σας δώσει μια απάντηση, αλλά μπορεί επίσης να υποθέσει ότι αυτό είναι σκόπιμο και να επηρεάσει την απάντηση.\n",
    "\n",
    "**Ελέγξτε τις ρυθμίσεις σας.** Οι ρυθμίσεις temperature και top_p ελέγχουν πόσο ντετερμινιστικό είναι το μοντέλο στην παραγωγή απάντησης. Αν ζητάτε μια απάντηση όπου υπάρχει μόνο μία σωστή λύση, τότε θα θέλετε να τις ορίσετε χαμηλά. Αν θέλετε πιο ποικίλες απαντήσεις, ίσως να τις ορίσετε πιο ψηλά. Το πιο συχνό λάθος που κάνουν οι χρήστες με αυτές τις ρυθμίσεις είναι να υποθέτουν ότι ελέγχουν την «ευφυΐα» ή τη «δημιουργικότητα» του μοντέλου.\n",
    "\n",
    "Πηγή: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Περίληψη Κειμένου  \n",
    "#### Πρόκληση  \n",
    "Περίληψε το κείμενο προσθέτοντας ένα 'tl;dr:' στο τέλος ενός αποσπάσματος. Παρατήρησε πώς το μοντέλο καταλαβαίνει πώς να εκτελέσει διάφορες εργασίες χωρίς επιπλέον οδηγίες. Μπορείς να πειραματιστείς με πιο περιγραφικές εντολές από το tl;dr για να τροποποιήσεις τη συμπεριφορά του μοντέλου και να προσαρμόσεις την περίληψη που λαμβάνεις(3).  \n",
    "\n",
    "Πρόσφατες μελέτες έχουν δείξει σημαντική πρόοδο σε πολλές εργασίες και αξιολογήσεις NLP μέσω προεκπαίδευσης σε μεγάλο όγκο κειμένου και στη συνέχεια εξειδικευμένης εκπαίδευσης για συγκεκριμένη εργασία. Αν και η αρχιτεκτονική είναι συνήθως ανεξάρτητη από την εργασία, η μέθοδος αυτή εξακολουθεί να απαιτεί εξειδικευμένα σύνολα δεδομένων με χιλιάδες ή δεκάδες χιλιάδες παραδείγματα. Αντίθετα, οι άνθρωποι μπορούν συνήθως να εκτελέσουν μια νέα γλωσσική εργασία με λίγα μόνο παραδείγματα ή απλές οδηγίες – κάτι που τα σημερινά συστήματα NLP δυσκολεύονται ακόμα να πετύχουν. Εδώ δείχνουμε ότι η αύξηση της κλίμακας των γλωσσικών μοντέλων βελτιώνει σημαντικά την απόδοση σε εργασίες με λίγα παραδείγματα, συχνά φτάνοντας σε ανταγωνιστικά επίπεδα με προηγούμενες κορυφαίες μεθόδους εξειδικευμένης εκπαίδευσης. \n",
    "\n",
    "\n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Ασκήσεις για διάφορες περιπτώσεις χρήσης  \n",
    "1. Περίληψη Κειμένου  \n",
    "2. Κατηγοριοποίηση Κειμένου  \n",
    "3. Δημιουργία Νέων Ονομάτων Προϊόντων\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Κατηγοριοποίηση Κειμένου  \n",
    "#### Πρόκληση  \n",
    "Κατατάξτε αντικείμενα σε κατηγορίες που δίνονται κατά τη διάρκεια της πρόβλεψης. Στο παρακάτω παράδειγμα, παρέχουμε τόσο τις κατηγορίες όσο και το κείμενο που θα κατηγοριοποιηθεί στην προτροπή (*playground_reference).\n",
    "\n",
    "Ερώτηση πελάτη: Γεια σας, ένα από τα πλήκτρα του πληκτρολογίου του φορητού μου υπολογιστή έσπασε πρόσφατα και θα χρειαστώ αντικατάσταση:\n",
    "\n",
    "Κατηγοριοποιημένη κατηγορία:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Δημιουργία Νέων Ονομάτων Προϊόντων\n",
    "#### Πρόκληση\n",
    "Δημιουργήστε ονόματα προϊόντων από παραδείγματα λέξεων. Εδώ συμπεριλαμβάνουμε στην προτροπή πληροφορίες για το προϊόν για το οποίο θα δημιουργήσουμε ονόματα. Παρέχουμε επίσης ένα παρόμοιο παράδειγμα για να δείξουμε το μοτίβο που θέλουμε να λάβουμε. Έχουμε επίσης ορίσει υψηλή τιμή για το temperature ώστε να αυξήσουμε την τυχαιότητα και να έχουμε πιο καινοτόμες απαντήσεις.\n",
    "\n",
    "Περιγραφή προϊόντος: Μια οικιακή συσκευή για milkshake\n",
    "Λέξεις-κλειδιά: γρήγορο, υγιεινό, compact.\n",
    "Ονόματα προϊόντων: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Περιγραφή προϊόντος: Ένα ζευγάρι παπούτσια που μπορεί να ταιριάξει σε κάθε μέγεθος ποδιού.\n",
    "Λέξεις-κλειδιά: προσαρμοστικό, εφαρμογή, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Αναφορές  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Παραδείγματα από το OpenAI Studio](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Βέλτιστες πρακτικές για fine-tuning του GPT-3 για ταξινόμηση κειμένου](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Για περισσότερη βοήθεια  \n",
    "[Ομάδα Εμπορικής Αξιοποίησης OpenAI](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Συντελεστές\n",
    "* [Chew-Yean Yam](https://www.linkedin.com/in/cyyam/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Αποποίηση Ευθύνης**:  \nΑυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να γνωρίζετε ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρανοήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "3eeb8e5cad61b52a8366f6259a53ed49",
   "translation_date": "2025-08-25T17:37:33+00:00",
   "source_file": "07-building-chat-applications/python/githubmodels-assignment.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}