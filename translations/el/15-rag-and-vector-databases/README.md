<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2210a0466c812d9defc4df2d9a709ff9",
  "translation_date": "2026-01-18T17:55:07+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "el"
}
-->
# Retrieval Augmented Generation (RAG) και Βάσεις Δεδομένων Διανυσμάτων

[![Retrieval Augmented Generation (RAG) και Βάσεις Δεδομένων Διανυσμάτων](../../../../../translated_images/el/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

Στο μάθημα των εφαρμογών αναζήτησης, μάθαμε συνοπτικά πώς να ενσωματώνουμε τα δικά μας δεδομένα σε Μεγάλα Γλωσσικά Μοντέλα (LLMs). Σε αυτό το μάθημα, θα εμβαθύνουμε περαιτέρω στις έννοιες της θεμελίωσης των δεδομένων σας στην εφαρμογή LLM, τους μηχανισμούς της διαδικασίας και τις μεθόδους αποθήκευσης δεδομένων, συμπεριλαμβανομένων τόσο των embeddings όσο και του κειμένου.

> **Βίντεο Έρχεται Σύντομα**

## Εισαγωγή

Σε αυτό το μάθημα θα καλύψουμε τα εξής:

- Μια εισαγωγή στο RAG, τι είναι και γιατί χρησιμοποιείται στην τεχνητή νοημοσύνη (AI).

- Κατανόηση των βάσεων δεδομένων διανυσμάτων και δημιουργία μίας για την εφαρμογή μας.

- Ένα πρακτικό παράδειγμα για το πώς να ενσωματώσετε το RAG σε μια εφαρμογή.

## Στόχοι Μάθησης

Μετά την ολοκλήρωση αυτού του μαθήματος, θα είστε ικανοί να:

- Εξηγήσετε τη σημασία του RAG στην ανάκτηση και επεξεργασία δεδομένων.

- Ρυθμίσετε μια εφαρμογή RAG και να θεμελιώσετε τα δεδομένα σας σε ένα LLM.

- Εφαρμόσετε αποτελεσματικά το RAG και τις Βάσεις Δεδομένων Διανυσμάτων σε εφαρμογές LLM.

## Το Σενάριό μας: βελτίωση των LLMs μας με τα δικά μας δεδομένα

Για αυτό το μάθημα, θέλουμε να προσθέσουμε τα δικά μας σημειώματα στην εκπαίδευση της νεοφυούς επιχείρησης, έτσι ώστε το chatbot να λαμβάνει περισσότερες πληροφορίες για τα διάφορα θέματα. Χρησιμοποιώντας τις σημειώσεις που έχουμε, οι μαθητές θα μπορούν να μελετούν καλύτερα και να κατανοούν τα διάφορα θέματα, καθιστώντας ευκολότερη την αναθεώρηση για τις εξετάσεις τους. Για να δημιουργήσουμε το σενάριό μας, θα χρησιμοποιήσουμε:

- `Azure OpenAI:` το LLM που θα χρησιμοποιήσουμε για να δημιουργήσουμε το chatbot μας

- `Μάθημα AI για αρχάριους στα Νευρωνικά Δίκτυα:` αυτό θα είναι το δεδομένο πάνω στο οποίο θα θεμελιώσουμε το LLM μας

- `Azure AI Search` και `Azure Cosmos DB:` βάση δεδομένων διανυσμάτων για αποθήκευση των δεδομένων μας και δημιουργία ευρετηρίου αναζήτησης

Οι χρήστες θα μπορούν να δημιουργούν πρακτικά κουίζ από τις σημειώσεις τους, κάρτες επανάληψης και να συνοψίζουν τα σε σύντομες επισκοπήσεις. Για να ξεκινήσουμε, ας δούμε τι είναι το RAG και πώς λειτουργεί:

## Retrieval Augmented Generation (RAG)

Ένα chatbot τροφοδοτούμενο από LLM επεξεργάζεται τις ερωτήσεις των χρηστών για να δημιουργήσει απαντήσεις. Σχεδιάζεται να είναι διαδραστικό και να αλληλεπιδρά με τους χρήστες σε πλήθος θεμάτων. Ωστόσο, οι απαντήσεις του περιορίζονται στο παρεχόμενο πλαίσιο και στα βασικά δεδομένα εκπαίδευσης. Για παράδειγμα, το όριο γνώσης του GPT-4 είναι ο Σεπτέμβριος 2021, που σημαίνει ότι δεν γνωρίζει γεγονότα μετά από αυτήν την περίοδο. Επιπλέον, τα δεδομένα που χρησιμοποιούνται για την εκπαίδευση των LLM εξαιρούν εμπιστευτικές πληροφορίες όπως προσωπικές σημειώσεις ή εγχειρίδια προϊόντων εταιρείας.

### Πώς λειτουργούν τα RAG (Retrieval Augmented Generation)

![drawing showing how RAGs work](../../../../../translated_images/el/how-rag-works.f5d0ff63942bd3a6.webp)

Ας υποθέσουμε ότι θέλετε να αναπτύξετε ένα chatbot που να δημιουργεί κουίζ από τις σημειώσεις σας, θα χρειαστείτε μια σύνδεση με τη βάση γνώσης. Εκεί μπαίνει το RAG. Τα RAG λειτουργούν ως εξής:

- **Βάση γνώσης:** Πριν την ανάκτηση, αυτά τα έγγραφα πρέπει να εισαχθούν και να προεπεξεργαστούν, συνήθως διασπώντας μεγάλα έγγραφα σε μικρότερα τμήματα, μετατρέποντάς τα σε text embeddings και αποθηκεύοντάς τα σε βάση δεδομένων.

- **Ερώτημα χρήστη:** ο χρήστης υποβάλλει μια ερώτηση

- **Ανάκτηση:** Όταν ο χρήστης υποβάλλει ερώτηση, το μοντέλο embedding ανακτά σχετικές πληροφορίες από τη βάση γνώσης για να παρέχει περισσότερο πλαίσιο που θα ενσωματωθεί στην ερώτηση.

- **Ενισχυμένη Γενιά:** Το LLM βελτιώνει την απάντησή του βάσει των ανακτημένων δεδομένων. Αυτό επιτρέπει η απάντηση να βασίζεται όχι μόνο σε προεκπαιδευμένα δεδομένα αλλά και σε σχετικές πληροφορίες από το πρόσθετο πλαίσιο. Τα ανακτημένα δεδομένα χρησιμοποιούνται για να ενισχύσουν τις απαντήσεις του LLM. Στη συνέχεια, το LLM επιστρέφει μια απάντηση στην ερώτηση του χρήστη.

![drawing showing how RAGs architecture](../../../../../translated_images/el/encoder-decode.f2658c25d0eadee2.webp)

Η αρχιτεκτονική των RAG υλοποιείται με χρήση μετασχηματιστών που αποτελούνται από δύο μέρη: έναν κωδικοποιητή (encoder) και έναν αποκωδικοποιητή (decoder). Για παράδειγμα, όταν ένας χρήστης υποβάλλει ερώτηση, το εισαγόμενο κείμενο "κωδικοποιείται" σε διανύσματα που συλλαμβάνουν το νόημα των λέξεων και τα διανύσματα "αποκωδικοποιούνται" στο ευρετήριο εγγράφων και παράγουν νέο κείμενο βασισμένο στην ερώτηση του χρήστη. Το LLM χρησιμοποιεί τόσο μοντέλο κωδικοποιητή-αποκωδικοποιητή για να δημιουργήσει την έξοδο.

Δύο προσεγγίσεις κατά την υλοποίηση RAG σύμφωνα με την προτεινόμενη εργασία: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) είναι:

- **_RAG-Sequence_** χρησιμοποιώντας ανακτημένα έγγραφα για να προβλέψει την καλύτερη δυνατή απάντηση σε μια ερώτηση χρήστη

- **RAG-Token** χρησιμοποιώντας έγγραφα για να δημιουργήσει το επόμενο token, στη συνέχεια ανακτώντας τα για να απαντήσει στην ερώτηση του χρήστη

### Γιατί να χρησιμοποιήσετε RAGs;

- **Πλούτος πληροφοριών:** εξασφαλίζει ότι οι απαντήσεις κειμένου είναι ενημερωμένες και τρέχουσες. Επομένως, βελτιώνει την απόδοση σε συγκεκριμένα πεδία πρόσβασης στη βάση γνώσης.

- Μειώνει την κατασκευή ψευδών πληροφοριών χρησιμοποιώντας **επαληθεύσιμα δεδομένα** στη βάση γνώσης για να παρέχει πλαίσιο στις ερωτήσεις των χρηστών.

- Είναι **οικονομικά αποδοτικό** καθώς είναι πιο οικονομικά σε σύγκριση με τη λεπτομερή προσαρμογή ενός LLM

## Δημιουργία βάσης γνώσης

Η εφαρμογή μας βασίζεται στα προσωπικά μας δεδομένα, δηλαδή το μάθημα Νευρωνικών Δικτύων στο πρόγραμμα AI For Beginners.

### Βάσεις Δεδομένων Διανυσμάτων

Μια βάση δεδομένων διανυσμάτων, σε αντίθεση με τις παραδοσιακές βάσεις δεδομένων, είναι εξειδικευμένη βάση σχεδιασμένη για αποθήκευση, διαχείριση και αναζήτηση ενσωματωμένων διανυσμάτων. Αποθηκεύει αριθμητικές αναπαραστάσεις εγγράφων. Η διάσπαση των δεδομένων σε αριθμητικά embeddings καθιστά πιο εύκολη την κατανόηση και επεξεργασία των δεδομένων από το σύστημα AI μας.

Αποθηκεύουμε τα embeddings μας σε βάσεις δεδομένων διανυσμάτων διότι τα LLM έχουν όριο στον αριθμό των tokens που δέχονται ως είσοδο. Καθώς δεν μπορείτε να περάσετε τα πλήρη embeddings σε ένα LLM, θα χρειαστεί να τα κόψετε σε κομμάτια και όταν ο χρήστης υποβάλλει ερώτηση, τα embeddings που είναι πιο πιθανό να σχετίζονται με την ερώτηση θα επιστραφούν μαζί με το prompt. Η διάσπαση σε τμήματα μειώνει επίσης το κόστος στον αριθμό των tokens που περνούν μέσω του LLM.

Μερικές δημοφιλείς βάσεις δεδομένων διανυσμάτων περιλαμβάνουν το Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant και DeepLake. Μπορείτε να δημιουργήσετε ένα μοντέλο Azure Cosmos DB χρησιμοποιώντας το Azure CLI με την ακόλουθη εντολή:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Από κείμενο σε embeddings

Πριν αποθηκεύσουμε τα δεδομένα μας, θα χρειαστεί να τα μετατρέψουμε σε vector embeddings πριν αποθηκευτούν στη βάση δεδομένων. Αν εργάζεστε με μεγάλα έγγραφα ή μεγάλα κείμενα, μπορείτε να τα κόψετε σε τμήματα βάσει των ερωτημάτων που περιμένετε. Η διάσπαση μπορεί να γίνει σε επίπεδο πρότασης ή σε επίπεδο παραγράφου. Καθώς η διάσπαση απορρέει νόημα από τις γύρω λέξεις, μπορείτε να προσθέσετε και άλλο πλαίσιο σε ένα τμήμα, για παράδειγμα, προσθέτοντας τον τίτλο του εγγράφου ή περιλαμβάνοντας λίγο κείμενο πριν ή μετά το τμήμα. Μπορείτε να κόψετε τα δεδομένα ως εξής:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Αν το τελευταίο κομμάτι δεν έφτασε το ελάχιστο μήκος, πρόσθεσέ το ούτως ή άλλως
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Αφού κοπούν, μπορούμε να ενσωματώσουμε το κείμενό μας χρησιμοποιώντας διαφορετικά μοντέλα ενσωμάτωσης. Μερικά μοντέλα που μπορείτε να χρησιμοποιήσετε περιλαμβάνουν: word2vec, ada-002 από το OpenAI, Azure Computer Vision και πολλά άλλα. Η επιλογή μοντέλου εξαρτάται από τις γλώσσες που χρησιμοποιείτε, τον τύπο του περιεχομένου που κωδικοποιείται (κείμενο/εικόνες/ήχος), το μέγεθος της εισόδου που μπορεί να κωδικοποιήσει και το μήκος της εξόδου του embedding.

Ένα παράδειγμα ενσωματωμένου κειμένου χρησιμοποιώντας το μοντέλο `text-embedding-ada-002` της OpenAI είναι:
![an embedding of the word cat](../../../../../translated_images/el/cat.74cbd7946bc9ca38.webp)

## Ανάκτηση και Αναζήτηση Διανυσμάτων

Όταν ένας χρήστης υποβάλλει ερώτηση, ο ανακτών την μετατρέπει σε διάνυσμα χρησιμοποιώντας τον κωδικοποιητή ερωτήματος (query encoder), στη συνέχεια αναζητά στο ευρετήριο αναζήτησης εγγράφων μας για συναφή διανύσματα που σχετίζονται με την είσοδο. Μόλις γίνει αυτό, μετατρέπει και τα διανύσματα εισόδου και εγγράφων σε κείμενο και το περνά μέσα από το LLM.

### Ανάκτηση

Η ανάκτηση συμβαίνει όταν το σύστημα προσπαθεί γρήγορα να βρει τα έγγραφα μέσα στο ευρετήριο που ικανοποιούν τα κριτήρια αναζήτησης. Ο στόχος του ανακτώντα είναι να φέρει έγγραφα που θα χρησιμοποιηθούν για να παρέχουν πλαίσιο και να θεμελιώσουν το LLM στα δεδομένα σας.

Υπάρχουν αρκετοί τρόποι για να πραγματοποιηθεί αναζήτηση στη βάση δεδομένων μας, όπως:

- **Αναζήτηση με λέξεις-κλειδιά** - χρησιμοποιείται για αναζητήσεις κειμένου

- **Αναζήτηση διανυσμάτων** - μετατρέπει τα έγγραφα από κείμενο σε διανυσματικές αναπαραστάσεις χρησιμοποιώντας μοντέλα ενσωμάτωσης, επιτρέποντας μια **σημασιολογική αναζήτηση** χρησιμοποιώντας το νόημα των λέξεων. Η ανάκτηση θα γίνει ερωτώντας τα έγγραφα των οποίων οι διανυσματικές αναπαραστάσεις είναι πιο κοντά στην ερώτηση του χρήστη.

- **Υβριδική** - συνδυασμός τόσο αναζήτησης με λέξεις-κλειδιά όσο και αναζήτησης διανυσμάτων.

Μια δυσκολία με την ανάκτηση εμφανίζεται όταν δεν υπάρχει παρόμοια απάντηση στο ερώτημα στη βάση δεδομένων, το σύστημα τότε επιστρέφει τις καλύτερες δυνατές πληροφορίες που μπορεί να βρει, ωστόσο, μπορείτε να χρησιμοποιήσετε τακτικές όπως ο καθορισμός μέγιστης απόστασης σχετικότητας ή χρήση της υβριδικής αναζήτησης που συνδυάζει τόσο λέξεις-κλειδιά όσο και αναζήτηση διανυσμάτων. Σε αυτό το μάθημα θα χρησιμοποιήσουμε υβριδική αναζήτηση, συνδυασμό των δύο. Θα αποθηκεύσουμε τα δεδομένα μας σε ένα dataframe με στήλες που περιέχουν τα τμήματα καθώς και τα embeddings.

### Ομοιότητα Διανυσμάτων

Ο ανακτών θα αναζητήσει στη βάση γνώσης εγγραφές embeddings που είναι κοντά μεταξύ τους, τον κοντινότερο γείτονα, καθώς είναι κείμενα που είναι παρόμοια. Στο σενάριο, όταν ένας χρήστης υποβάλλει ερώτημα, αυτό πρώτα ενσωματώνεται και στη συνέχεια ταιριάζεται με παρόμοια embeddings. Το κοινό μέτρο που χρησιμοποιείται για να βρεθεί πόσο παρόμοια είναι τα διανύσματα είναι η ομοιότητα συνημιτών (cosine similarity) που βασίζεται στη γωνία μεταξύ δύο διανυσμάτων.

Μπορούμε να μετρήσουμε ομοιότητα χρησιμοποιώντας και άλλες εναλλακτικές όπως η ευκλείδεια απόσταση που είναι η ευθεία γραμμή μεταξύ των άκρων των διανυσμάτων και το εσωτερικό γινόμενο (dot product) που μετρά το άθροισμα των γινομένων των αντίστοιχων στοιχείων δύο διανυσμάτων.

### Ευρετήριο Αναζήτησης

Όταν κάνετε ανάκτηση, θα πρέπει να δημιουργήσουμε ένα ευρετήριο αναζήτησης για τη βάση γνώσης μας πριν την αναζήτηση. Ένα ευρετήριο θα αποθηκεύει τα embeddings μας και μπορεί να ανακτήσει γρήγορα τα πιο όμοια τμήματα ακόμα και σε μεγάλη βάση δεδομένων. Μπορούμε να δημιουργήσουμε το ευρετήριό μας τοπικά χρησιμοποιώντας:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Δημιουργήστε τον δείκτη αναζήτησης
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Για να κάνετε ερώτημα στον δείκτη, μπορείτε να χρησιμοποιήσετε τη μέθοδο kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```

### Επανακατάταξη

Αφού κάνετε ερώτημα στη βάση δεδομένων, ίσως χρειαστεί να ταξινομήσετε τα αποτελέσματα από τα πιο σχετικά. Ένα reranking LLM χρησιμοποιεί Μηχανική Μάθηση για να βελτιώσει τη σχετικότητα των αποτελεσμάτων αναζήτησης ταξινομώντας τα από το πιο σχετικά. Χρησιμοποιώντας το Azure AI Search, η επανακατάταξη γίνεται αυτόματα μέσω ενός σημασιολογικού επαναταξινομητή. Ένα παράδειγμα του πώς λειτουργεί η επανακατάταξη χρησιμοποιώντας κοντινούς γείτονες:

```python
# Βρείτε τα πιο παρόμοια έγγραφα
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Εκτυπώστε τα πιο παρόμοια έγγραφα
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Όλα μαζί

Το τελευταίο βήμα είναι να προσθέσουμε το LLM στο μείγμα ώστε να λάβουμε απαντήσεις θεμελιωμένες στα δεδομένα μας. Μπορούμε να το υλοποιήσουμε ως εξής:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Μετατρέψτε την ερώτηση σε διάνυσμα ερωτήματος
    query_vector = create_embeddings(user_input)

    # Βρείτε τα πιο παρόμοια έγγραφα
    distances, indices = nbrs.kneighbors([query_vector])

    # προσθέστε έγγραφα στο ερώτημα για να παρέχετε πλαίσιο
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # συνδυάστε το ιστορικό και την είσοδο του χρήστη
    history.append(user_input)

    # δημιουργήστε ένα αντικείμενο μηνύματος
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # χρησιμοποιήστε την ολοκλήρωση συνομιλίας για να δημιουργήσετε μια απάντηση
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Αξιολόγηση της εφαρμογής μας

### Μετρικές Αξιολόγησης

- Ποιότητα των απαντήσεων εξασφαλίζοντας ότι ακούγονται φυσικές, ρέουσες και ανθρώπινες

- Θεμελίωση των δεδομένων: αξιολόγηση εάν η απάντηση προέρχεται από τα παρεχόμενα έγγραφα

- Σχετικότητα: αξιολόγηση εάν η απάντηση ταιριάζει και σχετίζεται με την ερώτηση

- Ροή - εάν η απάντηση έχει γραμματικό νόημα

## Περιπτώσεις Χρήσης για χρήση του RAG (Retrieval Augmented Generation) και βάσεων δεδομένων διανυσμάτων

Υπάρχουν πολλές διαφορετικές περιπτώσεις χρήσης όπου οι κλήσεις συναρτήσεων μπορούν να βελτιώσουν την εφαρμογή σας, όπως:

- Ερωτήσεις και Απαντήσεις: θεμελίωση των δεδομένων της εταιρείας σας σε ένα chat που μπορούν να χρησιμοποιήσουν οι υπάλληλοι για να υποβάλουν ερωτήσεις.

- Συστήματα Συστάσεων: όπου μπορείτε να δημιουργήσετε ένα σύστημα που ταιριάζει τις πιο όμοιες τιμές π.χ. ταινίες, εστιατόρια και πολλά ακόμα.

- Υπηρεσίες chatbot: μπορείτε να αποθηκεύσετε το ιστορικό συνομιλιών και να προσωποποιήσετε την συνομιλία βάσει των δεδομένων του χρήστη.

- Αναζήτηση εικόνων βασισμένη σε embeddings διανυσμάτων, χρήσιμη σε αναγνώριση εικόνας και ανίχνευση ανωμαλιών.

## Περίληψη

Καλύψαμε τις βασικές περιοχές του RAG από την προσθήκη των δεδομένων μας στην εφαρμογή, την ερώτηση χρήστη και την έξοδο. Για να απλοποιήσετε τη δημιουργία RAG, μπορείτε να χρησιμοποιήσετε frameworks όπως το Semantic Kernel, Langchain ή Autogen.

## Άσκηση

Για να συνεχίσετε τη μάθησή σας στο Retrieval Augmented Generation (RAG), μπορείτε να δημιουργήσετε:

- Δημιουργήστε ένα front-end για την εφαρμογή χρησιμοποιώντας το framework της επιλογής σας

- Χρησιμοποιήστε ένα framework, είτε LangChain είτε Semantic Kernel, και αναδημιουργήστε την εφαρμογή σας.

Συγχαρητήρια για την ολοκλήρωση του μαθήματος 👏.

## Η μάθηση δεν σταματά εδώ, συνεχίστε το ταξίδι

Μετά την ολοκλήρωση αυτού του μαθήματος, επισκεφτείτε τη [Συλλογή Μαθημάτων για Γενετική AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) για να συνεχίσετε να αυξάνετε τις γνώσεις σας στην Γενετική Τεχνητή Νοημοσύνη!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ενώ προσπαθούμε για ακρίβεια, παρακαλούμε να λάβετε υπόψη ότι οι αυτόματες μεταφράσεις μπορεί να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη γλώσσα του θα πρέπει να θεωρείται η αξιόπιστη πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική μετάφραση από άνθρωπο. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή λανθασμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->