{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyendo con los modelos de la familia Meta\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En esta lección veremos:\n",
    "\n",
    "- Exploración de los dos principales modelos de la familia Meta: Llama 3.1 y Llama 3.2\n",
    "- Comprender los casos de uso y escenarios para cada modelo\n",
    "- Ejemplo de código para mostrar las características únicas de cada modelo\n",
    "\n",
    "## La familia de modelos Meta\n",
    "\n",
    "En esta lección, exploraremos 2 modelos de la familia Meta o \"Llama Herd\": Llama 3.1 y Llama 3.2\n",
    "\n",
    "Estos modelos tienen diferentes variantes y están disponibles en el marketplace de modelos de Github. Aquí tienes más información sobre cómo usar Github Models para [prototipar con modelos de IA](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Variantes de modelos:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Nota: Llama 3 también está disponible en Github Models pero no se cubrirá en esta lección*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Con 405 mil millones de parámetros, Llama 3.1 entra en la categoría de LLMs de código abierto.\n",
    "\n",
    "Este modelo es una mejora respecto a la versión anterior, Llama 3, ya que ofrece:\n",
    "\n",
    "- Ventana de contexto más grande: 128k tokens frente a 8k tokens\n",
    "- Mayor cantidad máxima de tokens de salida: 4096 frente a 2048\n",
    "- Mejor soporte multilingüe: gracias al aumento de tokens de entrenamiento\n",
    "\n",
    "Estas mejoras permiten que Llama 3.1 maneje casos de uso más complejos al crear aplicaciones de GenAI, incluyendo:\n",
    "- Llamadas nativas a funciones: la capacidad de invocar herramientas y funciones externas fuera del flujo de trabajo del LLM\n",
    "- Mejor rendimiento en RAG: gracias a la ventana de contexto más amplia\n",
    "- Generación de datos sintéticos: la capacidad de crear datos efectivos para tareas como el ajuste fino\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llamadas Nativas a Funciones\n",
    "\n",
    "Llama 3.1 ha sido ajustado para ser más efectivo al realizar llamadas a funciones o herramientas. Además, cuenta con dos herramientas integradas que el modelo puede identificar como necesarias según la solicitud del usuario. Estas herramientas son:\n",
    "\n",
    "- **Brave Search** - Se puede usar para obtener información actualizada como el clima realizando una búsqueda en la web\n",
    "- **Wolfram Alpha** - Se puede usar para cálculos matemáticos más complejos, por lo que no es necesario escribir tus propias funciones.\n",
    "\n",
    "También puedes crear tus propias herramientas personalizadas que el LLM pueda utilizar.\n",
    "\n",
    "En el siguiente ejemplo de código:\n",
    "\n",
    "- Definimos las herramientas disponibles (brave_search, wolfram_alpha) en el prompt del sistema.\n",
    "- Enviamos un prompt de usuario que pregunta por el clima en una ciudad específica.\n",
    "- El LLM responderá con una llamada a la herramienta Brave Search que se verá así `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Nota: Este ejemplo solo realiza la llamada a la herramienta; si deseas obtener los resultados, necesitarás crear una cuenta gratuita en la página de la API de Brave y definir la función tú mismo*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "A pesar de ser un LLM, una de las limitaciones que tiene Llama 3.1 es la multimodalidad. Es decir, la capacidad de usar diferentes tipos de entrada como imágenes como indicaciones y proporcionar respuestas. Esta capacidad es una de las principales características de Llama 3.2. Estas características también incluyen:\n",
    "\n",
    "- Multimodalidad: tiene la capacidad de evaluar tanto indicaciones de texto como de imagen\n",
    "- Variaciones de tamaño pequeño a mediano (11B y 90B): esto ofrece opciones de implementación flexibles,\n",
    "- Variaciones solo de texto (1B y 3B): esto permite que el modelo se implemente en dispositivos edge o móviles y ofrece baja latencia\n",
    "\n",
    "El soporte multimodal representa un gran avance en el mundo de los modelos de código abierto. El siguiente ejemplo de código toma tanto una imagen como una indicación de texto para obtener un análisis de la imagen por parte de Llama 3.2 90B.\n",
    "\n",
    "### Soporte multimodal con Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El aprendizaje no termina aquí, continúa el viaje\n",
    "\n",
    "Después de completar esta lección, visita nuestra [colección de aprendizaje sobre IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para seguir ampliando tus conocimientos sobre IA Generativa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Descargo de responsabilidad**:  \nEste documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que surjan del uso de esta traducción.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:36:02+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "es"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}