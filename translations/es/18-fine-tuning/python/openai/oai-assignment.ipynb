{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste fino de modelos de Open AI\n",
    "\n",
    "Este cuaderno se basa en la guía actual proporcionada en la documentación de [Ajuste Fino](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst) de Open AI.\n",
    "\n",
    "El ajuste fino mejora el rendimiento de los modelos base para tu aplicación al reentrenarlos con datos y contexto adicionales relevantes para ese caso de uso o escenario específico. Ten en cuenta que técnicas de ingeniería de prompts como _few shot learning_ y _retrieval augmented generation_ te permiten mejorar el prompt predeterminado con datos relevantes para aumentar la calidad. Sin embargo, estos enfoques están limitados por el tamaño máximo de la ventana de tokens del modelo base seleccionado.\n",
    "\n",
    "Con el ajuste fino, en realidad estamos reentrenando el propio modelo con los datos necesarios (lo que nos permite usar muchos más ejemplos de los que caben en la ventana máxima de tokens) y desplegando una versión _personalizada_ del modelo que ya no necesita que se le proporcionen ejemplos en tiempo de inferencia. Esto no solo mejora la efectividad del diseño de nuestros prompts (tenemos más flexibilidad para usar la ventana de tokens en otras cosas), sino que también puede reducir nuestros costos (al disminuir la cantidad de tokens que necesitamos enviar al modelo durante la inferencia).\n",
    "\n",
    "El ajuste fino tiene 4 pasos:\n",
    "1. Preparar los datos de entrenamiento y subirlos.\n",
    "1. Ejecutar el trabajo de entrenamiento para obtener un modelo ajustado.\n",
    "1. Evaluar el modelo ajustado y repetir para mejorar la calidad.\n",
    "1. Desplegar el modelo ajustado para inferencia cuando estés satisfecho.\n",
    "\n",
    "Ten en cuenta que no todos los modelos base admiten ajuste fino; [consulta la documentación de OpenAI](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned?WT.mc_id=academic-105485-koreyst) para obtener la información más reciente. También puedes ajustar un modelo que ya ha sido ajustado previamente. En este tutorial, usaremos `gpt-35-turbo` como nuestro modelo base objetivo para el ajuste fino.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1.1: Prepara tu conjunto de datos\n",
    "\n",
    "Vamos a crear un chatbot que te ayude a entender la tabla periódica de los elementos respondiendo preguntas sobre un elemento con una limerick. En _este_ sencillo tutorial, solo crearemos un conjunto de datos para entrenar el modelo con algunos ejemplos de respuestas que muestran el formato esperado de los datos. En un caso de uso real, necesitarías crear un conjunto de datos con muchos más ejemplos. También podrías usar un conjunto de datos abierto (para tu dominio de aplicación) si existe alguno, y reformatearlo para usarlo en el ajuste fino.\n",
    "\n",
    "Como nos estamos enfocando en `gpt-35-turbo` y buscamos una respuesta de un solo turno (chat completion), podemos crear ejemplos usando [este formato sugerido](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?WT.mc_id=academic-105485-koreyst) que refleja los requisitos de chat completion de OpenAI. Si esperas contenido conversacional de varios turnos, deberías usar el [formato de ejemplo de varios turnos](https://platform.openai.com/docs/guides/fine-tuning/multi-turn-chat-examples?WT.mc_id=academic-105485-koreyst), que incluye un parámetro `weight` para indicar qué mensajes deben usarse (o no) en el proceso de ajuste fino.\n",
    "\n",
    "Usaremos el formato más sencillo de un solo turno para nuestro tutorial aquí. Los datos están en el [formato jsonl](https://jsonlines.org/?WT.mc_id=academic-105485-koreyst) con 1 registro por línea, cada uno representado como un objeto en formato JSON. El fragmento a continuación muestra 2 registros como ejemplo; consulta [training-data.jsonl](../../../../../18-fine-tuning/python/openai/training-data.jsonl) para ver el conjunto completo de ejemplos (10 ejemplos) que usaremos en nuestro tutorial de ajuste fino. **Nota:** Cada registro _debe_ estar definido en una sola línea (no dividido en varias líneas como es típico en un archivo JSON formateado)\n",
    "\n",
    "```json\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Gallium\"}, {\"role\": \"assistant\", \"content\": \"Gallium, oh gallium, so light - Melts in your hand, oh what a sight - At 86 degrees - Its liquid with ease - And in semiconductors, it's out of sight\"}]}\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Hydrogen\"}, {\"role\": \"assistant\", \"content\": \"Hydrogen, the first in the line - The lightest of all, so divine - It's in water, you see - And in stars, it's the key - The universe's most common sign\"}]}\n",
    "```\n",
    "\n",
    "En un caso de uso real necesitarás un conjunto de ejemplos mucho más grande para obtener buenos resultados; el equilibrio estará entre la calidad de las respuestas y el tiempo/costo del ajuste fino. Estamos usando un conjunto pequeño para poder completar el ajuste fino rápidamente y así ilustrar el proceso. Consulta [este ejemplo del OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst) para un tutorial de ajuste fino más complejo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1.2 Sube tu conjunto de datos\n",
    "\n",
    "Sube los datos usando la API de Archivos [como se describe aquí](https://platform.openai.com/docs/guides/fine-tuning/upload-a-training-file). Ten en cuenta que para ejecutar este código, primero debes haber realizado los siguientes pasos:\n",
    " - Haber instalado el paquete de Python `openai` (asegúrate de usar una versión >=0.28.0 para contar con las funciones más recientes)\n",
    " - Haber configurado la variable de entorno `OPENAI_API_KEY` con tu clave de API de OpenAI\n",
    "Para más información, consulta la [Guía de configuración](./../../../00-course-setup/02-setup-local.md?WT.mc_id=academic-105485-koreyst) proporcionada para el curso.\n",
    "\n",
    "Ahora, ejecuta el código para crear un archivo a partir de tu archivo JSONL local y subirlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-JdAJcagdOTG6ACNlFWzuzmyV', bytes=4021, created_at=1715566183, filename='training-data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
      "Training File ID: file-JdAJcagdOTG6ACNlFWzuzmyV\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_file = client.files.create(\n",
    "  file=open(\"./training-data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(ft_file)\n",
    "print(\"Training File ID: \" + ft_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2.1: Crea el trabajo de ajuste fino con el SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', created_at=1715566184, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-EZ6ag0n0S6Zm8eV9BSWKmE6l', result_files=[], seed=830529052, status='validating_files', trained_tokens=None, training_file='file-JdAJcagdOTG6ACNlFWzuzmyV', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)\n",
      "Fine-tuning Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_filejob = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file.id, \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "print(ft_filejob)\n",
    "print(\"Fine-tuning Job ID: \" + ft_filejob.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2.2: Verificar el estado del trabajo\n",
    "\n",
    "Aquí tienes algunas cosas que puedes hacer con la API `client.fine_tuning.jobs`:\n",
    "- `client.fine_tuning.jobs.list(limit=<n>)` - Lista los últimos n trabajos de ajuste fino\n",
    "- `client.fine_tuning.jobs.retrieve(<job_id>)` - Obtiene los detalles de un trabajo de ajuste fino específico\n",
    "- `client.fine_tuning.jobs.cancel(<job_id>)` - Cancela un trabajo de ajuste fino\n",
    "- `client.fine_tuning.jobs.list_events(fine_tuning_job_id=<job_id>, limit=<b>)` - Muestra hasta n eventos del trabajo\n",
    "- `client.fine_tuning.jobs.create(model=\"gpt-35-turbo\", training_file=\"your-training-file.jsonl\", ...)`\n",
    "\n",
    "El primer paso del proceso es _validar el archivo de entrenamiento_ para asegurarse de que los datos estén en el formato correcto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-GkWiDgZmOsuv4q5cSTEGscY6', created_at=1715566184, level='info', message='Validating training file: file-JdAJcagdOTG6ACNlFWzuzmyV', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-3899xdVTO3LN7Q7LkKLMJUnb', created_at=1715566184, level='info', message='Created fine-tuning job: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_filejob.id, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n",
      "Status: running\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "# Once the training data is validated\n",
    "# Track the job status to see if it is running and when it is complete\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(\"Trained Tokens:\", response.trained_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2.3: Haz seguimiento de eventos para monitorear el progreso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 85/100: training loss=0.14\n",
      "Step 86/100: training loss=0.00\n",
      "Step 87/100: training loss=0.00\n",
      "Step 88/100: training loss=0.07\n",
      "Step 89/100: training loss=0.00\n",
      "Step 90/100: training loss=0.00\n",
      "Step 91/100: training loss=0.00\n",
      "Step 92/100: training loss=0.00\n",
      "Step 93/100: training loss=0.00\n",
      "Step 94/100: training loss=0.00\n",
      "Step 95/100: training loss=0.08\n",
      "Step 96/100: training loss=0.05\n",
      "Step 97/100: training loss=0.00\n",
      "Step 98/100: training loss=0.00\n",
      "Step 99/100: training loss=0.00\n",
      "Step 100/100: training loss=0.00\n",
      "Checkpoint created at step 80 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyyF2:ckpt-step-80\n",
      "Checkpoint created at step 90 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyzhK:ckpt-step-90\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "# You can also track progress in a more granular way by checking for events\n",
    "# Refresh this code till you get the `The job has successfully completed` message\n",
    "response = client.fine_tuning.jobs.list_events(ft_filejob.id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2.4: Ver el estado en el panel de OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También puedes ver el estado visitando el sitio web de OpenAI y explorando la sección _Fine-tuning_ de la plataforma. Allí podrás ver el estado del trabajo actual y también seguir el historial de ejecuciones anteriores. En esta captura de pantalla, puedes notar que la ejecución previa falló y la segunda se realizó con éxito. Para dar contexto, esto ocurrió porque la primera ejecución utilizó un archivo JSON con registros mal formateados; una vez corregido, la segunda ejecución terminó correctamente y el modelo quedó disponible para su uso.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fine-tuned-model-status.563271727bf7bfba7e3f73a201f8712fae3cea1c08f7c7f12ca469c06d234122.es.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También puedes ver los mensajes de estado y las métricas desplazándote hacia abajo en el panel visual, como se muestra:\n",
    "\n",
    "| Mensajes | Métricas |\n",
    "|:---|:---|\n",
    "| ![Mensajes](../../../../../translated_images/fine-tuned-messages-panel.4ed0c2da5ea1313b3a706a66f66bf5007c379cd9219cfb74cb30c0b04b90c4c8.es.png) |  ![Métricas](../../../../../translated_images/fine-tuned-metrics-panel.700d7e4995a652299584ab181536a6cfb67691a897a518b6c7a2aa0a17f1a30d.es.png)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3.1: Recuperar ID y probar el modelo ajustado en el código\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the identity of the fine-tuned model once ready\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "fine_tuned_model_id = response.fine_tuned_model\n",
    "print(\"Fine-tuned Model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Strontium, a metal so bright - It's in fireworks, a dazzling sight - It's in bones, you see - And in tea, it's the key - It's the fortieth, so pure, that's the right\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# You can then use that model to generate completions from the SDK as shown\n",
    "# Or you can load that model into the OpenAI Playground (in the UI) to validate it from there.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Elle, a factual chatbot that answers questions about elements in the periodic table with a limerick\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Strontium\"},\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3.2: Cargar y probar el modelo ajustado en Playground\n",
    "\n",
    "Ahora puedes probar el modelo ajustado de dos maneras. Primero, puedes ir a Playground y usar el menú desplegable de Modelos para seleccionar tu modelo ajustado de la lista de opciones. La otra opción es usar la opción \"Playground\" que aparece en el panel de Fine-tuning (ver la captura de pantalla arriba), la cual abre una vista _comparativa_ que muestra las versiones del modelo base y el modelo ajustado lado a lado para una evaluación rápida.\n",
    "\n",
    "![Estado del trabajo de fine-tuning](../../../../../translated_images/fine-tuned-playground-compare.56e06f0ad8922016497d39ced3d84ea296eec89073503f2bf346ec9718f913b5.es.png)\n",
    "\n",
    "Simplemente completa el contexto del sistema que usaste en tus datos de entrenamiento y proporciona tu pregunta de prueba. Notarás que ambos lados se actualizan con el mismo contexto y pregunta. Ejecuta la comparación y verás la diferencia en las respuestas entre ambos modelos. _Observa cómo el modelo ajustado genera la respuesta en el formato que diste en tus ejemplos, mientras que el modelo base simplemente sigue la indicación del sistema_.\n",
    "\n",
    "![Estado del trabajo de fine-tuning](../../../../../translated_images/fine-tuned-playground-launch.5a26495c983c6350c227e05700a47a89002d132949a56fa4ff37f266ebe997b2.es.png)\n",
    "\n",
    "También notarás que la comparación muestra la cantidad de tokens para cada modelo y el tiempo que tarda la inferencia. **Este ejemplo específico es muy simple y solo busca mostrar el proceso, pero no refleja un conjunto de datos o escenario real**. Puede que notes que ambos ejemplos muestran la misma cantidad de tokens (el contexto del sistema y la indicación del usuario son idénticos), pero el modelo ajustado tarda más en la inferencia (modelo personalizado).\n",
    "\n",
    "En escenarios reales, no usarás un ejemplo tan simple como este, sino que harás fine-tuning con datos reales (por ejemplo, un catálogo de productos para atención al cliente), donde la calidad de la respuesta será mucho más evidente. En _ese_ contexto, lograr una calidad de respuesta equivalente con el modelo base requerirá una mayor personalización de los prompts, lo que aumentará el uso de tokens y posiblemente el tiempo de procesamiento para la inferencia. _Para probar esto, revisa los ejemplos de fine-tuning en el OpenAI Cookbook para comenzar._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Descargo de responsabilidad**:  \nEste documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que surjan del uso de esta traducción.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "69b527f1e605a10fb9c7e00ae841021d",
   "translation_date": "2025-08-25T21:29:01+00:00",
   "source_file": "18-fine-tuning/python/openai/oai-assignment.ipynb",
   "language_code": "es"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}