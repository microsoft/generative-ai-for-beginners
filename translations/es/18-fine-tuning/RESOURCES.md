# Recursos Para Aprendizaje Autodidacta

La lección fue construida utilizando una serie de recursos fundamentales de OpenAI y Azure OpenAI como referencias para la terminología y los tutoriales. Aquí hay una lista no exhaustiva, para tus propias jornadas de aprendizaje autodidacta.

## 1. Recursos Primarios

| Título/Enlace                                                                                                                                                                                                                  | Descripción                                                                                                                                                                                                                                                                                                                   |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning with OpenAI Models](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                         | El ajuste fino mejora el aprendizaje con pocos ejemplos al entrenar con muchos más ejemplos de los que caben en el prompt, ahorrando costos, mejorando la calidad de respuesta y permitiendo solicitudes de menor latencia. **Obtén una visión general del ajuste fino de OpenAI.**                                                                                     |
| [What is Fine-Tuning with Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                      | Comprende **qué es el ajuste fino (concepto)**, por qué deberías considerarlo (problema motivador), qué datos usar (entrenamiento) y cómo medir la calidad.                                                                                                                                                                    |
| [Customize a model with fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)  | El Servicio de Azure OpenAI te permite adaptar nuestros modelos a tus conjuntos de datos personales usando el ajuste fino. Aprende **cómo ajustar finamente (proceso)** modelos seleccionados usando Azure AI Studio, Python SDK o REST API.                                                                                                                          |
| [Recommendations for LLM fine-tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                     | Los LLMs pueden no funcionar bien en dominios, tareas o conjuntos de datos específicos, o pueden producir resultados inexactos o engañosos. **¿Cuándo deberías considerar el ajuste fino** como una posible solución a esto?                                                                                                                                           |
| [Continuous Fine Tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)              | El ajuste fino continuo es el proceso iterativo de seleccionar un modelo ya ajustado finamente como modelo base y **ajustarlo finamente aún más** en nuevos conjuntos de ejemplos de entrenamiento.                                                                                                                                                                   |
| [Fine-tuning and function calling](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                        | Ajustar finamente tu modelo **con ejemplos de llamadas a funciones** puede mejorar la salida del modelo al obtener resultados más precisos y consistentes, con respuestas formateadas de manera similar y ahorro de costos.                                                                                                                                             |
| [Fine-tuning Models: Azure OpenAI Guidance](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                         | Consulta esta tabla para entender **qué modelos se pueden ajustar finamente** en Azure OpenAI, y en qué regiones están disponibles. Consulta sus límites de tokens y fechas de expiración de los datos de entrenamiento si es necesario.                                                                                                                               |
| [To Fine Tune or Not To Fine Tune? That is the Question](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                       | Este episodio de 30 minutos de **Oct 2023** del AI Show discute beneficios, desventajas y perspectivas prácticas que te ayudarán a tomar esta decisión.                                                                                                                                                                        |
| [Getting Started With LLM Fine-Tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning?WT.mc_id=academic-105485-koreyst)                                              | Este recurso del **AI Playbook** te guía a través de los requisitos de datos, el formato, el ajuste fino de hiperparámetros y los desafíos/limitaciones que deberías conocer.                                                                                                                                                                                       |
| **Tutorial**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                   | Aprende a crear un conjunto de datos de ajuste fino de muestra, prepararte para el ajuste fino, crear un trabajo de ajuste fino y desplegar el modelo ajustado finamente en Azure.                                                                                                                                                                                   |
| **Tutorial**: [Fine-tune a Llama 2 model in Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                       | Azure AI Studio te permite adaptar modelos de lenguaje grande a tus conjuntos de datos personales _usando un flujo de trabajo basado en UI adecuado para desarrolladores de bajo código_. Mira este ejemplo.                                                                                                                                                          |
| **Tutorial**:[Fine-tune Hugging Face models for a single GPU on Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)                | Este artículo describe cómo ajustar finamente un modelo de Hugging Face con la biblioteca de transformadores de Hugging Face en una sola GPU con Azure DataBricks + bibliotecas Hugging Face Trainer.                                                                                                                                                                |
| **Training:** [Fine-tune a foundation model with Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)                | El catálogo de modelos en Azure Machine Learning ofrece muchos modelos de código abierto que puedes ajustar finamente para tu tarea específica. Prueba este módulo que es [del Camino de Aprendizaje de Generative AI de AzureML](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst). |
| **Tutorial:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                 | Ajustar finamente modelos GPT-3.5 o GPT-4 en Microsoft Azure usando W&B permite un seguimiento y análisis detallado del rendimiento del modelo. Esta guía extiende los conceptos de la guía de Ajuste Fino de OpenAI con pasos y características específicas para Azure OpenAI.                                                                                     |
|                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                               |

## 2. Recursos Secundarios

Esta sección captura recursos adicionales que valen la pena explorar, pero que no tuvimos tiempo de cubrir en esta lección. Pueden ser cubiertos en una lección futura, o como una opción de tarea secundaria, en una fecha posterior. Por ahora, úsalos para construir tu propia experiencia y conocimiento sobre este tema.

| Título/Enlace                                                                                                                                                                                                              | Descripción                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Data preparation and analysis for chat model fine-tuning](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                           | Este notebook sirve como una herramienta para preprocesar y analizar el conjunto de datos de chat usado para el ajuste fino de un modelo de chat. Verifica errores de formato, proporciona estadísticas básicas y estima el conteo de tokens para los costos de ajuste fino. Ver: [Fine-tuning method for gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                 |
| **OpenAI Cookbook**: [Fine-Tuning for Retrieval Augmented Generation (RAG) with Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst)      | El objetivo de este notebook es guiarte a través de un ejemplo comprensivo de cómo ajustar finamente modelos de OpenAI para la Generación Aumentada por Recuperación (RAG). También integraremos Qdrant y el Aprendizaje con Pocos Ejemplos para mejorar el rendimiento del modelo y reducir fabricaciones.                                                                                                                                                                                                                         |
| **OpenAI Cookbook**: [Fine-tuning GPT with Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                                  | Weights & Biases (W&B) es la plataforma de desarrolladores de IA, con herramientas para entrenar modelos, ajustar finamente modelos y aprovechar modelos base. Lee su guía de [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai?WT.mc_id=academic-105485-koreyst) primero, luego prueba el ejercicio del Cookbook.                                                                                                                                                                                      |
| **Community Tutorial** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - ajuste fino para Modelos de Lenguaje Pequeños                                                | Conoce a [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), el nuevo modelo pequeño de Microsoft, notablemente poderoso pero compacto. Este tutorial te guiará a través del ajuste fino de Phi-2, demostrando cómo construir un conjunto de datos único y ajustar finamente el modelo usando QLoRA.                                                                                                                                                                      |
| **Hugging Face Tutorial** [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                                    | Esta publicación de blog te guía sobre cómo ajustar finamente LLMs abiertos usando Hugging Face TRL, Transformers y conjuntos de datos en 2024. Defines un caso de uso, configuras un entorno de desarrollo, preparas un conjunto de datos, ajustas finamente el modelo, lo pruebas y evalúas, y luego lo despliegas en producción.                                                                                                                                                                                            |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                                 | Trae un entrenamiento y despliegues más rápidos y fáciles de [modelos de aprendizaje automático de última generación](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). El repositorio tiene tutoriales amigables para Colab con orientación en video de YouTube, para ajuste fino. **Refleja la reciente actualización de [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)**. Lee la [documentación de AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst). |
|                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando servicios de traducción automatizada por IA. Aunque nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda la traducción profesional humana. No nos hacemos responsables de malentendidos o interpretaciones erróneas que surjan del uso de esta traducción.