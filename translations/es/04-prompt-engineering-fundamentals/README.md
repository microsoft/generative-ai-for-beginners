# Fundamentos de la Ingenier√≠a de Prompts

[![Fundamentos de la Ingenier√≠a de Prompts](../../../translated_images/04-lesson-banner.png?WT.d904d510033d5f0283f2caff5f735050f929dd196a1fc25fefa18433347fe463.es.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson4-gh?WT.mc_id=academic-105485-koreyst)

## Introducci√≥n
Este m√≥dulo cubre conceptos y t√©cnicas esenciales para crear prompts efectivos en modelos de IA generativa. La forma en que escribes tu prompt a un LLM tambi√©n importa. Un prompt cuidadosamente elaborado puede lograr una mejor calidad de respuesta. Pero, ¬øqu√© significan exactamente t√©rminos como _prompt_ e _ingenier√≠a de prompts_? ¬øY c√≥mo puedo mejorar el _input_ del prompt que env√≠o al LLM? Estas son las preguntas que intentaremos responder en este cap√≠tulo y el siguiente.

La _IA generativa_ es capaz de crear nuevo contenido (por ejemplo, texto, im√°genes, audio, c√≥digo, etc.) en respuesta a solicitudes de los usuarios. Lo logra utilizando _Modelos de Lenguaje Extensos_ como la serie GPT ("Generative Pre-trained Transformer") de OpenAI, que est√°n entrenados para usar lenguaje natural y c√≥digo.

Los usuarios ahora pueden interactuar con estos modelos usando paradigmas familiares como el chat, sin necesidad de conocimientos t√©cnicos o capacitaci√≥n. Los modelos est√°n basados en _prompts_ - los usuarios env√≠an una entrada de texto (prompt) y reciben la respuesta de la IA (completaci√≥n). Luego pueden "chatear con la IA" de manera iterativa, en conversaciones de m√∫ltiples turnos, refinando su prompt hasta que la respuesta cumpla con sus expectativas.

Los "prompts" ahora se convierten en la principal _interfaz de programaci√≥n_ para aplicaciones de IA generativa, indicando a los modelos qu√© hacer e influyendo en la calidad de las respuestas devueltas. La "Ingenier√≠a de Prompts" es un campo de estudio de r√°pido crecimiento que se centra en el _dise√±o y optimizaci√≥n_ de prompts para ofrecer respuestas consistentes y de calidad a gran escala.

## Objetivos de Aprendizaje

En esta lecci√≥n, aprenderemos qu√© es la Ingenier√≠a de Prompts, por qu√© es importante y c√≥mo podemos crear prompts m√°s efectivos para un modelo y objetivo de aplicaci√≥n dados. Entenderemos conceptos b√°sicos y mejores pr√°cticas para la ingenier√≠a de prompts, y aprenderemos sobre un entorno interactivo de "sandbox" en Jupyter Notebooks donde podemos ver estos conceptos aplicados a ejemplos reales.

Al final de esta lecci√≥n, seremos capaces de:

1. Explicar qu√© es la ingenier√≠a de prompts y por qu√© es importante.
2. Describir los componentes de un prompt y c√≥mo se utilizan.
3. Aprender mejores pr√°cticas y t√©cnicas para la ingenier√≠a de prompts.
4. Aplicar las t√©cnicas aprendidas a ejemplos reales, utilizando un endpoint de OpenAI.

## T√©rminos Clave

Ingenier√≠a de Prompts: La pr√°ctica de dise√±ar y refinar entradas para guiar a los modelos de IA hacia la producci√≥n de salidas deseadas.
Tokenizaci√≥n: El proceso de convertir texto en unidades m√°s peque√±as, llamadas tokens, que un modelo puede entender y procesar.
LLMs Ajustados por Instrucci√≥n: Modelos de Lenguaje Extensos (LLMs) que han sido ajustados con instrucciones espec√≠ficas para mejorar la precisi√≥n y relevancia de sus respuestas.

## Sandbox de Aprendizaje

La ingenier√≠a de prompts es actualmente m√°s arte que ciencia. La mejor manera de mejorar nuestra intuici√≥n al respecto es _practicar m√°s_ y adoptar un enfoque de prueba y error que combine la experiencia en el dominio de la aplicaci√≥n con t√©cnicas recomendadas y optimizaciones espec√≠ficas del modelo.

El Jupyter Notebook que acompa√±a esta lecci√≥n proporciona un entorno de _sandbox_ donde puedes probar lo que aprendes, ya sea mientras avanzas o como parte del desaf√≠o de c√≥digo al final. Para ejecutar los ejercicios, necesitar√°s:

1. **Una clave API de Azure OpenAI** - el endpoint del servicio para un LLM desplegado.
2. **Un Entorno de Ejecuci√≥n de Python** - en el cual se pueda ejecutar el Notebook.
3. **Variables de Entorno Local** - _completa los pasos de [SETUP](./../00-course-setup/SETUP.md?WT.mc_id=academic-105485-koreyst) ahora para prepararte_.

El notebook viene con ejercicios _iniciales_ - pero se te anima a agregar tus propias secciones de _Markdown_ (descripci√≥n) y _C√≥digo_ (solicitudes de prompts) para probar m√°s ejemplos o ideas - y construir tu intuici√≥n para el dise√±o de prompts.

## Gu√≠a Ilustrada

¬øQuieres obtener una visi√≥n general de lo que cubre esta lecci√≥n antes de profundizar? Echa un vistazo a esta gu√≠a ilustrada, que te da una idea de los temas principales cubiertos y los puntos clave para que reflexiones en cada uno. El mapa de la lecci√≥n te lleva desde la comprensi√≥n de los conceptos y desaf√≠os b√°sicos hasta abordarlos con t√©cnicas de ingenier√≠a de prompts relevantes y mejores pr√°cticas. Ten en cuenta que la secci√≥n de "T√©cnicas Avanzadas" en esta gu√≠a se refiere a contenido cubierto en el _pr√≥ximo_ cap√≠tulo de este curr√≠culo.

![Gu√≠a Ilustrada de Ingenier√≠a de Prompts](../../../translated_images/04-prompt-engineering-sketchnote.png?WT.a936f69bc33c7a783015f6747ea56d0f0071349644cd9031f9b8d20a3eec8696.es.mc_id=academic-105485-koreyst)

## Nuestra Startup

Ahora, hablemos sobre c√≥mo _este tema_ se relaciona con nuestra misi√≥n de startup para [llevar la innovaci√≥n de IA a la educaci√≥n](https://educationblog.microsoft.com/2023/06/collaborating-to-bring-ai-innovation-to-education?WT.mc_id=academic-105485-koreyst). Queremos construir aplicaciones de aprendizaje personalizado impulsadas por IA, as√≠ que pensemos en c√≥mo los diferentes usuarios de nuestra aplicaci√≥n podr√≠an "dise√±ar" prompts:

- **Administradores** podr√≠an pedir a la IA que _analice datos del curr√≠culo para identificar brechas en la cobertura_. La IA puede resumir resultados o visualizarlos con c√≥digo.
- **Educadores** podr√≠an pedir a la IA que _genere un plan de lecci√≥n para una audiencia y tema espec√≠ficos_. La IA puede construir el plan personalizado en un formato especificado.
- **Estudiantes** podr√≠an pedir a la IA que _les ense√±e una materia dif√≠cil_. La IA ahora puede guiar a los estudiantes con lecciones, pistas y ejemplos adaptados a su nivel.

Eso es solo la punta del iceberg. Echa un vistazo a [Prompts Para Educaci√≥n](https://github.com/microsoft/prompts-for-edu/tree/main?WT.mc_id=academic-105485-koreyst) - una biblioteca de prompts de c√≥digo abierto curada por expertos en educaci√≥n - para obtener una visi√≥n m√°s amplia de las posibilidades. _¬°Intenta ejecutar algunos de esos prompts en el sandbox o utilizando el OpenAI Playground para ver qu√© sucede!_

<!--
LESSON TEMPLATE:
This unit should cover core concept #1.
Reinforce the concept with examples and references.

CONCEPT #1:
Prompt Engineering.
Define it and explain why it is needed.
-->

## ¬øQu√© es la Ingenier√≠a de Prompts?

Comenzamos esta lecci√≥n definiendo la **Ingenier√≠a de Prompts** como el proceso de _dise√±ar y optimizar_ entradas de texto (prompts) para ofrecer respuestas consistentes y de calidad (completaciones) para un objetivo de aplicaci√≥n y modelo dados. Podemos pensar en esto como un proceso de 2 pasos:

- _dise√±ar_ el prompt inicial para un modelo y objetivo dados
- _refinar_ el prompt de manera iterativa para mejorar la calidad de la respuesta

Este es necesariamente un proceso de prueba y error que requiere intuici√≥n y esfuerzo del usuario para obtener resultados √≥ptimos. Entonces, ¬øpor qu√© es importante? Para responder a esa pregunta, primero necesitamos entender tres conceptos:

- _Tokenizaci√≥n_ = c√≥mo el modelo "ve" el prompt
- _LLMs Base_ = c√≥mo el modelo base "procesa" un prompt
- _LLMs Ajustados por Instrucci√≥n_ = c√≥mo el modelo ahora puede ver "tareas"

### Tokenizaci√≥n

Un LLM ve los prompts como una _secuencia de tokens_ donde diferentes modelos (o versiones de un modelo) pueden tokenizar el mismo prompt de diferentes maneras. Dado que los LLMs est√°n entrenados en tokens (y no en texto en bruto), la forma en que los prompts se tokenizan tiene un impacto directo en la calidad de la respuesta generada.

Para obtener una intuici√≥n de c√≥mo funciona la tokenizaci√≥n, prueba herramientas como el [OpenAI Tokenizer](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst) que se muestra a continuaci√≥n. Copia tu prompt - y observa c√≥mo se convierte en tokens, prestando atenci√≥n a c√≥mo se manejan los caracteres de espacio en blanco y los signos de puntuaci√≥n. Ten en cuenta que este ejemplo muestra un LLM m√°s antiguo (GPT-3) - por lo que probar esto con un modelo m√°s nuevo puede producir un resultado diferente.

![Tokenizaci√≥n](../../../translated_images/04-tokenizer-example.png?WT.f5399316da400747ffe3af9c95e61dc1a85508d57378da23a77538270c4cabf1.es.mc_id=academic-105485-koreyst)

### Concepto: Modelos Base

Una vez que un prompt est√° tokenizado, la funci√≥n principal del ["LLM Base"](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) (o modelo base) es predecir el token en esa secuencia. Dado que los LLMs est√°n entrenados en conjuntos de datos de texto masivos, tienen una buena noci√≥n de las relaciones estad√≠sticas entre los tokens y pueden hacer esa predicci√≥n con cierta confianza. Ten en cuenta que no entienden el _significado_ de las palabras en el prompt o token; solo ven un patr√≥n que pueden "completar" con su pr√≥xima predicci√≥n. Pueden continuar prediciendo la secuencia hasta que el usuario intervenga o se alcance una condici√≥n preestablecida.

¬øQuieres ver c√≥mo funciona la completaci√≥n basada en prompts? Ingresa el prompt anterior en el [_Chat Playground_](https://oai.azure.com/playground?WT.mc_id=academic-105485-koreyst) de Azure OpenAI Studio con la configuraci√≥n predeterminada. El sistema est√° configurado para tratar los prompts como solicitudes de informaci√≥n, por lo que deber√≠as ver una completaci√≥n que satisfaga este contexto.

Pero, ¬øqu√© pasa si el usuario quiere ver algo espec√≠fico que cumpla con algunos criterios u objetivo de tarea? Aqu√≠ es donde entran en juego los LLMs _ajustados por instrucci√≥n_.

![Completaci√≥n de Chat de LLM Base](../../../translated_images/04-playground-chat-base.png?WT.7645a03d7989b1c410f2e9e6b503d18e4624f82d9cbf108dac999b8c8988f0ad.es.mc_id=academic-105485-koreyst)

### Concepto: LLMs Ajustados por Instrucci√≥n

Un [LLM Ajustado por Instrucci√≥n](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) comienza con el modelo base y lo ajusta con ejemplos o pares de entrada/salida (por ejemplo, "mensajes" de m√∫ltiples turnos) que pueden contener instrucciones claras, y la respuesta de la IA intenta seguir esa instrucci√≥n.

Esto utiliza t√©cnicas como el Aprendizaje por Refuerzo con Retroalimentaci√≥n Humana (RLHF) que pueden entrenar al modelo para _seguir instrucciones_ y _aprender de la retroalimentaci√≥n_ de modo que produzca respuestas que est√©n mejor adaptadas a aplicaciones pr√°cticas y sean m√°s relevantes para los objetivos del usuario.

Vamos a probarlo - vuelve al prompt anterior, pero ahora cambia el _mensaje del sistema_ para proporcionar la siguiente instrucci√≥n como contexto:

> _Resume el contenido que se te proporcione para un estudiante de segundo grado. Mant√©n el resultado en un p√°rrafo con 3-5 vi√±etas._

¬øVes c√≥mo el resultado ahora est√° ajustado para reflejar el objetivo y formato deseados? Un educador ahora puede usar directamente esta respuesta en sus diapositivas para esa clase.

![Completaci√≥n de Chat de LLM Ajustado por Instrucci√≥n](../../../translated_images/04-playground-chat-instructions.png?WT.d9c80b15e90815a83ce665bf4418e70205d30318a5a5bcf407b2c92769743593.es.mc_id=academic-105485-koreyst)

## ¬øPor qu√© necesitamos la Ingenier√≠a de Prompts?

Ahora que sabemos c√≥mo los LLMs procesan los prompts, hablemos de _por qu√©_ necesitamos la ingenier√≠a de prompts. La respuesta radica en el hecho de que los LLMs actuales presentan una serie de desaf√≠os que hacen que lograr _completaciones confiables y consistentes_ sea m√°s dif√≠cil sin dedicar esfuerzo a la construcci√≥n y optimizaci√≥n de prompts. Por ejemplo:

1. **Las respuestas del modelo son estoc√°sticas.** Es probable que el _mismo prompt_ produzca diferentes respuestas con diferentes modelos o versiones de modelos. Y puede incluso producir diferentes resultados con el _mismo modelo_ en diferentes momentos. _Las t√©cnicas de ingenier√≠a de prompts pueden ayudarnos a minimizar estas variaciones proporcionando mejores l√≠mites_.

1. **Los modelos pueden fabricar respuestas.** Los modelos est√°n preentrenados con _conjuntos de datos grandes pero finitos_, lo que significa que carecen de conocimiento sobre conceptos fuera de ese √°mbito de entrenamiento. Como resultado, pueden producir completaciones que son inexactas, imaginarias o directamente contradictorias a hechos conocidos. _Las t√©cnicas de ingenier√≠a de prompts ayudan a los usuarios a identificar y mitigar tales fabricaciones, por ejemplo, pidiendo a la IA citas o razonamientos_.

1. **Las capacidades de los modelos variar√°n.** Los modelos m√°s nuevos o generaciones de modelos tendr√°n capacidades m√°s ricas, pero tambi√©n traer√°n peculiaridades √∫nicas y compensaciones en costo y complejidad. _La ingenier√≠a de prompts puede ayudarnos a desarrollar mejores pr√°cticas y flujos de trabajo que abstraigan las diferencias y se adapten a los requisitos espec√≠ficos del modelo de manera escalable y sin problemas_.

Veamos esto en acci√≥n en el OpenAI o Azure OpenAI Playground:

- Usa el mismo prompt con diferentes implementaciones de LLM (por ejemplo, OpenAI, Azure OpenAI, Hugging Face) - ¬øviste las variaciones?
- Usa el mismo prompt repetidamente con la _misma_ implementaci√≥n de LLM (por ejemplo, Azure OpenAI playground) - ¬øc√≥mo difer√≠an estas variaciones?

### Ejemplo de Fabricaciones

En este curso, usamos el t√©rmino **"fabricaci√≥n"** para referirnos al fen√≥meno en el que los LLMs a veces generan informaci√≥n incorrecta debido a limitaciones en su entrenamiento u otras restricciones. Tambi√©n puedes haber o√≠do esto referido como _"alucinaciones"_ en art√≠culos populares o trabajos de investigaci√≥n. Sin embargo, recomendamos encarecidamente usar _"fabricaci√≥n"_ como t√©rmino para no antropomorfizar accidentalmente el comportamiento al atribuir un rasgo humano a un resultado impulsado por una m√°quina. Esto tambi√©n refuerza las [directrices de IA Responsable](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-105485-koreyst) desde una perspectiva de terminolog√≠a, eliminando t√©rminos que tambi√©n pueden considerarse ofensivos o no inclusivos en algunos contextos.

¬øQuieres tener una idea de c√≥mo funcionan las fabricaciones? Piensa en un prompt que instruya a la IA a generar contenido para un tema inexistente (para asegurarte de que no se encuentre en el conjunto de datos de entrenamiento). Por ejemplo, intent√© este prompt:

> **Prompt:** genera un plan de lecci√≥n sobre la Guerra Marciana de 2076.

Una b√∫squeda en la web me mostr√≥ que hab√≠a relatos ficticios (por ejemplo, series de televisi√≥n o libros) sobre guerras marcianas, pero ninguno en 2076. El sentido com√∫n tambi√©n nos dice que 2076 est√° _en el futuro_ y, por lo tanto, no puede asociarse con un evento real.

Entonces, ¬øqu√© sucede cuando ejecutamos este prompt con diferentes proveedores de LLM?

> **Respuesta 1**: OpenAI Playground (GPT-35)

![Respuesta 1](../../../translated_images/04-fabrication-oai.png?WT.08cc3e01259a6b46725a800e67de50c37b7fdd6b1f932f027881cbe32f80bcf1.es.mc_id=academic-105485-koreyst)

> **Respuesta 2**: Azure OpenAI Playground (GPT-35)

![Respuesta 2](../../../translated_images/04-fabrication-aoai.png?WT.81e0d286a351c87c804aaca6e5f8251a6deed5105d11bca035f8cead8c52d299.es.mc_id=academic-105485-koreyst)

> **Respuesta 3**: : Hugging Face Chat Playground (LLama-2)

![Respuesta 3](../../../translated_images/04-fabrication-huggingchat.png?WT.992b3a675cc7ed0dbe53e308b93df8165048fb7c4516e6bb00611d05c92e8fd5.es.mc_id=academic-105485-koreyst)

Como era de esperar, cada modelo (o versi√≥n de modelo) produce respuestas ligeramente diferentes gracias al comportamiento estoc√°stico y las variaciones en las capacidades del modelo. Por ejemplo, un modelo se dirige a una audiencia de octavo grado, mientras que el otro asume un estudiante de secundaria. Pero los tres modelos generaron respuestas que podr√≠an convencer a un usuario desinformado de que el evento era real.

Las t√©cnicas de ingenier√≠a de prompts como _metaprompting_ y _configuraci√≥n de temperatura_ pueden reducir las fabricaciones del modelo hasta cierto punto. Nuevas _arquitecturas_ de ingenier√≠a de prompts tambi√©n incorporan nuevas herramientas y t√©cnicas de manera fluida en el flujo de prompts, para mitigar o reducir algunos de estos efectos.

## Estudio de Caso: GitHub Copilot

Concluyamos esta secci√≥n obteniendo una idea de c√≥mo se utiliza la ingenier√≠a de prompts en soluciones del mundo real al observar un Estudio de Caso: [GitHub Copilot](https://github.com/features/copilot?WT.mc_id=academic-105485-koreyst).

GitHub Copilot es tu "Programador de Pareja de IA" - convierte prompts de texto en completaciones de c√≥digo y est√° integrado en tu entorno de desarrollo (por ejemplo, Visual Studio Code) para una experiencia de usuario fluida. Como se documenta en la serie de blogs a continuaci√≥n, la versi√≥n m√°s temprana se bas√≥ en el modelo OpenAI Codex, y los ingenieros r√°pidamente se dieron cuenta de la necesidad de ajustar el modelo y desarrollar mejores t√©cnicas de ingenier√≠a de prompts para mejorar la calidad del c√≥digo. En julio, [debutaron un modelo de IA mejorado que va m√°s all√° de Codex](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/?WT.mc_id=academic-105485-koreyst) para sugerencias a√∫n m√°s r√°pidas.

Lee las publicaciones en orden, para seguir su viaje de aprendizaje.

- **Mayo 2023** | [GitHub Copilot est√° Mejorando en Entender tu C√≥digo](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/?WT.mc_id=academic-105485-koreyst)
- **Mayo 2023** | [Dentro de GitHub: Trabajando con los LLMs detr√°s de GitHub Copilot](https://github.blog/2023-05-17-inside-github-working-with-the-llms-behind-github-copilot/?WT.mc_id=academic-105485-koreyst).
- **Junio 2023** | [C√≥mo escribir mejores prompts para GitHub Copilot](https://github.blog/2023-06-20-how-to-write-better-prompts-for-github-copilot/?WT.mc_id=academic-105485-koreyst).
- **Julio 2023** | [..
Finalmente, el verdadero valor de las plantillas radica en la capacidad de crear y publicar _bibliotecas de prompts_ para dominios de aplicaciones verticales, donde la plantilla de prompt ahora est√° _optimizada_ para reflejar un contexto o ejemplos espec√≠ficos de la aplicaci√≥n que hacen que las respuestas sean m√°s relevantes y precisas para la audiencia de usuarios objetivo. El repositorio [Prompts For Edu](https://github.com/microsoft/prompts-for-edu?WT.mc_id=academic-105485-koreyst) es un gran ejemplo de este enfoque, curando una biblioteca de prompts para el dominio de la educaci√≥n con √©nfasis en objetivos clave como la planificaci√≥n de lecciones, el dise√±o curricular, la tutor√≠a de estudiantes, etc.

## Contenido de Apoyo

Si pensamos en la construcci√≥n de prompts como tener una instrucci√≥n (tarea) y un objetivo (contenido principal), entonces el _contenido secundario_ es como el contexto adicional que proporcionamos para **influenciar la salida de alguna manera**. Podr√≠a ser la sintonizaci√≥n de par√°metros, instrucciones de formato, taxonom√≠as de temas, etc., que pueden ayudar al modelo a _adaptar_ su respuesta para adecuarse a los objetivos o expectativas deseadas por el usuario.

Por ejemplo: Dado un cat√°logo de cursos con metadatos extensos (nombre, descripci√≥n, nivel, etiquetas de metadatos, instructor, etc.) sobre todos los cursos disponibles en el curr√≠culo:

- podemos definir una instrucci√≥n para "resumir el cat√°logo de cursos para el oto√±o de 2023"
- podemos usar el contenido principal para proporcionar algunos ejemplos del resultado deseado
- podemos usar el contenido secundario para identificar las 5 principales "etiquetas" de inter√©s.

Ahora, el modelo puede proporcionar un resumen en el formato mostrado por los pocos ejemplos, pero si un resultado tiene m√∫ltiples etiquetas, puede priorizar las 5 etiquetas identificadas en el contenido secundario.

---

<!--
PLANTILLA DE LECCI√ìN:
Esta unidad debe cubrir el concepto central #1.
Reforzar el concepto con ejemplos y referencias.

CONCEPTO #3:
T√©cnicas de Ingenier√≠a de Prompts.
¬øCu√°les son algunas t√©cnicas b√°sicas para la ingenier√≠a de prompts?
Il√∫stralo con algunos ejercicios.
-->

## Mejores Pr√°cticas de Prompts

Ahora que sabemos c√≥mo se pueden _construir_ los prompts, podemos comenzar a pensar en c√≥mo _dise√±arlos_ para reflejar las mejores pr√°cticas. Podemos pensar en esto en dos partes: tener la _mentalidad_ correcta y aplicar las _t√©cnicas_ adecuadas.

### Mentalidad de Ingenier√≠a de Prompts

La Ingenier√≠a de Prompts es un proceso de prueba y error, as√≠ que ten en cuenta tres factores amplios que gu√≠an:

1. **La Comprensi√≥n del Dominio Importa.** La precisi√≥n y relevancia de la respuesta es una funci√≥n del _dominio_ en el que opera esa aplicaci√≥n o usuario. Aplica tu intuici√≥n y experiencia en el dominio para **personalizar las t√©cnicas** a√∫n m√°s. Por ejemplo, define _personalidades espec√≠ficas del dominio_ en tus prompts del sistema, o usa _plantillas espec√≠ficas del dominio_ en tus prompts de usuario. Proporciona contenido secundario que refleje contextos espec√≠ficos del dominio, o utiliza _pistas y ejemplos espec√≠ficos del dominio_ para guiar al modelo hacia patrones de uso familiares.

2. **La Comprensi√≥n del Modelo Importa.** Sabemos que los modelos son estoc√°sticos por naturaleza. Pero las implementaciones de modelos tambi√©n pueden variar en t√©rminos del conjunto de datos de entrenamiento que utilizan (conocimiento preentrenado), las capacidades que proporcionan (por ejemplo, a trav√©s de API o SDK) y el tipo de contenido para el que est√°n optimizados (por ejemplo, c√≥digo vs. im√°genes vs. texto). Comprende las fortalezas y limitaciones del modelo que est√°s utilizando y utiliza ese conocimiento para _priorizar tareas_ o construir _plantillas personalizadas_ que est√©n optimizadas para las capacidades del modelo.

3. **La Iteraci√≥n y Validaci√≥n Importa.** Los modelos est√°n evolucionando r√°pidamente, al igual que las t√©cnicas para la ingenier√≠a de prompts. Como experto en el dominio, es posible que tengas otro contexto o criterios para _tu_ aplicaci√≥n espec√≠fica, que puede no aplicarse a la comunidad en general. Usa herramientas y t√©cnicas de ingenier√≠a de prompts para "dar un salto inicial" en la construcci√≥n de prompts, luego itera y valida los resultados utilizando tu propia intuici√≥n y experiencia en el dominio. Registra tus conocimientos y crea una **base de conocimientos** (por ejemplo, bibliotecas de prompts) que puedan ser utilizadas como una nueva l√≠nea base por otros, para iteraciones m√°s r√°pidas en el futuro.

## Mejores Pr√°cticas

Ahora veamos las pr√°cticas comunes recomendadas por los profesionales de [OpenAI](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api?WT.mc_id=academic-105485-koreyst) y [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering#best-practices?WT.mc_id=academic-105485-koreyst).

| Qu√©                               | Por qu√©                                                                                                                                                                                                                                             |
| :-------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Evaluar los modelos m√°s recientes. | Las nuevas generaciones de modelos probablemente tengan caracter√≠sticas y calidad mejoradas, pero tambi√©n pueden tener costos m√°s altos. Eval√∫alos por su impacto, luego toma decisiones de migraci√≥n.                                               |
| Separar instrucciones y contexto  | Verifica si tu modelo/proveedor define _delimitadores_ para distinguir instrucciones, contenido principal y secundario de manera m√°s clara. Esto puede ayudar a los modelos a asignar pesos m√°s precisos a los tokens.                               |
| Ser espec√≠fico y claro            | Da m√°s detalles sobre el contexto deseado, resultado, longitud, formato, estilo, etc. Esto mejorar√° tanto la calidad como la consistencia de las respuestas. Captura recetas en plantillas reutilizables.                                            |
| Ser descriptivo, usar ejemplos    | Los modelos pueden responder mejor a un enfoque de "mostrar y contar". Comienza con un `zero-shot` approach where you give it an instruction (but no examples) then try `few-shot` as a refinement, providing a few examples of the desired output. Use analogies. |
| Use cues to jumpstart completions | Nudge it towards a desired outcome by giving it some leading words or phrases that it can use as a starting point for the response.                                                                                                               |
| Double Down                       | Sometimes you may need to repeat yourself to the model. Give instructions before and after your primary content, use an instruction and a cue, etc. Iterate & validate to see what works.                                                         |
| Order Matters                     | The order in which you present information to the model may impact the output, even in the learning examples, thanks to recency bias. Try different options to see what works best.                                                               |
| Give the model an ‚Äúout‚Äù           | Give the model a _fallback_ completion response it can provide if it cannot complete the task for any reason. This can reduce chances of models generating false or fabricated responses.                                                         |
|                                   |                                                                                                                                                                                                                                                   |

As with any best practice, remember that _your mileage may vary_ based on the model, the task and the domain. Use these as a starting point, and iterate to find what works best for you. Constantly re-evaluate your prompt engineering process as new models and tools become available, with a focus on process scalability and response quality.

<!--
LESSON TEMPLATE:
This unit should provide a code challenge if applicable

CHALLENGE:
Link to a Jupyter Notebook with only the code comments in the instructions (code sections are empty).

SOLUTION:
Link to a copy of that Notebook with the prompts filled in and run, showing what one example could be.
-->

## Assignment

Congratulations! You made it to the end of the lesson! It's time to put some of those concepts and techniques to the test with real examples!

For our assignment, we'll be using a Jupyter Notebook with exercises you can complete interactively. You can also extend the Notebook with your own Markdown and Code cells to explore ideas and techniques on your own.

### To get started, fork the repo, then

- (Recommended) Launch GitHub Codespaces
- (Alternatively) Clone the repo to your local device and use it with Docker Desktop
- (Alternatively) Open the Notebook with your preferred Notebook runtime environment.

### Next, configure your environment variables

- Copy the `.env.copy` file in repo root to `.env` and fill in the `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_DEPLOYMENT` valores. Regresa a la [secci√≥n de Sandbox de Aprendizaje](../../../04-prompt-engineering-fundamentals/04-prompt-engineering-fundamentals) para aprender c√≥mo.

### A continuaci√≥n, abre el Jupyter Notebook

- Selecciona el n√∫cleo de tiempo de ejecuci√≥n. Si usas las opciones 1 o 2, simplemente selecciona el n√∫cleo predeterminado de Python 3.10.x proporcionado por el contenedor de desarrollo.

Est√°s listo para ejecutar los ejercicios. Ten en cuenta que aqu√≠ no hay respuestas _correctas o incorrectas_, solo se exploran opciones mediante prueba y error y se construye intuici√≥n sobre lo que funciona para un modelo y dominio de aplicaci√≥n dados.

_Por esta raz√≥n, no hay segmentos de Soluci√≥n de C√≥digo en esta lecci√≥n. En su lugar, el Notebook tendr√° celdas de Markdown tituladas "Mi Soluci√≥n:" que muestran un ejemplo de salida para referencia._

<!--
PLANTILLA DE LECCI√ìN:
Cierra la secci√≥n con un resumen y recursos para el aprendizaje autodirigido.
-->

## Verificaci√≥n de Conocimiento

¬øCu√°l de los siguientes es un buen prompt siguiendo algunas pr√°cticas razonables?

1. Mu√©strame una imagen de un coche rojo
2. Mu√©strame una imagen de un coche rojo de la marca Volvo y modelo XC90 estacionado junto a un acantilado con el sol poni√©ndose
3. Mu√©strame una imagen de un coche rojo de la marca Volvo y modelo XC90

A: 2, es el mejor prompt ya que proporciona detalles sobre "qu√©" y entra en especificaciones (no solo cualquier coche, sino una marca y modelo espec√≠ficos) y tambi√©n describe el entorno general. 3 es el siguiente mejor ya que tambi√©n contiene mucha descripci√≥n.

## üöÄ Desaf√≠o

Ve si puedes aprovechar la t√©cnica de "pista" con el prompt: Completa la oraci√≥n "Mu√©strame una imagen de un coche rojo de la marca Volvo y ". ¬øCon qu√© responde, y c√≥mo lo mejorar√≠as?

## ¬°Gran Trabajo! Contin√∫a Tu Aprendizaje

¬øQuieres aprender m√°s sobre diferentes conceptos de Ingenier√≠a de Prompts? Ve a la [p√°gina de aprendizaje continuo](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para encontrar otros recursos excelentes sobre este tema.

Dir√≠gete a la Lecci√≥n 5 donde veremos [t√©cnicas avanzadas de prompts](../05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst)!

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando servicios de traducci√≥n autom√°tica basados en inteligencia artificial. Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda la traducci√≥n profesional humana. No nos hacemos responsables de ning√∫n malentendido o interpretaci√≥n err√≥nea que surja del uso de esta traducci√≥n.