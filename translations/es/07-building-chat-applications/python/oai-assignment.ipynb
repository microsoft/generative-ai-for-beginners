{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Capítulo 7: Creando aplicaciones de chat\n",
    "## Guía rápida de la API de OpenAI\n",
    "\n",
    "Este cuaderno está adaptado del [Repositorio de ejemplos de Azure OpenAI](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst), que incluye cuadernos que acceden a los servicios de [Azure OpenAI](notebook-azure-openai.ipynb).\n",
    "\n",
    "La API de OpenAI para Python también funciona con los modelos de Azure OpenAI, con algunas modificaciones. Puedes obtener más información sobre las diferencias aquí: [Cómo cambiar entre los endpoints de OpenAI y Azure OpenAI con Python](https://learn.microsoft.com/azure/ai-services/openai/how-to/switching-endpoints?WT.mc_id=academic-109527-jasmineg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Descripción general  \n",
    "\"Los modelos de lenguaje grandes son funciones que asignan texto a texto. Dado un texto de entrada, un modelo de lenguaje grande intenta predecir el texto que seguirá\"(1). Este cuaderno de inicio rápido presentará a los usuarios conceptos generales sobre los LLM, los requisitos principales de los paquetes para comenzar con AML, una introducción sencilla al diseño de prompts y varios ejemplos breves de diferentes casos de uso.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Tabla de Contenidos  \n",
    "\n",
    "[Descripción general](../../../../07-building-chat-applications/python)  \n",
    "[Cómo usar el servicio de OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[1. Crear tu servicio de OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[2. Instalación](../../../../07-building-chat-applications/python)    \n",
    "[3. Credenciales](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Casos de uso](../../../../07-building-chat-applications/python)    \n",
    "[1. Resumir texto](../../../../07-building-chat-applications/python)  \n",
    "[2. Clasificar texto](../../../../07-building-chat-applications/python)  \n",
    "[3. Generar nuevos nombres de productos](../../../../07-building-chat-applications/python)  \n",
    "[4. Ajustar un clasificador](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Referencias](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Crea tu primer prompt  \n",
    "Este breve ejercicio te dará una introducción básica para enviar prompts a un modelo de OpenAI para una tarea sencilla: \"resumir\".\n",
    "\n",
    "\n",
    "**Pasos**:  \n",
    "1. Instala la biblioteca de OpenAI en tu entorno de Python  \n",
    "2. Carga las bibliotecas auxiliares estándar y configura tus credenciales de seguridad habituales para el servicio de OpenAI que hayas creado  \n",
    "3. Elige un modelo para tu tarea  \n",
    "4. Crea un prompt sencillo para el modelo  \n",
    "5. ¡Envía tu solicitud a la API del modelo!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\")\n",
    "assert API_KEY, \"ERROR: OpenAI Key is missing\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Encontrar el modelo adecuado  \n",
    "Los modelos GPT-3.5-turbo o GPT-4 pueden comprender y generar lenguaje natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Diseño de Prompts  \n",
    "\n",
    "\"La magia de los grandes modelos de lenguaje es que, al ser entrenados para minimizar este error de predicción sobre enormes cantidades de texto, los modelos terminan aprendiendo conceptos útiles para estas predicciones. Por ejemplo, aprenden conceptos como\"(1):\n",
    "\n",
    "* cómo se escribe correctamente\n",
    "* cómo funciona la gramática\n",
    "* cómo parafrasear\n",
    "* cómo responder preguntas\n",
    "* cómo mantener una conversación\n",
    "* cómo escribir en muchos idiomas\n",
    "* cómo programar\n",
    "* etc.\n",
    "\n",
    "#### Cómo controlar un gran modelo de lenguaje  \n",
    "\"De todos los insumos de un gran modelo de lenguaje, por mucho el más influyente es el prompt de texto(1).\n",
    "\n",
    "Se puede guiar a los grandes modelos de lenguaje para que generen resultados de varias maneras:\n",
    "\n",
    "Instrucción: Dile al modelo lo que quieres\n",
    "Completado: Haz que el modelo complete el inicio de lo que deseas\n",
    "Demostración: Muéstrale al modelo lo que quieres, ya sea con:\n",
    "Algunos ejemplos en el prompt\n",
    "Cientos o miles de ejemplos en un conjunto de datos de entrenamiento para ajuste fino\"\n",
    "\n",
    "\n",
    "\n",
    "#### Hay tres pautas básicas para crear prompts:\n",
    "\n",
    "**Muestra y explica**. Deja claro lo que quieres, ya sea a través de instrucciones, ejemplos o una combinación de ambos. Si quieres que el modelo ordene una lista de elementos alfabéticamente o que clasifique un párrafo según su sentimiento, muéstrale que eso es lo que buscas.\n",
    "\n",
    "**Proporciona datos de calidad**. Si intentas crear un clasificador o que el modelo siga un patrón, asegúrate de que haya suficientes ejemplos. Revisa bien tus ejemplos: el modelo suele ser lo suficientemente inteligente como para detectar errores ortográficos básicos y darte una respuesta, pero también podría asumir que es intencional y eso puede afectar la respuesta.\n",
    "\n",
    "**Revisa tu configuración.** Los parámetros temperature y top_p controlan cuán determinista es el modelo al generar una respuesta. Si le pides una respuesta donde solo hay una opción correcta, deberías poner estos valores bajos. Si buscas respuestas más variadas, podrías configurarlos más altos. El error más común con estos parámetros es asumir que controlan la \"inteligencia\" o \"creatividad\" del modelo.\n",
    "\n",
    "\n",
    "Fuente: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Resumir texto  \n",
    "#### Desafío  \n",
    "Resume un texto añadiendo un 'tl;dr:' al final de un pasaje. Observa cómo el modelo entiende cómo realizar varias tareas sin instrucciones adicionales. Puedes probar con indicaciones más descriptivas que tl;dr para modificar el comportamiento del modelo y personalizar el resumen que recibes(3).  \n",
    "\n",
    "Trabajos recientes han demostrado mejoras significativas en muchas tareas y pruebas de PLN al preentrenar con un gran corpus de texto y luego ajustar el modelo para una tarea específica. Aunque normalmente la arquitectura es independiente de la tarea, este método aún requiere conjuntos de datos de ajuste fino con miles o decenas de miles de ejemplos. En cambio, los humanos suelen poder realizar una nueva tarea de lenguaje con solo unos pocos ejemplos o instrucciones simples, algo con lo que los sistemas actuales de PLN todavía tienen dificultades. Aquí mostramos que aumentar la escala de los modelos de lenguaje mejora notablemente el rendimiento independiente de la tarea y con pocos ejemplos, llegando en ocasiones a ser tan competitivos como los enfoques previos de ajuste fino más avanzados.  \n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Ejercicios para varios casos de uso  \n",
    "1. Resumir texto  \n",
    "2. Clasificar texto  \n",
    "3. Generar nuevos nombres de productos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Clasificar texto  \n",
    "#### Desafío  \n",
    "Clasifica elementos en categorías que se proporcionan en el momento de la inferencia. En el siguiente ejemplo, proporcionamos tanto las categorías como el texto a clasificar en el prompt (*playground_reference).\n",
    "\n",
    "Consulta del cliente: Hola, una de las teclas de mi teclado de portátil se rompió recientemente y necesito un reemplazo:\n",
    "\n",
    "Categoría clasificada:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Genera nuevos nombres de productos\n",
    "#### Desafío\n",
    "Crea nombres de productos a partir de palabras de ejemplo. Aquí incluimos en la indicación información sobre el producto para el que vamos a generar nombres. También proporcionamos un ejemplo similar para mostrar el patrón que queremos recibir. Además, hemos configurado el valor de temperatura alto para aumentar la aleatoriedad y obtener respuestas más innovadoras.\n",
    "\n",
    "Descripción del producto: Una máquina para hacer batidos en casa\n",
    "Palabras clave: rápido, saludable, compacto.\n",
    "Nombres de productos: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Descripción del producto: Un par de zapatos que se adapta a cualquier talla de pie.\n",
    "Palabras clave: adaptable, ajuste, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Referencias  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Ejemplos de OpenAI Studio](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Mejores prácticas para ajustar GPT-3 y clasificar texto](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Para más ayuda  \n",
    "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Colaboradores\n",
    "* Louis Li\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Descargo de responsabilidad**:  \nEste documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de ningún malentendido o interpretación errónea que surja del uso de esta traducción.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "1854bdde1dd56b366f1f6c9122f7d304",
   "translation_date": "2025-08-25T17:59:43+00:00",
   "source_file": "07-building-chat-applications/python/oai-assignment.ipynb",
   "language_code": "es"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}