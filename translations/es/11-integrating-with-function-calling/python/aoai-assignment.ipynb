{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En esta lección se cubrirá:\n",
    "- Qué es la llamada a funciones y en qué casos se utiliza\n",
    "- Cómo crear una llamada a función usando Azure OpenAI\n",
    "- Cómo integrar una llamada a función en una aplicación\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Después de completar esta lección, sabrás cómo y entenderás:\n",
    "\n",
    "- El propósito de utilizar la llamada a funciones\n",
    "- Configurar la llamada a funciones usando el servicio Azure OpenAI\n",
    "- Diseñar llamadas a funciones efectivas para el caso de uso de tu aplicación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entendiendo las llamadas a funciones\n",
    "\n",
    "En esta lección, queremos crear una función para nuestra startup educativa que permita a los usuarios usar un chatbot para encontrar cursos técnicos. Recomendaremos cursos que se ajusten a su nivel de habilidad, rol actual y tecnología de interés.\n",
    "\n",
    "Para lograr esto, usaremos una combinación de:\n",
    " - `Azure Open AI` para crear una experiencia de chat para el usuario\n",
    " - `Microsoft Learn Catalog API` para ayudar a los usuarios a encontrar cursos según su solicitud\n",
    " - `Function Calling` para tomar la consulta del usuario y enviarla a una función que realice la solicitud a la API.\n",
    "\n",
    "Para empezar, veamos por qué querríamos usar llamadas a funciones en primer lugar:\n",
    "\n",
    "print(\"Mensajes en la siguiente solicitud:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # obtener una nueva respuesta de GPT donde puede ver la respuesta de la función\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por qué usar Function Calling\n",
    "\n",
    "Si ya completaste alguna otra lección de este curso, probablemente ya entiendes el poder de usar Modelos de Lenguaje Grande (LLMs). Esperamos que también puedas notar algunas de sus limitaciones.\n",
    "\n",
    "Function Calling es una función del Azure Open AI Service que ayuda a superar las siguientes limitaciones:\n",
    "1) Formato de respuesta consistente\n",
    "2) Capacidad de usar datos de otras fuentes de una aplicación en un contexto de chat\n",
    "\n",
    "Antes de function calling, las respuestas de un LLM eran no estructuradas e inconsistentes. Los desarrolladores tenían que escribir código de validación complejo para poder manejar cada variación de una respuesta.\n",
    "\n",
    "Los usuarios no podían obtener respuestas como \"¿Cuál es el clima actual en Estocolmo?\". Esto se debe a que los modelos estaban limitados a la época en la que se entrenaron los datos.\n",
    "\n",
    "Veamos el siguiente ejemplo que ilustra este problema:\n",
    "\n",
    "Supongamos que queremos crear una base de datos de datos de estudiantes para poder sugerirles el curso adecuado. A continuación tenemos dos descripciones de estudiantes que son muy similares en los datos que contienen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos enviar esto a un LLM para analizar los datos. Esto se puede usar más adelante en nuestra aplicación para enviarlo a una API o almacenarlo en una base de datos.\n",
    "\n",
    "Vamos a crear dos prompts idénticos en los que le indicamos al LLM qué información nos interesa:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos enviar esto a un LLM para analizar las partes que son importantes para nuestro producto. Así podemos crear dos indicaciones idénticas para instruir al LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de crear estos dos prompts, los enviaremos al LLM usando `openai.ChatCompletion`. Almacenamos el prompt en la variable `messages` y asignamos el rol a `user`. Esto es para imitar un mensaje de un usuario escrito a un chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque los prompts sean los mismos y las descripciones parecidas, podemos obtener diferentes formatos para la propiedad `Grades`.\n",
    "\n",
    "Si ejecutas la celda anterior varias veces, el formato puede ser `3.7` o `3.7 GPA`.\n",
    "\n",
    "Esto ocurre porque el LLM toma datos no estructurados en forma de prompt escrito y también devuelve datos no estructurados. Necesitamos tener un formato estructurado para saber qué esperar al almacenar o usar estos datos.\n",
    "\n",
    "Al usar llamadas funcionales, podemos asegurarnos de recibir datos estructurados. Cuando usamos llamadas a funciones, el LLM en realidad no llama ni ejecuta ninguna función. En su lugar, creamos una estructura que el LLM debe seguir en sus respuestas. Luego usamos esas respuestas estructuradas para saber qué función ejecutar en nuestras aplicaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama de flujo de llamada de función](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.es.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casos de uso para llamadas a funciones\n",
    "\n",
    "**Llamar herramientas externas**\n",
    "Los chatbots son muy útiles para responder preguntas de los usuarios. Al usar llamadas a funciones, los chatbots pueden aprovechar los mensajes de los usuarios para realizar ciertas tareas. Por ejemplo, un estudiante puede pedirle al chatbot: \"Envía un correo a mi profesor diciendo que necesito más ayuda con esta materia\". Esto puede hacer una llamada a la función `send_email(to: string, body: string)`\n",
    "\n",
    "**Crear consultas a API o bases de datos**\n",
    "Los usuarios pueden buscar información usando lenguaje natural que se convierte en una consulta o solicitud de API con formato. Un ejemplo podría ser un profesor que pregunta: \"¿Quiénes son los estudiantes que completaron la última tarea?\", lo que podría llamar a una función llamada `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Crear datos estructurados**\n",
    "Los usuarios pueden tomar un bloque de texto o un archivo CSV y usar el LLM para extraer información relevante. Por ejemplo, un estudiante puede convertir un artículo de Wikipedia sobre acuerdos de paz para crear tarjetas de estudio con IA. Esto se puede lograr usando una función llamada `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creando tu primera llamada a una función\n",
    "\n",
    "El proceso para crear una llamada a una función incluye 3 pasos principales:\n",
    "1. Llamar a la API de Chat Completions con una lista de tus funciones y un mensaje del usuario\n",
    "2. Leer la respuesta del modelo para realizar una acción, es decir, ejecutar una función o una llamada a una API\n",
    "3. Hacer otra llamada a la API de Chat Completions con la respuesta de tu función para usar esa información y crear una respuesta para el usuario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flujo de una llamada a función](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.es.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementos de una llamada a función\n",
    "\n",
    "#### Entrada del usuario\n",
    "\n",
    "El primer paso es crear un mensaje de usuario. Este puede asignarse dinámicamente tomando el valor de una entrada de texto, o puedes asignar un valor aquí. Si es la primera vez que trabajas con la API de Chat Completions, necesitamos definir el `role` y el `content` del mensaje.\n",
    "\n",
    "El `role` puede ser `system` (creando reglas), `assistant` (el modelo) o `user` (el usuario final). Para la llamada a función, lo asignaremos como `user` y una pregunta de ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando funciones.\n",
    "\n",
    "A continuación definiremos una función y los parámetros de esa función. Aquí usaremos solo una función llamada `search_courses`, pero puedes crear varias funciones.\n",
    "\n",
    "**Importante**: Las funciones se incluyen en el mensaje del sistema para el LLM y contarán dentro de la cantidad de tokens disponibles que tienes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definiciones**\n",
    "\n",
    "`name` - El nombre de la función que queremos que se llame.\n",
    "\n",
    "`description` - Esta es la descripción de cómo funciona la función. Aquí es importante ser específico y claro.\n",
    "\n",
    "`parameters` - Una lista de valores y el formato que quieres que el modelo genere en su respuesta.\n",
    "\n",
    "`type` - El tipo de dato en el que se almacenarán las propiedades.\n",
    "\n",
    "`properties` - Lista de los valores específicos que el modelo usará para su respuesta.\n",
    "\n",
    "`name` - El nombre de la propiedad que el modelo usará en su respuesta formateada.\n",
    "\n",
    "`type` - El tipo de dato de esta propiedad.\n",
    "\n",
    "`description` - Descripción de la propiedad específica.\n",
    "\n",
    "**Opcional**\n",
    "\n",
    "`required` - Propiedad obligatoria para que se complete la llamada a la función.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar la llamada a la función\n",
    "Después de definir una función, ahora necesitamos incluirla en la llamada a la API de Chat Completion. Hacemos esto agregando `functions` a la solicitud. En este caso, `functions=functions`.\n",
    "\n",
    "También existe la opción de establecer `function_call` en `auto`. Esto significa que dejaremos que el LLM decida qué función debe llamarse según el mensaje del usuario en lugar de asignarla nosotros mismos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos la respuesta y observemos cómo está formateada:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Puedes ver que se llama al nombre de la función y, a partir del mensaje del usuario, el LLM pudo encontrar los datos para completar los argumentos de la función.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrando llamadas a funciones en una aplicación.\n",
    "\n",
    "Después de haber probado la respuesta formateada del LLM, ahora podemos integrarla en una aplicación.\n",
    "\n",
    "### Gestionando el flujo\n",
    "\n",
    "Para integrar esto en nuestra aplicación, sigamos los siguientes pasos:\n",
    "\n",
    "Primero, hagamos la llamada a los servicios de Open AI y guardemos el mensaje en una variable llamada `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definiremos la función que llamará a la API de Microsoft Learn para obtener una lista de cursos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como buena práctica, veremos si el modelo quiere llamar a una función. Después de eso, crearemos una de las funciones disponibles y la asignaremos a la función que se está llamando.\n",
    "Luego tomaremos los argumentos de la función y los relacionaremos con los argumentos provenientes del LLM.\n",
    "\n",
    "Por último, agregaremos el mensaje de llamada a la función y los valores que fueron devueltos por el mensaje `search_courses`. Esto le da al LLM toda la información que necesita para\n",
    "responder al usuario usando lenguaje natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafío de Código\n",
    "\n",
    "¡Buen trabajo! Para seguir aprendiendo sobre Azure Open AI Function Calling puedes construir: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Más parámetros de la función que puedan ayudar a los estudiantes a encontrar más cursos. Puedes encontrar los parámetros disponibles de la API aquí:\n",
    " - Crear otra llamada de función que tome más información del estudiante, como su idioma nativo\n",
    " - Crear manejo de errores cuando la llamada de función y/o la llamada a la API no devuelvan cursos adecuados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Descargo de responsabilidad**:  \nEste documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de ningún malentendido o interpretación errónea que surja del uso de esta traducción.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-08-25T19:52:38+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "es"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}