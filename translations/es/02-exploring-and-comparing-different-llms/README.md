<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2f686f2eb794941761252ac5e8e090b",
  "translation_date": "2025-07-09T08:08:08+00:00",
  "source_file": "02-exploring-and-comparing-different-llms/README.md",
  "language_code": "es"
}
-->
# Explorando y comparando diferentes LLMs

[![Explorando y comparando diferentes LLMs](../../../translated_images/02-lesson-banner.ef94c84979f97f60f07e27d905e708cbcbdf78707120553ccab27d91c947805b.es.png)](https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst)

> _Haz clic en la imagen de arriba para ver el video de esta lecci√≥n_

Con la lecci√≥n anterior, vimos c√≥mo la IA Generativa est√° cambiando el panorama tecnol√≥gico, c√≥mo funcionan los Modelos de Lenguaje Grande (LLMs) y c√≥mo un negocio, como nuestra startup, puede aplicarlos a sus casos de uso y crecer. En este cap√≠tulo, vamos a comparar y contrastar diferentes tipos de modelos de lenguaje grande (LLMs) para entender sus ventajas y desventajas.

El siguiente paso en el camino de nuestra startup es explorar el panorama actual de los LLMs y entender cu√°les son adecuados para nuestro caso de uso.

## Introducci√≥n

Esta lecci√≥n cubrir√°:

- Diferentes tipos de LLMs en el panorama actual.
- C√≥mo probar, iterar y comparar diferentes modelos para tu caso de uso en Azure.
- C√≥mo desplegar un LLM.

## Objetivos de aprendizaje

Despu√©s de completar esta lecci√≥n, podr√°s:

- Seleccionar el modelo adecuado para tu caso de uso.
- Entender c√≥mo probar, iterar y mejorar el rendimiento de tu modelo.
- Saber c√≥mo las empresas despliegan modelos.

## Entender los diferentes tipos de LLMs

Los LLMs pueden clasificarse de varias formas seg√∫n su arquitectura, datos de entrenamiento y caso de uso. Comprender estas diferencias ayudar√° a nuestra startup a elegir el modelo correcto para el escenario y a entender c√≥mo probar, iterar y mejorar el rendimiento.

Existen muchos tipos diferentes de modelos LLM; la elecci√≥n depende de para qu√© los quieras usar, tus datos, cu√°nto est√°s dispuesto a pagar y m√°s.

Dependiendo de si quieres usar los modelos para texto, audio, video, generaci√≥n de im√°genes, etc., podr√≠as optar por un tipo diferente de modelo.

- **Reconocimiento de audio y voz**. Para este prop√≥sito, los modelos tipo Whisper son una excelente opci√≥n, ya que son de prop√≥sito general y est√°n orientados al reconocimiento de voz. Est√°n entrenados con audio diverso y pueden realizar reconocimiento de voz multiling√ºe. Aprende m√°s sobre [modelos tipo Whisper aqu√≠](https://platform.openai.com/docs/models/whisper?WT.mc_id=academic-105485-koreyst).

- **Generaci√≥n de im√°genes**. Para generaci√≥n de im√°genes, DALL-E y Midjourney son dos opciones muy conocidas. DALL-E es ofrecido por Azure OpenAI. [Lee m√°s sobre DALL-E aqu√≠](https://platform.openai.com/docs/models/dall-e?WT.mc_id=academic-105485-koreyst) y tambi√©n en el Cap√≠tulo 9 de este curr√≠culo.

- **Generaci√≥n de texto**. La mayor√≠a de los modelos est√°n entrenados para generaci√≥n de texto y tienes una gran variedad de opciones desde GPT-3.5 hasta GPT-4. Tienen diferentes costos, siendo GPT-4 el m√°s caro. Vale la pena explorar el [playground de Azure OpenAI](https://oai.azure.com/portal/playground?WT.mc_id=academic-105485-koreyst) para evaluar qu√© modelos se ajustan mejor a tus necesidades en t√©rminos de capacidad y costo.

- **Multimodalidad**. Si buscas manejar m√∫ltiples tipos de datos en entrada y salida, podr√≠as interesarte en modelos como [gpt-4 turbo con visi√≥n o gpt-4o](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models?WT.mc_id=academic-105485-koreyst), las √∫ltimas versiones de los modelos de OpenAI, que combinan procesamiento de lenguaje natural con comprensi√≥n visual, permitiendo interacciones a trav√©s de interfaces multimodales.

Seleccionar un modelo significa obtener ciertas capacidades b√°sicas, que a veces no son suficientes. A menudo tienes datos espec√≠ficos de la empresa que necesitas comunicar al LLM. Hay varias formas de abordar esto, que veremos en las pr√≥ximas secciones.

### Modelos Foundation versus LLMs

El t√©rmino Foundation Model fue [acu√±ado por investigadores de Stanford](https://arxiv.org/abs/2108.07258?WT.mc_id=academic-105485-koreyst) y se define como un modelo de IA que cumple ciertos criterios, tales como:

- **Se entrenan usando aprendizaje no supervisado o auto-supervisado**, es decir, se entrenan con datos multimodales sin etiquetar y no requieren anotaci√≥n humana para su entrenamiento.
- **Son modelos muy grandes**, basados en redes neuronales profundas entrenadas con miles de millones de par√°metros.
- **Normalmente est√°n destinados a servir como ‚Äòfundamento‚Äô para otros modelos**, lo que significa que pueden usarse como punto de partida para construir otros modelos encima, mediante fine-tuning.

![Foundation Models versus LLMs](../../../translated_images/FoundationModel.e4859dbb7a825c94b284f17eae1c186aabc21d4d8644331f5b007d809cf8d0f2.es.png)

Fuente de la imagen: [Essential Guide to Foundation Models and Large Language Models | por Babar M Bhatti | Medium](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404)

Para aclarar esta distinci√≥n, tomemos ChatGPT como ejemplo. Para construir la primera versi√≥n de ChatGPT, se us√≥ un modelo llamado GPT-3.5 como foundation model. Esto significa que OpenAI utiliz√≥ datos espec√≠ficos de chat para crear una versi√≥n ajustada de GPT-3.5 especializada en desempe√±arse bien en escenarios conversacionales, como chatbots.

![Foundation Model](../../../translated_images/Multimodal.2c389c6439e0fc51b0b7b226d95d7d900d372ae66902d71b8ce5ec4951b8efbe.es.png)

Fuente de la imagen: [2108.07258.pdf (arxiv.org)](https://arxiv.org/pdf/2108.07258.pdf?WT.mc_id=academic-105485-koreyst)

### Modelos Open Source versus Propietarios

Otra forma de categorizar los LLMs es si son open source o propietarios.

Los modelos open source son aquellos que est√°n disponibles al p√∫blico y pueden ser usados por cualquiera. A menudo son liberados por la empresa que los cre√≥ o por la comunidad investigadora. Estos modelos pueden ser inspeccionados, modificados y personalizados para distintos casos de uso. Sin embargo, no siempre est√°n optimizados para producci√≥n y pueden no ser tan eficientes como los modelos propietarios. Adem√°s, la financiaci√≥n para modelos open source puede ser limitada, y puede que no se mantengan a largo plazo ni se actualicen con la investigaci√≥n m√°s reciente. Ejemplos populares de modelos open source incluyen [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html?WT.mc_id=academic-105485-koreyst), [Bloom](https://huggingface.co/bigscience/bloom) y [LLaMA](https://llama.meta.com).

Los modelos propietarios son propiedad de una empresa y no est√°n disponibles al p√∫blico. Estos modelos suelen estar optimizados para uso en producci√≥n. Sin embargo, no se permite inspeccionarlos, modificarlos ni personalizarlos para diferentes casos de uso. Adem√°s, no siempre son gratuitos y pueden requerir suscripci√≥n o pago para usarlos. Los usuarios tampoco tienen control sobre los datos usados para entrenar el modelo, por lo que deben confiar en que el propietario del modelo garantice la privacidad de los datos y el uso responsable de la IA. Ejemplos populares de modelos propietarios incluyen [modelos de OpenAI](https://platform.openai.com/docs/models/overview?WT.mc_id=academic-105485-koreyst), [Google Bard](https://sapling.ai/llm/bard?WT.mc_id=academic-105485-koreyst) o [Claude 2](https://www.anthropic.com/index/claude-2?WT.mc_id=academic-105485-koreyst).

### Embedding versus generaci√≥n de im√°genes versus generaci√≥n de texto y c√≥digo

Los LLMs tambi√©n pueden clasificarse seg√∫n el tipo de salida que generan.

Los embeddings son un conjunto de modelos que convierten texto en una forma num√©rica llamada embedding, que es una representaci√≥n num√©rica del texto de entrada. Los embeddings facilitan que las m√°quinas entiendan las relaciones entre palabras o frases y pueden usarse como entradas para otros modelos, como modelos de clasificaci√≥n o de clustering que funcionan mejor con datos num√©ricos. Los modelos de embedding se usan a menudo para transfer learning, donde se construye un modelo para una tarea sustituta con abundancia de datos, y luego se reutilizan los pesos (embeddings) para otras tareas posteriores. Un ejemplo de esta categor√≠a son los [OpenAI embeddings](https://platform.openai.com/docs/models/embeddings?WT.mc_id=academic-105485-koreyst).

![Embedding](../../../translated_images/Embedding.c3708fe988ccf76073d348483dbb7569f622211104f073e22e43106075c04800.es.png)

Los modelos de generaci√≥n de im√°genes son aquellos que generan im√°genes. Se usan para edici√≥n, s√≠ntesis y traducci√≥n de im√°genes. Estos modelos suelen entrenarse con grandes conjuntos de datos de im√°genes, como [LAION-5B](https://laion.ai/blog/laion-5b/?WT.mc_id=academic-105485-koreyst), y pueden generar im√°genes nuevas o editar im√°genes existentes con t√©cnicas como inpainting, superresoluci√≥n y colorizaci√≥n. Ejemplos incluyen [DALL-E-3](https://openai.com/dall-e-3?WT.mc_id=academic-105485-koreyst) y [modelos Stable Diffusion](https://github.com/Stability-AI/StableDiffusion?WT.mc_id=academic-105485-koreyst).

![Generaci√≥n de im√°genes](../../../translated_images/Image.349c080266a763fd255b840a921cd8fc526ed78dc58708fa569ff1873d302345.es.png)

Los modelos de generaci√≥n de texto y c√≥digo son aquellos que generan texto o c√≥digo. Se usan para resumen de texto, traducci√≥n y respuesta a preguntas. Los modelos de generaci√≥n de texto suelen entrenarse con grandes conjuntos de datos de texto, como [BookCorpus](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html?WT.mc_id=academic-105485-koreyst), y pueden generar texto nuevo o responder preguntas. Los modelos de generaci√≥n de c√≥digo, como [CodeParrot](https://huggingface.co/codeparrot?WT.mc_id=academic-105485-koreyst), se entrenan con grandes conjuntos de datos de c√≥digo, como GitHub, y pueden generar c√≥digo nuevo o corregir errores en c√≥digo existente.

![Generaci√≥n de texto y c√≥digo](../../../translated_images/Text.a8c0cf139e5cc2a0cd3edaba8d675103774e6ddcb3c9fc5a98bb17c9a450e31d.es.png)

### Encoder-Decoder versus solo Decoder

Para hablar de los diferentes tipos de arquitecturas de LLMs, usemos una analog√≠a.

Imagina que tu jefe te asigna la tarea de escribir un cuestionario para los estudiantes. Tienes dos colegas; uno se encarga de crear el contenido y el otro de revisarlo.

El creador de contenido es como un modelo solo Decoder, puede ver el tema y lo que ya escribiste, y luego escribir un curso basado en eso. Son muy buenos escribiendo contenido atractivo e informativo, pero no son tan buenos entendiendo el tema y los objetivos de aprendizaje. Algunos ejemplos de modelos Decoder son los de la familia GPT, como GPT-3.

El revisor es como un modelo solo Encoder, revisa el curso escrito y las respuestas, notando la relaci√≥n entre ellos y entendiendo el contexto, pero no es bueno generando contenido. Un ejemplo de modelo solo Encoder ser√≠a BERT.

Imagina que pudi√©ramos tener a alguien que pueda crear y revisar el cuestionario, este ser√≠a un modelo Encoder-Decoder. Algunos ejemplos ser√≠an BART y T5.

### Servicio versus Modelo

Ahora, hablemos de la diferencia entre un servicio y un modelo. Un servicio es un producto ofrecido por un proveedor de servicios en la nube, y suele ser una combinaci√≥n de modelos, datos y otros componentes. Un modelo es el componente central de un servicio, y suele ser un foundation model, como un LLM.

Los servicios suelen estar optimizados para uso en producci√≥n y son m√°s f√°ciles de usar que los modelos, a trav√©s de una interfaz gr√°fica. Sin embargo, los servicios no siempre son gratuitos y pueden requerir suscripci√≥n o pago, a cambio de aprovechar el equipo y recursos del propietario del servicio, optimizando gastos y facilitando la escalabilidad. Un ejemplo de servicio es [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=academic-105485-koreyst), que ofrece un plan de pago por uso, es decir, los usuarios pagan proporcionalmente a cu√°nto usan el servicio. Adem√°s, Azure OpenAI Service ofrece seguridad de nivel empresarial y un marco de IA responsable sobre las capacidades de los modelos.

Los modelos son solo la red neuronal, con par√°metros, pesos y dem√°s. Permiten que las empresas los ejecuten localmente, pero necesitar√≠an comprar equipo, construir una infraestructura para escalar y comprar una licencia o usar un modelo open source. Un modelo como LLaMA est√° disponible para usarse, requiriendo potencia computacional para ejecutarlo.

## C√≥mo probar e iterar con diferentes modelos para entender el rendimiento en Azure

Una vez que nuestro equipo ha explorado el panorama actual de LLMs e identificado algunos buenos candidatos para sus escenarios, el siguiente paso es probarlos con sus datos y carga de trabajo. Este es un proceso iterativo, realizado mediante experimentos y mediciones.
La mayor√≠a de los modelos que mencionamos en los p√°rrafos anteriores (modelos de OpenAI, modelos de c√≥digo abierto como Llama2 y transformers de Hugging Face) est√°n disponibles en el [Cat√°logo de Modelos](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview?WT.mc_id=academic-105485-koreyst) en [Azure AI Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreyst).

[Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/what-is-ai-studio?WT.mc_id=academic-105485-koreyst) es una plataforma en la nube dise√±ada para que los desarrolladores construyan aplicaciones de IA generativa y gestionen todo el ciclo de vida del desarrollo, desde la experimentaci√≥n hasta la evaluaci√≥n, combinando todos los servicios de Azure AI en un solo centro con una interfaz gr√°fica f√°cil de usar. El Cat√°logo de Modelos en Azure AI Studio permite al usuario:

- Encontrar el Modelo Base de inter√©s en el cat√°logo, ya sea propietario o de c√≥digo abierto, filtrando por tarea, licencia o nombre. Para mejorar la b√∫squeda, los modelos est√°n organizados en colecciones, como la colecci√≥n Azure OpenAI, la colecci√≥n Hugging Face y m√°s.

![Model catalog](../../../translated_images/AzureAIStudioModelCatalog.3cf8a499aa8ba0314f2c73d4048b3225d324165f547525f5b7cfa5f6c9c68941.es.png)

- Revisar la ficha del modelo, que incluye una descripci√≥n detallada del uso previsto y los datos de entrenamiento, ejemplos de c√≥digo y resultados de evaluaci√≥n en la biblioteca interna de evaluaciones.

![Model card](../../../translated_images/ModelCard.598051692c6e400d681a713ba7717e8b6e5e65f08d12131556fcec0f1789459b.es.png)

- Comparar benchmarks entre modelos y conjuntos de datos disponibles en la industria para evaluar cu√°l se ajusta mejor al escenario de negocio, a trav√©s del panel de [Model Benchmarks](https://learn.microsoft.com/azure/ai-studio/how-to/model-benchmarks?WT.mc_id=academic-105485-koreyst).

![Model benchmarks](../../../translated_images/ModelBenchmarks.254cb20fbd06c03a4ca53994585c5ea4300a88bcec8eff0450f2866ee2ac5ff3.es.png)

- Ajustar finamente el modelo con datos de entrenamiento personalizados para mejorar su rendimiento en una carga de trabajo espec√≠fica, aprovechando las capacidades de experimentaci√≥n y seguimiento de Azure AI Studio.

![Model fine-tuning](../../../translated_images/FineTuning.aac48f07142e36fddc6571b1f43ea2e003325c9c6d8e3fc9d8834b771e308dbf.es.png)

- Desplegar el modelo preentrenado original o la versi√≥n ajustada finamente en un endpoint remoto de inferencia en tiempo real ‚Äî ya sea en c√≥mputo gestionado o en una API serverless ‚Äî con un modelo de [pago por uso](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview#model-deployment-managed-compute-and-serverless-api-pay-as-you-go?WT.mc_id=academic-105485-koreyst) para que las aplicaciones puedan consumirlo.

![Model deployment](../../../translated_images/ModelDeploy.890da48cbd0bccdb4abfc9257f3d884831e5d41b723e7d1ceeac9d60c3c4f984.es.png)


> [!NOTE]
> No todos los modelos en el cat√°logo est√°n disponibles actualmente para ajuste fino y/o despliegue con pago por uso. Consulta la ficha del modelo para detalles sobre sus capacidades y limitaciones.

## Mejorando los resultados de LLM

Hemos explorado con nuestro equipo startup diferentes tipos de LLM y una plataforma en la nube (Azure Machine Learning) que nos permite comparar distintos modelos, evaluarlos con datos de prueba, mejorar su rendimiento y desplegarlos en endpoints de inferencia.

Pero, ¬øcu√°ndo deber√≠an considerar ajustar finamente un modelo en lugar de usar uno preentrenado? ¬øExisten otras formas de mejorar el rendimiento del modelo en cargas de trabajo espec√≠ficas?

Hay varias estrategias que una empresa puede usar para obtener los resultados que necesita de un LLM. Puedes seleccionar diferentes tipos de modelos con distintos grados de entrenamiento al desplegar un LLM en producci√≥n, con diferentes niveles de complejidad, costo y calidad. Aqu√≠ algunas opciones:

- **Ingenier√≠a de prompts con contexto**. La idea es proporcionar suficiente contexto al prompt para asegurar que obtienes las respuestas que necesitas.

- **Retrieval Augmented Generation, RAG**. Tus datos podr√≠an estar en una base de datos o en un endpoint web, por ejemplo. Para asegurar que estos datos, o un subconjunto de ellos, se incluyan al momento de hacer el prompt, puedes recuperar la informaci√≥n relevante y hacer que forme parte del prompt del usuario.

- **Modelo ajustado finamente**. Aqu√≠, entrenas el modelo m√°s a fondo con tus propios datos, lo que hace que el modelo sea m√°s preciso y responda mejor a tus necesidades, aunque puede ser costoso.

![LLMs deployment](../../../translated_images/Deploy.18b2d27412ec8c02871386cbe91097c7f2190a8c6e2be88f66392b411609a48c.es.png)

Fuente de la imagen: [Four Ways that Enterprises Deploy LLMs | Fiddler AI Blog](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)

### Ingenier√≠a de prompts con contexto

Los LLM preentrenados funcionan muy bien en tareas generales de lenguaje natural, incluso con un prompt corto, como una frase para completar o una pregunta ‚Äî el llamado aprendizaje ‚Äúzero-shot‚Äù.

Sin embargo, cuanto m√°s pueda el usuario enmarcar su consulta, con una petici√≥n detallada y ejemplos ‚Äî el Contexto ‚Äî m√°s precisa y cercana a las expectativas del usuario ser√° la respuesta. En este caso, hablamos de aprendizaje ‚Äúone-shot‚Äù si el prompt incluye un solo ejemplo, y ‚Äúfew-shot‚Äù si incluye varios ejemplos. La ingenier√≠a de prompts con contexto es la forma m√°s rentable para comenzar.

### Retrieval Augmented Generation (RAG)

Los LLM tienen la limitaci√≥n de que solo pueden usar los datos con los que fueron entrenados para generar una respuesta. Esto significa que no saben nada sobre hechos ocurridos despu√©s de su entrenamiento, ni pueden acceder a informaci√≥n no p√∫blica (como datos de la empresa).
Esto se puede superar con RAG, una t√©cnica que ampl√≠a el prompt con datos externos en forma de fragmentos de documentos, considerando los l√≠mites de longitud del prompt. Esto es posible gracias a herramientas de bases de datos vectoriales (como [Azure Vector Search](https://learn.microsoft.com/azure/search/vector-search-overview?WT.mc_id=academic-105485-koreyst)) que recuperan los fragmentos √∫tiles de diversas fuentes de datos predefinidas y los a√±aden al Contexto del prompt.

Esta t√©cnica es muy √∫til cuando una empresa no tiene suficientes datos, tiempo o recursos para ajustar finamente un LLM, pero a√∫n as√≠ quiere mejorar el rendimiento en una carga de trabajo espec√≠fica y reducir riesgos de invenciones, es decir, distorsiones de la realidad o contenido da√±ino.

### Modelo ajustado finamente

El ajuste fino es un proceso que aprovecha el aprendizaje por transferencia para ‚Äòadaptar‚Äô el modelo a una tarea espec√≠fica o para resolver un problema concreto. A diferencia del aprendizaje few-shot y RAG, genera un nuevo modelo con pesos y sesgos actualizados. Requiere un conjunto de ejemplos de entrenamiento que consisten en una entrada √∫nica (el prompt) y su salida asociada (la respuesta).
Esta ser√≠a la opci√≥n preferida si:

- **Usar modelos ajustados finamente**. Una empresa quiere usar modelos ajustados menos potentes (como modelos de embeddings) en lugar de modelos de alto rendimiento, logrando una soluci√≥n m√°s econ√≥mica y r√°pida.

- **Considerar la latencia**. La latencia es importante para un caso de uso espec√≠fico, por lo que no es posible usar prompts muy largos o la cantidad de ejemplos que el modelo deber√≠a aprender no encaja con el l√≠mite de longitud del prompt.

- **Mantenerse actualizado**. Una empresa dispone de muchos datos de alta calidad y etiquetas de verdad de terreno, y los recursos necesarios para mantener estos datos actualizados a lo largo del tiempo.

### Modelo entrenado

Entrenar un LLM desde cero es sin duda el enfoque m√°s dif√≠cil y complejo, que requiere enormes cantidades de datos, recursos especializados y potencia computacional adecuada. Esta opci√≥n solo deber√≠a considerarse en un escenario donde una empresa tenga un caso de uso espec√≠fico de dominio y una gran cantidad de datos centrados en ese dominio.

## Verificaci√≥n de conocimientos

¬øCu√°l podr√≠a ser un buen enfoque para mejorar los resultados de completado de un LLM?

1. Ingenier√≠a de prompts con contexto  
1. RAG  
1. Modelo ajustado finamente

R:3, si tienes tiempo, recursos y datos de alta calidad, el ajuste fino es la mejor opci√≥n para mantenerse actualizado. Sin embargo, si buscas mejorar las cosas y te falta tiempo, vale la pena considerar primero RAG.

## üöÄ Desaf√≠o

Lee m√°s sobre c√≥mo puedes [usar RAG](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview?WT.mc_id=academic-105485-koreyst) para tu negocio.

## Excelente trabajo, contin√∫a aprendiendo

Despu√©s de completar esta lecci√≥n, visita nuestra [colecci√≥n de aprendizaje de IA generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para seguir mejorando tus conocimientos en IA generativa.

¬°Dir√≠gete a la Lecci√≥n 3 donde veremos c√≥mo [construir con IA generativa de forma responsable](../03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)!

**Aviso legal**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por la precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas derivadas del uso de esta traducci√≥n.