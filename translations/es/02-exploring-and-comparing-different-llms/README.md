<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2f686f2eb794941761252ac5e8e090b",
  "translation_date": "2025-05-19T09:14:11+00:00",
  "source_file": "02-exploring-and-comparing-different-llms/README.md",
  "language_code": "es"
}
-->
# Explorando y comparando diferentes LLMs

[![Explorando y comparando diferentes LLMs](../../../translated_images/02-lesson-banner.722fb0fdf701564d4479112ef4c4fa964c98dce0c241decbe12aae32e9fb4659.es.png)](https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst)

> _Haz clic en la imagen de arriba para ver el video de esta lecci칩n_

En la lecci칩n anterior, vimos c칩mo la IA generativa est치 cambiando el panorama tecnol칩gico, c칩mo funcionan los Modelos de Lenguaje de Gran Escala (LLMs) y c칩mo un negocio, como nuestra startup, puede aplicarlos a sus casos de uso y crecer. En este cap칤tulo, buscamos comparar y contrastar diferentes tipos de modelos de lenguaje de gran escala (LLMs) para entender sus pros y contras.

El siguiente paso en el viaje de nuestra startup es explorar el panorama actual de los LLMs y entender cu치les son adecuados para nuestro caso de uso.

## Introducci칩n

Esta lecci칩n cubrir치:

- Diferentes tipos de LLMs en el panorama actual.
- Probar, iterar y comparar diferentes modelos para tu caso de uso en Azure.
- C칩mo desplegar un LLM.

## Objetivos de aprendizaje

Despu칠s de completar esta lecci칩n, podr치s:

- Seleccionar el modelo adecuado para tu caso de uso.
- Entender c칩mo probar, iterar y mejorar el rendimiento de tu modelo.
- Saber c칩mo las empresas despliegan modelos.

## Entender diferentes tipos de LLMs

Los LLMs pueden tener m칰ltiples categorizaciones basadas en su arquitectura, datos de entrenamiento y caso de uso. Entender estas diferencias ayudar치 a nuestra startup a seleccionar el modelo adecuado para el escenario y a entender c칩mo probar, iterar y mejorar el rendimiento.

Existen muchos tipos diferentes de modelos LLM, tu elecci칩n de modelo depende de para qu칠 planeas usarlos, tus datos, cu치nto est치s dispuesto a pagar y m치s.

Dependiendo de si planeas usar los modelos para generaci칩n de texto, audio, video, im치genes, etc., podr칤as optar por un tipo diferente de modelo.

- **Reconocimiento de audio y voz**. Para este prop칩sito, los modelos tipo Whisper son una gran elecci칩n, ya que son de prop칩sito general y est치n orientados al reconocimiento de voz. Est치n entrenados en audio diverso y pueden realizar reconocimiento de voz multiling칲e. Aprende m치s sobre [modelos tipo Whisper aqu칤](https://platform.openai.com/docs/models/whisper?WT.mc_id=academic-105485-koreyst).

- **Generaci칩n de im치genes**. Para la generaci칩n de im치genes, DALL-E y Midjourney son dos opciones muy conocidas. DALL-E es ofrecido por Azure OpenAI. [Lee m치s sobre DALL-E aqu칤](https://platform.openai.com/docs/models/dall-e?WT.mc_id=academic-105485-koreyst) y tambi칠n en el Cap칤tulo 9 de este curr칤culo.

- **Generaci칩n de texto**. La mayor칤a de los modelos est치n entrenados en generaci칩n de texto y tienes una gran variedad de opciones desde GPT-3.5 hasta GPT-4. Tienen diferentes costos, siendo GPT-4 el m치s caro. Vale la pena echar un vistazo al [Azure OpenAI playground](https://oai.azure.com/portal/playground?WT.mc_id=academic-105485-koreyst) para evaluar qu칠 modelos se ajustan mejor a tus necesidades en t칠rminos de capacidad y costo.

- **Multimodalidad**. Si buscas manejar m칰ltiples tipos de datos en entrada y salida, podr칤as querer mirar modelos como [gpt-4 turbo con visi칩n o gpt-4o](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models?WT.mc_id=academic-105485-koreyst) - las 칰ltimas versiones de los modelos de OpenAI - que son capaces de combinar el procesamiento del lenguaje natural con la comprensi칩n visual, permitiendo interacciones a trav칠s de interfaces multimodales.

Seleccionar un modelo significa que obtienes algunas capacidades b치sicas, que sin embargo podr칤an no ser suficientes. A menudo tienes datos espec칤ficos de la empresa que de alguna manera necesitas comunicar al LLM. Hay algunas opciones diferentes sobre c칩mo abordar eso, m치s sobre eso en las secciones siguientes.

### Modelos Fundamentales versus LLMs

El t칠rmino Modelo Fundamental fue [acu침ado por investigadores de Stanford](https://arxiv.org/abs/2108.07258?WT.mc_id=academic-105485-koreyst) y se define como un modelo de IA que sigue algunos criterios, tales como:

- **Son entrenados usando aprendizaje no supervisado o auto-supervisado**, lo que significa que est치n entrenados en datos multimodales no etiquetados y no requieren anotaci칩n o etiquetado humano de datos para su proceso de entrenamiento.
- **Son modelos muy grandes**, basados en redes neuronales muy profundas entrenadas en miles de millones de par치metros.
- **Normalmente est치n destinados a servir como una 'base' para otros modelos**, lo que significa que pueden usarse como punto de partida para que otros modelos se construyan sobre ellos, lo cual se puede hacer mediante ajuste fino.

![Modelos Fundamentales versus LLMs](../../../translated_images/FoundationModel.1b89e9d94c6a60a9af557b1c0a10faa3a55c0cbc6bb357eb144512ab833d162c.es.png)

Fuente de la imagen: [Gu칤a Esencial de Modelos Fundamentales y Modelos de Lenguaje de Gran Escala | por Babar M Bhatti | Medium](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404)

Para aclarar a칰n m치s esta distinci칩n, tomemos ChatGPT como ejemplo. Para construir la primera versi칩n de ChatGPT, un modelo llamado GPT-3.5 sirvi칩 como el modelo fundamental. Esto significa que OpenAI us칩 algunos datos espec칤ficos de chat para crear una versi칩n ajustada de GPT-3.5 que estaba especializada en desempe침arse bien en escenarios conversacionales, como chatbots.

![Modelo Fundamental](../../../translated_images/Multimodal.41df52bb0de979b80e9643ba34f8f1b53d7791cebd88bceedda6497241495f27.es.png)

Fuente de la imagen: [2108.07258.pdf (arxiv.org)](https://arxiv.org/pdf/2108.07258.pdf?WT.mc_id=academic-105485-koreyst)

### Modelos de C칩digo Abierto versus Propietarios

Otra forma de categorizar los LLMs es si son de c칩digo abierto o propietarios.

Los modelos de c칩digo abierto son modelos que est치n disponibles para el p칰blico y pueden ser usados por cualquiera. A menudo son puestos a disposici칩n por la empresa que los cre칩 o por la comunidad de investigaci칩n. Estos modelos pueden ser inspeccionados, modificados y personalizados para los diversos casos de uso en LLMs. Sin embargo, no siempre est치n optimizados para uso en producci칩n y pueden no ser tan eficientes como los modelos propietarios. Adem치s, la financiaci칩n para los modelos de c칩digo abierto puede ser limitada y pueden no mantenerse a largo plazo o no actualizarse con la 칰ltima investigaci칩n. Ejemplos de modelos de c칩digo abierto populares incluyen [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html?WT.mc_id=academic-105485-koreyst), [Bloom](https://huggingface.co/bigscience/bloom) y [LLaMA](https://llama.meta.com).

Los modelos propietarios son modelos que son propiedad de una empresa y no est치n disponibles para el p칰blico. Estos modelos a menudo est치n optimizados para uso en producci칩n. Sin embargo, no se permite que sean inspeccionados, modificados o personalizados para diferentes casos de uso. Adem치s, no siempre est치n disponibles de forma gratuita y pueden requerir una suscripci칩n o pago para usarlos. Tambi칠n, los usuarios no tienen control sobre los datos que se utilizan para entrenar el modelo, lo que significa que deben confiar en el propietario del modelo para garantizar el compromiso con la privacidad de los datos y el uso responsable de la IA. Ejemplos de modelos propietarios populares incluyen [modelos de OpenAI](https://platform.openai.com/docs/models/overview?WT.mc_id=academic-105485-koreyst), [Google Bard](https://sapling.ai/llm/bard?WT.mc_id=academic-105485-koreyst) o [Claude 2](https://www.anthropic.com/index/claude-2?WT.mc_id=academic-105485-koreyst).

### Embedding versus Generaci칩n de Im치genes versus Generaci칩n de Texto y C칩digo

Los LLMs tambi칠n pueden categorizarse por el tipo de salida que generan.

Las incrustaciones son un conjunto de modelos que pueden convertir texto en una forma num칠rica, llamada incrustaci칩n, que es una representaci칩n num칠rica del texto de entrada. Las incrustaciones facilitan a las m치quinas entender las relaciones entre palabras o frases y pueden ser consumidas como entradas por otros modelos, como modelos de clasificaci칩n o modelos de agrupamiento que tienen mejor rendimiento en datos num칠ricos. Los modelos de incrustaci칩n a menudo se usan para el aprendizaje por transferencia, donde un modelo se construye para una tarea sustituta para la cual hay abundancia de datos, y luego los pesos del modelo (incrustaciones) se reutilizan para otras tareas posteriores. Un ejemplo de esta categor칤a es [incrustaciones de OpenAI](https://platform.openai.com/docs/models/embeddings?WT.mc_id=academic-105485-koreyst).

![Embedding](../../../translated_images/Embedding.fbf261f314681a51994056854fd928b69b253616bb313e68a9ce19a2b15c8768.es.png)

Los modelos de generaci칩n de im치genes son modelos que generan im치genes. Estos modelos a menudo se usan para edici칩n de im치genes, s칤ntesis de im치genes y traducci칩n de im치genes. Los modelos de generaci칩n de im치genes a menudo se entrenan en grandes conjuntos de datos de im치genes, como [LAION-5B](https://laion.ai/blog/laion-5b/?WT.mc_id=academic-105485-koreyst), y pueden usarse para generar nuevas im치genes o para editar im치genes existentes con t칠cnicas de inpainting, super-resoluci칩n y colorizaci칩n. Ejemplos incluyen [DALL-E-3](https://openai.com/dall-e-3?WT.mc_id=academic-105485-koreyst) y [modelos de Stable Diffusion](https://github.com/Stability-AI/StableDiffusion?WT.mc_id=academic-105485-koreyst).

![Generaci칩n de im치genes](../../../translated_images/Image.fffee8e361cc35ed409975f6fc85502ae3d20b8eb01273cd327294e26318a049.es.png)

Los modelos de generaci칩n de texto y c칩digo son modelos que generan texto o c칩digo. Estos modelos a menudo se usan para resumir texto, traducir y responder preguntas. Los modelos de generaci칩n de texto a menudo se entrenan en grandes conjuntos de datos de texto, como [BookCorpus](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html?WT.mc_id=academic-105485-koreyst), y pueden usarse para generar nuevo texto o para responder preguntas. Los modelos de generaci칩n de c칩digo, como [CodeParrot](https://huggingface.co/codeparrot?WT.mc_id=academic-105485-koreyst), a menudo se entrenan en grandes conjuntos de datos de c칩digo, como GitHub, y pueden usarse para generar nuevo c칩digo o para corregir errores en c칩digo existente.

![Generaci칩n de texto y c칩digo](../../../translated_images/Text.35cfbe12e08d5b5615cf7db5174fe477bf96f45c5b82d53c29523bd8b94bdc17.es.png)

### Encoder-Decoder versus Solo Decoder

Para hablar sobre los diferentes tipos de arquitecturas de LLMs, usemos una analog칤a.

Imagina que tu gerente te dio la tarea de escribir un cuestionario para los estudiantes. Tienes dos colegas; uno se encarga de crear el contenido y el otro de revisarlo.

El creador de contenido es como un modelo Solo Decoder, pueden mirar el tema y ver lo que ya escribiste y luego pueden escribir un curso basado en eso. Son muy buenos escribiendo contenido atractivo e informativo, pero no son muy buenos entendiendo el tema y los objetivos de aprendizaje. Algunos ejemplos de modelos Decoder son los modelos de la familia GPT, como GPT-3.

El revisor es como un modelo Solo Encoder, miran el curso escrito y las respuestas, notando la relaci칩n entre ellos y entendiendo el contexto, pero no son buenos generando contenido. Un ejemplo de modelo Solo Encoder ser칤a BERT.

Imagina que tambi칠n podemos tener a alguien que pueda crear y revisar el cuestionario, este es un modelo Encoder-Decoder. Algunos ejemplos ser칤an BART y T5.

### Servicio versus Modelo

Ahora, hablemos sobre la diferencia entre un servicio y un modelo. Un servicio es un producto que ofrece un Proveedor de Servicios en la Nube y a menudo es una combinaci칩n de modelos, datos y otros componentes. Un modelo es el componente central de un servicio y a menudo es un modelo fundamental, como un LLM.

Los servicios a menudo est치n optimizados para uso en producci칩n y a menudo son m치s f치ciles de usar que los modelos, a trav칠s de una interfaz gr치fica de usuario. Sin embargo, los servicios no siempre est치n disponibles de forma gratuita y pueden requerir una suscripci칩n o pago para usarlos, a cambio de aprovechar el equipo y los recursos del propietario del servicio, optimizando los gastos y escalando f치cilmente. Un ejemplo de un servicio es [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=academic-105485-koreyst), que ofrece un plan de tarifas de pago por uso, lo que significa que los usuarios son cobrados proporcionalmente a cu치nto usan el servicio. Adem치s, Azure OpenAI Service ofrece seguridad de grado empresarial y un marco de IA responsable adem치s de las capacidades de los modelos.

Los modelos son solo la Red Neuronal, con los par치metros, pesos y otros. Permiten a las empresas ejecutarse localmente, sin embargo, necesitar칤an comprar equipo, construir una estructura para escalar y comprar una licencia o usar un modelo de c칩digo abierto. Un modelo como LLaMA est치 disponible para ser usado, requiriendo poder computacional para ejecutar el modelo.

## C칩mo probar e iterar con diferentes modelos para entender el rendimiento en Azure

Una vez que nuestro equipo ha explorado el panorama actual de LLMs e identificado algunos buenos candidatos para sus escenarios, el siguiente paso es probarlos en sus datos y en su carga de trabajo. Este es un proceso iterativo, realizado mediante experimentos y medidas. La mayor칤a de los modelos que mencionamos en p치rrafos anteriores (modelos de OpenAI, modelos de c칩digo abierto como Llama2 y transformadores de Hugging Face) est치n disponibles en el [Cat치logo de Modelos](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview?WT.mc_id=academic-105485-koreyst) en [Azure AI Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreyst).

[Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/what-is-ai-studio?WT.mc_id=academic-105485-koreyst) es una Plataforma en la Nube dise침ada para desarrolladores para construir aplicaciones de IA generativa y gestionar todo el ciclo de vida del desarrollo, desde la experimentaci칩n hasta la evaluaci칩n, combinando todos los servicios de Azure AI en un 칰nico centro con una interfaz gr치fica de usuario pr치ctica. El Cat치logo de Modelos en Azure AI Studio permite al usuario:

- Encontrar el Modelo Fundamental de inter칠s en el cat치logo, ya sea propietario o de c칩digo abierto, filtrando por tarea, licencia o nombre. Para mejorar la b칰squeda, los modelos est치n organizados en colecciones, como la colecci칩n Azure OpenAI, la colecci칩n Hugging Face y m치s.

![Cat치logo de modelos](../../../translated_images/AzureAIStudioModelCatalog.e34ac207ac348d31e74246c4f91d10086444783b72bbee3658e0453918aa5d22.es.png)

- Revisar la tarjeta del modelo, incluyendo una descripci칩n detallada del uso previsto y los datos de entrenamiento, ejemplos de c칩digo y resultados de evaluaci칩n en la biblioteca de evaluaciones internas.

![Tarjeta del modelo](../../../translated_images/ModelCard.8b25784bb406028655a12ea87d1ef3d52302e5d692ae4ec559c2dce7682027c7.es.png)
- Compara los puntos de referencia entre modelos y conjuntos de datos disponibles en la industria para evaluar cu치l se ajusta al escenario empresarial, a trav칠s del panel de [Puntos de Referencia del Modelo](https://learn.microsoft.com/azure/ai-studio/how-to/model-benchmarks?WT.mc_id=academic-105485-koreyst).

![Puntos de referencia del modelo](../../../translated_images/ModelBenchmarks.b3b4182f762db04b59267af64ce77cc936d38adf40fb032f12acec9063578008.es.png)

- Ajusta el modelo con datos de entrenamiento personalizados para mejorar el rendimiento del modelo en una carga de trabajo espec칤fica, aprovechando las capacidades de experimentaci칩n y seguimiento de Azure AI Studio.

![Ajuste del modelo](../../../translated_images/FineTuning.f93db4ecbdc85b4a20ff1198fb82f5e2daa3a1ee328733b17d603727db20f5c0.es.png)

- Despliega el modelo preentrenado original o la versi칩n ajustada para una inferencia remota en tiempo real - c칩mputo gestionado - o un punto final de API sin servidor - [pago por uso](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview#model-deployment-managed-compute-and-serverless-api-pay-as-you-go?WT.mc_id=academic-105485-koreyst) - para permitir que las aplicaciones lo consuman.

![Despliegue del modelo](../../../translated_images/ModelDeploy.7c78c2c5841567abf820d5da8354be454d3f20b62168905645aeac99e50c2562.es.png)

> [!NOTE]
> No todos los modelos en el cat치logo est치n actualmente disponibles para ajuste y/o despliegue de pago por uso. Consulta la tarjeta del modelo para obtener detalles sobre las capacidades y limitaciones del modelo.

## Mejorando los resultados de LLM

Hemos explorado con nuestro equipo de startup diferentes tipos de LLMs y una plataforma en la nube (Azure Machine Learning) que nos permite comparar diferentes modelos, evaluarlos con datos de prueba, mejorar el rendimiento y desplegarlos en puntos finales de inferencia.

Pero, 쯖u치ndo deber칤an considerar ajustar un modelo en lugar de usar uno preentrenado? 쮼xisten otros enfoques para mejorar el rendimiento del modelo en cargas de trabajo espec칤ficas?

Hay varios enfoques que una empresa puede utilizar para obtener los resultados que necesita de un LLM. Puedes seleccionar diferentes tipos de modelos con diferentes grados de entrenamiento al desplegar un LLM en producci칩n, con diferentes niveles de complejidad, costo y calidad. Aqu칤 hay algunos enfoques diferentes:

- **Ingenier칤a de prompts con contexto**. La idea es proporcionar suficiente contexto cuando haces un prompt para asegurar que obtienes las respuestas que necesitas.

- **Generaci칩n Aumentada por Recuperaci칩n, RAG**. Tus datos podr칤an existir en una base de datos o punto final web, por ejemplo, para asegurar que estos datos, o un subconjunto de ellos, se incluyan en el momento del prompt, puedes recuperar los datos relevantes y hacer que formen parte del prompt del usuario.

- **Modelo ajustado**. Aqu칤, entrenaste el modelo m치s a fondo con tus propios datos, lo que llev칩 a que el modelo sea m치s exacto y responda mejor a tus necesidades, pero podr칤a ser costoso.

![Despliegue de LLMs](../../../translated_images/Deploy.09224ecfe6a5ef47996fd0a44288772990139305451440c430662d43ac323ecd.es.png)

Fuente de la imagen: [Cuatro maneras en que las empresas despliegan LLMs | Blog de Fiddler AI](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)

### Ingenier칤a de Prompts con Contexto

Los LLMs preentrenados funcionan muy bien en tareas de lenguaje natural generalizadas, incluso al llamarlos con un prompt corto, como una oraci칩n para completar o una pregunta: el llamado aprendizaje "zero-shot".

Sin embargo, cuanto m치s pueda el usuario enmarcar su consulta, con una solicitud detallada y ejemplos - el Contexto - m치s precisa y cercana a las expectativas del usuario ser치 la respuesta. En este caso, hablamos de aprendizaje "one-shot" si el prompt incluye solo un ejemplo y "few-shot learning" si incluye m칰ltiples ejemplos. La ingenier칤a de prompts con contexto es el enfoque m치s rentable para comenzar.

### Generaci칩n Aumentada por Recuperaci칩n (RAG)

Los LLMs tienen la limitaci칩n de que solo pueden usar los datos que se han utilizado durante su entrenamiento para generar una respuesta. Esto significa que no saben nada sobre los hechos que ocurrieron despu칠s de su proceso de entrenamiento, y no pueden acceder a informaci칩n no p칰blica (como datos de la empresa). Esto se puede superar a trav칠s de RAG, una t칠cnica que aumenta el prompt con datos externos en forma de fragmentos de documentos, considerando los l칤mites de longitud del prompt. Esto es compatible con herramientas de bases de datos vectoriales (como [Azure Vector Search](https://learn.microsoft.com/azure/search/vector-search-overview?WT.mc_id=academic-105485-koreyst)) que recuperan los fragmentos 칰tiles de fuentes de datos predefinidas variadas y los agregan al Contexto del prompt.

Esta t칠cnica es muy 칰til cuando una empresa no tiene suficientes datos, tiempo o recursos para ajustar un LLM, pero a칰n desea mejorar el rendimiento en una carga de trabajo espec칤fica y reducir los riesgos de fabricaciones, es decir, mistificaci칩n de la realidad o contenido da침ino.

### Modelo Ajustado

El ajuste es un proceso que aprovecha el aprendizaje por transferencia para 'adaptar' el modelo a una tarea descendente o para resolver un problema espec칤fico. A diferencia del aprendizaje "few-shot" y RAG, resulta en un nuevo modelo generado, con pesos y sesgos actualizados. Requiere un conjunto de ejemplos de entrenamiento que consisten en una sola entrada (el prompt) y su salida asociada (la finalizaci칩n). Este ser칤a el enfoque preferido si:

- **Usar modelos ajustados**. Una empresa quisiera usar modelos ajustados menos capaces (como modelos de incrustaci칩n) en lugar de modelos de alto rendimiento, resultando en una soluci칩n m치s rentable y r치pida.

- **Considerar la latencia**. La latencia es importante para un caso de uso espec칤fico, por lo que no es posible usar prompts muy largos o el n칰mero de ejemplos que deber칤an aprenderse del modelo no encaja con el l칤mite de longitud del prompt.

- **Mantenerse actualizado**. Una empresa tiene muchos datos de alta calidad y etiquetas de verdad de base y los recursos necesarios para mantener estos datos actualizados a lo largo del tiempo.

### Modelo Entrenado

Entrenar un LLM desde cero es sin duda el enfoque m치s dif칤cil y complejo de adoptar, requiriendo enormes cantidades de datos, recursos calificados y la potencia computacional adecuada. Esta opci칩n solo deber칤a considerarse en un escenario donde una empresa tenga un caso de uso espec칤fico de dominio y una gran cantidad de datos centrados en el dominio.

## Comprobaci칩n de Conocimientos

쯈u칠 podr칤a ser un buen enfoque para mejorar los resultados de finalizaci칩n de LLM?

1. Ingenier칤a de prompts con contexto
1. RAG
1. Modelo ajustado

A:3, si tienes el tiempo y los recursos y datos de alta calidad, el ajuste es la mejor opci칩n para mantenerse actualizado. Sin embargo, si est치s buscando mejorar las cosas y te falta tiempo, vale la pena considerar primero RAG.

## 游 Desaf칤o

Lee m치s sobre c칩mo puedes [usar RAG](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview?WT.mc_id=academic-105485-koreyst) para tu negocio.

## Gran Trabajo, Contin칰a Tu Aprendizaje

Despu칠s de completar esta lecci칩n, consulta nuestra [colecci칩n de Aprendizaje de IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para continuar mejorando tu conocimiento de IA Generativa.

Dir칤gete a la Lecci칩n 3 donde veremos c칩mo [construir con IA Generativa de manera Responsable](../03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)!

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci칩n autom치tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por lograr precisi칩n, tenga en cuenta que las traducciones autom치ticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci칩n cr칤tica, se recomienda la traducci칩n profesional humana. No nos hacemos responsables de ning칰n malentendido o interpretaci칩n err칩nea que surja del uso de esta traducci칩n.