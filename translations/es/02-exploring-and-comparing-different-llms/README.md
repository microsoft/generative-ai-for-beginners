<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6b7629b8ee4d7d874a27213e903d86a7",
  "translation_date": "2025-10-17T22:48:15+00:00",
  "source_file": "02-exploring-and-comparing-different-llms/README.md",
  "language_code": "es"
}
-->
# Explorando y comparando diferentes LLMs

[![Explorando y comparando diferentes LLMs](../../../translated_images/02-lesson-banner.ef94c84979f97f60f07e27d905e708cbcbdf78707120553ccab27d91c947805b.es.png)](https://youtu.be/KIRUeDKscfI?si=8BHX1zvwzQBn-PlK)

> _Haz clic en la imagen de arriba para ver el video de esta lecci√≥n_

En la lecci√≥n anterior, vimos c√≥mo la IA generativa est√° transformando el panorama tecnol√≥gico, c√≥mo funcionan los Modelos de Lenguaje Extenso (LLMs) y c√≥mo una empresa, como nuestra startup, puede aplicarlos a sus casos de uso y crecer. En este cap√≠tulo, vamos a comparar y contrastar diferentes tipos de modelos de lenguaje extenso (LLMs) para entender sus ventajas y desventajas.

El siguiente paso en el camino de nuestra startup es explorar el panorama actual de los LLMs y entender cu√°les son adecuados para nuestro caso de uso.

## Introducci√≥n

Esta lecci√≥n cubrir√°:

- Diferentes tipos de LLMs en el panorama actual.
- Probar, iterar y comparar diferentes modelos para tu caso de uso en Azure.
- C√≥mo implementar un LLM.

## Objetivos de aprendizaje

Despu√©s de completar esta lecci√≥n, podr√°s:

- Seleccionar el modelo adecuado para tu caso de uso.
- Entender c√≥mo probar, iterar y mejorar el rendimiento de tu modelo.
- Saber c√≥mo las empresas implementan modelos.

## Entender los diferentes tipos de LLMs

Los LLMs pueden clasificarse de varias maneras seg√∫n su arquitectura, datos de entrenamiento y caso de uso. Entender estas diferencias ayudar√° a nuestra startup a seleccionar el modelo adecuado para el escenario y a comprender c√≥mo probar, iterar y mejorar el rendimiento.

Existen muchos tipos diferentes de modelos LLM, y tu elecci√≥n depende de para qu√© planeas usarlos, tus datos, cu√°nto est√°s dispuesto a pagar y m√°s.

Dependiendo de si planeas usar los modelos para texto, audio, video, generaci√≥n de im√°genes, etc., podr√≠as optar por un tipo diferente de modelo.

- **Reconocimiento de audio y voz**. Para este prop√≥sito, los modelos tipo Whisper son una excelente opci√≥n, ya que son de prop√≥sito general y est√°n orientados al reconocimiento de voz. Est√°n entrenados en audio diverso y pueden realizar reconocimiento de voz multiling√ºe. Aprende m√°s sobre [modelos tipo Whisper aqu√≠](https://platform.openai.com/docs/models/whisper?WT.mc_id=academic-105485-koreyst).

- **Generaci√≥n de im√°genes**. Para la generaci√≥n de im√°genes, DALL-E y Midjourney son dos opciones muy conocidas. DALL-E es ofrecido por Azure OpenAI. [Lee m√°s sobre DALL-E aqu√≠](https://platform.openai.com/docs/models/dall-e?WT.mc_id=academic-105485-koreyst) y tambi√©n en el Cap√≠tulo 9 de este curr√≠culo.

- **Generaci√≥n de texto**. La mayor√≠a de los modelos est√°n entrenados en generaci√≥n de texto y tienes una gran variedad de opciones desde GPT-3.5 hasta GPT-4. Tienen diferentes costos, siendo GPT-4 el m√°s caro. Vale la pena explorar el [Azure OpenAI playground](https://oai.azure.com/portal/playground?WT.mc_id=academic-105485-koreyst) para evaluar qu√© modelos se ajustan mejor a tus necesidades en t√©rminos de capacidad y costo.

- **Multimodalidad**. Si buscas manejar m√∫ltiples tipos de datos en entrada y salida, podr√≠as considerar modelos como [gpt-4 turbo con visi√≥n o gpt-4o](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models?WT.mc_id=academic-105485-koreyst), las √∫ltimas versiones de los modelos de OpenAI, que son capaces de combinar el procesamiento de lenguaje natural con la comprensi√≥n visual, permitiendo interacciones a trav√©s de interfaces multimodales.

Seleccionar un modelo significa obtener algunas capacidades b√°sicas, que pueden no ser suficientes. A menudo tienes datos espec√≠ficos de la empresa que de alguna manera necesitas comunicar al LLM. Hay varias opciones sobre c√≥mo abordar esto, m√°s sobre esto en las pr√≥ximas secciones.

### Modelos Fundamentales versus LLMs

El t√©rmino Modelo Fundamental fue [acu√±ado por investigadores de Stanford](https://arxiv.org/abs/2108.07258?WT.mc_id=academic-105485-koreyst) y se define como un modelo de IA que sigue ciertos criterios, como:

- **Est√°n entrenados utilizando aprendizaje no supervisado o aprendizaje auto-supervisado**, lo que significa que est√°n entrenados con datos multimodales no etiquetados y no requieren anotaciones o etiquetado humano de datos para su proceso de entrenamiento.
- **Son modelos muy grandes**, basados en redes neuronales muy profundas entrenadas con miles de millones de par√°metros.
- **Normalmente est√°n destinados a servir como una ‚Äòbase‚Äô para otros modelos**, lo que significa que pueden usarse como punto de partida para construir otros modelos encima, lo cual puede hacerse mediante ajuste fino.

![Modelos Fundamentales versus LLMs](../../../translated_images/FoundationModel.e4859dbb7a825c94b284f17eae1c186aabc21d4d8644331f5b007d809cf8d0f2.es.png)

Fuente de la imagen: [Essential Guide to Foundation Models and Large Language Models | por Babar M Bhatti | Medium
](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404)

Para aclarar m√°s esta distinci√≥n, tomemos ChatGPT como ejemplo. Para construir la primera versi√≥n de ChatGPT, se utiliz√≥ un modelo llamado GPT-3.5 como modelo fundamental. Esto significa que OpenAI utiliz√≥ algunos datos espec√≠ficos de chat para crear una versi√≥n ajustada de GPT-3.5 que estaba especializada en desempe√±arse bien en escenarios conversacionales, como los chatbots.

![Modelo Fundamental](../../../translated_images/Multimodal.2c389c6439e0fc51b0b7b226d95d7d900d372ae66902d71b8ce5ec4951b8efbe.es.png)

Fuente de la imagen: [2108.07258.pdf (arxiv.org)](https://arxiv.org/pdf/2108.07258.pdf?WT.mc_id=academic-105485-koreyst)

### Modelos de c√≥digo abierto versus modelos propietarios

Otra forma de categorizar los LLMs es si son de c√≥digo abierto o propietarios.

Los modelos de c√≥digo abierto son modelos que est√°n disponibles para el p√∫blico y pueden ser utilizados por cualquier persona. A menudo son puestos a disposici√≥n por la empresa que los cre√≥ o por la comunidad investigadora. Estos modelos pueden ser inspeccionados, modificados y personalizados para los diversos casos de uso en LLMs. Sin embargo, no siempre est√°n optimizados para uso en producci√≥n y pueden no ser tan eficientes como los modelos propietarios. Adem√°s, la financiaci√≥n para los modelos de c√≥digo abierto puede ser limitada y es posible que no se mantengan a largo plazo o que no se actualicen con las √∫ltimas investigaciones. Ejemplos de modelos populares de c√≥digo abierto incluyen [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html?WT.mc_id=academic-105485-koreyst), [Bloom](https://huggingface.co/bigscience/bloom) y [LLaMA](https://llama.meta.com).

Los modelos propietarios son modelos que son propiedad de una empresa y no est√°n disponibles para el p√∫blico. Estos modelos suelen estar optimizados para uso en producci√≥n. Sin embargo, no se permite que sean inspeccionados, modificados o personalizados para diferentes casos de uso. Adem√°s, no siempre est√°n disponibles de forma gratuita y pueden requerir una suscripci√≥n o pago para su uso. Tambi√©n, los usuarios no tienen control sobre los datos que se utilizan para entrenar el modelo, lo que significa que deben confiar en el propietario del modelo para garantizar el compromiso con la privacidad de los datos y el uso responsable de la IA. Ejemplos de modelos propietarios populares incluyen [modelos de OpenAI](https://platform.openai.com/docs/models/overview?WT.mc_id=academic-105485-koreyst), [Google Bard](https://sapling.ai/llm/bard?WT.mc_id=academic-105485-koreyst) o [Claude 2](https://www.anthropic.com/index/claude-2?WT.mc_id=academic-105485-koreyst).

### Embedding versus generaci√≥n de im√°genes versus generaci√≥n de texto y c√≥digo

Los LLMs tambi√©n pueden categorizarse seg√∫n el tipo de salida que generan.

Los embeddings son un conjunto de modelos que pueden convertir texto en una forma num√©rica, llamada embedding, que es una representaci√≥n num√©rica del texto de entrada. Los embeddings facilitan que las m√°quinas entiendan las relaciones entre palabras o frases y pueden ser consumidos como entradas por otros modelos, como modelos de clasificaci√≥n o modelos de agrupamiento que tienen mejor rendimiento con datos num√©ricos. Los modelos de embedding se utilizan a menudo para el aprendizaje por transferencia, donde se construye un modelo para una tarea sustituta para la cual hay una abundancia de datos, y luego los pesos del modelo (embeddings) se reutilizan para otras tareas posteriores. Un ejemplo de esta categor√≠a es [OpenAI embeddings](https://platform.openai.com/docs/models/embeddings?WT.mc_id=academic-105485-koreyst).

![Embedding](../../../translated_images/Embedding.c3708fe988ccf76073d348483dbb7569f622211104f073e22e43106075c04800.es.png)

Los modelos de generaci√≥n de im√°genes son modelos que generan im√°genes. Estos modelos se utilizan a menudo para edici√≥n de im√°genes, s√≠ntesis de im√°genes y traducci√≥n de im√°genes. Los modelos de generaci√≥n de im√°genes suelen estar entrenados en grandes conjuntos de datos de im√°genes, como [LAION-5B](https://laion.ai/blog/laion-5b/?WT.mc_id=academic-105485-koreyst), y pueden usarse para generar nuevas im√°genes o para editar im√°genes existentes con t√©cnicas de inpainting, super-resoluci√≥n y colorizaci√≥n. Ejemplos incluyen [DALL-E-3](https://openai.com/dall-e-3?WT.mc_id=academic-105485-koreyst) y [modelos de Stable Diffusion](https://github.com/Stability-AI/StableDiffusion?WT.mc_id=academic-105485-koreyst).

![Generaci√≥n de im√°genes](../../../translated_images/Image.349c080266a763fd255b840a921cd8fc526ed78dc58708fa569ff1873d302345.es.png)

Los modelos de generaci√≥n de texto y c√≥digo son modelos que generan texto o c√≥digo. Estos modelos se utilizan a menudo para resumen de texto, traducci√≥n y respuesta a preguntas. Los modelos de generaci√≥n de texto suelen estar entrenados en grandes conjuntos de datos de texto, como [BookCorpus](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html?WT.mc_id=academic-105485-koreyst), y pueden usarse para generar nuevo texto o para responder preguntas. Los modelos de generaci√≥n de c√≥digo, como [CodeParrot](https://huggingface.co/codeparrot?WT.mc_id=academic-105485-koreyst), suelen estar entrenados en grandes conjuntos de datos de c√≥digo, como GitHub, y pueden usarse para generar nuevo c√≥digo o para corregir errores en c√≥digo existente.

![Generaci√≥n de texto y c√≥digo](../../../translated_images/Text.a8c0cf139e5cc2a0cd3edaba8d675103774e6ddcb3c9fc5a98bb17c9a450e31d.es.png)

### Encoder-Decoder versus solo Decoder

Para hablar sobre los diferentes tipos de arquitecturas de LLMs, usemos una analog√≠a.

Imagina que tu gerente te dio la tarea de escribir un cuestionario para los estudiantes. Tienes dos colegas; uno se encarga de crear el contenido y el otro de revisarlo.

El creador de contenido es como un modelo solo Decoder, puede mirar el tema y ver lo que ya escribiste y luego escribir un curso basado en eso. Son muy buenos escribiendo contenido atractivo e informativo, pero no son muy buenos entendiendo el tema y los objetivos de aprendizaje. Algunos ejemplos de modelos Decoder son los modelos de la familia GPT, como GPT-3.

El revisor es como un modelo solo Encoder, mira el curso escrito y las respuestas, notando la relaci√≥n entre ellos y entendiendo el contexto, pero no es bueno generando contenido. Un ejemplo de modelo solo Encoder ser√≠a BERT.

Imagina que tambi√©n podemos tener a alguien que pueda crear y revisar el cuestionario, este es un modelo Encoder-Decoder. Algunos ejemplos ser√≠an BART y T5.

### Servicio versus Modelo

Ahora, hablemos de la diferencia entre un servicio y un modelo. Un servicio es un producto que ofrece un proveedor de servicios en la nube y suele ser una combinaci√≥n de modelos, datos y otros componentes. Un modelo es el componente central de un servicio y suele ser un modelo fundamental, como un LLM.

Los servicios suelen estar optimizados para uso en producci√≥n y son m√°s f√°ciles de usar que los modelos, a trav√©s de una interfaz gr√°fica de usuario. Sin embargo, los servicios no siempre est√°n disponibles de forma gratuita y pueden requerir una suscripci√≥n o pago para su uso, a cambio de aprovechar el equipo y los recursos del propietario del servicio, optimizando gastos y escalando f√°cilmente. Un ejemplo de servicio es [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=academic-105485-koreyst), que ofrece un plan de tarifas de pago por uso, lo que significa que los usuarios son cobrados proporcionalmente a cu√°nto usan el servicio. Adem√°s, Azure OpenAI Service ofrece seguridad de nivel empresarial y un marco de IA responsable sobre las capacidades de los modelos.

Los modelos son solo la red neuronal, con los par√°metros, pesos y otros. Permiten a las empresas ejecutarlos localmente, aunque necesitar√≠an comprar equipos, construir una estructura para escalar y comprar una licencia o usar un modelo de c√≥digo abierto. Un modelo como LLaMA est√° disponible para ser usado, requiriendo poder computacional para ejecutar el modelo.

## C√≥mo probar e iterar con diferentes modelos para entender el rendimiento en Azure

Una vez que nuestro equipo ha explorado el panorama actual de los LLMs e identificado algunos buenos candidatos para sus escenarios, el siguiente paso es probarlos con sus datos y su carga de trabajo. Este es un proceso iterativo, realizado mediante experimentos y mediciones.
La mayor√≠a de los modelos que mencionamos en los p√°rrafos anteriores (modelos de OpenAI, modelos de c√≥digo abierto como Llama2 y transformadores de Hugging Face) est√°n disponibles en el [Cat√°logo de Modelos](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview?WT.mc_id=academic-105485-koreyst) en [Azure AI Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreyst).

[Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/what-is-ai-studio?WT.mc_id=academic-105485-koreyst) es una plataforma en la nube dise√±ada para que los desarrolladores construyan aplicaciones de IA generativa y gestionen todo el ciclo de desarrollo, desde la experimentaci√≥n hasta la evaluaci√≥n, combinando todos los servicios de Azure AI en un √∫nico centro con una interfaz gr√°fica f√°cil de usar. El Cat√°logo de Modelos en Azure AI Studio permite al usuario:

- Encontrar el modelo base de inter√©s en el cat√°logo, ya sea propietario o de c√≥digo abierto, filtrando por tarea, licencia o nombre. Para mejorar la b√∫squeda, los modelos est√°n organizados en colecciones, como la colecci√≥n Azure OpenAI, la colecci√≥n Hugging Face y m√°s.

![Cat√°logo de modelos](../../../translated_images/AzureAIStudioModelCatalog.3cf8a499aa8ba0314f2c73d4048b3225d324165f547525f5b7cfa5f6c9c68941.es.png)

- Revisar la tarjeta del modelo, que incluye una descripci√≥n detallada del uso previsto y los datos de entrenamiento, ejemplos de c√≥digo y resultados de evaluaci√≥n en la biblioteca interna de evaluaciones.

![Tarjeta del modelo](../../../translated_images/ModelCard.598051692c6e400d681a713ba7717e8b6e5e65f08d12131556fcec0f1789459b.es.png)

- Comparar benchmarks entre modelos y conjuntos de datos disponibles en la industria para evaluar cu√°l se adapta mejor al escenario empresarial, a trav√©s del panel de [Benchmarks de Modelos](https://learn.microsoft.com/azure/ai-studio/how-to/model-benchmarks?WT.mc_id=academic-105485-koreyst).

![Benchmarks de modelos](../../../translated_images/ModelBenchmarks.254cb20fbd06c03a4ca53994585c5ea4300a88bcec8eff0450f2866ee2ac5ff3.es.png)

- Ajustar el modelo con datos de entrenamiento personalizados para mejorar el rendimiento del modelo en una carga de trabajo espec√≠fica, aprovechando las capacidades de experimentaci√≥n y seguimiento de Azure AI Studio.

![Ajuste del modelo](../../../translated_images/FineTuning.aac48f07142e36fddc6571b1f43ea2e003325c9c6d8e3fc9d8834b771e308dbf.es.png)

- Desplegar el modelo preentrenado original o la versi√≥n ajustada para una inferencia en tiempo real en un entorno de c√≥mputo gestionado o en un endpoint de API sin servidor - [pago por uso](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview#model-deployment-managed-compute-and-serverless-api-pay-as-you-go?WT.mc_id=academic-105485-koreyst) - para habilitar su consumo en aplicaciones.

![Despliegue del modelo](../../../translated_images/ModelDeploy.890da48cbd0bccdb4abfc9257f3d884831e5d41b723e7d1ceeac9d60c3c4f984.es.png)

> [!NOTE]
> No todos los modelos en el cat√°logo est√°n disponibles actualmente para ajuste y/o despliegue bajo el modelo de pago por uso. Consulta la tarjeta del modelo para obtener detalles sobre las capacidades y limitaciones del modelo.

## Mejorando los resultados de los LLM

Hemos explorado con nuestro equipo de startups diferentes tipos de LLMs y una plataforma en la nube (Azure Machine Learning) que nos permite comparar diferentes modelos, evaluarlos con datos de prueba, mejorar su rendimiento y desplegarlos en endpoints de inferencia.

Pero, ¬øcu√°ndo deber√≠an considerar ajustar un modelo en lugar de usar uno preentrenado? ¬øExisten otros enfoques para mejorar el rendimiento del modelo en cargas de trabajo espec√≠ficas?

Hay varios enfoques que una empresa puede utilizar para obtener los resultados que necesita de un LLM. Puedes seleccionar diferentes tipos de modelos con distintos grados de entrenamiento al desplegar un LLM en producci√≥n, con diferentes niveles de complejidad, costo y calidad. Aqu√≠ hay algunos enfoques diferentes:

- **Ingenier√≠a de prompts con contexto**. La idea es proporcionar suficiente contexto al realizar el prompt para garantizar que obtengas las respuestas que necesitas.

- **Generaci√≥n Aumentada por Recuperaci√≥n, RAG**. Tus datos podr√≠an estar en una base de datos o un endpoint web, por ejemplo. Para garantizar que estos datos, o un subconjunto de ellos, se incluyan al momento de realizar el prompt, puedes recuperar los datos relevantes y hacer que formen parte del prompt del usuario.

- **Modelo ajustado**. Aqu√≠, entrenas el modelo adicionalmente con tus propios datos, lo que hace que el modelo sea m√°s preciso y responda mejor a tus necesidades, aunque puede ser costoso.

![Despliegue de LLMs](../../../translated_images/Deploy.18b2d27412ec8c02871386cbe91097c7f2190a8c6e2be88f66392b411609a48c.es.png)

Fuente de la imagen: [Cuatro formas en que las empresas despliegan LLMs | Blog de Fiddler AI](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)

### Ingenier√≠a de Prompts con Contexto

Los LLM preentrenados funcionan muy bien en tareas generales de lenguaje natural, incluso llam√°ndolos con un prompt corto, como una oraci√≥n para completar o una pregunta: el llamado aprendizaje "zero-shot".

Sin embargo, cuanto m√°s pueda el usuario enmarcar su consulta, con una solicitud detallada y ejemplos - el Contexto - m√°s precisa y cercana a las expectativas del usuario ser√° la respuesta. En este caso, hablamos de aprendizaje "one-shot" si el prompt incluye solo un ejemplo y de "few-shot learning" si incluye m√∫ltiples ejemplos. La ingenier√≠a de prompts con contexto es el enfoque m√°s rentable para comenzar.

### Generaci√≥n Aumentada por Recuperaci√≥n (RAG)

Los LLMs tienen la limitaci√≥n de que solo pueden usar los datos que se han utilizado durante su entrenamiento para generar una respuesta. Esto significa que no saben nada sobre los hechos que ocurrieron despu√©s de su proceso de entrenamiento y no pueden acceder a informaci√≥n no p√∫blica (como datos de la empresa).  
Esto se puede superar mediante RAG, una t√©cnica que aumenta el prompt con datos externos en forma de fragmentos de documentos, considerando los l√≠mites de longitud del prompt. Esto es compatible con herramientas de bases de datos vectoriales (como [Azure Vector Search](https://learn.microsoft.com/azure/search/vector-search-overview?WT.mc_id=academic-105485-koreyst)) que recuperan los fragmentos √∫tiles de diversas fuentes de datos predefinidas y los agregan al Contexto del prompt.

Esta t√©cnica es muy √∫til cuando una empresa no tiene suficientes datos, tiempo o recursos para ajustar un LLM, pero a√∫n desea mejorar el rendimiento en una carga de trabajo espec√≠fica y reducir los riesgos de fabricaciones, es decir, distorsi√≥n de la realidad o contenido da√±ino.

### Modelo ajustado

El ajuste es un proceso que aprovecha el aprendizaje por transferencia para "adaptar" el modelo a una tarea espec√≠fica o resolver un problema concreto. A diferencia del aprendizaje de pocos ejemplos y RAG, resulta en la generaci√≥n de un nuevo modelo, con pesos y sesgos actualizados. Requiere un conjunto de ejemplos de entrenamiento que consisten en una √∫nica entrada (el prompt) y su salida asociada (la respuesta).  
Este ser√≠a el enfoque preferido si:

- **Uso de modelos ajustados**. Una empresa desea utilizar modelos ajustados menos capaces (como modelos de embeddings) en lugar de modelos de alto rendimiento, lo que resulta en una soluci√≥n m√°s rentable y r√°pida.

- **Considerando la latencia**. La latencia es importante para un caso de uso espec√≠fico, por lo que no es posible usar prompts muy largos o el n√∫mero de ejemplos que deber√≠an ser aprendidos por el modelo no encaja con el l√≠mite de longitud del prompt.

- **Mantenerse actualizado**. Una empresa tiene muchos datos de alta calidad y etiquetas de verdad base, as√≠ como los recursos necesarios para mantener estos datos actualizados con el tiempo.

### Modelo entrenado

Entrenar un LLM desde cero es, sin duda, el enfoque m√°s dif√≠cil y complejo de adoptar, ya que requiere cantidades masivas de datos, recursos calificados y potencia computacional adecuada. Esta opci√≥n solo deber√≠a considerarse en un escenario donde una empresa tenga un caso de uso espec√≠fico para un dominio y una gran cantidad de datos centrados en ese dominio.

## Comprobaci√≥n de conocimientos

¬øCu√°l podr√≠a ser un buen enfoque para mejorar los resultados de las respuestas de un LLM?

1. Ingenier√≠a de prompts con contexto  
1. RAG  
1. Modelo ajustado  

A:3, si tienes el tiempo, los recursos y datos de alta calidad, el ajuste es la mejor opci√≥n para mantenerse actualizado. Sin embargo, si est√°s buscando mejorar las cosas y te falta tiempo, vale la pena considerar primero RAG.

## üöÄ Desaf√≠o

Investiga m√°s sobre c√≥mo puedes [usar RAG](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview?WT.mc_id=academic-105485-koreyst) para tu negocio.

## ¬°Buen trabajo! Contin√∫a aprendiendo

Despu√©s de completar esta lecci√≥n, consulta nuestra [colecci√≥n de aprendizaje sobre IA generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para seguir ampliando tus conocimientos sobre IA generativa.

Dir√≠gete a la Lecci√≥n 3, donde veremos c√≥mo [construir con IA generativa de manera responsable](../03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst).

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que surjan del uso de esta traducci√≥n.