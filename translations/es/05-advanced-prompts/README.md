<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b2651fb16bcfbc62b8e518751ed90fdb",
  "translation_date": "2025-10-17T22:44:50+00:00",
  "source_file": "05-advanced-prompts/README.md",
  "language_code": "es"
}
-->
# Creando prompts avanzados

[![Creando prompts avanzados](../../../translated_images/05-lesson-banner.522610fd4a2cd82dbed66bb7e6fe104ed6da172e085dbb4d9100b28dc73ed435.es.png)](https://youtu.be/BAjzkaCdRok?si=NmUIyRf7-cDgbjtt)

Repasemos algunos aprendizajes del cap√≠tulo anterior:

> La _ingenier√≠a de prompts_ es el proceso mediante el cual **guiamos al modelo hacia respuestas m√°s relevantes** proporcionando instrucciones o contexto m√°s √∫tiles.

Tambi√©n hay dos pasos para escribir prompts: construir el prompt, proporcionando un contexto relevante, y la _optimizaci√≥n_, que consiste en mejorar gradualmente el prompt.

En este punto, tenemos una comprensi√≥n b√°sica de c√≥mo escribir prompts, pero necesitamos profundizar m√°s. En este cap√≠tulo, pasar√°s de probar varios prompts a entender por qu√© un prompt es mejor que otro. Aprender√°s c√≥mo construir prompts siguiendo algunas t√©cnicas b√°sicas que se pueden aplicar a cualquier LLM.

## Introducci√≥n

En este cap√≠tulo, cubriremos los siguientes temas:

- Ampliar tu conocimiento sobre ingenier√≠a de prompts aplicando diferentes t√©cnicas a tus prompts.
- Configurar tus prompts para variar el resultado.

## Objetivos de aprendizaje

Despu√©s de completar esta lecci√≥n, podr√°s:

- Aplicar t√©cnicas de ingenier√≠a de prompts que mejoren el resultado de tus prompts.
- Realizar prompts que sean variados o deterministas.

## Ingenier√≠a de prompts

La ingenier√≠a de prompts es el proceso de crear prompts que produzcan el resultado deseado. Hay m√°s en la ingenier√≠a de prompts que simplemente escribir un texto. La ingenier√≠a de prompts no es una disciplina de ingenier√≠a, es m√°s bien un conjunto de t√©cnicas que puedes aplicar para obtener el resultado deseado.

### Un ejemplo de un prompt

Tomemos un prompt b√°sico como este:

> Genera 10 preguntas sobre geograf√≠a.

En este prompt, en realidad est√°s aplicando un conjunto de diferentes t√©cnicas de prompts.

Desglos√©moslo.

- **Contexto**, especificas que debe ser sobre "geograf√≠a".
- **Limitar el resultado**, quieres no m√°s de 10 preguntas.

### Limitaciones de los prompts simples

Es posible que obtengas o no el resultado deseado. Se generar√°n tus preguntas, pero la geograf√≠a es un tema amplio y es posible que no obtengas lo que deseas debido a las siguientes razones:

- **Tema amplio**, no sabes si ser√° sobre pa√≠ses, capitales, r√≠os, etc.
- **Formato**, ¬øqu√© pasa si quer√≠as que las preguntas tuvieran un formato espec√≠fico?

Como puedes ver, hay mucho que considerar al crear prompts.

Hasta ahora, hemos visto un ejemplo de prompt simple, pero la IA generativa es capaz de mucho m√°s para ayudar a las personas en una variedad de roles e industrias. Exploremos algunas t√©cnicas b√°sicas a continuaci√≥n.

### T√©cnicas para crear prompts

Primero, necesitamos entender que la creaci√≥n de prompts es una propiedad _emergente_ de un LLM, lo que significa que no es una caracter√≠stica incorporada en el modelo, sino algo que descubrimos al usar el modelo.

Hay algunas t√©cnicas b√°sicas que podemos usar para crear prompts en un LLM. Vamos a explorarlas.

- **Prompt sin ejemplos (Zero-shot prompting)**, esta es la forma m√°s b√°sica de crear prompts. Es un √∫nico prompt que solicita una respuesta del LLM basada √∫nicamente en sus datos de entrenamiento.
- **Prompt con pocos ejemplos (Few-shot prompting)**, este tipo de prompt gu√≠a al LLM proporcionando uno o m√°s ejemplos en los que puede basarse para generar su respuesta.
- **Cadena de pensamiento (Chain-of-thought)**, este tipo de prompt indica al LLM c√≥mo descomponer un problema en pasos.
- **Conocimiento generado**, para mejorar la respuesta de un prompt, puedes proporcionar hechos o conocimientos generados adicionalmente a tu prompt.
- **De menos a m√°s (Least to most)**, al igual que la cadena de pensamiento, esta t√©cnica consiste en descomponer un problema en una serie de pasos y luego pedir que se realicen en orden.
- **Auto-refinamiento (Self-refine)**, esta t√©cnica consiste en criticar la salida del LLM y luego pedirle que la mejore.
- **Prompt may√©utico (Maieutic prompting)**. Aqu√≠ lo que quieres es asegurarte de que la respuesta del LLM sea correcta y le pides que explique varias partes de la respuesta. Esta es una forma de auto-refinamiento.

### Prompt sin ejemplos (Zero-shot prompting)

Este estilo de prompt es muy simple, consiste en un √∫nico prompt. Esta t√©cnica es probablemente la que est√°s utilizando mientras comienzas a aprender sobre los LLM. Aqu√≠ hay un ejemplo:

- Prompt: "¬øQu√© es el √°lgebra?"
- Respuesta: "El √°lgebra es una rama de las matem√°ticas que estudia los s√≠mbolos matem√°ticos y las reglas para manipular estos s√≠mbolos."

### Prompt con pocos ejemplos (Few-shot prompting)

Este estilo de prompt ayuda al modelo proporcionando algunos ejemplos junto con la solicitud. Consiste en un √∫nico prompt con datos adicionales espec√≠ficos de la tarea. Aqu√≠ hay un ejemplo:

- Prompt: "Escribe un poema al estilo de Shakespeare. Aqu√≠ hay algunos ejemplos de sonetos de Shakespeare:
  Soneto 18: '¬øCompararte con un d√≠a de verano? Eres m√°s hermoso y m√°s templado...'
  Soneto 116: 'No permitas que el matrimonio de mentes verdaderas admita impedimentos. El amor no es amor que cambia cuando encuentra cambio...'
  Soneto 132: 'Tus ojos amo, y ellos, como compadeci√©ndome, sabiendo que tu coraz√≥n me atormenta con desd√©n,...'
  Ahora, escribe un soneto sobre la belleza de la luna."
- Respuesta: "Sobre el cielo, la luna suavemente brilla, En luz plateada que lanza su gentil gracia,..."

Los ejemplos proporcionan al LLM el contexto, formato o estilo del resultado deseado. Ayudan al modelo a entender la tarea espec√≠fica y generar respuestas m√°s precisas y relevantes.

### Cadena de pensamiento (Chain-of-thought)

La cadena de pensamiento es una t√©cnica muy interesante ya que se trata de llevar al LLM a trav√©s de una serie de pasos. La idea es instruir al LLM de tal manera que entienda c√≥mo hacer algo. Considera el siguiente ejemplo, con y sin cadena de pensamiento:

    - Prompt: "Alice tiene 5 manzanas, tira 3 manzanas, da 2 a Bob y Bob le devuelve una, ¬øcu√°ntas manzanas tiene Alice?"
    - Respuesta: 5

El LLM responde con 5, lo cual es incorrecto. La respuesta correcta es 1 manzana, dado el c√°lculo (5 -3 -2 + 1 = 1).

Entonces, ¬øc√≥mo podemos ense√±ar al LLM a hacer esto correctamente?

Intentemos la cadena de pensamiento. Aplicar la cadena de pensamiento significa:

1. Dar al LLM un ejemplo similar.
1. Mostrar el c√°lculo y c√≥mo calcularlo correctamente.
1. Proporcionar el prompt original.

As√≠ es como se hace:

- Prompt: "Lisa tiene 7 manzanas, tira 1 manzana, da 4 manzanas a Bart y Bart le devuelve una:
  7 -1 = 6
  6 -4 = 2
  2 +1 = 3  
  Alice tiene 5 manzanas, tira 3 manzanas, da 2 a Bob y Bob le devuelve una, ¬øcu√°ntas manzanas tiene Alice?"
  Respuesta: 1

Nota c√≥mo escribimos prompts sustancialmente m√°s largos con otro ejemplo, un c√°lculo y luego el prompt original, y llegamos a la respuesta correcta: 1.

Como puedes ver, la cadena de pensamiento es una t√©cnica muy poderosa.

### Conocimiento generado

Muchas veces, cuando quieres construir un prompt, quieres hacerlo utilizando los datos de tu propia empresa. Quieres que parte del prompt provenga de la empresa y la otra parte sea el prompt que te interesa.

Como ejemplo, as√≠ es como podr√≠a verse tu prompt si est√°s en el negocio de seguros:

```text
{{company}}: {{company_name}}
{{products}}:
{{products_list}}
Please suggest an insurance given the following budget and requirements:
Budget: {{budget}}
Requirements: {{requirements}}
```

Arriba, puedes ver c√≥mo se construye el prompt utilizando una plantilla. En la plantilla hay una serie de variables, denotadas por `{{variable}}`, que ser√°n reemplazadas con valores reales de una API de la empresa.

Aqu√≠ hay un ejemplo de c√≥mo podr√≠a verse el prompt una vez que las variables han sido reemplazadas por contenido de tu empresa:

```text
Insurance company: ACME Insurance
Insurance products (cost per month):
- Car, cheap, 500 USD
- Car, expensive, 1100 USD
- Home, cheap, 600 USD
- Home, expensive, 1200 USD
- Life, cheap, 100 USD

Please suggest an insurance given the following budget and requirements:
Budget: $1000
Requirements: Car, Home, and Life insurance
```

Ejecutar este prompt a trav√©s de un LLM producir√° una respuesta como esta:

```output
Given the budget and requirements, we suggest the following insurance package from ACME Insurance:
- Car, cheap, 500 USD
- Home, cheap, 600 USD
- Life, cheap, 100 USD
Total cost: $1,200 USD
```

Como puedes ver, tambi√©n sugiere el seguro de vida, lo cual no deber√≠a. Este resultado es una indicaci√≥n de que necesitamos optimizar el prompt cambi√°ndolo para que sea m√°s claro sobre lo que puede permitir. Despu√©s de algunos _ensayos y errores_, llegamos al siguiente prompt:

```text
Insurance company: ACME Insurance
Insurance products (cost per month):
- type: Car, cheap, cost: 500 USD
- type: Car, expensive, cost: 1100 USD
- type: Home, cheap, cost: 600 USD
- type: Home, expensive, cost: 1200 USD
- type: Life, cheap, cost: 100 USD

Please suggest an insurance given the following budget and requirements:
Budget: $1000 restrict choice to types: Car, Home
```

Nota c√≥mo agregar _tipo_ y _costo_ y tambi√©n usar la palabra clave _restringir_ ayuda al LLM a entender lo que queremos.

Ahora obtenemos la siguiente respuesta:

```output
Given the budget and requirements, we suggest the Car, Cheap insurance product which costs 500 USD per month.
```

El objetivo de este ejemplo era mostrar que, aunque estamos utilizando una t√©cnica b√°sica como el _conocimiento generado_, a√∫n necesitamos optimizar el prompt en la mayor√≠a de los casos para obtener el resultado deseado.

### De menos a m√°s (Least-to-most)

La idea del prompting de menos a m√°s es descomponer un problema m√°s grande en subproblemas. De esa manera, ayudas a guiar al LLM sobre c√≥mo "resolver" el problema m√°s grande. Un buen ejemplo podr√≠a ser en ciencia de datos, donde puedes pedirle al LLM que divida un problema de esta manera:

> Prompt: ¬øC√≥mo realizar ciencia de datos en 5 pasos?

Con tu asistente de IA respondiendo con:

1. Recopilar datos
1. Limpiar datos
1. Analizar datos
1. Graficar datos
1. Presentar datos

### Auto-refinamiento, criticar los resultados

Con las IAs generativas y los LLMs, no puedes confiar ciegamente en el resultado. Necesitas verificarlo. Despu√©s de todo, el LLM solo te presenta lo que es m√°s probable que diga a continuaci√≥n, no lo que es correcto. Por lo tanto, una buena idea es pedirle al LLM que se critique a s√≠ mismo, lo que nos lleva a la t√©cnica de auto-refinamiento.

C√≥mo funciona es que sigues los siguientes pasos:

1. Prompt inicial pidiendo al LLM que resuelva un problema.
1. El LLM responde.
1. Criticas la respuesta y pides a la IA que la mejore.
1. El LLM responde nuevamente, esta vez considerando la cr√≠tica y sugiriendo soluciones que se le ocurrieron.

Puedes repetir este proceso tantas veces como desees.

Aqu√≠ hay un ejemplo usando esta t√©cnica:

> Prompt: "Crea una API web en Python con rutas para productos y clientes"

Respuesta de la IA:

```python
import flask
app = Flask(__name__)

@app.route('/products')
def products():
    return 'Products'

@app.route('/customers')
def customers():
    return 'Customers'
```

> Prompt: sugiere 3 mejoras para el c√≥digo anterior

Respuesta de la IA:

```text
Sure, here are three suggestions to improve the code:

Import the correct module: The first line should import Flask instead of flask. The correct import statement should be from flask import Flask.

Add a main function: Add a if __name__ == '__main__': block to the end of the file to run the application when the file is executed.

Use f-strings: Use f-strings to format the return statements in the route functions. For example, return 'Products' can be changed to return f'This is the {request.path} page'.
```

```python
from flask import Flask, request

app = Flask(__name__)

@app.route('/products')
def products():
    return f'This is the {request.path} page'

@app.route('/customers')
def customers():
    return f'This is the {request.path} page'

if __name__ == '__main__':
    app.run()
```

Como puedes ver, la respuesta de la IA anterior mejora el primer c√≥digo sugerido gracias a la cr√≠tica de la primera respuesta.

### Prompt may√©utico

El prompting may√©utico es una t√©cnica similar al auto-refinamiento, pero se trata m√°s de pedirle al LLM que se explique a s√≠ mismo. El objetivo es reducir las inconsistencias en la salida del LLM para asegurarse de que llegue a la respuesta correcta. El flujo de trabajo a seguir es:

1. Pedir al LLM que responda una pregunta.
1. Para cada parte de la respuesta, pedir al LLM que la explique m√°s a fondo.
1. Si hay inconsistencias, descartar las partes que son inconsistentes.

Repite los pasos 2 y 3 hasta que hayas revisado todas las partes y est√©s satisfecho con la respuesta.

Aqu√≠ hay un ejemplo de prompt:

> Prompt: ¬øC√≥mo puedo crear un plan de crisis para mitigar una pandemia en 5 pasos?
> Respuesta del LLM:

```text
1. Identify the risks
1. Identify the stakeholders
1. Identify the resources
1. Identify the actions
1. Identify the communication channels
```

Ha identificado 5 pasos, pero ¬øpodemos determinar si esto es correcto? Pidamos al LLM que explique cada paso:

> Prompt: Explica el primer paso con m√°s detalle, ¬øcu√°les son los riesgos en detalle de una pandemia?

```text
The risks are the following:

- Loss of life
- Loss of business
- Loss of reputation
- Loss of customers
- Loss of employees
```

En este punto, queremos asegurarnos de que el LLM sea correcto, as√≠ que le pedimos que explique el primer riesgo con m√°s detalle y esperamos que sea consistente con la respuesta anterior:

> Prompt: En una pandemia, ¬øcu√°l es el mayor riesgo y por qu√©?

```text
The biggest risk is loss of life because it's the most important thing to protect.
```

> ¬øCu√°les son los dos mayores riesgos en una pandemia?

```text
The two biggest risks are loss of life and loss of business.
```

En este punto, el LLM es consistente y menciona "vida" y "negocios" como los dos principales riesgos. Ahora podemos continuar con el siguiente paso y sentirnos bastante seguros. Sin embargo, no debemos confiar ciegamente en el LLM, siempre debemos verificar el resultado.

## Variar tu resultado

Los LLMs son por naturaleza no deterministas, lo que significa que obtendr√°s diferentes resultados cada vez que ejecutes el mismo prompt. Prueba el siguiente prompt, por ejemplo:

> "Genera c√≥digo para una API web en Python"

```python
# Import necessary modules
import flask
from flask import request, jsonify

# Create a Flask app
app = flask.Flask(__name__)

# Create a route for the API
@app.route('/api', methods=['GET'])
def api():
    # Get the data from the request
    data = request.args

    # Process the data
    result = process_data(data)

    # Return the result as JSON
    return jsonify(result)

# Function to process the data
def process_data(data):
    # Do something with the data
    result = data + 1

    # Return the result
    return result

# Run the app
if __name__ == '__main__':
    app.run()

```

Ejecutar el mismo prompt nuevamente genera una respuesta ligeramente diferente:

```python
#import necessary packages
import flask
from flask import request, jsonify

#create the Flask application
app = flask.Flask(__name__)
app.config["DEBUG"] = True

#create a list of books
books = [
    {'id': 0,
     'title': 'A Fire Upon the Deep',
     'author': 'Vernor Vinge',
     'first_sentence': 'The coldsleep itself was dreamless.',
     'year_published': '1992'},
    {'id': 1,
     'title': 'The Ones Who Walk Away From Omelas',
     'author': 'Ursula K. Le Guin',
     'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.',
     'published': '1973'},
    {'id': 2,
     'title': 'Dhalgren',
     'author': 'Samuel R. Delany',
     'first_sentence': 'to wound the autumnal city.',
     'published': '1975'}
]

#create an endpoint for the API
@app.route('/', methods=['GET'])
def home():
    return '''<h1>Books API</h1>
<p>A prototype API for retrieving books.</p>'''

#create an endpoint to return all books
@app.route('/api/v1/resources/books/all', methods=['GET'])
def api_all():
    return jsonify(books)

#create an endpoint to return a single book
@app.route('/api/v1/resources/books', methods=['GET'])
def api_id():
    #check if an ID was provided as part of the URL
    #if ID is provided, assign it to a variable
    #if no ID is provided, display an error
    if 'id' in request.args:
        id = int(request.args['id'])
    else:
        return "Error: No id field provided. Please specify an id."

    #create an empty list for our results
    results = []

    #loop through the data and match results that fit the requested ID
    #IDs are unique, but other fields might return many results
    for book in books:
        if book['id'] == id:
            results.append(book)

    #use the jsonify function from Flask to convert our list of
    #Python dictionaries to the JSON format
    return jsonify(results)

app.run()
```

> Entonces, ¬øes un problema el resultado variado?

Depende de lo que est√©s tratando de hacer. Si deseas una respuesta espec√≠fica, entonces es un problema. Si est√°s de acuerdo con un resultado variado como "Genera cualquier 3 preguntas sobre geograf√≠a", entonces no es un problema.

### Usar la temperatura para variar tu resultado

Bien, entonces hemos decidido que queremos limitar el resultado para que sea m√°s predecible, es decir, m√°s determinista. ¬øC√≥mo hacemos eso?

La temperatura es un valor entre 0 y 1, donde 0 es el m√°s determinista y 1 es el m√°s variado. El valor predeterminado es 0.7. Veamos qu√© sucede con dos ejecuciones del mismo prompt con la temperatura configurada en 0.1:

> "Genera c√≥digo para una API web en Python"

```python
#import necessary libraries
import flask
from flask import request, jsonify

#create an instance of the Flask class
app = flask.Flask(__name__)

#create an endpoint for the API
@app.route('/api/v1/endpoint', methods=['GET'])
def api_endpoint():
    #get the request data
    data = request.get_json()

    #process the data
    result = process_data(data)

    #return the result
    return jsonify(result)

#function to process the data
def process_data(data):
    #process the data
    result = {'result': 'success'}

    #return the result
    return result

#run the Flask app
if __name__ == '__main__':
    app.run()

```

Ejecutar el prompt nuevamente nos da este resultado:

```python
#import necessary libraries
import flask
from flask import request, jsonify

#create a Flask app
app = flask.Flask(__name__)

#create a route for the API
@app.route('/api', methods=['GET'])
def api_call():
    #get the data from the request
    data = request.get_json()

    #process the data
    result = process_data(data)

    #return the result
    return jsonify(result)

#define the process_data function
def process_data(data):
    #do something with the data
    result = data + 1

    #return the result
    return result

#run the app
if __name__ == '__main__':
    app.run()

```

Solo hay una peque√±a diferencia entre estos dos resultados. Hagamos lo contrario esta vez, configuremos la temperatura en 0.9:

```python
# Import necessary libraries
import flask
from flask import request, jsonify

# Create a Flask app
app = flask.Flask(__name__)

# Create a route for the API
@app.route('/api', methods=['GET'])
def api_call():
    # Get the data from the request
    data = request.args

    # Process the data
    result = process_data(data)

    # Return the result
    return jsonify(result)

# Function to process the data
def process_data(data):
    # Do something with the data
    result = data + 1

    # Return the result
    return result

# Run the app
if __name__ == '__main__':
    app.run()

```

y el segundo intento con un valor de temperatura de 0.9:

```python
import flask
from flask import request, jsonify

# create the Flask app
app = flask.Flask(__name__)
app.config['DEBUG'] = True

# create some test data
books = [
    {'id': 0, 'title': 'A Fire Upon The Deep', 'author': 'Vernor Vinge', 'first_sentence': 'The coldsleep itself was dreamless.', 'year_published': '1992'},
    {'id': 1, 'title': 'The Ones Who Walk Away From Omelas', 'author': 'Ursula K. Le Guin', 'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.', 'published': '1973'},
    {'id': 2, 'title': 'Dhalgren', 'author': 'Samuel R. Delany', 'first_sentence': 'to wound the autumnal city.', 'published': '1975'}
]

# create an endpoint
@app.route('/', methods=['GET'])
def home():
    return '''<h1>Welcome to our book API!</h1>'''

@app.route('/api/v1/resources/books

```

Como puedes ver, los resultados no podr√≠an ser m√°s variados.

> Nota que hay m√°s par√°metros que puedes cambiar para variar el resultado, como top-k, top-p, penalizaci√≥n por repetici√≥n, penalizaci√≥n por longitud y penalizaci√≥n por diversidad, pero estos est√°n fuera del alcance de este curso.

## Buenas pr√°cticas

Existen muchas pr√°cticas que puedes aplicar para intentar obtener lo que deseas. Encontrar√°s tu propio estilo a medida que uses m√°s y m√°s el prompting.

Adem√°s de las t√©cnicas que hemos cubierto, hay algunas buenas pr√°cticas a considerar al realizar un prompting a un LLM.

Aqu√≠ tienes algunas buenas pr√°cticas a tener en cuenta:

- **Especifica el contexto**. El contexto importa, mientras m√°s detalles puedas especificar como el dominio, tema, etc., mejor.
- Limita el resultado. Si deseas un n√∫mero espec√≠fico de elementos o una longitud espec√≠fica, especif√≠calo.
- **Especifica qu√© y c√≥mo**. Recuerda mencionar tanto lo que quieres como c√≥mo lo quieres, por ejemplo: "Crea una API web en Python con rutas para productos y clientes, div√≠dela en 3 archivos".
- **Usa plantillas**. A menudo, querr√°s enriquecer tus prompts con datos de tu empresa. Usa plantillas para hacerlo. Las plantillas pueden tener variables que reemplaces con datos reales.
- **Escribe correctamente**. Los LLMs podr√≠an proporcionarte una respuesta correcta, pero si escribes correctamente, obtendr√°s una mejor respuesta.

## Tarea

Aqu√≠ tienes un c√≥digo en Python que muestra c√≥mo construir una API simple usando Flask:

```python
from flask import Flask, request

app = Flask(__name__)

@app.route('/')
def hello():
    name = request.args.get('name', 'World')
    return f'Hello, {name}!'

if __name__ == '__main__':
    app.run()
```

Usa un asistente de IA como GitHub Copilot o ChatGPT y aplica la t√©cnica de "auto-refinamiento" para mejorar el c√≥digo.

## Soluci√≥n

Por favor, intenta resolver la tarea a√±adiendo prompts adecuados al c√≥digo.

> [!TIP]
> Formula un prompt para pedir que se mejore, es una buena idea limitar cu√°ntas mejoras. Tambi√©n puedes pedir que se mejore de una manera espec√≠fica, por ejemplo, arquitectura, rendimiento, seguridad, etc.

[Soluci√≥n](../../../05-advanced-prompts/python/aoai-solution.py)

## Verificaci√≥n de conocimiento

¬øPor qu√© usar√≠a el prompting de cadena de pensamiento? Mu√©strame 1 respuesta correcta y 2 respuestas incorrectas.

1. Para ense√±ar al LLM c√≥mo resolver un problema.
1. B, Para ense√±ar al LLM a encontrar errores en el c√≥digo.
1. C, Para instruir al LLM a proponer diferentes soluciones.

A: 1, porque la cadena de pensamiento trata de mostrar al LLM c√≥mo resolver un problema proporcion√°ndole una serie de pasos, problemas similares y c√≥mo fueron resueltos.

## üöÄ Desaf√≠o

Acabas de usar la t√©cnica de auto-refinamiento en la tarea. Toma cualquier programa que hayas creado y considera qu√© mejoras te gustar√≠a aplicarle. Ahora usa la t√©cnica de auto-refinamiento para aplicar los cambios propuestos. ¬øQu√© te pareci√≥ el resultado, mejor o peor?

## ¬°Gran trabajo! Contin√∫a tu aprendizaje

Despu√©s de completar esta lecci√≥n, revisa nuestra [colecci√≥n de aprendizaje de IA generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para seguir ampliando tus conocimientos sobre IA generativa.

Dir√≠gete a la Lecci√≥n 6 donde aplicaremos nuestro conocimiento de Ingenier√≠a de Prompts [construyendo aplicaciones de generaci√≥n de texto](../06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que surjan del uso de esta traducci√≥n.