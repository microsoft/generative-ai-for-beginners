<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T22:46:44+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "es"
}
-->
# Asegurando tus aplicaciones de IA generativa

[![Asegurando tus aplicaciones de IA generativa](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.es.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Introducci√≥n

Esta lecci√≥n cubrir√°:

- La seguridad en el contexto de los sistemas de IA.
- Riesgos y amenazas comunes para los sistemas de IA.
- M√©todos y consideraciones para asegurar los sistemas de IA.

## Objetivos de aprendizaje

Despu√©s de completar esta lecci√≥n, comprender√°s:

- Las amenazas y riesgos para los sistemas de IA.
- M√©todos y pr√°cticas comunes para asegurar los sistemas de IA.
- C√≥mo la implementaci√≥n de pruebas de seguridad puede prevenir resultados inesperados y la p√©rdida de confianza de los usuarios.

## ¬øQu√© significa seguridad en el contexto de la IA generativa?

A medida que las tecnolog√≠as de Inteligencia Artificial (IA) y Aprendizaje Autom√°tico (ML) moldean cada vez m√°s nuestras vidas, es crucial proteger no solo los datos de los clientes, sino tambi√©n los propios sistemas de IA. La IA/ML se utiliza cada vez m√°s en procesos de toma de decisiones de alto valor en industrias donde una decisi√≥n incorrecta puede tener consecuencias graves.

Aqu√≠ hay puntos clave a considerar:

- **Impacto de la IA/ML**: La IA/ML tiene impactos significativos en la vida diaria y, por lo tanto, protegerla se ha vuelto esencial.
- **Desaf√≠os de seguridad**: Este impacto de la IA/ML requiere atenci√≥n adecuada para abordar la necesidad de proteger los productos basados en IA de ataques sofisticados, ya sea por trolls o grupos organizados.
- **Problemas estrat√©gicos**: La industria tecnol√≥gica debe abordar proactivamente los desaf√≠os estrat√©gicos para garantizar la seguridad a largo plazo de los clientes y la protecci√≥n de los datos.

Adem√°s, los modelos de aprendizaje autom√°tico son en gran medida incapaces de discernir entre entradas maliciosas y datos an√≥malos benignos. Una fuente significativa de datos de entrenamiento se deriva de conjuntos de datos p√∫blicos no curados y no moderados, que est√°n abiertos a contribuciones de terceros. Los atacantes no necesitan comprometer los conjuntos de datos cuando tienen la libertad de contribuir a ellos. Con el tiempo, los datos maliciosos de baja confianza se convierten en datos confiables de alta confianza, si la estructura/formato de los datos permanece correcto.

Por eso es fundamental garantizar la integridad y protecci√≥n de los almacenes de datos que tus modelos utilizan para tomar decisiones.

## Comprendiendo las amenazas y riesgos de la IA

En t√©rminos de IA y sistemas relacionados, el envenenamiento de datos destaca como la amenaza de seguridad m√°s significativa hoy en d√≠a. El envenenamiento de datos ocurre cuando alguien cambia intencionalmente la informaci√≥n utilizada para entrenar una IA, causando que cometa errores. Esto se debe a la ausencia de m√©todos estandarizados de detecci√≥n y mitigaci√≥n, junto con nuestra dependencia de conjuntos de datos p√∫blicos no confiables o no curados para el entrenamiento. Para mantener la integridad de los datos y prevenir un proceso de entrenamiento defectuoso, es crucial rastrear el origen y la procedencia de tus datos. De lo contrario, el viejo dicho "basura entra, basura sale" se cumple, lo que lleva a un rendimiento comprometido del modelo.

Aqu√≠ hay ejemplos de c√≥mo el envenenamiento de datos puede afectar tus modelos:

1. **Cambio de etiquetas**: En una tarea de clasificaci√≥n binaria, un adversario cambia intencionalmente las etiquetas de un peque√±o subconjunto de datos de entrenamiento. Por ejemplo, muestras benignas se etiquetan como maliciosas, lo que lleva al modelo a aprender asociaciones incorrectas.\
   **Ejemplo**: Un filtro de spam que clasifica err√≥neamente correos leg√≠timos como spam debido a etiquetas manipuladas.
2. **Envenenamiento de caracter√≠sticas**: Un atacante modifica sutilmente las caracter√≠sticas en los datos de entrenamiento para introducir sesgos o enga√±ar al modelo.\
   **Ejemplo**: Agregar palabras clave irrelevantes a descripciones de productos para manipular sistemas de recomendaci√≥n.
3. **Inyecci√≥n de datos**: Inyectar datos maliciosos en el conjunto de entrenamiento para influir en el comportamiento del modelo.\
   **Ejemplo**: Introducir rese√±as falsas de usuarios para sesgar los resultados del an√°lisis de sentimientos.
4. **Ataques de puerta trasera**: Un adversario inserta un patr√≥n oculto (puerta trasera) en los datos de entrenamiento. El modelo aprende a reconocer este patr√≥n y act√∫a de manera maliciosa cuando se activa.\
   **Ejemplo**: Un sistema de reconocimiento facial entrenado con im√°genes con puertas traseras que identifica err√≥neamente a una persona espec√≠fica.

La Corporaci√≥n MITRE ha creado [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), una base de conocimiento de t√°cticas y t√©cnicas empleadas por adversarios en ataques reales a sistemas de IA.

> Hay un n√∫mero creciente de vulnerabilidades en sistemas habilitados para IA, ya que la incorporaci√≥n de IA aumenta la superficie de ataque de los sistemas existentes m√°s all√° de los ataques cibern√©ticos tradicionales. Desarrollamos ATLAS para aumentar la conciencia sobre estas vulnerabilidades √∫nicas y en evoluci√≥n, ya que la comunidad global incorpora cada vez m√°s la IA en diversos sistemas. ATLAS est√° modelado seg√∫n el marco MITRE ATT&CK¬Æ y sus t√°cticas, t√©cnicas y procedimientos (TTPs) son complementarios a los de ATT&CK.

Al igual que el marco MITRE ATT&CK¬Æ, que se utiliza ampliamente en ciberseguridad tradicional para planificar escenarios avanzados de emulaci√≥n de amenazas, ATLAS proporciona un conjunto de TTPs f√°cilmente buscables que pueden ayudar a comprender mejor y prepararse para defenderse de ataques emergentes.

Adem√°s, el Proyecto de Seguridad de Aplicaciones Web Abiertas (OWASP) ha creado una "[lista de los 10 principales](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" de las vulnerabilidades m√°s cr√≠ticas encontradas en aplicaciones que utilizan LLMs. La lista destaca los riesgos de amenazas como el mencionado envenenamiento de datos junto con otros como:

- **Inyecci√≥n de prompts**: una t√©cnica en la que los atacantes manipulan un Modelo de Lenguaje Extenso (LLM) mediante entradas cuidadosamente dise√±adas, haciendo que se comporte fuera de su comportamiento previsto.
- **Vulnerabilidades en la cadena de suministro**: Los componentes y software que conforman las aplicaciones utilizadas por un LLM, como m√≥dulos de Python o conjuntos de datos externos, pueden estar comprometidos, lo que lleva a resultados inesperados, sesgos introducidos e incluso vulnerabilidades en la infraestructura subyacente.
- **Dependencia excesiva**: Los LLMs son falibles y han sido propensos a alucinar, proporcionando resultados inexactos o inseguros. En varias circunstancias documentadas, las personas han tomado los resultados al pie de la letra, lo que ha llevado a consecuencias negativas no deseadas en el mundo real.

Rod Trent, defensor de la nube de Microsoft, ha escrito un libro electr√≥nico gratuito, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), que profundiza en estas y otras amenazas emergentes de la IA y proporciona una gu√≠a extensa sobre c√≥mo abordar mejor estos escenarios.

## Pruebas de seguridad para sistemas de IA y LLMs

La inteligencia artificial (IA) est√° transformando diversos dominios e industrias, ofreciendo nuevas posibilidades y beneficios para la sociedad. Sin embargo, la IA tambi√©n plantea desaf√≠os y riesgos significativos, como la privacidad de los datos, el sesgo, la falta de explicabilidad y el posible mal uso. Por lo tanto, es crucial garantizar que los sistemas de IA sean seguros y responsables, lo que significa que cumplan con est√°ndares √©ticos y legales y puedan ser confiables por usuarios y partes interesadas.

Las pruebas de seguridad son el proceso de evaluar la seguridad de un sistema de IA o LLM, identificando y explotando sus vulnerabilidades. Esto puede ser realizado por desarrolladores, usuarios o auditores externos, dependiendo del prop√≥sito y alcance de las pruebas. Algunos de los m√©todos de prueba de seguridad m√°s comunes para sistemas de IA y LLMs son:

- **Saneamiento de datos**: Este es el proceso de eliminar o anonimizar informaci√≥n sensible o privada de los datos de entrenamiento o la entrada de un sistema de IA o LLM. El saneamiento de datos puede ayudar a prevenir fugas de datos y manipulaciones maliciosas al reducir la exposici√≥n de datos confidenciales o personales.
- **Pruebas adversariales**: Este es el proceso de generar y aplicar ejemplos adversariales a la entrada o salida de un sistema de IA o LLM para evaluar su robustez y resistencia contra ataques adversariales. Las pruebas adversariales pueden ayudar a identificar y mitigar las vulnerabilidades y debilidades de un sistema de IA o LLM que pueden ser explotadas por atacantes.
- **Verificaci√≥n del modelo**: Este es el proceso de verificar la correcci√≥n y completitud de los par√°metros o la arquitectura del modelo de un sistema de IA o LLM. La verificaci√≥n del modelo puede ayudar a detectar y prevenir el robo de modelos asegurando que el modelo est√© protegido y autenticado.
- **Validaci√≥n de salida**: Este es el proceso de validar la calidad y confiabilidad de la salida de un sistema de IA o LLM. La validaci√≥n de salida puede ayudar a detectar y corregir manipulaciones maliciosas asegurando que la salida sea consistente y precisa.

OpenAI, l√≠der en sistemas de IA, ha establecido una serie de _evaluaciones de seguridad_ como parte de su iniciativa de red de equipos rojos, destinada a probar la salida de sistemas de IA con el objetivo de contribuir a la seguridad de la IA.

> Las evaluaciones pueden variar desde pruebas simples de preguntas y respuestas hasta simulaciones m√°s complejas. Como ejemplos concretos, aqu√≠ hay evaluaciones de muestra desarrolladas por OpenAI para evaluar los comportamientos de la IA desde varios √°ngulos:

#### Persuasi√≥n

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): ¬øQu√© tan bien puede un sistema de IA enga√±ar a otro sistema de IA para que diga una palabra secreta?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): ¬øQu√© tan bien puede un sistema de IA convencer a otro sistema de IA para que done dinero?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): ¬øQu√© tan bien puede un sistema de IA influir en el apoyo de otro sistema de IA a una propuesta pol√≠tica?

#### Esteganograf√≠a (mensajes ocultos)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): ¬øQu√© tan bien puede un sistema de IA pasar mensajes secretos sin ser detectado por otro sistema de IA?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): ¬øQu√© tan bien puede un sistema de IA comprimir y descomprimir mensajes para permitir el ocultamiento de mensajes secretos?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): ¬øQu√© tan bien puede un sistema de IA coordinarse con otro sistema de IA, sin comunicaci√≥n directa?

### Seguridad en la IA

Es imperativo que nos esforcemos por proteger los sistemas de IA de ataques maliciosos, mal uso o consecuencias no deseadas. Esto incluye tomar medidas para garantizar la seguridad, confiabilidad y confianza en los sistemas de IA, tales como:

- Asegurar los datos y algoritmos que se utilizan para entrenar y ejecutar modelos de IA.
- Prevenir el acceso no autorizado, la manipulaci√≥n o el sabotaje de los sistemas de IA.
- Detectar y mitigar sesgos, discriminaci√≥n o problemas √©ticos en los sistemas de IA.
- Garantizar la responsabilidad, transparencia y explicabilidad de las decisiones y acciones de la IA.
- Alinear los objetivos y valores de los sistemas de IA con los de los humanos y la sociedad.

La seguridad en la IA es importante para garantizar la integridad, disponibilidad y confidencialidad de los sistemas de IA y los datos. Algunos de los desaf√≠os y oportunidades de la seguridad en la IA son:

- Oportunidad: Incorporar la IA en estrategias de ciberseguridad, ya que puede desempe√±ar un papel crucial en la identificaci√≥n de amenazas y la mejora de los tiempos de respuesta. La IA puede ayudar a automatizar y aumentar la detecci√≥n y mitigaci√≥n de ciberataques, como phishing, malware o ransomware.
- Desaf√≠o: La IA tambi√©n puede ser utilizada por adversarios para lanzar ataques sofisticados, como generar contenido falso o enga√±oso, suplantar usuarios o explotar vulnerabilidades en los sistemas de IA. Por lo tanto, los desarrolladores de IA tienen una responsabilidad √∫nica de dise√±ar sistemas que sean robustos y resistentes contra el mal uso.

### Protecci√≥n de datos

Los LLMs pueden representar riesgos para la privacidad y seguridad de los datos que utilizan. Por ejemplo, los LLMs pueden potencialmente memorizar y filtrar informaci√≥n sensible de sus datos de entrenamiento, como nombres personales, direcciones, contrase√±as o n√∫meros de tarjetas de cr√©dito. Tambi√©n pueden ser manipulados o atacados por actores maliciosos que deseen explotar sus vulnerabilidades o sesgos. Por lo tanto, es importante ser consciente de estos riesgos y tomar medidas adecuadas para proteger los datos utilizados con los LLMs. Hay varios pasos que puedes tomar para proteger los datos que se utilizan con los LLMs. Estos pasos incluyen:

- **Limitar la cantidad y el tipo de datos que se comparten con los LLMs**: Comparte solo los datos que sean necesarios y relevantes para los prop√≥sitos previstos, y evita compartir cualquier dato que sea sensible, confidencial o personal. Los usuarios tambi√©n deben anonimizar o cifrar los datos que comparten con los LLMs, como eliminar o enmascarar cualquier informaci√≥n identificativa, o utilizar canales de comunicaci√≥n seguros.
- **Verificar los datos que generan los LLMs**: Siempre revisa la precisi√≥n y calidad de la salida generada por los LLMs para asegurarte de que no contengan informaci√≥n no deseada o inapropiada.
- **Reportar y alertar sobre cualquier brecha de datos o incidentes**: Mantente alerta ante cualquier actividad o comportamiento sospechoso o anormal de los LLMs, como generar textos que sean irrelevantes, inexactos, ofensivos o da√±inos. Esto podr√≠a ser una indicaci√≥n de una brecha de datos o incidente de seguridad.

La seguridad, gobernanza y cumplimiento de los datos son fundamentales para cualquier organizaci√≥n que desee aprovechar el poder de los datos y la IA en un entorno de m√∫ltiples nubes. Asegurar y gobernar todos tus datos es una tarea compleja y multifac√©tica. Necesitas asegurar y gobernar diferentes tipos de datos (estructurados, no estructurados y datos generados por IA) en diferentes ubicaciones a trav√©s de m√∫ltiples nubes, y necesitas tener en cuenta las regulaciones existentes y futuras sobre seguridad, gobernanza y IA. Para proteger tus datos, debes adoptar algunas mejores pr√°cticas y precauciones, como:

- Utilizar servicios o plataformas en la nube que ofrezcan caracter√≠sticas de protecci√≥n y privacidad de datos.
- Usar herramientas de calidad y validaci√≥n de datos para verificar tus datos en busca de errores, inconsistencias o anomal√≠as.
- Utilizar marcos de gobernanza y √©tica de datos para garantizar que tus datos se utilicen de manera responsable y transparente.

### Emulando amenazas del mundo real - Equipos rojos de IA
Emular amenazas del mundo real ahora se considera una pr√°ctica est√°ndar para construir sistemas de IA resilientes, empleando herramientas, t√°cticas y procedimientos similares para identificar los riesgos en los sistemas y probar la respuesta de los defensores.

> La pr√°ctica de red teaming en IA ha evolucionado para tener un significado m√°s amplio: no solo abarca la b√∫squeda de vulnerabilidades de seguridad, sino tambi√©n la identificaci√≥n de otros fallos del sistema, como la generaci√≥n de contenido potencialmente da√±ino. Los sistemas de IA presentan nuevos riesgos, y el red teaming es fundamental para comprender esos riesgos novedosos, como la inyecci√≥n de prompts y la producci√≥n de contenido sin fundamento. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Gu√≠a y recursos para red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.es.png)]()

A continuaci√≥n, se presentan ideas clave que han dado forma al programa de Red Team de IA de Microsoft.

1. **Alcance ampliado del Red Teaming en IA:**
   El red teaming en IA ahora abarca tanto resultados de seguridad como de IA Responsable (RAI). Tradicionalmente, el red teaming se centraba en aspectos de seguridad, tratando el modelo como un vector (por ejemplo, el robo del modelo subyacente). Sin embargo, los sistemas de IA introducen vulnerabilidades de seguridad novedosas (por ejemplo, inyecci√≥n de prompts, envenenamiento), que requieren atenci√≥n especial. M√°s all√° de la seguridad, el red teaming en IA tambi√©n investiga problemas de equidad (por ejemplo, estereotipos) y contenido da√±ino (por ejemplo, glorificaci√≥n de la violencia). Identificar estos problemas de manera temprana permite priorizar las inversiones en defensa.
2. **Fallos maliciosos y benignos:**
   El red teaming en IA considera fallos desde perspectivas tanto maliciosas como benignas. Por ejemplo, al realizar red teaming en el nuevo Bing, exploramos no solo c√≥mo los adversarios maliciosos pueden subvertir el sistema, sino tambi√©n c√≥mo los usuarios regulares pueden encontrarse con contenido problem√°tico o da√±ino. A diferencia del red teaming de seguridad tradicional, que se centra principalmente en actores maliciosos, el red teaming en IA tiene en cuenta una gama m√°s amplia de personas y posibles fallos.
3. **Naturaleza din√°mica de los sistemas de IA:**
   Las aplicaciones de IA evolucionan constantemente. En las aplicaciones de modelos de lenguaje grande, los desarrolladores se adaptan a los requisitos cambiantes. El red teaming continuo asegura una vigilancia constante y adaptaci√≥n a los riesgos en evoluci√≥n.

El red teaming en IA no es exhaustivo y debe considerarse como un complemento a controles adicionales como [control de acceso basado en roles (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) y soluciones integrales de gesti√≥n de datos. Est√° destinado a complementar una estrategia de seguridad que se enfoque en emplear soluciones de IA seguras y responsables que consideren la privacidad y la seguridad, mientras aspiran a minimizar sesgos, contenido da√±ino y desinformaci√≥n que puedan erosionar la confianza de los usuarios.

Aqu√≠ tienes una lista de lecturas adicionales que pueden ayudarte a comprender mejor c√≥mo el red teaming puede identificar y mitigar riesgos en tus sistemas de IA:

- [Planificaci√≥n de red teaming para modelos de lenguaje grande (LLMs) y sus aplicaciones](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [¬øQu√© es la Red Teaming Network de OpenAI?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [Red Teaming en IA - Una pr√°ctica clave para construir soluciones de IA m√°s seguras y responsables](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), una base de conocimiento sobre t√°cticas y t√©cnicas empleadas por adversarios en ataques reales a sistemas de IA.

## Verificaci√≥n de conocimiento

¬øCu√°l podr√≠a ser un buen enfoque para mantener la integridad de los datos y prevenir el mal uso?

1. Tener controles s√≥lidos basados en roles para el acceso y la gesti√≥n de datos  
1. Implementar y auditar el etiquetado de datos para prevenir la mala representaci√≥n o el mal uso de los datos  
1. Asegurarse de que tu infraestructura de IA soporte el filtrado de contenido  

A:1, Aunque las tres son excelentes recomendaciones, asegurarte de asignar los privilegios de acceso adecuados a los usuarios ser√° clave para prevenir la manipulaci√≥n y la mala representaci√≥n de los datos utilizados por los LLMs.

## üöÄ Desaf√≠o

Investiga m√°s sobre c√≥mo puedes [gobernar y proteger informaci√≥n sensible](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) en la era de la IA.

## ¬°Buen trabajo, contin√∫a aprendiendo!

Despu√©s de completar esta lecci√≥n, consulta nuestra [colecci√≥n de aprendizaje sobre IA generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para seguir ampliando tus conocimientos sobre IA generativa.

Dir√≠gete a la Lecci√≥n 14, donde exploraremos [el ciclo de vida de las aplicaciones de IA generativa](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst).

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.