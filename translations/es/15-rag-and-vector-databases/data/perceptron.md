# Introducci√≥n a Redes Neuronales: Perceptr√≥n

Uno de los primeros intentos de implementar algo similar a una red neuronal moderna fue realizado por Frank Rosenblatt del Laboratorio Aeron√°utico de Cornell en 1957. Fue una implementaci√≥n de hardware llamada "Mark-1", dise√±ada para reconocer figuras geom√©tricas primitivas, como tri√°ngulos, cuadrados y c√≠rculos.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='El Perceptr√≥n Mark 1' />|

> Im√°genes de Wikipedia

Una imagen de entrada se representaba mediante una matriz de fotoc√©lulas de 20x20, por lo que la red neuronal ten√≠a 400 entradas y una salida binaria. Una red simple conten√≠a una neurona, tambi√©n llamada una **unidad l√≥gica de umbral**. Los pesos de la red neuronal actuaban como potenci√≥metros que requer√≠an ajuste manual durante la fase de entrenamiento.

> ‚úÖ Un potenci√≥metro es un dispositivo que permite al usuario ajustar la resistencia de un circuito.

> El New York Times escribi√≥ sobre el perceptr√≥n en ese momento: *el embri√≥n de una computadora electr√≥nica que [la Marina] espera que sea capaz de caminar, hablar, ver, escribir, reproducirse a s√≠ misma y ser consciente de su existencia.*

## Modelo de Perceptr√≥n

Supongamos que tenemos N caracter√≠sticas en nuestro modelo, en cuyo caso el vector de entrada ser√≠a un vector de tama√±o N. Un perceptr√≥n es un modelo de **clasificaci√≥n binaria**, es decir, puede distinguir entre dos clases de datos de entrada. Supondremos que para cada vector de entrada x, la salida de nuestro perceptr√≥n ser√° +1 o -1, dependiendo de la clase. La salida se calcular√° usando la f√≥rmula:

y(x) = f(w<sup>T</sup>x)

donde f es una funci√≥n de activaci√≥n escalonada.

## Entrenamiento del Perceptr√≥n

Para entrenar un perceptr√≥n necesitamos encontrar un vector de pesos w que clasifique la mayor√≠a de los valores correctamente, es decir, que resulte en el menor **error**. Este error se define por el **criterio del perceptr√≥n** de la siguiente manera:

E(w) = -‚àëw<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

donde:

* la suma se toma sobre aquellos puntos de datos de entrenamiento i que resultan en una clasificaci√≥n incorrecta
* x<sub>i</sub> es el dato de entrada, y t<sub>i</sub> es -1 o +1 para ejemplos negativos y positivos respectivamente.

Este criterio se considera como una funci√≥n de los pesos w, y necesitamos minimizarlo. A menudo se utiliza un m√©todo llamado **descenso de gradiente**, en el cual comenzamos con algunos pesos iniciales w<sup>(0)</sup>, y luego en cada paso actualizamos los pesos de acuerdo con la f√≥rmula:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Œ∑‚àáE(w)

Aqu√≠ Œ∑ es la llamada **tasa de aprendizaje**, y ‚àáE(w) denota el **gradiente** de E. Despu√©s de calcular el gradiente, terminamos con

w<sup>(t+1)</sup> = w<sup>(t)</sup> + ‚àëŒ∑x<sub>i</sub>t<sub>i</sub>

El algoritmo en Python se ve as√≠:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Conclusi√≥n

En esta lecci√≥n, aprendiste sobre un perceptr√≥n, que es un modelo de clasificaci√≥n binaria, y c√≥mo entrenarlo usando un vector de pesos.

## üöÄ Desaf√≠o

Si deseas intentar construir tu propio perceptr√≥n, prueba este laboratorio en Microsoft Learn que utiliza el dise√±ador de Azure ML.

## Revisi√≥n y Autoestudio

Para ver c√≥mo podemos usar el perceptr√≥n para resolver un problema simple as√≠ como problemas de la vida real, y para continuar aprendiendo, dir√≠gete al cuaderno de Perceptr√≥n.

Aqu√≠ tambi√©n hay un art√≠culo interesante sobre perceptrones.

## Tarea

En esta lecci√≥n, hemos implementado un perceptr√≥n para una tarea de clasificaci√≥n binaria y lo hemos usado para clasificar entre dos d√≠gitos escritos a mano. En este laboratorio, se te pide que resuelvas completamente el problema de clasificaci√≥n de d√≠gitos, es decir, determinar qu√© d√≠gito es m√°s probable que corresponda a una imagen dada.

* Instrucciones
* Cuaderno

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando servicios de traducci√≥n autom√°tica basados en inteligencia artificial. Aunque nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n humana profesional. No somos responsables de ning√∫n malentendido o interpretaci√≥n err√≥nea que surja del uso de esta traducci√≥n.