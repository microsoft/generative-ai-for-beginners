<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-07-09T16:53:29+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "es"
}
-->
# Introducci√≥n a las Redes Neuronales: Perceptr√≥n

Uno de los primeros intentos de implementar algo similar a una red neuronal moderna fue realizado por Frank Rosenblatt del Cornell Aeronautical Laboratory en 1957. Fue una implementaci√≥n en hardware llamada "Mark-1", dise√±ada para reconocer figuras geom√©tricas primitivas, como tri√°ngulos, cuadrados y c√≠rculos.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='El Perceptr√≥n Mark 1' />|

> Im√°genes de Wikipedia

Una imagen de entrada estaba representada por una matriz de 20x20 fotoceldas, por lo que la red neuronal ten√≠a 400 entradas y una salida binaria. Una red simple conten√≠a un solo neurona, tambi√©n llamada **unidad l√≥gica umbral**. Los pesos de la red neuronal actuaban como potenci√≥metros que requer√≠an ajuste manual durante la fase de entrenamiento.

> ‚úÖ Un potenci√≥metro es un dispositivo que permite al usuario ajustar la resistencia de un circuito.

> The New York Times escribi√≥ sobre el perceptr√≥n en esa √©poca: *el embri√≥n de una computadora electr√≥nica que [la Marina] espera que pueda caminar, hablar, ver, escribir, reproducirse y ser consciente de su existencia.*

## Modelo de Perceptr√≥n

Supongamos que tenemos N caracter√≠sticas en nuestro modelo, en cuyo caso el vector de entrada ser√≠a un vector de tama√±o N. Un perceptr√≥n es un modelo de **clasificaci√≥n binaria**, es decir, puede distinguir entre dos clases de datos de entrada. Supondremos que para cada vector de entrada x la salida de nuestro perceptr√≥n ser√° +1 o -1, dependiendo de la clase. La salida se calcular√° usando la f√≥rmula:

y(x) = f(w<sup>T</sup>x)

donde f es una funci√≥n de activaci√≥n escal√≥n

## Entrenamiento del Perceptr√≥n

Para entrenar un perceptr√≥n necesitamos encontrar un vector de pesos w que clasifique correctamente la mayor√≠a de los valores, es decir, que resulte en el menor **error**. Este error se define mediante el **criterio del perceptr√≥n** de la siguiente manera:

E(w) = -‚àëw<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

donde:

* la suma se toma sobre aquellos puntos de datos de entrenamiento i que resultan en una clasificaci√≥n incorrecta
* x<sub>i</sub> es el dato de entrada, y t<sub>i</sub> es -1 o +1 para ejemplos negativos y positivos respectivamente.

Este criterio se considera como una funci√≥n de los pesos w, y necesitamos minimizarlo. A menudo, se utiliza un m√©todo llamado **descenso por gradiente**, en el que comenzamos con algunos pesos iniciales w<sup>(0)</sup>, y luego en cada paso actualizamos los pesos seg√∫n la f√≥rmula:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Œ∑‚àáE(w)

Aqu√≠ Œ∑ es la llamada **tasa de aprendizaje**, y ‚àáE(w) denota el **gradiente** de E. Despu√©s de calcular el gradiente, obtenemos

w<sup>(t+1)</sup> = w<sup>(t)</sup> + ‚àëŒ∑x<sub>i</sub>t<sub>i</sub>

El algoritmo en Python se ve as√≠:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Conclusi√≥n

En esta lecci√≥n, aprendiste sobre el perceptr√≥n, que es un modelo de clasificaci√≥n binaria, y c√≥mo entrenarlo usando un vector de pesos.

## üöÄ Desaf√≠o

Si quieres intentar construir tu propio perceptr√≥n, prueba este laboratorio en Microsoft Learn que utiliza el dise√±ador de Azure ML


## Repaso y Autoestudio

Para ver c√≥mo podemos usar el perceptr√≥n para resolver un problema sencillo as√≠ como problemas de la vida real, y para continuar aprendiendo, ve al cuaderno Perceptron.

Tambi√©n aqu√≠ tienes un art√≠culo interesante sobre perceptrones.

## Tarea

En esta lecci√≥n, hemos implementado un perceptr√≥n para una tarea de clasificaci√≥n binaria, y lo hemos usado para clasificar entre dos d√≠gitos escritos a mano. En este laboratorio, se te pide resolver el problema de clasificaci√≥n de d√≠gitos completamente, es decir, determinar qu√© d√≠gito es el que m√°s probablemente corresponde a una imagen dada.

* Instrucciones
* Cuaderno

**Aviso legal**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por la precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda la traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas derivadas del uso de esta traducci√≥n.