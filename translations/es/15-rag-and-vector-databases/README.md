<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2210a0466c812d9defc4df2d9a709ff9",
  "translation_date": "2026-01-18T16:43:09+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "es"
}
-->
# Generaci칩n Aumentada por Recuperaci칩n (RAG) y Bases de Datos Vectoriales

[![Generaci칩n Aumentada por Recuperaci칩n (RAG) y Bases de Datos Vectoriales](../../../../../translated_images/es/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

En la lecci칩n de aplicaciones de b칰squeda, aprendimos brevemente c칩mo integrar tus propios datos en Modelos de Lenguaje Grande (LLMs). En esta lecci칩n, profundizaremos en los conceptos de fundamentar tus datos en tu aplicaci칩n LLM, la mec치nica del proceso y los m칠todos para almacenar datos, incluyendo tanto embeddings como texto.

> **Video Pronto Disponible**

## Introducci칩n

En esta lecci칩n cubriremos lo siguiente:

- Una introducci칩n a RAG, qu칠 es y por qu칠 se usa en IA (inteligencia artificial).

- Entender qu칠 son las bases de datos vectoriales y crear una para nuestra aplicaci칩n.

- Un ejemplo pr치ctico sobre c칩mo integrar RAG en una aplicaci칩n.

## Objetivos de aprendizaje

Despu칠s de completar esta lecci칩n, ser치s capaz de:

- Explicar la importancia de RAG en la recuperaci칩n y procesamiento de datos.

- Configurar una aplicaci칩n RAG y fundamentar tus datos en un LLM.

- Integrar efectivamente RAG y Bases de Datos Vectoriales en Aplicaciones LLM.

## Nuestro escenario: mejorando nuestros LLMs con nuestros propios datos

Para esta lecci칩n, queremos a침adir nuestras propias notas a la startup educativa, lo que permite que el chatbot obtenga m치s informaci칩n sobre las diferentes materias. Usando las notas que tenemos, los estudiantes podr치n estudiar mejor y entender los distintos temas, facilitando la revisi칩n para sus ex치menes. Para crear nuestro escenario usaremos:

- `Azure OpenAI:` el LLM que usaremos para crear nuestro chatbot.

- `Lecci칩n AI para principiantes sobre Redes Neuronales:` estos ser치n los datos en los que fundamentaremos nuestro LLM.

- `Azure AI Search` y `Azure Cosmos DB:` base de datos vectorial para almacenar nuestros datos y crear un 칤ndice de b칰squeda.

Los usuarios podr치n crear cuestionarios de pr치ctica a partir de sus notas, tarjetas de repaso y resumirlos en vistas concisas. Para comenzar, veamos qu칠 es RAG y c칩mo funciona:

## Generaci칩n Aumentada por Recuperaci칩n (RAG)

Un chatbot potenciado por LLM procesa las indicaciones del usuario para generar respuestas. Est치 dise침ado para ser interactivo y conversar con los usuarios sobre una amplia variedad de temas. Sin embargo, sus respuestas est치n limitadas al contexto proporcionado y sus datos de entrenamiento fundamentales. Por ejemplo, el conocimiento de GPT-4 se corta en septiembre de 2021, lo que significa que carece de conocimiento sobre eventos posteriores a ese periodo. Adem치s, los datos usados para entrenar LLMs excluyen informaci칩n confidencial como notas personales o el manual de productos de una empresa.

### C칩mo funcionan los RAGs (Generaci칩n Aumentada por Recuperaci칩n)

![dibujo que muestra c칩mo funcionan los RAGs](../../../../../translated_images/es/how-rag-works.f5d0ff63942bd3a6.webp)

Supongamos que quieres desplegar un chatbot que crea cuestionarios a partir de tus notas, necesitar치s una conexi칩n a la base de conocimiento. Aqu칤 es donde RAG viene al rescate. Los RAGs operan de la siguiente manera:

- **Base de conocimiento:** Antes de la recuperaci칩n, estos documentos necesitan ser ingeridos y preprocesados, normalmente dividiendo documentos grandes en trozos m치s peque침os, transform치ndolos en embeddings de texto y almacen치ndolos en una base de datos.

- **Consulta del usuario:** el usuario hace una pregunta.

- **Recuperaci칩n:** Cuando un usuario hace una pregunta, el modelo de embeddings recupera informaci칩n relevante de nuestra base de conocimiento para proporcionar m치s contexto que se incorporar치 en la indicaci칩n.

- **Generaci칩n aumentada:** el LLM mejora su respuesta con base en los datos recuperados. Permite que la respuesta generada no solo se base en datos preentrenados sino tambi칠n en informaci칩n relevante del contexto a침adido. Los datos recuperados se usan para aumentar las respuestas del LLM. Luego el LLM retorna una respuesta a la pregunta del usuario.

![dibujo que muestra la arquitectura de RAGs](../../../../../translated_images/es/encoder-decode.f2658c25d0eadee2.webp)

La arquitectura para RAGs se implementa usando transformadores que constan de dos partes: un codificador y un decodificador. Por ejemplo, cuando un usuario hace una pregunta, el texto de entrada es 'codificado' en vectores que capturan el significado de las palabras y los vectores son 'decodificados' en nuestro 칤ndice de documentos y genera texto nuevo basado en la consulta del usuario. El LLM usa un modelo codificador-decodificador para generar la salida.

Dos enfoques al implementar RAG seg칰n el art칤culo propuesto: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) son:

- **_RAG-Sequence_** usando documentos recuperados para predecir la mejor respuesta posible a una consulta de usuario.

- **RAG-Token** usando documentos para generar el siguiente token, luego recuperarlos para responder a la consulta del usuario.

### 쯇or qu칠 usar칤as RAGs?

- **Riqueza de informaci칩n:** asegura que las respuestas de texto est칠n actualizadas y sean actuales. Por lo tanto, mejora el rendimiento en tareas espec칤ficas del dominio accediendo a la base de conocimiento interna.

- Reduce la fabricaci칩n usando **datos verificables** en la base de conocimiento para proporcionar contexto a las consultas de los usuarios.

- Es **rentable** ya que es m치s econ칩mico que ajustar finamente un LLM.

## Creando una base de conocimiento

Nuestra aplicaci칩n est치 basada en nuestros datos personales, es decir, la lecci칩n de Redes Neuronales del curr칤culo AI Para Principiantes.

### Bases de Datos Vectoriales

Una base de datos vectorial, a diferencia de bases de datos tradicionales, es una base de datos especializada dise침ada para almacenar, gestionar y buscar vectores embebidos. Almacena representaciones num칠ricas de documentos. Descomponer los datos en embeddings num칠ricos facilita que nuestro sistema de IA entienda y procese los datos.

Almacenamos nuestros embeddings en bases de datos vectoriales ya que los LLMs tienen un l칤mite en el n칰mero de tokens que aceptan como entrada. Como no puedes pasar todos los embeddings completos a un LLM, necesitaremos dividirlos en trozos y cuando un usuario haga una pregunta, se devolver치n los embeddings m치s parecidos a la consulta junto con la indicaci칩n. Dividir tambi칠n reduce costos en la cantidad de tokens que pasan por un LLM.

Algunas bases de datos vectoriales populares incluyen Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant y DeepLake. Puedes crear un modelo Azure Cosmos DB usando Azure CLI con el siguiente comando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```


### De texto a embeddings

Antes de almacenar nuestros datos, necesitaremos convertirlos en vectores embedding antes de almacenarlos en la base de datos. Si trabajas con documentos grandes o textos largos, puedes dividirlos en fragmentos basados en las consultas que esperas. La divisi칩n puede hacerse a nivel de oraciones o p치rrafos. Como dividir extrae significados de las palabras alrededor, puedes a침adir otro contexto a un fragmento, por ejemplo, a침adiendo el t칤tulo del documento o incluyendo algo de texto antes o despu칠s del fragmento. Puedes dividir los datos como sigue:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Si el 칰ltimo fragmento no alcanz칩 la longitud m칤nima, agr칠guelo de todos modos
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```


Una vez divididos, podemos incrustar nuestro texto usando diferentes modelos de embeddings. Algunos modelos que puedes usar incluyen: word2vec, ada-002 de OpenAI, Azure Computer Vision y muchos m치s. La selecci칩n de un modelo depender치 de los idiomas que uses, el tipo de contenido codificado (texto/im치genes/audio), el tama침o de entrada que puede codificar y la longitud de salida del embedding.

Un ejemplo de texto embebido usando el modelo `text-embedding-ada-002` de OpenAI es:
![un embedding de la palabra gato](../../../../../translated_images/es/cat.74cbd7946bc9ca38.webp)

## Recuperaci칩n y b칰squeda vectorial

Cuando un usuario hace una pregunta, el recuperador la transforma en un vector usando el codificador de consultas, luego busca en nuestro 칤ndice de b칰squeda de documentos aquellos vectores relevantes en el documento relacionados con la entrada. Una vez hecho, convierte tanto el vector de entrada como los vectores del documento en texto y los pasa a trav칠s del LLM.

### Recuperaci칩n

La recuperaci칩n ocurre cuando el sistema intenta encontrar r치pidamente los documentos del 칤ndice que cumplan con los criterios de b칰squeda. El objetivo del recuperador es obtener documentos que se usar치n para proporcionar contexto y fundamentar el LLM en tus datos.

Hay varias formas de realizar b칰squedas dentro de nuestra base de datos tales como:

- **B칰squeda por palabra clave**: usada para b칰squedas de texto.

- **B칰squeda vectorial**: convierte documentos de texto a representaciones vectoriales usando modelos de embeddings, permitiendo una **b칰squeda sem치ntica** basada en el significado de las palabras. La recuperaci칩n se hace consultando los documentos cuyas representaciones vectoriales est칠n m치s cerca de la pregunta del usuario.

- **H칤brida**: una combinaci칩n de b칰squeda por palabra clave y vectorial.

Un desaf칤o con la recuperaci칩n surge cuando no hay una respuesta similar a la consulta en la base de datos, el sistema entonces devolver치 la mejor informaci칩n que pueda obtener; sin embargo, puedes usar t치cticas como establecer la distancia m치xima para relevancia o usar b칰squeda h칤brida que combina ambas b칰squedas: por palabras clave y vectoriales. En esta lecci칩n usaremos b칰squeda h칤brida, una combinaci칩n de ambas. Almacenaremos nuestros datos en un dataframe con columnas que contienen los fragmentos as칤 como embeddings.

### Similitud Vectorial

El recuperador buscar치 en la base de conocimiento los embeddings que est칠n cercanos entre s칤, el vecino m치s cercano, ya que son textos similares. En el escenario, cuando un usuario hace una consulta, primero se embebe y luego se empareja con embeddings similares. La medida com칰nmente usada para encontrar cu치n similares son diferentes vectores es la similitud coseno, que se basa en el 치ngulo entre dos vectores.

Podemos medir similitud usando otras alternativas como distancia euclidiana, que es la l칤nea recta entre los puntos finales de vectores, y producto punto, que mide la suma de los productos de los elementos correspondientes de dos vectores.

### 칈ndice de b칰squeda

Al realizar recuperaci칩n, necesitaremos construir un 칤ndice de b칰squeda para nuestra base de conocimiento antes de realizar la b칰squeda. Un 칤ndice almacenar치 nuestros embeddings y podr치 recuperar r치pidamente los fragmentos m치s similares incluso en una base de datos grande. Podemos crear nuestro 칤ndice localmente usando:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Crear el 칤ndice de b칰squeda
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Para consultar el 칤ndice, puedes usar el m칠todo kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```


### Reordenamiento

Una vez que has consultado la base de datos, podr칤as necesitar ordenar los resultados desde los m치s relevantes. Un LLM de reordenamiento utiliza aprendizaje autom치tico para mejorar la relevancia de los resultados de b칰squeda orden치ndolos desde los m치s relevantes. Usando Azure AI Search, el reordenamiento se realiza autom치ticamente para ti usando un reranker sem치ntico. Un ejemplo de c칩mo funciona el reordenamiento usando vecinos m치s cercanos:

```python
# Encuentra los documentos m치s similares
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Imprime los documentos m치s similares
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```


## Integr치ndolo todo

El 칰ltimo paso es agregar nuestro LLM a la mezcla para poder obtener respuestas fundamentadas en nuestros datos. Podemos implementarlo como sigue:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convertir la pregunta a un vector de consulta
    query_vector = create_embeddings(user_input)

    # Encontrar los documentos m치s similares
    distances, indices = nbrs.kneighbors([query_vector])

    # agregar documentos a la consulta para proporcionar contexto
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combinar el historial y la entrada del usuario
    history.append(user_input)

    # crear un objeto de mensaje
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # usar la finalizaci칩n de chat para generar una respuesta
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```


## Evaluando nuestra aplicaci칩n

### M칠tricas de evaluaci칩n

- Calidad de las respuestas suministradas asegurando que suene natural, fluido y humano.

- Fundamentaci칩n de los datos: evaluar si la respuesta proviene de los documentos suministrados.

- Relevancia: evaluar que la respuesta coincida y est칠 relacionada con la pregunta formulada.

- Fluidez: si la respuesta tiene sentido gramaticalmente.

## Casos de uso para usar RAG (Generaci칩n Aumentada por Recuperaci칩n) y bases de datos vectoriales

Hay muchos casos de uso diferentes donde las llamadas a funciones pueden mejorar tu app como:

- Preguntas y respuestas: fundamentar los datos de tu empresa para un chat que pueden usar los empleados para hacer preguntas.

- Sistemas de recomendaci칩n: donde puedes crear un sistema que coincida con los valores m치s similares, por ejemplo, pel칤culas, restaurantes y muchos m치s.

- Servicios de chatbot: puedes almacenar el historial de chat y personalizar la conversaci칩n bas치ndote en los datos del usuario.

- B칰squeda de im치genes basada en embeddings vectoriales, 칰til para reconocimiento de im치genes y detecci칩n de anomal칤as.

## Resumen

Hemos cubierto las 치reas fundamentales de RAG desde a침adir nuestros datos a la aplicaci칩n, la consulta del usuario y la salida. Para simplificar la creaci칩n de RAG, puedes usar frameworks como Semantic Kernel, Langchain o Autogen.

## Tarea

Para continuar tu aprendizaje de Generaci칩n Aumentada por Recuperaci칩n (RAG) puedes construir:

- Construir un front-end para la aplicaci칩n usando el framework de tu elecci칩n.

- Utilizar un framework, ya sea LangChain o Semantic Kernel, y recrear tu aplicaci칩n.

Felicidades por completar la lecci칩n 游녪.

## El aprendizaje no termina aqu칤, contin칰a el viaje

Despu칠s de completar esta lecci칩n, echa un vistazo a nuestra [colecci칩n de Aprendizaje de IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para seguir aumentando tus conocimientos en IA Generativa!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci칩n autom치tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por la precisi칩n, tenga en cuenta que las traducciones automatizadas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente oficial. Para informaci칩n cr칤tica, se recomienda una traducci칩n profesional realizada por humanos. No nos hacemos responsables por malentendidos o interpretaciones err칩neas derivadas del uso de esta traducci칩n.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->