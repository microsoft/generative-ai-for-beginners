<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-05-20T00:59:43+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "es"
}
-->
# Generaci칩n Aumentada por Recuperaci칩n (RAG) y Bases de Datos Vectoriales

[![Generaci칩n Aumentada por Recuperaci칩n (RAG) y Bases de Datos Vectoriales](../../../translated_images/15-lesson-banner.799d0cd2229970edb365f6667a4c7b3a0f526eb8698baa7d2e05c3bd49a5d83f.es.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

En la lecci칩n de aplicaciones de b칰squeda, aprendimos brevemente c칩mo integrar tus propios datos en Modelos de Lenguaje Extenso (LLMs). En esta lecci칩n, profundizaremos en los conceptos de fundamentar tus datos en tu aplicaci칩n LLM, la mec치nica del proceso y los m칠todos para almacenar datos, incluyendo tanto incrustaciones como texto.

> **Video Pr칩ximamente**

## Introducci칩n

En esta lecci칩n cubriremos lo siguiente:

- Una introducci칩n a RAG, qu칠 es y por qu칠 se utiliza en la inteligencia artificial (IA).

- Comprender qu칠 son las bases de datos vectoriales y crear una para nuestra aplicaci칩n.

- Un ejemplo pr치ctico de c칩mo integrar RAG en una aplicaci칩n.

## Objetivos de Aprendizaje

Despu칠s de completar esta lecci칩n, podr치s:

- Explicar la importancia de RAG en la recuperaci칩n y procesamiento de datos.

- Configurar una aplicaci칩n RAG y fundamentar tus datos a un LLM

- Integraci칩n efectiva de RAG y Bases de Datos Vectoriales en Aplicaciones LLM.

## Nuestro Escenario: mejorando nuestros LLMs con nuestros propios datos

Para esta lecci칩n, queremos agregar nuestras propias notas a la startup educativa, lo que permite al chatbot obtener m치s informaci칩n sobre los diferentes temas. Usando las notas que tenemos, los estudiantes podr치n estudiar mejor y comprender los diferentes temas, facilitando la revisi칩n para sus ex치menes. Para crear nuestro escenario, utilizaremos:

- `Azure OpenAI:` el LLM que utilizaremos para crear nuestro chatbot

- `AI for beginners' lesson on Neural Networks`: estos ser치n los datos en los que fundamentaremos nuestro LLM

- `Azure AI Search` y `Azure Cosmos DB:` base de datos vectorial para almacenar nuestros datos y crear un 칤ndice de b칰squeda

Los usuarios podr치n crear cuestionarios de pr치ctica a partir de sus notas, tarjetas de repaso y resumirlas en visiones generales concisas. Para comenzar, veamos qu칠 es RAG y c칩mo funciona:

## Generaci칩n Aumentada por Recuperaci칩n (RAG)

Un chatbot potenciado por LLM procesa las indicaciones del usuario para generar respuestas. Est치 dise침ado para ser interactivo y se involucra con los usuarios en una amplia gama de temas. Sin embargo, sus respuestas est치n limitadas al contexto proporcionado y sus datos de entrenamiento fundamentales. Por ejemplo, el l칤mite de conocimiento de GPT-4 es septiembre de 2021, lo que significa que carece de conocimiento de eventos que han ocurrido despu칠s de este per칤odo. Adem치s, los datos utilizados para entrenar los LLMs excluyen informaci칩n confidencial como notas personales o el manual de productos de una empresa.

### C칩mo funcionan los RAGs (Generaci칩n Aumentada por Recuperaci칩n)

![dibujo mostrando c칩mo funcionan los RAGs](../../../translated_images/how-rag-works.d87a7ed9c30f43126bb9e8e259be5d66e16cd1fef65374e6914746ba9bfb0b2f.es.png)

Supongamos que quieres desplegar un chatbot que cree cuestionarios a partir de tus notas, necesitar치s una conexi칩n con la base de conocimiento. Aqu칤 es donde RAG viene al rescate. Los RAGs operan de la siguiente manera:

- **Base de conocimiento:** Antes de la recuperaci칩n, estos documentos deben ser ingeridos y preprocesados, t칤picamente dividiendo documentos grandes en partes m치s peque침as, transform치ndolos en incrustaciones de texto y almacen치ndolos en una base de datos.

- **Consulta del usuario:** el usuario hace una pregunta

- **Recuperaci칩n:** Cuando un usuario hace una pregunta, el modelo de incrustaci칩n recupera informaci칩n relevante de nuestra base de conocimiento para proporcionar m치s contexto que se incorporar치 a la indicaci칩n.

- **Generaci칩n Aumentada:** el LLM mejora su respuesta basado en los datos recuperados. Permite que la respuesta generada no solo se base en datos pre-entrenados, sino tambi칠n en informaci칩n relevante del contexto agregado. Los datos recuperados se utilizan para aumentar las respuestas del LLM. Luego, el LLM devuelve una respuesta a la pregunta del usuario.

![dibujo mostrando la arquitectura de los RAGs](../../../translated_images/encoder-decode.75eebc7093ccefec17568eebc80d3d0b831ecf2ea204566377a04c77a5a57ebb.es.png)

La arquitectura de los RAGs se implementa utilizando transformadores que constan de dos partes: un codificador y un decodificador. Por ejemplo, cuando un usuario hace una pregunta, el texto de entrada se 'codifica' en vectores que capturan el significado de las palabras y los vectores se 'decodifican' en nuestro 칤ndice de documentos y generan un nuevo texto basado en la consulta del usuario. El LLM utiliza tanto un modelo codificador-decodificador para generar la salida.

Dos enfoques al implementar RAG seg칰n el documento propuesto: [Generaci칩n Aumentada por Recuperaci칩n para Tareas de Procesamiento de Lenguaje Natural Intensivo en Conocimiento](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) son:

- **_RAG-Secuencia_** utilizando documentos recuperados para predecir la mejor respuesta posible a una consulta del usuario

- **RAG-Token** utilizando documentos para generar el siguiente token, luego recuperarlos para responder a la consulta del usuario

### 쯇or qu칠 usar칤as RAGs?

- **Riqueza de informaci칩n:** asegura que las respuestas de texto est칠n actualizadas y sean actuales. Por lo tanto, mejora el rendimiento en tareas espec칤ficas del dominio al acceder a la base de conocimiento interna.

- Reduce la fabricaci칩n utilizando **datos verificables** en la base de conocimiento para proporcionar contexto a las consultas del usuario.

- Es **rentable** ya que son m치s econ칩micos en comparaci칩n con el ajuste fino de un LLM

## Creaci칩n de una base de conocimiento

Nuestra aplicaci칩n se basa en nuestros datos personales, es decir, la lecci칩n de Redes Neuronales en el curr칤culo de AI For Beginners.

### Bases de Datos Vectoriales

Una base de datos vectorial, a diferencia de las bases de datos tradicionales, es una base de datos especializada dise침ada para almacenar, gestionar y buscar vectores incrustados. Almacena representaciones num칠ricas de documentos. Desglosar datos en incrustaciones num칠ricas facilita que nuestro sistema de IA entienda y procese los datos.

Almacenamos nuestras incrustaciones en bases de datos vectoriales ya que los LLMs tienen un l칤mite en la cantidad de tokens que aceptan como entrada. Como no puedes pasar todas las incrustaciones a un LLM, necesitaremos dividirlas en partes y cuando un usuario haga una pregunta, las incrustaciones m치s parecidas a la pregunta se devolver치n junto con la indicaci칩n. La divisi칩n tambi칠n reduce costos en el n칰mero de tokens que se pasan a trav칠s de un LLM.

Algunas bases de datos vectoriales populares incluyen Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant y DeepLake. Puedes crear un modelo de Azure Cosmos DB usando Azure CLI con el siguiente comando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### De texto a incrustaciones

Antes de almacenar nuestros datos, necesitaremos convertirlos en incrustaciones vectoriales antes de que se almacenen en la base de datos. Si est치s trabajando con documentos grandes o textos largos, puedes dividirlos seg칰n las consultas que esperes. La divisi칩n se puede hacer a nivel de oraci칩n o a nivel de p치rrafo. Dado que la divisi칩n deriva significados de las palabras que los rodean, puedes agregar alg칰n otro contexto a una parte, por ejemplo, agregando el t칤tulo del documento o incluyendo alg칰n texto antes o despu칠s de la parte. Puedes dividir los datos de la siguiente manera:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Una vez divididos, podemos incrustar nuestro texto usando diferentes modelos de incrustaci칩n. Algunos modelos que puedes usar incluyen: word2vec, ada-002 de OpenAI, Azure Computer Vision y muchos m치s. La selecci칩n de un modelo para usar depender치 de los idiomas que est칠s utilizando, el tipo de contenido codificado (texto/im치genes/audio), el tama침o de entrada que puede codificar y la longitud de la salida de la incrustaci칩n.

Un ejemplo de texto incrustado usando el modelo `text-embedding-ada-002` de OpenAI es:
![una incrustaci칩n de la palabra gato](../../../translated_images/cat.3db013cbca4fd5d90438ea7b312ad0364f7686cf79931ab15cd5922151aea53e.es.png)

## Recuperaci칩n y B칰squeda Vectorial

Cuando un usuario hace una pregunta, el recuperador la transforma en un vector usando el codificador de consultas, luego busca a trav칠s de nuestro 칤ndice de b칰squeda de documentos vectores relevantes en el documento que est치n relacionados con la entrada. Una vez hecho, convierte tanto el vector de entrada como los vectores de documento en texto y lo pasa a trav칠s del LLM.

### Recuperaci칩n

La recuperaci칩n ocurre cuando el sistema intenta encontrar r치pidamente los documentos del 칤ndice que satisfacen los criterios de b칰squeda. El objetivo del recuperador es obtener documentos que se utilizar치n para proporcionar contexto y fundamentar el LLM en tus datos.

Hay varias formas de realizar b칰squedas dentro de nuestra base de datos, tales como:

- **B칰squeda por palabras clave** - utilizada para b칰squedas de texto

- **B칰squeda sem치ntica** - utiliza el significado sem치ntico de las palabras

- **B칰squeda vectorial** - convierte documentos de texto a representaciones vectoriales usando modelos de incrustaci칩n. La recuperaci칩n se realizar치 consultando los documentos cuyas representaciones vectoriales est칠n m치s cerca de la pregunta del usuario.

- **H칤brida** - una combinaci칩n de b칰squeda por palabras clave y b칰squeda vectorial.

Un desaf칤o con la recuperaci칩n surge cuando no hay una respuesta similar a la consulta en la base de datos, el sistema entonces devolver치 la mejor informaci칩n que pueda obtener, sin embargo, puedes usar t치cticas como establecer la distancia m치xima para la relevancia o usar una b칰squeda h칤brida que combine tanto palabras clave como b칰squeda vectorial. En esta lecci칩n utilizaremos la b칰squeda h칤brida, una combinaci칩n de b칰squeda vectorial y por palabras clave. Almacenaremos nuestros datos en un dataframe con columnas que contienen las partes as칤 como las incrustaciones.

### Similitud Vectorial

El recuperador buscar치 en la base de datos de conocimiento incrustaciones que est칠n cerca, el vecino m치s cercano, ya que son textos similares. En el escenario en que un usuario hace una consulta, primero se incrusta y luego se empareja con incrustaciones similares. La medida com칰n que se utiliza para encontrar cu치n similares son diferentes vectores es la similitud coseno que se basa en el 치ngulo entre dos vectores.

Podemos medir la similitud usando otras alternativas como la distancia Euclidiana que es la l칤nea recta entre los puntos finales de los vectores y el producto punto que mide la suma de los productos de los elementos correspondientes de dos vectores.

### 칈ndice de b칰squeda

Al realizar la recuperaci칩n, necesitaremos construir un 칤ndice de b칰squeda para nuestra base de conocimiento antes de realizar la b칰squeda. Un 칤ndice almacenar치 nuestras incrustaciones y podr치 recuperar r치pidamente las partes m치s similares incluso en una base de datos grande. Podemos crear nuestro 칤ndice localmente usando:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Reordenamiento

Una vez que hayas consultado la base de datos, es posible que necesites ordenar los resultados de los m치s relevantes. Un LLM de reordenamiento utiliza el aprendizaje autom치tico para mejorar la relevancia de los resultados de b칰squeda orden치ndolos de los m치s relevantes. Usando Azure AI Search, el reordenamiento se realiza autom치ticamente para ti usando un reordenador sem치ntico. Un ejemplo de c칩mo funciona el reordenamiento utilizando vecinos m치s cercanos:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Integr치ndolo todo

El 칰ltimo paso es agregar nuestro LLM a la mezcla para poder obtener respuestas que est칠n fundamentadas en nuestros datos. Podemos implementarlo de la siguiente manera:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluando nuestra aplicaci칩n

### M칠tricas de Evaluaci칩n

- Calidad de las respuestas proporcionadas asegurando que suene natural, fluido y humano

- Fundamentaci칩n de los datos: evaluando si la respuesta provino de los documentos proporcionados

- Relevancia: evaluando si la respuesta coincide y est치 relacionada con la pregunta hecha

- Fluidez - si la respuesta tiene sentido gramaticalmente

## Casos de Uso para utilizar RAG (Generaci칩n Aumentada por Recuperaci칩n) y bases de datos vectoriales

Hay muchos casos de uso diferentes donde las llamadas a funciones pueden mejorar tu aplicaci칩n como:

- Preguntas y Respuestas: fundamentando los datos de tu empresa a un chat que puede ser utilizado por los empleados para hacer preguntas.

- Sistemas de Recomendaci칩n: donde puedes crear un sistema que coincida con los valores m치s similares, por ejemplo, pel칤culas, restaurantes y muchos m치s.

- Servicios de Chatbot: puedes almacenar el historial de chat y personalizar la conversaci칩n basada en los datos del usuario.

- B칰squeda de im치genes basada en incrustaciones vectoriales, 칰til cuando se realiza reconocimiento de im치genes y detecci칩n de anomal칤as.

## Resumen

Hemos cubierto las 치reas fundamentales de RAG desde agregar nuestros datos a la aplicaci칩n, la consulta del usuario y la salida. Para simplificar la creaci칩n de RAG, puedes usar marcos como Semanti Kernel, Langchain o Autogen.

## Tarea

Para continuar tu aprendizaje de Generaci칩n Aumentada por Recuperaci칩n (RAG) puedes construir:

- Construir un front-end para la aplicaci칩n usando el marco de tu elecci칩n

- Utilizar un marco, ya sea LangChain o Semantic Kernel, y recrear tu aplicaci칩n.

Felicitaciones por completar la lecci칩n 游녪.

## El aprendizaje no se detiene aqu칤, contin칰a el viaje

Despu칠s de completar esta lecci칩n, revisa nuestra [colecci칩n de Aprendizaje de IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para seguir mejorando tus conocimientos de IA Generativa.

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci칩n autom치tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por lograr precisi칩n, tenga en cuenta que las traducciones autom치ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci칩n cr칤tica, se recomienda una traducci칩n profesional humana. No nos hacemos responsables de malentendidos o interpretaciones err칩neas que surjan del uso de esta traducci칩n.