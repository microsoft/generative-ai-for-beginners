# Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) e Bancos de Dados Vetoriais

[![Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) e Bancos de Dados Vetoriais](../../../translated_images/pt-BR/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

Na li√ß√£o de aplica√ß√µes de busca, aprendemos brevemente como integrar seus pr√≥prios dados em Grandes Modelos de Linguagem (LLMs). Nesta li√ß√£o, exploraremos mais a fundo os conceitos de fundamenta√ß√£o dos seus dados na aplica√ß√£o LLM, a mec√¢nica do processo e os m√©todos para armazenamento de dados, incluindo tanto embeddings quanto texto.

> **V√≠deo em Breve**

## Introdu√ß√£o

Nesta li√ß√£o, abordaremos o seguinte:

- Uma introdu√ß√£o ao RAG, o que √© e por que √© usado em IA (intelig√™ncia artificial).

- Compreens√£o do que s√£o bancos de dados vetoriais e cria√ß√£o de um para nossa aplica√ß√£o.

- Um exemplo pr√°tico de como integrar RAG em uma aplica√ß√£o.

## Objetivos de Aprendizagem

Ap√≥s concluir esta li√ß√£o, voc√™ ser√° capaz de:

- Explicar a import√¢ncia do RAG na recupera√ß√£o e processamento de dados.

- Configurar a aplica√ß√£o RAG e fundamentar seus dados em um LLM.

- Integrar efetivamente RAG e Bancos de Dados Vetoriais em aplica√ß√µes LLM.

## Nosso Cen√°rio: aprimorando nossos LLMs com nossos pr√≥prios dados

Para esta li√ß√£o, queremos adicionar nossas pr√≥prias notas na startup educacional, permitindo que o chatbot obtenha mais informa√ß√µes sobre os diferentes assuntos. Usando as notas que temos, os alunos poder√£o estudar melhor e entender os diferentes t√≥picos, facilitando a revis√£o para seus exames. Para criar nosso cen√°rio, usaremos:

- `Azure OpenAI:` o LLM que usaremos para criar nosso chatbot

- `Li√ß√£o AI para iniciantes sobre Redes Neurais:` esses ser√£o os dados nos quais fundamentaremos nosso LLM

- `Azure AI Search` e `Azure Cosmos DB:` banco de dados vetorial para armazenar nossos dados e criar um √≠ndice de pesquisa

Os usu√°rios poder√£o criar quizzes de pr√°tica a partir de suas notas, cart√µes de revis√£o e resumi-los em vis√µes concisas. Para come√ßar, vamos entender o que √© RAG e como funciona:

## Gera√ß√£o Aumentada por Recupera√ß√£o (RAG)

Um chatbot movido por LLM processa prompts dos usu√°rios para gerar respostas. Ele √© projetado para ser interativo e engajar-se com usu√°rios em uma ampla variedade de t√≥picos. Contudo, suas respostas s√£o limitadas ao contexto fornecido e aos seus dados de treinamento base. Por exemplo, o corte de conhecimento do GPT-4 √© setembro de 2021, o que significa que ele n√£o possui conhecimento de eventos ocorridos ap√≥s esse per√≠odo. Al√©m disso, os dados usados para treinar LLMs excluem informa√ß√µes confidenciais, como notas pessoais ou o manual de produto de uma empresa.

### Como funcionam os RAGs (Gera√ß√£o Aumentada por Recupera√ß√£o)

![drawing showing how RAGs work](../../../translated_images/pt-BR/how-rag-works.f5d0ff63942bd3a6.webp)

Suponha que voc√™ queira implantar um chatbot que cria quizzes a partir de suas notas, ser√° necess√°ria uma conex√£o √† base de conhecimento. √â a√≠ que o RAG entra em a√ß√£o. Os RAGs operam da seguinte forma:

- **Base de conhecimento:** Antes da recupera√ß√£o, esses documentos precisam ser ingeridos e pr√©-processados, tipicamente dividindo grandes documentos em peda√ßos menores, transformando-os em embeddings de texto e armazenando-os em um banco de dados.

- **Consulta do usu√°rio:** o usu√°rio faz uma pergunta

- **Recupera√ß√£o:** Quando um usu√°rio faz uma pergunta, o modelo de embedding recupera informa√ß√µes relevantes da nossa base de conhecimento para fornecer mais contexto que ser√° incorporado ao prompt.

- **Gera√ß√£o Aumentada:** o LLM aprimora sua resposta com base nos dados recuperados. Isso permite que a resposta gerada n√£o seja apenas baseada em dados pr√©-treinados, mas tamb√©m em informa√ß√µes relevantes do contexto adicionado. Os dados recuperados s√£o usados para aumentar as respostas do LLM. O LLM ent√£o retorna uma resposta √† pergunta do usu√°rio.

![drawing showing how RAGs architecture](../../../translated_images/pt-BR/encoder-decode.f2658c25d0eadee2.webp)

A arquitetura para RAGs √© implementada usando transformers consistindo em duas partes: um codificador e um decodificador. Por exemplo, quando um usu√°rio faz uma pergunta, o texto de entrada √© 'codificado' em vetores que capturam o significado das palavras e os vetores s√£o 'decodificados' em nosso √≠ndice de documentos e geram um novo texto baseado na consulta do usu√°rio. O LLM usa tanto um modelo codificador-decodificador para gerar a sa√≠da.

Duas abordagens ao implementar RAG segundo o artigo proposto: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) s√£o:

- **_RAG-Sequence_** usando documentos recuperados para prever a melhor resposta poss√≠vel a uma consulta do usu√°rio

- **RAG-Token** usando documentos para gerar o pr√≥ximo token, depois recuper√°-los para responder √† consulta do usu√°rio

### Por que usar RAGs?

- **Riqueza de informa√ß√£o:** garante que as respostas em texto estejam atualizadas e atuais. Portanto, melhora o desempenho em tarefas espec√≠ficas de dom√≠nio ao acessar a base de conhecimento interna.

- Reduz a fabrica√ß√£o usando **dados verific√°veis** na base de conhecimento para fornecer contexto √†s consultas dos usu√°rios.

- √â **econ√¥mico** pois s√£o mais acess√≠veis comparados ao ajuste fino de um LLM.

## Criando uma base de conhecimento

Nossa aplica√ß√£o √© baseada em nossos dados pessoais, ou seja, a li√ß√£o de Redes Neurais no curr√≠culo AI para Iniciantes.

### Bancos de Dados Vetoriais

Um banco de dados vetorial, diferente dos bancos tradicionais, √© um banco especializado projetado para armazenar, gerenciar e buscar vetores embutidos (embeddings). Ele armazena representa√ß√µes num√©ricas dos documentos. Dividir os dados em embeddings num√©ricos torna mais f√°cil para nosso sistema de IA entender e processar os dados.

Armazenamos nossos embeddings em bancos de dados vetoriais, pois os LLMs t√™m um limite do n√∫mero de tokens que aceitam como entrada. Como n√£o √© poss√≠vel enviar o embedding inteiro para um LLM, precisamos dividi-los em peda√ßos e quando um usu√°rio faz uma pergunta, os embeddings mais relacionados √† pergunta ser√£o retornados junto com o prompt. A divis√£o em peda√ßos tamb√©m reduz custos relacionados ao n√∫mero de tokens enviados ao LLM.

Alguns bancos de dados vetoriais populares incluem Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant e DeepLake. Voc√™ pode criar um modelo Azure Cosmos DB usando Azure CLI com o seguinte comando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### De texto para embeddings

Antes de armazenar nossos dados, precisaremos convert√™-los para embeddings vetoriais antes de armazen√°-los no banco de dados. Se voc√™ estiver trabalhando com documentos grandes ou textos longos, pode dividi-los com base em consultas esperadas. A divis√£o pode ser feita ao n√≠vel de senten√ßa ou de par√°grafo. Como a divis√£o deriva significados das palavras ao redor, voc√™ pode adicionar algum outro contexto a um peda√ßo, por exemplo, adicionando o t√≠tulo do documento ou incluindo algum texto antes ou depois do peda√ßo. Voc√™ pode dividir os dados como segue:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Se o √∫ltimo peda√ßo n√£o atingiu o comprimento m√≠nimo, adicione-o mesmo assim
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Uma vez divididos, podemos ent√£o embutir nosso texto usando diferentes modelos de embedding. Alguns modelos que voc√™ pode usar incluem: word2vec, ada-002 da OpenAI, Azure Computer Vision e muitos outros. A escolha do modelo depender√° das l√≠nguas usadas, do tipo de conte√∫do codificado (texto/imagens/√°udio), do tamanho da entrada codific√°vel e do comprimento da sa√≠da do embedding.

Um exemplo de texto embutido usando o modelo `text-embedding-ada-002` da OpenAI √©:
![an embedding of the word cat](../../../translated_images/pt-BR/cat.74cbd7946bc9ca38.webp)

## Recupera√ß√£o e Busca Vetorial

Quando um usu√°rio faz uma pergunta, o recuperador a transforma em um vetor usando o codificador da consulta, em seguida, ele busca em nosso √≠ndice de busca de documentos por vetores relevantes relacionados √† entrada. Feito isso, converte o vetor de entrada e os vetores do documento em texto e os passa pelo LLM.

### Recupera√ß√£o

A recupera√ß√£o ocorre quando o sistema tenta rapidamente encontrar os documentos do √≠ndice que satisfa√ßam os crit√©rios de busca. O objetivo do recuperador √© obter documentos que ser√£o usados para fornecer contexto e fundamentar o LLM em seus dados.

Existem v√°rias formas de realizar buscas dentro do nosso banco de dados, tais como:

- **Busca por palavra-chave** - usada para buscas em texto

- **Busca vetorial** - converte documentos de texto para representa√ß√µes vetoriais usando modelos de embedding, permitindo uma **busca sem√¢ntica** baseada no significado das palavras. A recupera√ß√£o ser√° feita consultando os documentos cujas representa√ß√µes vetoriais est√£o mais pr√≥ximas da pergunta do usu√°rio.

- **H√≠brida** - uma combina√ß√£o de busca por palavra-chave e vetorial.

Um desafio na recupera√ß√£o ocorre quando n√£o h√° uma resposta semelhante √† consulta no banco de dados; o sistema ent√£o retorna a melhor informa√ß√£o dispon√≠vel. Contudo, voc√™ pode usar t√°ticas como definir a dist√¢ncia m√°xima para relev√¢ncia ou usar busca h√≠brida que combina palavra-chave e busca vetorial. Nesta li√ß√£o usaremos busca h√≠brida, uma combina√ß√£o de busca vetorial e por palavra-chave. Armazenaremos nossos dados em um dataframe com colunas contendo os peda√ßos (chunks) bem como os embeddings.

### Similaridade Vetorial

O recuperador buscar√° pela base de conhecimento embeddings que estejam pr√≥ximos, o vizinho mais pr√≥ximo, pois s√£o textos similares. No cen√°rio em que um usu√°rio faz uma consulta, primeiro ela √© embutida e ent√£o comparada com embeddings similares. A m√©trica comum usada para encontrar qu√£o similares s√£o os vetores √© a similaridade de cosseno, que se baseia no √¢ngulo entre dois vetores.

Podemos medir similaridade usando outras alternativas, como dist√¢ncia Euclidiana, que √© a linha reta entre os pontos finais dos vetores, e produto escalar, que mede a soma dos produtos dos elementos correspondentes de dois vetores.

### √çndice de busca

Ao fazer recupera√ß√µes, precisaremos construir um √≠ndice de busca para nossa base de conhecimento antes de realizar a consulta. Um √≠ndice armazenar√° nossos embeddings e poder√° recuperar rapidamente os peda√ßos mais similares mesmo em um banco de dados grande. Podemos criar nosso √≠ndice localmente usando:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Criar o √≠ndice de busca
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Para consultar o √≠ndice, voc√™ pode usar o m√©todo kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```

### Reordena√ß√£o

Depois de ter consultado o banco de dados, voc√™ pode precisar ordenar os resultados do mais relevante para o menos relevante. Um LLM de reordena√ß√£o utiliza Machine Learning para melhorar a relev√¢ncia dos resultados de busca ordenando-os do mais relevante. Usando Azure AI Search, a reordena√ß√£o √© feita automaticamente para voc√™ usando um reordenador sem√¢ntico. Um exemplo de como a reordena√ß√£o funciona usando vizinhos mais pr√≥ximos:

```python
# Encontrar os documentos mais semelhantes
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Imprimir os documentos mais semelhantes
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Juntando tudo

A √∫ltima etapa √© adicionar nosso LLM para conseguir respostas fundamentadas em nossos dados. Podemos implementar assim:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Converter a pergunta em um vetor de consulta
    query_vector = create_embeddings(user_input)

    # Encontrar os documentos mais semelhantes
    distances, indices = nbrs.kneighbors([query_vector])

    # adicionar documentos √† consulta para fornecer contexto
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combinar o hist√≥rico e a entrada do usu√°rio
    history.append(user_input)

    # criar um objeto de mensagem
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # usar o complemento de chat para gerar uma resposta
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Avaliando nossa aplica√ß√£o

### M√©tricas de Avalia√ß√£o

- Qualidade das respostas fornecidas, garantindo que soem naturais, fluentes e humanas

- Fundamenta√ß√£o dos dados: avaliando se a resposta veio dos documentos fornecidos

- Relev√¢ncia: verificando se a resposta corresponde e est√° relacionada √† pergunta feita

- Flu√™ncia - se a resposta faz sentido gramaticalmente

## Casos de Uso para utilizar RAG (Gera√ß√£o Aumentada por Recupera√ß√£o) e bancos de dados vetoriais

Existem muitos casos de uso onde chamadas de fun√ß√£o podem melhorar sua aplica√ß√£o, como:

- Perguntas e Respostas: fundamentando os dados da sua empresa em um chat que possa ser usado por funcion√°rios para fazer perguntas.

- Sistemas de Recomenda√ß√£o: onde voc√™ pode criar um sistema que combina os valores mais similares, por exemplo, filmes, restaurantes e muito mais.

- Servi√ßos de Chatbot: voc√™ pode armazenar hist√≥rico de conversas e personalizar o di√°logo com base nos dados do usu√°rio.

- Busca de imagens baseada em embeddings vetoriais, √∫til para reconhecimento de imagem e detec√ß√£o de anomalias.

## Resumo

Cobrimos as √°reas fundamentais do RAG desde a adi√ß√£o dos nossos dados na aplica√ß√£o, a consulta do usu√°rio at√© a sa√≠da. Para simplificar a cria√ß√£o do RAG, voc√™ pode usar frameworks como Semanti Kernel, Langchain ou Autogen.

## Tarefa

Para continuar seu aprendizado sobre Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) voc√™ pode construir:

- Criar uma interface para a aplica√ß√£o usando o framework de sua escolha

- Utilizar um framework, seja LangChain ou Semantic Kernel, e recriar sua aplica√ß√£o.

Parab√©ns por concluir a li√ß√£o üëè.

## O aprendizado n√£o para aqui, continue a jornada

Ap√≥s concluir esta li√ß√£o, confira nossa [cole√ß√£o de Aprendizagem de IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para continuar elevando seu conhecimento em IA Generativa!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original no idioma nativo deve ser considerado a fonte autorizada. Para informa√ß√µes cr√≠ticas, recomenda-se tradu√ß√£o profissional realizada por um humano. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->