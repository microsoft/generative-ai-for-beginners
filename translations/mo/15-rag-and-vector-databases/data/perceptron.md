<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-05-20T02:31:35+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "mo"
}
-->
# Introduction to Neural Networks: Perceptron

Má»™t trong nhá»¯ng ná»— lá»±c Ä‘áº§u tiÃªn Ä‘á»ƒ triá»ƒn khai thá»© gÃ¬ Ä‘Ã³ tÆ°Æ¡ng tá»± nhÆ° máº¡ng nÆ¡-ron hiá»‡n Ä‘áº¡i Ä‘Ã£ Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi Frank Rosenblatt tá»« Cornell Aeronautical Laboratory vÃ o nÄƒm 1957. ÄÃ³ lÃ  má»™t thiáº¿t bá»‹ pháº§n cá»©ng gá»i lÃ  "Mark-1", Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ nháº­n diá»‡n cÃ¡c hÃ¬nh há»c cÆ¡ báº£n nhÆ° tam giÃ¡c, vuÃ´ng vÃ  trÃ²n.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> HÃ¬nh áº£nh tá»« Wikipedia

Má»™t hÃ¬nh áº£nh Ä‘áº§u vÃ o Ä‘Æ°á»£c Ä‘áº¡i diá»‡n bá»Ÿi máº£ng táº¿ bÃ o quang 20x20, vÃ¬ váº­y máº¡ng nÆ¡-ron cÃ³ 400 Ä‘áº§u vÃ o vÃ  má»™t Ä‘áº§u ra nhá»‹ phÃ¢n. Má»™t máº¡ng Ä‘Æ¡n giáº£n chá»©a má»™t nÆ¡-ron, cÅ©ng gá»i lÃ  **Ä‘Æ¡n vá»‹ logic ngÆ°á»¡ng**. CÃ¡c trá»ng sá»‘ cá»§a máº¡ng nÆ¡-ron hoáº¡t Ä‘á»™ng nhÆ° cÃ¡c biáº¿n trá»Ÿ cáº§n Ä‘iá»u chá»‰nh thá»§ cÃ´ng trong giai Ä‘oáº¡n huáº¥n luyá»‡n.

> âœ… Biáº¿n trá»Ÿ lÃ  má»™t thiáº¿t bá»‹ cho phÃ©p ngÆ°á»i dÃ¹ng Ä‘iá»u chá»‰nh Ä‘iá»‡n trá»Ÿ cá»§a má»™t máº¡ch.

> Thá»i bÃ¡o New York Ä‘Ã£ viáº¿t vá» perceptron vÃ o thá»i Ä‘iá»ƒm Ä‘Ã³: *phÃ´i cá»§a má»™t mÃ¡y tÃ­nh Ä‘iá»‡n tá»­ mÃ  [Háº£i quÃ¢n] mong Ä‘á»£i sáº½ cÃ³ kháº£ nÄƒng Ä‘i láº¡i, nÃ³i chuyá»‡n, nhÃ¬n tháº¥y, viáº¿t, tá»± tÃ¡i táº¡o vÃ  nháº­n thá»©c Ä‘Æ°á»£c sá»± tá»“n táº¡i cá»§a nÃ³.*

## MÃ´ hÃ¬nh Perceptron

Giáº£ sá»­ chÃºng ta cÃ³ N Ä‘áº·c trÆ°ng trong mÃ´ hÃ¬nh cá»§a mÃ¬nh, trong trÆ°á»ng há»£p Ä‘Ã³ vector Ä‘áº§u vÃ o sáº½ lÃ  má»™t vector cÃ³ kÃ­ch thÆ°á»›c N. Perceptron lÃ  má»™t mÃ´ hÃ¬nh **phÃ¢n loáº¡i nhá»‹ phÃ¢n**, tá»©c lÃ  nÃ³ cÃ³ thá»ƒ phÃ¢n biá»‡t giá»¯a hai lá»›p dá»¯ liá»‡u Ä‘áº§u vÃ o. ChÃºng ta sáº½ giáº£ Ä‘á»‹nh ráº±ng Ä‘á»‘i vá»›i má»—i vector Ä‘áº§u vÃ o x, Ä‘áº§u ra cá»§a perceptron sáº½ lÃ  +1 hoáº·c -1, tÃ¹y thuá»™c vÃ o lá»›p. Äáº§u ra sáº½ Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c:

y(x) = f(w<sup>T</sup>x)

trong Ä‘Ã³ f lÃ  hÃ m kÃ­ch hoáº¡t bÆ°á»›c

## Huáº¥n luyá»‡n Perceptron

Äá»ƒ huáº¥n luyá»‡n perceptron, chÃºng ta cáº§n tÃ¬m má»™t vector trá»ng sá»‘ w phÃ¢n loáº¡i háº§u háº¿t cÃ¡c giÃ¡ trá»‹ Ä‘Ãºng, tá»©c lÃ  dáº«n Ä‘áº¿n lá»—i nhá» nháº¥t. Lá»—i nÃ y Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi **tiÃªu chÃ­ perceptron** theo cÃ¡ch sau:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

trong Ä‘Ã³:

* tá»•ng Ä‘Æ°á»£c láº¥y trÃªn nhá»¯ng Ä‘iá»ƒm dá»¯ liá»‡u huáº¥n luyá»‡n i dáº«n Ä‘áº¿n phÃ¢n loáº¡i sai
* x<sub>i</sub> lÃ  dá»¯ liá»‡u Ä‘áº§u vÃ o, vÃ  t<sub>i</sub> lÃ  -1 hoáº·c +1 cho cÃ¡c vÃ­ dá»¥ tiÃªu cá»±c vÃ  tÃ­ch cá»±c tÆ°Æ¡ng á»©ng.

TiÃªu chÃ­ nÃ y Ä‘Æ°á»£c xem nhÆ° má»™t hÃ m cá»§a trá»ng sá»‘ w, vÃ  chÃºng ta cáº§n tá»‘i thiá»ƒu hÃ³a nÃ³. ThÆ°á»ng thÃ¬ má»™t phÆ°Æ¡ng phÃ¡p gá»i lÃ  **gradient descent** Ä‘Æ°á»£c sá»­ dá»¥ng, trong Ä‘Ã³ chÃºng ta báº¯t Ä‘áº§u vá»›i má»™t sá»‘ trá»ng sá»‘ ban Ä‘áº§u w<sup>(0)</sup>, vÃ  sau Ä‘Ã³ á»Ÿ má»—i bÆ°á»›c cáº­p nháº­t trá»ng sá»‘ theo cÃ´ng thá»©c:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

á» Ä‘Ã¢y Î· lÃ  cÃ¡i gá»i lÃ  **tá»‘c Ä‘á»™ há»c**, vÃ  âˆ‡E(w) biá»ƒu thá»‹ **gradient** cá»§a E. Sau khi chÃºng ta tÃ­nh toÃ¡n gradient, chÃºng ta cÃ³

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Thuáº­t toÃ¡n trong Python trÃ´ng nhÆ° tháº¿ nÃ y:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Káº¿t luáº­n

Trong bÃ i há»c nÃ y, báº¡n Ä‘Ã£ há»c vá» perceptron, má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i nhá»‹ phÃ¢n, vÃ  cÃ¡ch huáº¥n luyá»‡n nÃ³ báº±ng cÃ¡ch sá»­ dá»¥ng vector trá»ng sá»‘.

## ğŸš€ Thá»­ thÃ¡ch

Náº¿u báº¡n muá»‘n thá»­ xÃ¢y dá»±ng perceptron cá»§a riÃªng mÃ¬nh, hÃ£y thá»­ bÃ i thá»±c hÃ nh nÃ y trÃªn Microsoft Learn sá»­ dá»¥ng Azure ML designer

## Ã”n táº­p & Tá»± há»c

Äá»ƒ xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng perceptron Ä‘á»ƒ giáº£i quyáº¿t má»™t váº¥n Ä‘á» Ä‘á»“ chÆ¡i cÅ©ng nhÆ° cÃ¡c váº¥n Ä‘á» thá»±c táº¿, vÃ  Ä‘á»ƒ tiáº¿p tá»¥c há»c - hÃ£y Ä‘áº¿n vá»›i notebook Perceptron.

ÄÃ¢y lÃ  má»™t bÃ i viáº¿t thÃº vá»‹ vá» perceptrons.

## BÃ i táº­p

Trong bÃ i há»c nÃ y, chÃºng ta Ä‘Ã£ triá»ƒn khai má»™t perceptron cho nhiá»‡m vá»¥ phÃ¢n loáº¡i nhá»‹ phÃ¢n, vÃ  chÃºng ta Ä‘Ã£ sá»­ dá»¥ng nÃ³ Ä‘á»ƒ phÃ¢n loáº¡i giá»¯a hai chá»¯ sá»‘ viáº¿t tay. Trong bÃ i thá»±c hÃ nh nÃ y, báº¡n Ä‘Æ°á»£c yÃªu cáº§u giáº£i quyáº¿t váº¥n Ä‘á» phÃ¢n loáº¡i chá»¯ sá»‘ hoÃ n toÃ n, tá»©c lÃ  xÃ¡c Ä‘á»‹nh chá»¯ sá»‘ nÃ o cÃ³ kháº£ nÄƒng tÆ°Æ¡ng á»©ng vá»›i má»™t hÃ¬nh áº£nh Ä‘Ã£ cho.

* HÆ°á»›ng dáº«n
* Notebook

I'm sorry, but I'm not familiar with a language called "mo." Could you please provide more context or specify the language you want the text translated into?