[![Open Source Models](../../../translated_images/zh-MO/18-lesson-banner.f30176815b1a5074.webp)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# 微調您的大型語言模型

使用大型語言模型來建立生成式 AI 應用程序同時帶來新的挑戰。關鍵問題是確保模型針對給定用戶請求所生成內容的回應質量（準確性和相關性）。在先前的課程中，我們討論了提示工程和增強檢索生成等技術，這些方法嘗試通過_修改模型的提示輸入_來解決問題。

在今天的課程中，我們討論第三種技術，**微調**，嘗試通過_使用額外數據重新訓練模型本身_來解決這個挑戰。讓我們深入了解細節。

## 學習目標

本課程介紹了預訓練語言模型的微調概念，探討了此方法的優點和挑戰，並提供了何時以及如何使用微調來提升生成式 AI 模型性能的指導。

完成本課程後，您應能回答以下問題：

- 什麼是語言模型的微調？
- 何時以及為什麼微調有用？
- 我如何微調一個預訓練模型？
- 微調有哪些限制？

準備好了嗎？讓我們開始吧。

## 圖解指南

想在深入之前先了解我們將涵蓋的整體內容嗎？請查看這份圖解指南，描述本課程的學習歷程——從了解微調的核心概念和動機，到理解微調執行流程及最佳實踐。這是一個令人著迷的探索主題，別忘了查看 [資源](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) 頁面以獲取更多支持您自主學習的連結！

![Illustrated Guide to Fine Tuning Language Models](../../../translated_images/zh-MO/18-fine-tuning-sketchnote.11b21f9ec8a70346.webp)

## 什麼是語言模型微調？

按照定義，大型語言模型是基於包含互聯網在內的多元來源大量文本進行_預訓練_的。如同我們在先前課程中學習的，要提升模型對用戶問題（「提示」）的回應質量，需要使用像是_提示工程_與_增強檢索生成_這樣的技術。

一種受歡迎的提示工程技巧是給模型更多指引，說明希望回應包含的內容，無論是透過提供_指示_（明確指引），或_給模型一些示例_（隱式指引）。這被稱為_少量示例學習_，但它有兩個限制：

- 模型的 token 限制會限制您能提供的示例數量，並限制效果。
- 模型 token 成本會使得每個提示都加示例代價高昂，限制彈性。

微調是機器學習系統中一項常見做法，我們會拿一個預訓練模型，用新數據重新訓練以提升其在特定任務上的性能。就語言模型而言，我們可以用_為特定任務或應用領域精心策劃的示例集_微調預訓練模型，創建一個**自訂模型**，它可能在特定任務或領域上會更準確和相關。微調的附帶效益是能降低少量示例學習所需的示例數量——減少 token 使用量及相關成本。

## 何時及為什麼我們應該微調模型？

在_此_情境下，當我們說到微調，指的是**監督式**微調，即通過**添加原始訓練數據集中未包含的新數據**來重新訓練模型。這與無監督微調不同，後者是在原有數據上用不同超參數進行重新訓練。

需要記住的關鍵是，微調是一種進階技術，需要一定的專業知識才能取得理想結果。若操作不當，可能無法帶來預期的改進，甚至會降低模型在目標領域的效能。

所以，在學習「如何」微調語言模型之前，您需要知道「為什麼」要採用這條路線，以及「何時」開始微調過程。先問自己這些問題：

- **使用案例**：您針對微調的_使用案例_是什麼？想改善當前預訓練模型的哪個方面？
- **替代方案**：您是否嘗試過_其他技術_達成預期目標？用它們來建立基準。
  - 提示工程：嘗試使用少量示例提示技術，帶入相關回應示例。評估回應質量。
  - 增強檢索生成：嘗試用查詢結果增強提示，評估回應質量。
- **成本**：您是否評估過微調的成本？
  - 可調整性 - 預訓練模型是否允許微調？
  - 工作量 - 用於準備訓練數據、評估及調整模型。
  - 計算資源 - 進行微調工作及部署微調模型所需。
  - 數據 - 是否具備足夠且高質量的示例以產生影響。
- **效益**：您是否確認了微調的好處？
  - 質量 - 微調模型是否超越了基準？
  - 成本 - 是否透過簡化提示降低了 token 使用？
  - 擴展性 - 是否能將基礎模型重用於新領域？

透過回答這些問題，您應能決定微調是否適合您的使用案例。理想情況是，只有當效益超過成本時，此方法才成立。一旦決定要進行，接著就是思考_如何_微調預訓練模型。

想深入了解決策過程？請觀看 [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## 我們如何微調預訓練模型？

微調預訓練模型需要：

- 一個可供微調的預訓練模型
- 用於微調的數據集
- 執行微調作業的訓練環境
- 部署微調模型的主機環境

## 微調實作演示

以下資源提供了逐步教學，帶您使用選定模型及精選數據集完成真實案例。要操作這些教程，您需在特定服務供應商擁有帳號，並能訪問相關模型與數據集。

| 供應商       | 教程                                                                                                                                                                        | 說明                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [如何微調聊天模型](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                         | 學習如何針對特定領域（「食譜助手」）微調 `gpt-35-turbo`，包括準備訓練數據、運行微調任務及使用微調後模型進行推論。                                                                                                                                                                                                                                                                                                                      |
| Azure OpenAI | [GPT 3.5 Turbo 微調教程](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst)           | 學習如何在 **Azure** 平台上微調 `gpt-35-turbo-0613` 模型，包括建立和上傳訓練數據、執行微調任務，部署並使用新模型。                                                                                                                                                                                                                                                                                                           |
| Hugging Face | [使用 Hugging Face 微調大型語言模型](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                         | 這篇部落格教你如何用 Hugging Face 的 [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) 函式庫和 [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) 以及 Hugging Face 上的公開 [datasets](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) 微調 _開放大型語言模型_（如 `CodeLlama 7B`）。 |
|              |                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| 🤗 AutoTrain | [使用 AutoTrain 微調大型語言模型](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                    | AutoTrain（或稱 AutoTrain Advanced）是由 Hugging Face 開發的 python 函式庫，支持許多任務的微調包括 LLM 微調。AutoTrain 是一個免寫代碼解決方案，微調可在您自己的雲端環境、Hugging Face Spaces 或本機進行。它同時支持基於網頁的 GUI、CLI 和 yaml 配置文件的訓練。                                                                                                                                                      |
|              |                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| 🦥 Unsloth   | [使用 Unsloth 微調大型語言模型](https://github.com/unslothai/unsloth)                                                                                                   | Unsloth 是一個開源框架，支持大型語言模型的微調及強化學習（RL）。Unsloth 簡化了本地訓練、評估與部署流程，附帶方便使用的 [notebooks](https://github.com/unslothai/notebooks)。還支持文本轉語音（TTS）、BERT 和多模態模型。想開始使用，請參考他們的逐步 [微調大型語言模型指南](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide)。                                                                                       |
|              |                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                   |
## 作業

從上方教程中選擇一個並進行實作。_我們可能會在本存儲庫中以 Jupyter 筆記本版本複製這些教程僅供參考，請直接使用原始來源以獲取最新版本_。

## 太好了！繼續您的學習。

完成本課程後，請查看我們的 [生成式 AI 學習合集](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，持續提升您的生成式 AI 知識！

恭喜您完成本課程的 v2 系列最終課！別停下學習和實作的腳步。**請查看 [資源](RESOURCES.md?WT.mc_id=academic-105485-koreyst) 頁面，裡面有更多這個主題的額外建議清單。

我們的 v1 系列課程也已更新，增加了更多作業與概念。花一點時間複習並請[分享您的問題和回饋](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst)，幫助我們為社群持續改善課程。

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**免責聲明**：  
本文件經由 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於提供準確的翻譯，但請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應視為權威來源。對於重要資訊，建議尋求專業人類翻譯。我們不對因使用本翻譯而產生的任何誤解或誤釋承擔責任。
<!-- CO-OP TRANSLATOR DISCLAIMER END -->