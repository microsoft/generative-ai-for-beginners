<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T02:08:29+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "sr"
}
-->
# Okviri neuronskih mreÅ¾a

Kao Å¡to smo veÄ‡ nauÄili, da bismo mogli efikasno trenirati neuronske mreÅ¾e, potrebno je da uradimo dve stvari:

* Da radimo sa tenzorima, npr. da mnoÅ¾imo, sabiramo i raÄunamo neke funkcije kao Å¡to su sigmoid ili softmax
* Da izraÄunamo gradijente svih izraza, kako bismo izvrÅ¡ili optimizaciju spuÅ¡tanjem gradijenta

Dok `numpy` biblioteka moÅ¾e da uradi prvi deo, potrebna nam je neka mehanizam za raÄunanje gradijenata. U naÅ¡em okviru koji smo razvili u prethodnom odeljku morali smo ruÄno da programiramo sve funkcije derivacija unutar `backward` metode, koja vrÅ¡i povratnu propagaciju. Idealno, okvir bi trebalo da nam pruÅ¾i moguÄ‡nost da izraÄunamo gradijente *bilo kog izraza* koji moÅ¾emo definisati.

JoÅ¡ jedna vaÅ¾na stvar je da moÅ¾emo da obavljamo proraÄune na GPU, ili bilo kojim drugim specijalizovanim jedinicama za proraÄun, kao Å¡to je TPU. Trening dubokih neuronskih mreÅ¾a zahteva *mnogo* proraÄuna, i moguÄ‡nost da se ti proraÄuni paralelizuju na GPU-ima je veoma vaÅ¾na.

> âœ… Termin 'paralelizacija' znaÄi raspodelu proraÄuna na viÅ¡e ureÄ‘aja.

Trenutno, dva najpopularnija okvira za neuronske mreÅ¾e su: TensorFlow i PyTorch. Oba pruÅ¾aju API niskog nivoa za rad sa tenzorima na CPU i GPU. Na vrhu API-ja niskog nivoa, postoji i API viÅ¡eg nivoa, koji se zove Keras i PyTorch Lightning, respektivno.

API niskog nivoa | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
API visokog nivoa| Keras| Pytorch

**API niskog nivoa** u oba okvira omoguÄ‡avaju vam da gradite takozvane **grafove proraÄuna**. Ovaj graf definiÅ¡e kako izraÄunati izlaz (obiÄno funkciju gubitka) sa datim ulaznim parametrima, i moÅ¾e biti gurnut za proraÄun na GPU, ako je dostupan. Postoje funkcije za diferencijaciju ovog grafika proraÄuna i raÄunanje gradijenata, koje se zatim mogu koristiti za optimizaciju parametara modela.

**API visokog nivoa** uglavnom smatraju neuronske mreÅ¾e kao **sekvencu slojeva**, i Äine konstrukciju veÄ‡ine neuronskih mreÅ¾a mnogo lakÅ¡om. Trening modela obiÄno zahteva pripremu podataka i zatim pozivanje `fit` funkcije da obavi posao.

API visokog nivoa omoguÄ‡ava vam da brzo konstruirate tipiÄne neuronske mreÅ¾e bez brige o mnogim detaljima. Istovremeno, API niskog nivoa pruÅ¾a mnogo viÅ¡e kontrole nad procesom treninga, i zbog toga se puno koriste u istraÅ¾ivanju, kada se bavite novim arhitekturama neuronskih mreÅ¾a.

TakoÄ‘e je vaÅ¾no razumeti da moÅ¾ete koristiti oba API-ja zajedno, npr. moÅ¾ete razviti svoju arhitekturu slojeva mreÅ¾e koristeÄ‡i API niskog nivoa, a zatim je koristiti unutar veÄ‡e mreÅ¾e konstruisane i trenirane sa API-jem visokog nivoa. Ili moÅ¾ete definisati mreÅ¾u koristeÄ‡i API visokog nivoa kao sekvencu slojeva, a zatim koristiti svoj sopstveni trening loop niskog nivoa da izvrÅ¡ite optimizaciju. Oba API-ja koriste iste osnovne koncepte, i dizajnirani su da dobro rade zajedno.

## UÄenje

U ovom kursu, nudimo veÄ‡inu sadrÅ¾aja kako za PyTorch, tako i za TensorFlow. MoÅ¾ete izabrati svoj preferirani okvir i proÄ‡i samo kroz odgovarajuÄ‡e beleÅ¾nice. Ako niste sigurni koji okvir da izaberete, proÄitajte neke diskusije na internetu o **PyTorch vs. TensorFlow**. TakoÄ‘e moÅ¾ete pogledati oba okvira da biste stekli bolje razumevanje.

Gde god je moguÄ‡e, koristiÄ‡emo API visokog nivoa radi jednostavnosti. MeÄ‘utim, verujemo da je vaÅ¾no razumeti kako neuronske mreÅ¾e funkcioniÅ¡u od temelja, stoga na poÄetku poÄinjemo sa radom sa API-jem niskog nivoa i tenzorima. MeÄ‘utim, ako Å¾elite brzo da krenete i ne Å¾elite da potroÅ¡ite puno vremena na uÄenje ovih detalja, moÅ¾ete ih preskoÄiti i odmah preÄ‡i na beleÅ¾nice API-ja visokog nivoa.

## âœï¸ VeÅ¾be: Okviri

Nastavite svoje uÄenje u sledeÄ‡im beleÅ¾nicama:

API niskog nivoa | BeleÅ¾nica TensorFlow+Keras | PyTorch
--------------|-------------------------------------|--------------------------------
API visokog nivoa| Keras | *PyTorch Lightning*

Nakon savladavanja okvira, hajde da ponovimo pojam prekomernog prilagoÄ‘avanja.

# Prekomerno prilagoÄ‘avanje

Prekomerno prilagoÄ‘avanje je izuzetno vaÅ¾an koncept u maÅ¡inskom uÄenju, i veoma je vaÅ¾no da ga pravilno razumemo!

Razmotrite sledeÄ‡i problem aproksimacije 5 taÄaka (predstavljenih sa `x` na grafikonima ispod):

!linear | prekomerno prilagoÄ‘avanje
-------------------------|--------------------------
**Linearni model, 2 parametra** | **Nelinearni model, 7 parametara**
GreÅ¡ka treninga = 5.3 | GreÅ¡ka treninga = 0
GreÅ¡ka validacije = 5.1 | GreÅ¡ka validacije = 20

* Levo vidimo dobru aproksimaciju pravom linijom. Zato Å¡to je broj parametara adekvatan, model pravilno razume distribuciju taÄaka.
* Desno, model je previÅ¡e moÄ‡an. Zato Å¡to imamo samo 5 taÄaka, a model ima 7 parametara, moÅ¾e se prilagoditi na takav naÄin da proÄ‘e kroz sve taÄke, ÄineÄ‡i greÅ¡ku treninga 0. MeÄ‘utim, to spreÄava model da razume ispravan obrazac podataka, stoga je greÅ¡ka validacije veoma visoka.

Veoma je vaÅ¾no postiÄ‡i pravilan balans izmeÄ‘u bogatstva modela (broja parametara) i broja uzoraka za trening.

## ZaÅ¡to dolazi do prekomernog prilagoÄ‘avanja

  * Nedovoljno podataka za trening
  * PreviÅ¡e moÄ‡an model
  * PreviÅ¡e Å¡uma u ulaznim podacima

## Kako otkriti prekomerno prilagoÄ‘avanje

Kao Å¡to moÅ¾ete videti sa grafikona iznad, prekomerno prilagoÄ‘avanje moÅ¾e se otkriti vrlo niskom greÅ¡kom treninga i visokom greÅ¡kom validacije. ObiÄno tokom treninga vidimo da greÅ¡ke treninga i validacije poÄinju da se smanjuju, a zatim u nekom trenutku greÅ¡ka validacije moÅ¾e prestati da se smanjuje i poÄeti da raste. To Ä‡e biti znak prekomernog prilagoÄ‘avanja i indikator da verovatno treba da prestanemo sa treningom u tom trenutku (ili barem da napravimo snimak modela).

## Kako spreÄiti prekomerno prilagoÄ‘avanje

Ako vidite da dolazi do prekomernog prilagoÄ‘avanja, moÅ¾ete uraditi jedno od sledeÄ‡eg:

 * PoveÄ‡ajte koliÄinu podataka za trening
 * Smanjite sloÅ¾enost modela
 * Koristite neku tehniku regularizacije, kao Å¡to je Dropout, koju Ä‡emo razmotriti kasnije.

## Prekomerno prilagoÄ‘avanje i kompromis pristrasnosti-varijanse

Prekomerno prilagoÄ‘avanje je zapravo sluÄaj opÅ¡teg problema u statistici koji se zove kompromis pristrasnosti-varijanse. Ako razmotrimo moguÄ‡e izvore greÅ¡ke u naÅ¡em modelu, moÅ¾emo videti dve vrste greÅ¡aka:

* **GreÅ¡ke pristrasnosti** su uzrokovane time Å¡to naÅ¡ algoritam nije u stanju da pravilno uhvati odnos izmeÄ‘u podataka za trening. To moÅ¾e biti rezultat Äinjenice da naÅ¡ model nije dovoljno moÄ‡an (**nedovoljno prilagoÄ‘avanje**).
* **GreÅ¡ke varijanse**, koje su uzrokovane time Å¡to model aproksimira Å¡um u ulaznim podacima umesto smislenog odnosa (**prekomerno prilagoÄ‘avanje**).

Tokom treninga, greÅ¡ka pristrasnosti se smanjuje (kako naÅ¡ model uÄi da aproksimira podatke), a greÅ¡ka varijanse se poveÄ‡ava. VaÅ¾no je prestati sa treningom - ili ruÄno (kada otkrijemo prekomerno prilagoÄ‘avanje) ili automatski (uvoÄ‘enjem regularizacije) - da bismo spreÄili prekomerno prilagoÄ‘avanje.

## ZakljuÄak

U ovoj lekciji ste nauÄili o razlikama izmeÄ‘u razliÄitih API-ja za dva najpopularnija AI okvira, TensorFlow i PyTorch. Pored toga, nauÄili ste o veoma vaÅ¾noj temi, prekomernom prilagoÄ‘avanju.

## ğŸš€ Izazov

U prateÄ‡im beleÅ¾nicama, pronaÄ‡i Ä‡ete 'zadatke' na dnu; proÄ‘ite kroz beleÅ¾nice i zavrÅ¡ite zadatke.

## Pregled i samostalno uÄenje

Uradite istraÅ¾ivanje o sledeÄ‡im temama:

- TensorFlow
- PyTorch
- Prekomerno prilagoÄ‘avanje

Postavite sebi sledeÄ‡a pitanja:

- Koja je razlika izmeÄ‘u TensorFlow i PyTorch?
- Koja je razlika izmeÄ‘u prekomernog prilagoÄ‘avanja i nedovoljnog prilagoÄ‘avanja?

## Zadatak

U ovoj laboratoriji, od vas se traÅ¾i da reÅ¡ite dva problema klasifikacije koristeÄ‡i jednostruke i viÅ¡eslojne potpuno povezane mreÅ¾e koristeÄ‡i PyTorch ili TensorFlow.

**ĞĞ´Ñ€Ğ¸Ñ†Ğ°ÑšĞµ Ğ¾Ğ´ Ğ¾Ğ´Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ğ¾ÑÑ‚Ğ¸**:  
ĞĞ²Ğ°Ñ˜ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ñ˜Ğµ Ğ¿Ñ€ĞµĞ²ĞµĞ´ĞµĞ½ ĞºĞ¾Ñ€Ğ¸ÑˆÑ›ĞµÑšĞµĞ¼ ÑƒÑĞ»ÑƒĞ³Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾Ñ’ĞµÑšĞ° Ğ²ĞµÑˆÑ‚Ğ°Ñ‡ĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ¸Ğ³ĞµĞ½Ñ†Ğ¸Ñ˜Ğ¾Ğ¼ [Co-op Translator](https://github.com/Azure/co-op-translator). Ğ˜Ğ°ĞºĞ¾ ÑĞµ Ñ‚Ñ€ÑƒĞ´Ğ¸Ğ¼Ğ¾ Ğ´Ğ° Ğ¾Ğ±ĞµĞ·Ğ±ĞµĞ´Ğ¸Ğ¼Ğ¾ Ñ‚Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚, Ğ¼Ğ¾Ğ»Ğ¸Ğ¼Ğ¾ Ğ²Ğ°Ñ Ğ´Ğ° Ğ±ÑƒĞ´ĞµÑ‚Ğµ ÑĞ²ĞµÑĞ½Ğ¸ Ğ´Ğ° Ğ°ÑƒÑ‚Ğ¾Ğ¼Ğ°Ñ‚ÑĞºĞ¸ Ğ¿Ñ€ĞµĞ²Ğ¾Ğ´Ğ¸ Ğ¼Ğ¾Ğ³Ñƒ ÑĞ°Ğ´Ñ€Ğ¶Ğ°Ñ‚Ğ¸ Ğ³Ñ€ĞµÑˆĞºĞµ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ‚Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ½Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ½Ğ° ÑšĞµĞ³Ğ¾Ğ²Ğ¾Ğ¼ Ğ¸Ğ·Ğ²Ğ¾Ñ€Ğ½Ğ¾Ğ¼ Ñ˜ĞµĞ·Ğ¸ĞºÑƒ Ñ‚Ñ€ĞµĞ±Ğ° ÑĞ¼Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ¸ Ğ°ÑƒÑ‚Ğ¾Ñ€Ğ¸Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¼ Ğ¸Ğ·Ğ²Ğ¾Ñ€Ğ¾Ğ¼. Ğ—Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ˜Ğµ, Ğ¿Ñ€ĞµĞ¿Ğ¾Ñ€ÑƒÑ‡ÑƒÑ˜Ğµ ÑĞµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ½Ğ¸ Ñ™ÑƒĞ´ÑĞºĞ¸ Ğ¿Ñ€ĞµĞ²Ğ¾Ğ´. ĞĞµ ÑĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ Ğ¾Ğ´Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ğ¾ÑÑ‚ Ğ·Ğ° Ğ±Ğ¸Ğ»Ğ¾ ĞºĞ°ĞºĞ²Ğ° Ğ½ĞµÑĞ¿Ğ¾Ñ€Ğ°Ğ·ÑƒĞ¼ĞµĞ²Ğ°ÑšĞ° Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ³Ñ€ĞµÑˆĞ½Ğ° Ñ‚ÑƒĞ¼Ğ°Ñ‡ĞµÑšĞ° ĞºĞ¾Ñ˜Ğ° Ğ¼Ğ¾Ğ³Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ°Ñ›Ğ¸ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¸ÑˆÑ›ĞµÑšĞ° Ğ¾Ğ²Ğ¾Ğ³ Ğ¿Ñ€ĞµĞ²Ğ¾Ğ´Ğ°.