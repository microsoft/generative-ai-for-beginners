{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рад са моделима из Meta породице\n",
    "\n",
    "## Увод\n",
    "\n",
    "Ова лекција обухвата:\n",
    "\n",
    "- Истраживање два главна модела из Meta породице - Llama 3.1 и Llama 3.2\n",
    "- Разумевање намена и сценарија за сваки модел\n",
    "- Пример кода који показује јединствене карактеристике сваког модела\n",
    "\n",
    "## Meta породица модела\n",
    "\n",
    "У овој лекцији ћемо истражити 2 модела из Meta породице или \"Llama Herd\" - Llama 3.1 и Llama 3.2\n",
    "\n",
    "Ови модели постоје у различитим варијантама и доступни су на Github Model marketplace-у. Више детаља о коришћењу Github Models за [прототипирање са AI моделима](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Варијанте модела:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Напомена: Llama 3 је такође доступан на Github Models, али неће бити обрађен у овој лекцији*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ллама 3.1\n",
    "\n",
    "Са 405 милијарди параметара, Ллама 3.1 спада у категорију отвореног кода LLM модела.\n",
    "\n",
    "Ова верзија је унапређена у односу на претходну Ллама 3 и доноси:\n",
    "\n",
    "- Већи контекстуални прозор – 128k токена уместо 8k токена\n",
    "- Већи максимални број излазних токена – 4096 уместо 2048\n",
    "- Бољу подршку за више језика – захваљујући већем броју токена у обуци\n",
    "\n",
    "Ове могућности омогућавају Ллама 3.1 да обрађује сложеније случајеве употребе при развоју GenAI апликација, укључујући:\n",
    "- Нативно позивање функција – могућност позивања спољних алата и функција ван LLM радног тока\n",
    "- Боље RAG перформансе – захваљујући већем контекстуалном прозору\n",
    "- Генерисање синтетичких података – могућност креирања ефикасних података за задатке као што је фино подешавање\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Позивање нативних функција\n",
    "\n",
    "Llama 3.1 је додатно прилагођена да буде ефикаснија у позивању функција или алата. Такође има два уграђена алата које модел може да препозна као потребне за коришћење на основу упита корисника. Ови алати су:\n",
    "\n",
    "- **Brave Search** – Може се користити за добијање најновијих информација као што је временска прогноза путем претраге на интернету\n",
    "- **Wolfram Alpha** – Може се користити за сложеније математичке прорачуне, тако да није потребно да сами пишете функције.\n",
    "\n",
    "Можете такође направити сопствене прилагођене алате које LLM може да позове.\n",
    "\n",
    "У примеру кода испод:\n",
    "\n",
    "- Дефинишемо доступне алате (brave_search, wolfram_alpha) у системском упиту.\n",
    "- Пошаљемо кориснички упит који пита за временску прогнозу у одређеном граду.\n",
    "- LLM ће одговорити позивом алата Brave Search који ће изгледати овако `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Напомена: Овај пример само позива алат, ако желите да добијете резултате, потребно је да направите бесплатан налог на Brave API страници и дефинишете саму функцију*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ллама 3.2\n",
    "\n",
    "Иако је Ллама 3.1 велики језички модел, једно од њених ограничења је мултимодалност. То значи да не може да користи различите типове улаза као што су слике као упите и да на њих одговара. Ова способност је једна од главних карактеристика Ллама 3.2. Ове могућности такође укључују:\n",
    "\n",
    "- Мултимодалност – има могућност да обрађује и текстуалне и сликовне упите\n",
    "- Варијанте мале и средње величине (11B и 90B) – ово омогућава флексибилне опције за имплементацију,\n",
    "- Варијанте само за текст (1B и 3B) – ово омогућава моделу да се користи на edge / мобилним уређајима и обезбеђује ниску латенцију\n",
    "\n",
    "Подршка за мултимодалност представља велики корак у свету open source модела. Пример кода испод прихвата и слику и текстуални упит како би добио анализу слике од Ллама 3.2 90B.\n",
    "\n",
    "### Мултимодална подршка са Ллама 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Учење се не завршава овде, наставите своје путовање\n",
    "\n",
    "Након што завршите ову лекцију, погледајте нашу [Збирку за учење о генеративној вештачкој интелигенцији](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) како бисте наставили да унапређујете своје знање о генеративној вештачкој интелигенцији!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Одрицање од одговорности**:  \nОвај документ је преведен коришћењем AI услуге за превођење [Co-op Translator](https://github.com/Azure/co-op-translator). Иако настојимо да обезбедимо тачност, имајте у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални људски превод. Не сносимо одговорност за било каква неспоразума или погрешна тумачења настала коришћењем овог превода.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:49:23+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "sr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}