<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-05-20T08:22:12+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "sr"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.8487555c3e3225eefc1dc84e72c8e00bce1ee76db867a080628fb0fbb04aa0d2.sr.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Fino pode코avanje va코eg LLM-a

Kori코캖enje velikih jezi캜kih modela za izgradnju generativnih AI aplikacija dolazi sa novim izazovima. Klju캜no pitanje je osiguranje kvaliteta odgovora (ta캜nost i relevantnost) u sadr쬬ju koji model generi코e za dati korisni캜ki zahtev. U prethodnim lekcijama smo diskutovali o tehnikama kao 코to su in쬰njering upita i generacija uz oboga캖ivanje podacima, koje poku코avaju da re코e problem _modifikovanjem ulaznog upita_ postoje캖em modelu.

U dana코njoj lekciji, diskutujemo o tre캖oj tehnici, **fino pode코avanje**, koja poku코ava da re코i izazov _preobu캜avanjem samog modela_ dodatnim podacima. Hajde da se udubimo u detalje.

## Ciljevi u캜enja

Ova lekcija uvodi koncept finog pode코avanja za unapred obu캜ene jezi캜ke modele, istra쬿je prednosti i izazove ovog pristupa i pru쬬 smernice o tome kada i kako koristiti fino pode코avanje da pobolj코ate performanse va코ih generativnih AI modela.

Na kraju ove lekcije, trebalo bi da mo쬰te odgovoriti na slede캖a pitanja:

- 맚a je fino pode코avanje za jezi캜ke modele?
- Kada i za코to je fino pode코avanje korisno?
- Kako mogu fino podesiti unapred obu캜eni model?
- Koja su ograni캜enja finog pode코avanja?

Spremni? Hajde da po캜nemo.

## Ilustrovani vodi캜

콯elite da dobijete celokupnu sliku onoga 코to 캖emo pokriti pre nego 코to se udubimo? Pogledajte ovaj ilustrovani vodi캜 koji opisuje put u캜enja za ovu lekciju - od u캜enja osnovnih koncepata i motivacije za fino pode코avanje, do razumevanja procesa i najboljih praksi za izvr코enje zadatka finog pode코avanja. Ovo je fascinantna tema za istra쬴vanje, pa ne zaboravite da pogledate stranicu [Resursi](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) za dodatne linkove koji 캖e podr쬬ti va코e samostalno u캜enje!

![Ilustrovani vodi캜 za fino pode코avanje jezi캜kih modela](../../../translated_images/18-fine-tuning-sketchnote.92733966235199dd260184b1aae3a84b877c7496bc872d8e63ad6fa2dd96bafc.sr.png)

## 맚a je fino pode코avanje za jezi캜ke modele?

Po definiciji, veliki jezi캜ki modeli su _unapred obu캜eni_ na velikim koli캜inama teksta prikupljenog iz raznih izvora, uklju캜uju캖i internet. Kao 코to smo nau캜ili u prethodnim lekcijama, potrebne su nam tehnike kao 코to su _in쬰njering upita_ i _generacija uz oboga캖ivanje podacima_ da bismo pobolj코ali kvalitet odgovora modela na korisni캜ka pitanja ("upite").

Popularna tehnika in쬰njeringa upita uklju캜uje davanje modelu vi코e smernica o tome 코ta se o캜ekuje u odgovoru bilo pru쬬njem _instrukcija_ (eksplicitne smernice) ili _davanjem nekoliko primera_ (implicitne smernice). Ovo se naziva _u캜enje sa nekoliko primera_, ali ima dva ograni캜enja:

- Ograni캜enja tokena modela mogu ograni캜iti broj primera koje mo쬰te dati i smanjiti efikasnost.
- Tro코kovi tokena modela mogu u캜initi skupo dodavanje primera svakom upitu i ograni캜iti fleksibilnost.

Fino pode코avanje je uobi캜ajena praksa u sistemima ma코inskog u캜enja gde uzimamo unapred obu캜eni model i ponovo ga obu캜avamo sa novim podacima kako bismo pobolj코ali njegovu performansu na odre캠enom zadatku. U kontekstu jezi캜kih modela, mo쬰mo fino podesiti unapred obu캜eni model _sa pa쬷jivo odabranim skupom primera za dati zadatak ili aplikacionu oblast_ kako bismo kreirali **prilago캠eni model** koji mo쬰 biti precizniji i relevantniji za taj specifi캜ni zadatak ili oblast. Sporedna korist finog pode코avanja je da mo쬰 smanjiti broj primera potrebnih za u캜enje sa nekoliko primera - smanjuju캖i upotrebu tokena i povezane tro코kove.

## Kada i za코to treba fino pode코avati modele?

U _ovom_ kontekstu, kada govorimo o finom pode코avanju, mislimo na **supervizirano** fino pode코avanje gde se preobuka vr코i dodavanjem **novih podataka** koji nisu bili deo originalnog skupa podataka za obuku. Ovo je druga캜ije od pristupa nesupervizovanog finog pode코avanja gde se model ponovo obu캜ava na originalnim podacima, ali sa razli캜itim hiperparametrima.

Klju캜na stvar koju treba zapamtiti je da je fino pode코avanje napredna tehnika koja zahteva odre캠eni nivo stru캜nosti da bi se postigli 쬰ljeni rezultati. Ako se ne uradi pravilno, mo쬯a ne캖e pru쬴ti o캜ekivana pobolj코anja, pa 캜ak mo쬰 i degradirati performanse modela za va코u ciljanu oblast.

Dakle, pre nego 코to nau캜ite "kako" fino pode코avati jezi캜ke modele, morate znati "za코to" biste krenuli tim putem i "kada" zapo캜eti proces finog pode코avanja. Po캜nite tako 코to 캖ete sebi postaviti ova pitanja:

- **Upotreba**: Koji je va코 _slu캜aj upotrebe_ za fino pode코avanje? Koji aspekt trenutnog unapred obu캜enog modela 쬰lite da pobolj코ate?
- **Alternative**: Da li ste probali _druge tehnike_ da postignete 쬰ljene rezultate? Koristite ih da kreirate osnovu za pore캠enje.
  - In쬰njering upita: Poku코ajte tehnike kao 코to su u캜enje sa nekoliko primera sa primerima relevantnih odgovora na upite. Procijenite kvalitet odgovora.
  - Generacija uz oboga캖ivanje podacima: Poku코ajte obogatiti upite rezultatima pretrage va코ih podataka. Procijenite kvalitet odgovora.
- **Tro코kovi**: Da li ste identifikovali tro코kove za fino pode코avanje?
  - Prilagodljivost - da li je unapred obu캜eni model dostupan za fino pode코avanje?
  - Napor - za pripremu podataka za obuku, evaluaciju i pobolj코anje modela.
  - Ra캜unanje - za izvo캠enje poslova finog pode코avanja i implementaciju fino pode코enog modela.
  - Podaci - pristup dovoljnim kvalitetnim primerima za uticaj finog pode코avanja.
- **Prednosti**: Da li ste potvrdili prednosti finog pode코avanja?
  - Kvalitet - da li je fino pode코eni model nadma코io osnovni model?
  - Tro코ak - da li smanjuje upotrebu tokena pojednostavljivanjem upita?
  - Pro코irivost - mo쬰te li prilagoditi osnovni model za nove oblasti?

Odgovaraju캖i na ova pitanja, trebalo bi da budete u mogu캖nosti da odlu캜ite da li je fino pode코avanje pravi pristup za va코 slu캜aj upotrebe. Idealno, pristup je validan samo ako prednosti nadma코uju tro코kove. Kada odlu캜ite da nastavite, vreme je da razmislite o tome _kako_ mo쬰te fino podesiti unapred obu캜eni model.

콯elite vi코e uvida u proces dono코enja odluka? Pogledajte [Da li fino pode코avati ili ne fino pode코avati](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Kako mo쬰mo fino podesiti unapred obu캜eni model?

Da biste fino podesili unapred obu캜eni model, potrebno je da imate:

- unapred obu캜eni model za fino pode코avanje
- skup podataka za kori코캖enje u finom pode코avanju
- okru쬰nje za obuku za pokretanje posla finog pode코avanja
- okru쬰nje za hosting za implementaciju fino pode코enog modela

## Fino pode코avanje u praksi

Slede캖i resursi pru쬬ju uputstva korak po korak koja vas vode kroz stvarni primer kori코캖enja odabranog modela sa pa쬷jivo odabranim skupom podataka. Da biste pro코li kroz ova uputstva, potrebno vam je nalog kod odre캠enog provajdera, zajedno sa pristupom relevantnom modelu i skupovima podataka.

| Provajder    | Uputstvo                                                                                                                                                                       | Opis                                                                                                                                                                                                                                                                                                                                                                                                                                |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [Kako fino podesiti modele za 캖askanje](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)       | Nau캜ite kako fino podesiti `gpt-35-turbo` za specifi캜nu oblast ("asistent za recepte") pripremom podataka za obuku, pokretanjem posla finog pode코avanja i kori코캖enjem fino pode코enog modela za izvo캠enje.                                                                                                                                                                                                                          |
| Azure OpenAI | [GPT 3.5 Turbo uputstvo za fino pode코avanje](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | Nau캜ite kako fino podesiti `gpt-35-turbo-0613` model **na Azure** preduzimanjem koraka za kreiranje i otpremanje podataka za obuku, pokretanje posla finog pode코avanja. Implementirajte i koristite novi model.                                                                                                                                                                                                                       |
| Hugging Face | [Fino pode코avanje LLM-ova sa Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                         | Ovaj blog post vas vodi kroz fino pode코avanje _otvorenog LLM-a_ (npr. `CodeLlama 7B`) koriste캖i biblioteku [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) i [Reinforcement Learning za transformere (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) sa otvorenim [skupovima podataka](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) na Hugging Face. |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 游뱅 AutoTrain | [Fino pode코avanje LLM-ova sa AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                   | AutoTrain (ili AutoTrain Advanced) je python biblioteka koju je razvio Hugging Face i omogu캖ava fino pode코avanje za mnoge razli캜ite zadatke, uklju캜uju캖i fino pode코avanje LLM-ova. AutoTrain je re코enje bez koda i fino pode코avanje se mo쬰 obaviti u va코em oblaku, na Hugging Face Spaces ili lokalno. Podr쬬va i web-bazirani GUI, CLI i obuku putem yaml konfiguracionih datoteka.                                                   |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Zadatak

Izaberite jedno od gore navedenih uputstava i pro캠ite kroz njega. _Mo쬯a 캖emo replicirati verziju ovih uputstava u Jupyter bele쬹icama u ovom repo-u samo za referencu. Molimo koristite originalne izvore direktno da biste dobili najnovije verzije_.

## Odli캜an posao! Nastavite sa u캜enjem.

Nakon 코to zavr코ite ovu lekciju, pogledajte na코u [Kolekciju u캜enja o generativnoj AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) da nastavite sa unapre캠ivanjem va코eg znanja o generativnoj AI!

캛estitamo!! Zavr코ili ste poslednju lekciju iz v2 serije za ovaj kurs! Nemojte prestati sa u캜enjem i izgradnjom. **Pogledajte [RESURSI](RESOURCES.md?WT.mc_id=academic-105485-koreyst) stranicu za listu dodatnih predloga samo za ovu temu.

Na코a v1 serija lekcija je tako캠e a쬿rirana sa vi코e zadataka i koncepata. Zato odvojite minut da osve쬴te svoje znanje - i molimo vas da [podelite svoja pitanja i povratne informacije](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) kako bismo pobolj코ali ove lekcije za zajednicu.

**뤯얨햦혢햣혴햣 쮏 쮏얧쮏쒫쮐햫쮐혝햦**:  
뤯쒫썜 햢쮏쥄햪햣햫혝 혲햣 햣쒫왏얧왏 햨쮐햦혣혵햣혴햣햪 AI 혞혜햩혞햡햣 향햟 햣쒫쮐뉋왐뛣 [Co-op Translator](https://github.com/Azure/co-op-translator). 햊햟햨 혜햣 혝혞햢햦햪 햢햟 쮐혝햦햡햫햣햪 혝햟혢햫쮐혝, 햪쮏햦햪 쒫썜 햢햟 햠혞햢햣혝햣 혜쒫왐햫햦 햢햟 햟혞혝쮏쨿썜햦향쮏쒫썛쫧 햣쒫쮏얧 햪쮏혞 혜햟햢햤햟혝햦 햡햣혣햨햣 햦햩햦 햫햣혝햟혢햫쮐혝햦. 뤰햦햡햦햫햟햩햫햦 햢쮏쥄햪햣햫혝 햫햟 혴햣햡쮏쒫쮏 햦향쒫쮐햫쮏 혲햣향햦햨혞 혝햣햠햟 혜햪햟혝햟혝햦 햟혞혝쮐햦혝햟혝햦쒫쫧쟳 햦향쒫쮐쮏. 행햟 햨햦혝햦혢햫햣 햦햫혟쮐햪햟혡햦혲햣, 햣쮐혞혢혞혲햣 혜햣 쮐햣혜햦쮏쫧썛햫햦 혳혞햢혜햨햦 햣쒫쮏. 햏햣 혜햫쮐햦햪 쮏얧쮏쒫쮐햫쮐혝 향햟 햠햦햩 햨햟햨쒫 햫햣혜쮐햟향혞햪햣 햦햩햦 쮏햣혣햫햟 혝혞햪햟혢햣혴햟 햨쮐떓 쮏쟳햦햩햟향햣 햦향 혞쮐햣햠햣 쮏쒫쮏 햣쒫쮏얧.