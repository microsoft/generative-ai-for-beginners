{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с моделите от семейството на Meta\n",
    "\n",
    "## Въведение\n",
    "\n",
    "В този урок ще разгледаме:\n",
    "\n",
    "- Двата основни модела от семейството на Meta – Llama 3.1 и Llama 3.2\n",
    "- За какви случаи и сценарии е подходящ всеки модел\n",
    "- Примерен код, който показва уникалните възможности на всеки модел\n",
    "\n",
    "## Семейството модели на Meta\n",
    "\n",
    "В този урок ще разгледаме два модела от семейството на Meta, наричано още \"Llama Herd\" – Llama 3.1 и Llama 3.2\n",
    "\n",
    "Тези модели имат различни варианти и са достъпни на пазара за модели в Github. Повече информация за използването на Github Models за [прототипиране с AI модели](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) можете да намерите тук.\n",
    "\n",
    "Варианти на моделите:\n",
    "- Llama 3.1 – 70B Instruct\n",
    "- Llama 3.1 – 405B Instruct\n",
    "- Llama 3.2 – 11B Vision Instruct\n",
    "- Llama 3.2 – 90B Vision Instruct\n",
    "\n",
    "*Забележка: Llama 3 също е наличен в Github Models, но няма да бъде разглеждан в този урок*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "С 405 милиарда параметъра, Llama 3.1 попада в категорията на отворените LLM модели.\n",
    "\n",
    "Този модел е надграждане на предишната версия Llama 3, като предлага:\n",
    "\n",
    "- По-голям контекстен прозорец – 128k токена срещу 8k токена\n",
    "- По-голям максимален брой изходни токени – 4096 срещу 2048\n",
    "- По-добра многоезична поддръжка – благодарение на увеличения брой тренировъчни токени\n",
    "\n",
    "Тези подобрения позволяват на Llama 3.1 да се справя с по-сложни случаи на употреба при създаване на GenAI приложения, включително:\n",
    "- Вградени извиквания на функции – възможност за използване на външни инструменти и функции извън работния процес на LLM\n",
    "- По-добра RAG производителност – благодарение на по-големия контекстен прозорец\n",
    "- Генериране на синтетични данни – възможност за създаване на ефективни данни за задачи като фино настройване\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извикване на собствени функции\n",
    "\n",
    "Llama 3.1 е допълнително обучен, за да бъде по-ефективен при извикване на функции или инструменти. Моделът разполага и с два вградени инструмента, които може да разпознае като необходими според подадената от потребителя заявка. Тези инструменти са:\n",
    "\n",
    "- **Brave Search** – Може да се използва за получаване на актуална информация, като например времето, чрез търсене в интернет\n",
    "- **Wolfram Alpha** – Може да се използва за по-сложни математически изчисления, така че не е нужно да пишете собствени функции.\n",
    "\n",
    "Можете също така да създадете свои собствени инструменти, които LLM да извиква.\n",
    "\n",
    "В примерния код по-долу:\n",
    "\n",
    "- Дефинираме наличните инструменти (brave_search, wolfram_alpha) в системния prompt.\n",
    "- Изпращаме потребителска заявка, която пита за времето в определен град.\n",
    "- LLM ще отговори с извикване на инструмент Brave Search, което ще изглежда така: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Забележка: Този пример само извиква инструмента. Ако искате да получите резултатите, ще трябва да си създадете безплатен акаунт в страницата на Brave API и да дефинирате самата функция.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Въпреки че е LLM, една от основните слабости на Llama 3.1 е липсата на мултимодалност. Тоест, невъзможността да използва различни типове вход, като например изображения, като подсказки и да предоставя отговори. Тази способност е една от основните характеристики на Llama 3.2. Тези възможности включват също:\n",
    "\n",
    "- Мултимодалност – може да обработва както текстови, така и визуални подсказки\n",
    "- Вариации с малък до среден размер (11B и 90B) – това осигурява гъвкави възможности за внедряване,\n",
    "- Само текстови вариации (1B и 3B) – позволява моделът да се използва на edge / мобилни устройства и осигурява ниска латентност\n",
    "\n",
    "Мултимодалната поддръжка е голяма крачка напред в света на отворения код. Примерът с код по-долу приема както изображение, така и текстова подсказка, за да получи анализ на изображението от Llama 3.2 90B.\n",
    "\n",
    "### Мултимодална поддръжка с Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Учението не спира тук, продължи своето пътешествие\n",
    "\n",
    "След като завършиш този урок, разгледай нашата [колекция за обучение по Генеративен ИИ](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), за да продължиш да развиваш знанията си в областта на Генеративния ИИ!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от отговорност**:  \nТози документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, имайте предвид, че автоматичните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия изходен език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Не носим отговорност за недоразумения или погрешни тълкувания, произтичащи от използването на този превод.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:49:03+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "bg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}