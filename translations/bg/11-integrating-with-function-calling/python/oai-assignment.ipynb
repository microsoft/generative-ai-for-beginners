{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Въведение\n",
    "\n",
    "Този урок ще обхване:\n",
    "- Какво е извикване на функция и кога се използва\n",
    "- Как да създадете извикване на функция с OpenAI\n",
    "- Как да интегрирате извикване на функция в приложение\n",
    "\n",
    "## Учебни цели\n",
    "\n",
    "След като завършите този урок, ще знаете и разбирате:\n",
    "\n",
    "- Защо се използва извикване на функция\n",
    "- Как да настроите извикване на функция чрез OpenAI Service\n",
    "- Как да проектирате ефективни извиквания на функции според нуждите на вашето приложение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиране на извикванията на функции\n",
    "\n",
    "В този урок ще изградим функционалност за нашия образователен стартъп, която позволява на потребителите да използват чатбот, за да намират технически курсове. Ще препоръчваме курсове, които отговарят на тяхното ниво на умения, текуща роля и интересуваща ги технология.\n",
    "\n",
    "За да реализираме това, ще използваме комбинация от:\n",
    " - `OpenAI` за създаване на чат изживяване за потребителя\n",
    " - `Microsoft Learn Catalog API`, за да помагаме на потребителите да намират курсове според тяхното запитване\n",
    " - `Function Calling`, за да вземем заявката на потребителя и да я изпратим към функция, която да направи API заявка.\n",
    "\n",
    "За да започнем, нека видим защо изобщо бихме искали да използваме извикване на функции:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # получаваме нов отговор от GPT, при който той може да види отговора на функцията\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Защо да използваме Function Calling\n",
    "\n",
    "Ако сте преминали през някой друг урок от този курс, вероятно вече разбирате колко мощни са големите езикови модели (LLMs). Надявам се също така да сте забелязали и някои от техните ограничения.\n",
    "\n",
    "Function Calling е функция на OpenAI Service, създадена да реши следните предизвикателства:\n",
    "\n",
    "Непоследователен формат на отговорите:\n",
    "- Преди function calling, отговорите от големите езикови модели бяха неструктурирани и непоследователни. Разработчиците трябваше да пишат сложен код за валидиране, за да обработят всяка вариация в изхода.\n",
    "\n",
    "Ограничена интеграция с външни данни:\n",
    "- Преди тази функция беше трудно да се включат данни от други части на приложението в чат контекста.\n",
    "\n",
    "Като стандартизира формата на отговорите и позволява лесна интеграция с външни данни, function calling улеснява разработката и намалява нуждата от допълнителна логика за валидиране.\n",
    "\n",
    "Потребителите не можеха да получат отговори като \"Какво е времето в момента в Стокхолм?\". Причината е, че моделите са ограничени до момента, в който са били обучени с данни.\n",
    "\n",
    "Нека разгледаме примера по-долу, който илюстрира този проблем:\n",
    "\n",
    "Да кажем, че искаме да създадем база данни с данни за студенти, за да можем да им препоръчаме подходящ курс. По-долу имаме две описания на студенти, които са много сходни по информацията, която съдържат.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Искаме да изпратим това към LLM, за да анализира данните. По-късно това може да се използва в нашето приложение, за да се изпрати към API или да се съхрани в база данни.\n",
    "\n",
    "Нека създадем два еднакви подканващи текста, с които инструктираме LLM каква информация ни интересува:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Искаме да изпратим това на LLM, за да анализира частите, които са важни за нашия продукт. Така че можем да създадем два еднакви подканващи текста, за да инструктираме LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "След като създадем тези два подканващи текста, ще ги изпратим към LLM, използвайки `openai.ChatCompletion`. Съхраняваме подканващия текст в променливата `messages` и задаваме ролята на `user`. Това е с цел да се имитира съобщение от потребител, написано към чатбот.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега можем да изпратим и двата заявки към LLM и да разгледаме получения отговор.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Въпреки че подканите са еднакви и описанията са сходни, можем да получим различни формати на свойството `Grades`.\n",
    "\n",
    "Ако изпълните горната клетка няколко пъти, форматът може да бъде `3.7` или `3.7 GPA`.\n",
    "\n",
    "Това е така, защото LLM приема неструктурирани данни под формата на написаната подканя и също така връща неструктурирани данни. Необходимо е да имаме структуриран формат, за да знаем какво да очакваме, когато съхраняваме или използваме тези данни.\n",
    "\n",
    "Чрез използване на функционално извикване можем да сме сигурни, че ще получим обратно структурирани данни. Когато използваме функционално извикване, LLM всъщност не извиква и не изпълнява никакви функции. Вместо това създаваме структура, която LLM да следва в отговорите си. След това използваме тези структурирани отговори, за да знаем коя функция да изпълним в нашите приложения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Диаграма на потока на извикване на функция](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.bg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примери за използване на извикване на функции\n",
    "\n",
    "**Извикване на външни инструменти**  \n",
    "Чатботовете са отлични в предоставянето на отговори на въпроси от потребителите. Чрез използване на извикване на функции, чатботовете могат да използват съобщенията от потребителите, за да изпълнят определени задачи. Например, студент може да помоли чатбота: \"Изпрати имейл на моя преподавател, че имам нужда от повече помощ по този предмет\". Това може да извика функцията `send_email(to: string, body: string)`\n",
    "\n",
    "**Създаване на API или заявки към база данни**  \n",
    "Потребителите могат да намират информация, използвайки естествен език, който се преобразува във форматирана заявка или API заявка. Пример за това е учител, който пита: \"Кои са учениците, които завършиха последното задание\", което може да извика функция с име `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Създаване на структурирани данни**  \n",
    "Потребителите могат да вземат текстов блок или CSV и да използват LLM, за да извлекат важна информация от него. Например, студент може да преобразува статия от Уикипедия за мирни споразумения, за да създаде AI флашкарти. Това може да се направи чрез функцията `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Създаване на първото ви извикване на функция\n",
    "\n",
    "Процесът на създаване на извикване на функция включва 3 основни стъпки:\n",
    "1. Извикване на Chat Completions API с списък от вашите функции и съобщение от потребителя\n",
    "2. Прочитане на отговора на модела, за да се извърши действие, например изпълнение на функция или API заявка\n",
    "3. Още едно извикване към Chat Completions API с отговора от вашата функция, за да използвате тази информация за създаване на отговор към потребителя.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Поток на извикване на функция](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.bg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Елементи на извикване на функция\n",
    "\n",
    "#### Вход от потребителя\n",
    "\n",
    "Първата стъпка е да създадете съобщение от потребителя. Това може да се зададе динамично чрез стойността на текстово поле или можете да зададете стойност тук. Ако за първи път работите с Chat Completions API, трябва да определим `role` и `content` на съобщението.\n",
    "\n",
    "`role` може да бъде `system` (за създаване на правила), `assistant` (моделът) или `user` (крайният потребител). За извикване на функция ще го зададем като `user` и ще дадем примерен въпрос.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Създаване на функции.\n",
    "\n",
    "Следва да дефинираме функция и нейните параметри. Тук ще използваме само една функция, наречена `search_courses`, но можете да създадете и повече функции.\n",
    "\n",
    "**Важно**: Функциите се включват в системното съобщение към LLM и ще бъдат част от броя налични токени, с които разполагате.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дефиниции**\n",
    "\n",
    "Структурата на дефиницията на функция има няколко нива, като всяко от тях има свои свойства. Ето как изглежда вложената структура:\n",
    "\n",
    "**Свойства на функцията на най-високо ниво:**\n",
    "\n",
    "`name` - Името на функцията, която искаме да бъде извикана.\n",
    "\n",
    "`description` - Описание на начина, по който работи функцията. Тук е важно да бъдете конкретни и ясни.\n",
    "\n",
    "`parameters` - Списък с стойности и формат, които искате моделът да използва в отговора си.\n",
    "\n",
    "**Свойства на обекта Parameters:**\n",
    "\n",
    "`type` - Типът данни на обекта parameters (обикновено \"object\")\n",
    "\n",
    "`properties` - Списък със специфичните стойности, които моделът ще използва за отговора си.\n",
    "\n",
    "**Свойства на отделен параметър:**\n",
    "\n",
    "`name` - Неявно дефинирано чрез ключа на свойството (например \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Типът данни на конкретния параметър (например \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - Описание на конкретния параметър\n",
    "\n",
    "**Опционални свойства:**\n",
    "\n",
    "`required` - Масив, който изброява кои параметри са задължителни, за да бъде извикването на функцията успешно\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извикване на функцията\n",
    "След като сме дефинирали функция, сега трябва да я включим в извикването към Chat Completion API. Това става като добавим `functions` към заявката. В този случай използваме `functions=functions`.\n",
    "\n",
    "Има и опция да зададете `function_call` на `auto`. Това означава, че ще оставим LLM да реши коя функция да извика според съобщението на потребителя, вместо ние да я избираме.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега нека разгледаме отговора и да видим как е форматиран:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Може да видите, че името на функцията е извикано и от съобщението на потребителя LLM е успял да намери данните, които да попълни като аргументи на функцията.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Интегриране на извиквания на функции в приложение.\n",
    "\n",
    "След като тествахме форматирания отговор от LLM, сега можем да го интегрираме в приложение.\n",
    "\n",
    "### Управление на процеса\n",
    "\n",
    "За да го интегрираме в нашето приложение, нека следваме следните стъпки:\n",
    "\n",
    "Първо, нека направим заявка към OpenAI услугите и запазим съобщението в променлива, наречена `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега ще дефинираме функцията, която ще извика Microsoft Learn API, за да получи списък с курсове:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Като добра практика, ще проверим дали моделът иска да извика функция. След това ще създадем една от наличните функции и ще я свържем с функцията, която се извиква.\n",
    "След това ще вземем аргументите на функцията и ще ги съпоставим с аргументите от LLM.\n",
    "\n",
    "Накрая ще добавим съобщението за извикване на функцията и стойностите, които са върнати от съобщението `search_courses`. Това дава на LLM цялата необходима информация,\n",
    "за да отговори на потребителя на естествен език.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предизвикателство с код\n",
    "\n",
    "Страхотна работа! За да продължите с ученето за OpenAI Function Calling, можете да изградите: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Още параметри на функцията, които могат да помогнат на обучаващите се да намерят повече курсове. Можете да намерите наличните API параметри тук:\n",
    " - Създайте още едно извикване на функция, което взема повече информация от обучаващия се, например неговия роден език\n",
    " - Добавете обработка на грешки, когато извикването на функцията и/или API не връща подходящи курсове\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от отговорност**:  \nТози документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия изходен език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Не носим отговорност за недоразумения или погрешни тълкувания, произтичащи от използването на този превод.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T21:20:49+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "bg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}