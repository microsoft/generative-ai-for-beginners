{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Въведение \n",
    "\n",
    "Този урок ще обхване: \n",
    "- Какво е извикване на функция и неговите случаи на използване \n",
    "- Как да създадете извикване на функция с помощта на OpenAI \n",
    "- Как да интегрирате извикване на функция в приложение \n",
    "\n",
    "## Цели на обучението \n",
    "\n",
    "След завършване на този урок ще знаете как да и ще разберете: \n",
    "\n",
    "- Целта на използването на извикване на функция \n",
    "- Настройване на извикване на функция с помощта на OpenAI Service \n",
    "- Проектиране на ефективни извиквания на функции за случая на използване на вашето приложение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиране на извикванията на функции\n",
    "\n",
    "За този урок искаме да създадем функция за нашия образователен стартъп, която позволява на потребителите да използват чатбот за намиране на технически курсове. Ще препоръчваме курсове, които отговарят на тяхното ниво на умения, текущата роля и интересуващата ги технология.\n",
    "\n",
    "За да завършим това, ще използваме комбинация от:\n",
    " - `OpenAI` за създаване на чат изживяване за потребителя\n",
    " - `Microsoft Learn Catalog API`, за да помогнем на потребителите да намерят курсове въз основа на заявката на потребителя\n",
    " - `Function Calling`, за да вземем заявката на потребителя и да я изпратим към функция за извършване на API заявката.\n",
    "\n",
    "За да започнем, нека разгледаме защо бихме искали да използваме извикване на функция на първо място:\n",
    "\n",
    "print(\"Съобщения в следващата заявка:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # получаване на нов отговор от GPT, където може да види отговора на функцията\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Защо извикване на функции\n",
    "\n",
    "Ако сте завършили някой друг урок в този курс, вероятно разбирате силата на използването на Големи езикови модели (LLMs). Надяваме се, че също така можете да видите някои от техните ограничения.\n",
    "\n",
    "Извикването на функции е функция на OpenAI Service, предназначена да адресира следните предизвикателства:\n",
    "\n",
    "Несъгласувано форматиране на отговорите:\n",
    "- Преди извикването на функции, отговорите от голям езиков модел бяха неструктурирани и несъгласувани. Разработчиците трябваше да пишат сложен код за валидиране, за да обработват всяка вариация в изхода.\n",
    "\n",
    "Ограничена интеграция с външни данни:\n",
    "- Преди тази функция беше трудно да се включат данни от други части на приложението в контекста на чат.\n",
    "\n",
    "Чрез стандартизиране на форматирането на отговорите и осигуряване на безпроблемна интеграция с външни данни, извикването на функции опростява разработката и намалява нуждата от допълнителна логика за валидиране.\n",
    "\n",
    "Потребителите не можеха да получат отговори като \"Какво е текущото време в Стокхолм?\". Това е така, защото моделите бяха ограничени до времето, на което са обучени данните.\n",
    "\n",
    "Нека разгледаме примера по-долу, който илюстрира този проблем:\n",
    "\n",
    "Да кажем, че искаме да създадем база данни със студентски данни, за да можем да им предложим подходящия курс. По-долу имаме две описания на студенти, които са много сходни по данните, които съдържат.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Искаме да изпратим това на LLM, за да анализира данните. Това по-късно може да се използва в нашето приложение за изпращане към API или за съхранение в база данни.\n",
    "\n",
    "Нека създадем два идентични подсказки, в които да инструктираме LLM каква информация ни интересува:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Искаме да изпратим това на LLM, за да анализира частите, които са важни за нашия продукт. Така можем да създадем два идентични подсказки, за да инструктираме LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "След създаването на тези два подсказки, ще ги изпратим към LLM, като използваме `openai.ChatCompletion`. Съхраняваме подсказката в променливата `messages` и задаваме ролята на `user`. Това е, за да се имитира съобщение от потребител, написано към чатбот.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега можем да изпратим и двете заявки към LLM и да разгледаме получения отговор.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Въпреки че подканите са еднакви и описанията са подобни, можем да получим различни формати на свойството `Grades`.\n",
    "\n",
    "Ако изпълните горната клетка няколко пъти, форматът може да бъде `3.7` или `3.7 GPA`.\n",
    "\n",
    "Това е така, защото LLM приема неструктурирани данни под формата на написаната подканваща команда и връща също неструктурирани данни. Трябва да имаме структуриран формат, за да знаем какво да очакваме при съхранение или използване на тези данни.\n",
    "\n",
    "Чрез използване на функционално извикване можем да сме сигурни, че ще получим обратно структуриран формат на данните. При използване на функционално извикване LLM всъщност не извиква или изпълнява никакви функции. Вместо това създаваме структура, която LLM трябва да следва за своите отговори. След това използваме тези структурирани отговори, за да знаем коя функция да изпълним в нашите приложения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Диаграма на потока на извикване на функция](../../../../translated_images/Function-Flow.083875364af4f4bb.bg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "След това можем да вземем това, което функцията връща, и да го изпратим обратно на LLM. LLM след това ще отговори, използвайки естествен език, за да отговори на запитването на потребителя.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примери за използване на извиквания на функции\n",
    "\n",
    "**Извикване на външни инструменти**  \n",
    "Чатботовете са отлични в предоставянето на отговори на въпроси от потребителите. Чрез използване на извикване на функции, чатботовете могат да използват съобщения от потребителите, за да изпълнят определени задачи. Например, студент може да помоли чатбота да „Изпрати имейл на моя преподавател, че имам нужда от повече помощ по този предмет“. Това може да направи извикване на функцията `send_email(to: string, body: string)`\n",
    "\n",
    "**Създаване на API или заявки към база данни**  \n",
    "Потребителите могат да намерят информация, използвайки естествен език, който се преобразува във форматирана заявка или API повикване. Пример за това може да бъде учител, който пита „Кои са студентите, които са завършили последното задание“, което може да извика функция с име `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Създаване на структурирани данни**  \n",
    "Потребителите могат да вземат блок текст или CSV и да използват LLM, за да извлекат важна информация от него. Например, студент може да преобразува статия от Уикипедия за мирни споразумения, за да създаде AI флашкарти. Това може да се направи чрез използване на функция, наречена `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Създаване на първото ви извикване на функция\n",
    "\n",
    "Процесът на създаване на извикване на функция включва 3 основни стъпки:  \n",
    "1. Извикване на Chat Completions API с списък на вашите функции и съобщение от потребителя  \n",
    "2. Прочитане на отговора на модела, за да се извърши действие, т.е. изпълнение на функция или API извикване  \n",
    "3. Направете друго извикване към Chat Completions API с отговора от вашата функция, за да използвате тази информация за създаване на отговор към потребителя.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Поток на извикване на функция](../../../../translated_images/LLM-Flow.3285ed8caf4796d7.bg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Елементи на извикване на функция \n",
    "\n",
    "#### Вход от потребителя \n",
    "\n",
    "Първата стъпка е да се създаде съобщение от потребителя. Това може да бъде динамично зададено чрез вземане на стойността от текстово поле или можете да зададете стойност тук. Ако това е първият път, когато работите с Chat Completions API, трябва да дефинираме `role` и `content` на съобщението. \n",
    "\n",
    "`role` може да бъде или `system` (създаване на правила), `assistant` (моделът) или `user` (крайният потребител). За извикване на функция ще зададем това като `user` и примерен въпрос. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Създаване на функции.\n",
    "\n",
    "След това ще дефинираме функция и параметрите на тази функция. Тук ще използваме само една функция, наречена `search_courses`, но можете да създадете и няколко функции.\n",
    "\n",
    "**Важно** : Функциите са включени в системното съобщение към LLM и ще бъдат включени в броя на наличните токени, с които разполагате.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дефиниции** \n",
    "\n",
    "Структурата на дефиницията на функцията има няколко нива, всяко със свои собствени свойства. Ето разбивка на вложената структура:\n",
    "\n",
    "**Основни свойства на функцията:**\n",
    "\n",
    "`name` - Името на функцията, която искаме да бъде извикана. \n",
    "\n",
    "`description` - Това е описанието на начина, по който функцията работи. Тук е важно да бъдете конкретни и ясни. \n",
    "\n",
    "`parameters` - Списък с стойности и формат, които искате моделът да генерира в своя отговор. \n",
    "\n",
    "**Свойства на обекта Parameters:**\n",
    "\n",
    "`type` - Типът данни на обекта parameters (обикновено \"object\")\n",
    "\n",
    "`properties` - Списък със специфичните стойности, които моделът ще използва за своя отговор. \n",
    "\n",
    "**Свойства на отделните параметри:**\n",
    "\n",
    "`name` - Имплицитно дефинирано чрез ключа на свойството (например \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Типът данни на конкретния параметър (например \"string\", \"number\", \"boolean\") \n",
    "\n",
    "`description` - Описание на конкретния параметър. \n",
    "\n",
    "**Опционални свойства:**\n",
    "\n",
    "`required` - Масив, изброяващ кои параметри са задължителни за изпълнението на извикването на функцията.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извикване на функцията  \n",
    "След като дефинираме функция, сега трябва да я включим в извикването на Chat Completion API. Правим това, като добавим `functions` към заявката. В този случай `functions=functions`. \n",
    "\n",
    "Има и опция да зададем `function_call` на `auto`. Това означава, че ще позволим на LLM да реши коя функция трябва да бъде извикана въз основа на съобщението на потребителя, вместо да я задаваме ние.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега нека разгледаме отговора и да видим как е форматиран:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Можете да видите, че името на функцията е извикано и от съобщението на потребителя LLM успя да намери данните, които да отговарят на аргументите на функцията.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Интегриране на извиквания на функции в приложение. \n",
    "\n",
    "\n",
    "След като тествахме форматирания отговор от LLM, сега можем да го интегрираме в приложение. \n",
    "\n",
    "### Управление на потока \n",
    "\n",
    "За да го интегрираме в нашето приложение, нека предприемем следните стъпки: \n",
    "\n",
    "Първо, нека направим извикването към услугите на OpenAI и съхраним съобщението в променлива, наречена `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега ще дефинираме функцията, която ще извика Microsoft Learn API, за да получи списък с курсове:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Като най-добра практика, след това ще видим дали моделът иска да извика функция. След това ще създадем една от наличните функции и ще я съпоставим с функцията, която се извиква.  \n",
    "След това ще вземем аргументите на функцията и ще ги свържем с аргументите от LLM.\n",
    "\n",
    "Накрая ще добавим съобщението за извикване на функцията и стойностите, които бяха върнати от съобщението `search_courses`. Това дава на LLM цялата необходима информация, за да отговори на потребителя, използвайки естествен език.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега ще изпратим актуализираното съобщение до LLM, за да получим отговор на естествен език вместо отговор във формат JSON от API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge \n",
    "\n",
    "Страхотна работа! За да продължите обучението си по OpenAI Function Calling, можете да изградите: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    " - Повече параметри на функцията, които могат да помогнат на учащите да намерят повече курсове. Можете да намерите наличните API параметри тук:  \n",
    " - Създайте друго извикване на функция, което приема повече информация от учащия, като например техния роден език  \n",
    " - Създайте обработка на грешки, когато извикването на функцията и/или извикването на API не връщат подходящи курсове  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Отказ от отговорност**:  \nТози документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T11:36:13+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "bg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}