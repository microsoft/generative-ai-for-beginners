{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This lesson will cover: \n",
    "- Що таке виклик функції та його випадки використання \n",
    "- Як створити виклик функції за допомогою OpenAI \n",
    "- Як інтегрувати виклик функції в додаток \n",
    "\n",
    "## Learning Goals \n",
    "\n",
    "After completing this lesson you will know how to and understand: \n",
    "\n",
    "-  Мету використання виклику функції \n",
    "- Налаштування виклику функції за допомогою сервісу OpenAI \n",
    "- Проектування ефективних викликів функцій для випадку використання вашого додатку\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Розуміння викликів функцій\n",
    "\n",
    "Для цього уроку ми хочемо створити функцію для нашого освітнього стартапу, яка дозволить користувачам використовувати чат-бота для пошуку технічних курсів. Ми рекомендуватимемо курси, які відповідають їхньому рівню навичок, поточній ролі та зацікавленій технології.\n",
    "\n",
    "Для цього ми використаємо комбінацію:\n",
    " - `OpenAI` для створення чат-досвіду для користувача\n",
    " - `Microsoft Learn Catalog API` для допомоги користувачам у пошуку курсів на основі запиту користувача\n",
    " - `Function Calling` для отримання запиту користувача та передачі його у функцію для виконання API-запиту.\n",
    "\n",
    "Щоб почати, давайте розглянемо, чому ми взагалі хочемо використовувати виклики функцій:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # отримати нову відповідь від GPT, де він може бачити відповідь функції\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чому Виклик Функцій\n",
    "\n",
    "Якщо ви пройшли будь-який інший урок цього курсу, ви, ймовірно, розумієте потужність використання Великих Мовних Моделей (LLM). Сподіваюсь, ви також бачите деякі їхні обмеження.\n",
    "\n",
    "Виклик Функцій — це функція сервісу OpenAI, створена для вирішення наступних проблем:\n",
    "\n",
    "Непослідовне Форматування Відповідей:\n",
    "- До впровадження виклику функцій відповіді від великої мовної моделі були неструктурованими та непослідовними. Розробникам доводилося писати складний код валідації для обробки кожного варіанту виводу.\n",
    "\n",
    "Обмежена Інтеграція з Зовнішніми Даними:\n",
    "- До цієї функції було складно інтегрувати дані з інших частин застосунку у контекст чату.\n",
    "\n",
    "Стандартизуючи формати відповідей і забезпечуючи безшовну інтеграцію із зовнішніми даними, виклик функцій спрощує розробку і зменшує потребу в додатковій логіці валідації.\n",
    "\n",
    "Користувачі не могли отримати відповіді на запитання типу \"Яка зараз погода в Стокгольмі?\". Це тому, що моделі були обмежені часом, на який були натреновані дані.\n",
    "\n",
    "Розглянемо приклад нижче, який ілюструє цю проблему:\n",
    "\n",
    "Припустимо, ми хочемо створити базу даних студентів, щоб пропонувати їм відповідний курс. Нижче наведено два описи студентів, які дуже схожі за даними, що вони містять.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми хочемо надіслати це до LLM для парсингу даних. Це пізніше можна буде використати в нашому застосунку для надсилання цього до API або збереження в базі даних.\n",
    "\n",
    "Давайте створимо два ідентичні запити, в яких ми інструктуємо LLM, яку інформацію нас цікавить:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми хочемо надіслати це до LLM, щоб розпарсити частини, які важливі для нашого продукту. Тож ми можемо створити два ідентичні підказки для інструктажу LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Після створення цих двох підказок ми надішлемо їх до LLM за допомогою `openai.ChatCompletion`. Ми зберігаємо підказку у змінній `messages` і призначаємо роль `user`. Це імітує повідомлення від користувача, яке надсилається чат-боту.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ми можемо надіслати обидва запити до LLM і проаналізувати отриману відповідь.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Навіть якщо підказки однакові, а описи схожі, ми можемо отримати різні формати властивості `Grades`.\n",
    "\n",
    "Якщо запустити наведений вище блок кілька разів, формат може бути `3.7` або `3.7 GPA`.\n",
    "\n",
    "Це тому, що LLM приймає неструктуровані дані у вигляді написаної підказки і також повертає неструктуровані дані. Нам потрібен структурований формат, щоб знати, чого очікувати при збереженні або використанні цих даних.\n",
    "\n",
    "Використовуючи функціональний виклик, ми можемо переконатися, що отримуємо структуровані дані у відповідь. При використанні функціонального виклику LLM фактично не виконує жодних функцій. Натомість ми створюємо структуру, якій LLM має слідувати у своїх відповідях. Потім ми використовуємо ці структуровані відповіді, щоб знати, яку функцію запускати у наших додатках.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Діаграма потоку виклику функції](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.uk.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми можемо взяти те, що повертається з функції, і надіслати це назад до LLM. LLM тоді відповість природною мовою, щоб відповісти на запит користувача.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Випадки використання викликів функцій\n",
    "\n",
    "**Виклик зовнішніх інструментів**  \n",
    "Чатботи чудово підходять для надання відповідей на запитання користувачів. Використовуючи виклики функцій, чатботи можуть використовувати повідомлення від користувачів для виконання певних завдань. Наприклад, студент може попросити чатбота \"Надіслати електронного листа моєму викладачу з проханням про додаткову допомогу з цього предмета\". Це може викликати функцію `send_email(to: string, body: string)`.\n",
    "\n",
    "**Створення запитів до API або бази даних**  \n",
    "Користувачі можуть знаходити інформацію, використовуючи природну мову, яка перетворюється у форматований запит або API-запит. Прикладом може бути вчитель, який запитує \"Хто з учнів виконав останнє завдання\", що може викликати функцію з назвою `get_completed(student_name: string, assignment: int, current_status: string)`.\n",
    "\n",
    "**Створення структурованих даних**  \n",
    "Користувачі можуть взяти блок тексту або CSV і використати LLM для вилучення важливої інформації з нього. Наприклад, студент може конвертувати статтю з Вікіпедії про мирні угоди для створення AI-флешкарток. Це можна зробити за допомогою функції `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Створення вашого першого виклику функції\n",
    "\n",
    "Процес створення виклику функції включає 3 основні кроки:  \n",
    "1. Виклик API Chat Completions зі списком ваших функцій та повідомленням користувача  \n",
    "2. Читання відповіді моделі для виконання дії, тобто виклику функції або API  \n",
    "3. Здійснення ще одного виклику до API Chat Completions з відповіддю вашої функції, щоб використати цю інформацію для створення відповіді користувачу.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Потік виклику функції](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.uk.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Елементи виклику функції \n",
    "\n",
    "#### Вхідні дані користувача \n",
    "\n",
    "Першим кроком є створення повідомлення користувача. Його можна динамічно призначити, взявши значення з текстового поля, або ви можете призначити значення тут. Якщо ви вперше працюєте з API Chat Completions, нам потрібно визначити `role` та `content` повідомлення. \n",
    "\n",
    "`role` може бути `system` (створення правил), `assistant` (модель) або `user` (кінцевий користувач). Для виклику функції ми призначимо це як `user` і приклад запитання. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Створення функцій.\n",
    "\n",
    "Далі ми визначимо функцію та параметри цієї функції. Тут ми використаємо лише одну функцію під назвою `search_courses`, але ви можете створювати кілька функцій.\n",
    "\n",
    "**Важливо** : Функції включені в системне повідомлення для LLM і будуть враховані у кількості доступних токенів, які у вас є.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Визначення** \n",
    "\n",
    "Структура визначення функції має кілька рівнів, кожен із яких має власні властивості. Ось розбір вкладеної структури:\n",
    "\n",
    "**Властивості функції верхнього рівня:**\n",
    "\n",
    "`name` - Назва функції, яку ми хочемо викликати. \n",
    "\n",
    "`description` - Опис того, як працює функція. Тут важливо бути конкретним і чітким. \n",
    "\n",
    "`parameters` - Список значень і формату, які ви хочете, щоб модель генерувала у своїй відповіді. \n",
    "\n",
    "**Властивості об’єкта параметрів:**\n",
    "\n",
    "`type` - Тип даних об’єкта параметрів (зазвичай \"object\")\n",
    "\n",
    "`properties` - Список конкретних значень, які модель використовуватиме у своїй відповіді. \n",
    "\n",
    "**Властивості окремих параметрів:**\n",
    "\n",
    "`name` - Неявно визначається ключем властивості (наприклад, \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Тип даних цього конкретного параметра (наприклад, \"string\", \"number\", \"boolean\") \n",
    "\n",
    "`description` - Опис конкретного параметра. \n",
    "\n",
    "**Необов’язкові властивості:**\n",
    "\n",
    "`required` - Масив, у якому перелічено, які параметри є обов’язковими для виконання виклику функції. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Виклик функції  \n",
    "Після визначення функції, тепер нам потрібно включити її у виклик API Chat Completion. Ми робимо це, додаючи `functions` до запиту. У цьому випадку `functions=functions`.  \n",
    "\n",
    "Також є опція встановити `function_call` у `auto`. Це означає, що ми дозволимо LLM самостійно вирішити, яку функцію слід викликати на основі повідомлення користувача, замість того, щоб призначати це самостійно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер давайте подивимося на відповідь і побачимо, як вона відформатована:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Ви можете побачити, що викликається ім'я функції, і з повідомлення користувача LLM змогла знайти дані, які відповідають аргументам функції.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Інтеграція викликів функцій у додаток. \n",
    "\n",
    "\n",
    "Після того, як ми протестували відформатовану відповідь від LLM, тепер ми можемо інтегрувати це у додаток. \n",
    "\n",
    "### Керування потоком \n",
    "\n",
    "Щоб інтегрувати це у наш додаток, давайте виконаємо наступні кроки: \n",
    "\n",
    "Спочатку зробимо виклик до сервісів OpenAI і збережемо повідомлення у змінній під назвою `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ми визначимо функцію, яка викликатиме API Microsoft Learn для отримання списку курсів:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як найкраща практика, ми потім перевіримо, чи модель хоче викликати функцію. Після цього ми створимо одну з доступних функцій і зіставимо її з функцією, яку викликають.  \n",
    "Потім ми візьмемо аргументи функції і зіставимо їх з аргументами з LLM.\n",
    "\n",
    "Нарешті, ми додамо повідомлення про виклик функції та значення, які були повернені повідомленням `search_courses`. Це дає LLM всю необхідну інформацію для  \n",
    "відповіді користувачу природною мовою.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ми надішлемо оновлене повідомлення до LLM, щоб отримати відповідь природною мовою замість відповіді у форматі JSON API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Виклик коду\n",
    "\n",
    "Чудова робота! Щоб продовжити навчання з OpenAI Function Calling, ви можете створити: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    " - Більше параметрів функції, які можуть допомогти учням знайти більше курсів. Ви можете знайти доступні параметри API тут:  \n",
    " - Створіть ще один виклик функції, який приймає більше інформації від учня, наприклад їх рідну мову  \n",
    " - Створіть обробку помилок, коли виклик функції та/або виклик API не повертає жодних відповідних курсів  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Відмова від відповідальності**:  \nЦей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується звертатися до професійного людського перекладу. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T12:03:56+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "uk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}