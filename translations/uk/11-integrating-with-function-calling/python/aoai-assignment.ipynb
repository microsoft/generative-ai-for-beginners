{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вступ\n",
    "\n",
    "У цьому уроці розглянемо:\n",
    "- Що таке виклик функції та для чого він потрібен\n",
    "- Як створити виклик функції за допомогою Azure OpenAI\n",
    "- Як інтегрувати виклик функції у ваш додаток\n",
    "\n",
    "## Навчальні цілі\n",
    "\n",
    "Після проходження цього уроку ви знатимете та розумітимете:\n",
    "\n",
    "- Для чого використовувати виклик функції\n",
    "- Як налаштувати виклик функції через сервіс Azure Open AI\n",
    "- Як розробити ефективні виклики функцій для вашого додатку\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Розуміння викликів функцій\n",
    "\n",
    "У цьому уроці ми створюємо функцію для нашого освітнього стартапу, яка дозволяє користувачам знаходити технічні курси за допомогою чат-бота. Ми будемо рекомендувати курси, які відповідають їхньому рівню знань, поточній ролі та цікавій технології.\n",
    "\n",
    "Для цього ми використаємо комбінацію:\n",
    " - `Azure Open AI` для створення чат-досвіду для користувача\n",
    " - `Microsoft Learn Catalog API` для допомоги у пошуку курсів на основі запиту користувача\n",
    " - `Function Calling` для того, щоб взяти запит користувача і передати його у функцію для виконання API-запиту\n",
    "\n",
    "Щоб розпочати, давайте розглянемо, навіщо взагалі використовувати виклики функцій:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # отримати нову відповідь від GPT, де він може бачити відповідь функції\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чому варто використовувати Function Calling\n",
    "\n",
    "Якщо ви вже проходили інші уроки цього курсу, ви, ймовірно, розумієте, наскільки потужними є великі мовні моделі (LLMs). Сподіваємось, ви також помітили деякі їхні обмеження.\n",
    "\n",
    "Function Calling — це функція Azure Open AI Service, яка допомагає подолати такі обмеження:\n",
    "1) Послідовний формат відповідей\n",
    "2) Можливість використовувати дані з інших джерел додатка у чаті\n",
    "\n",
    "До появи function calling відповіді LLM були неструктурованими та непослідовними. Розробникам доводилося писати складний код для перевірки, щоб обробляти всі можливі варіації відповідей.\n",
    "\n",
    "Користувачі не могли отримати відповіді на запитання на кшталт \"Яка зараз погода у Стокгольмі?\". Це тому, що моделі обмежувалися даними, на яких їх навчали.\n",
    "\n",
    "Розглянемо приклад нижче, який ілюструє цю проблему:\n",
    "\n",
    "Припустимо, ми хочемо створити базу даних студентів, щоб рекомендувати їм відповідні курси. Нижче наведено два описи студентів, які містять дуже схожі дані.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми хочемо надіслати це LLM для обробки даних. Згодом це можна буде використати у нашому застосунку для надсилання до API або зберігання у базі даних.\n",
    "\n",
    "Давайте створимо два однакових запити, якими ми пояснюємо LLM, яка саме інформація нас цікавить:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ми хочемо надіслати це LLM, щоб проаналізувати частини, які важливі для нашого продукту. Тому ми можемо створити два ідентичні підказки, щоб дати інструкції LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Після створення цих двох підказок ми надішлемо їх до LLM, використовуючи `openai.ChatCompletion`. Ми зберігаємо підказку у змінній `messages` і призначаємо роль `user`. Це потрібно для імітації повідомлення від користувача, яке надсилається чат-боту.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Навіть якщо підказки однакові, а описи схожі, ми можемо отримати різні формати властивості `Grades`.\n",
    "\n",
    "Якщо ви запустите вищенаведену комірку кілька разів, формат може бути як `3.7`, так і `3.7 GPA`.\n",
    "\n",
    "Це відбувається тому, що LLM отримує неструктуровані дані у вигляді текстової підказки і також повертає неструктуровані дані. Нам потрібно мати структурований формат, щоб знати, чого очікувати при збереженні чи використанні цих даних.\n",
    "\n",
    "Використовуючи функціональні виклики, ми можемо бути впевнені, що отримаємо структуровані дані у відповідь. При використанні функціональних викликів LLM насправді не викликає і не запускає жодних функцій. Натомість ми створюємо структуру, якої LLM має дотримуватися у своїх відповідях. Потім ми використовуємо ці структуровані відповіді, щоб знати, яку функцію запускати у наших застосунках.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Схема потоку виклику функції](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.uk.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Варіанти використання викликів функцій\n",
    "\n",
    "**Виклик зовнішніх інструментів**  \n",
    "Чат-боти чудово відповідають на запитання користувачів. Завдяки виклику функцій, чат-боти можуть використовувати повідомлення від користувачів для виконання певних завдань. Наприклад, студент може попросити чат-бота: \"Надішли лист моєму викладачу з проханням допомогти мені з цією темою\". Для цього можна викликати функцію `send_email(to: string, body: string)`\n",
    "\n",
    "**Створення API або запитів до бази даних**  \n",
    "Користувачі можуть знаходити інформацію, використовуючи природну мову, яка перетворюється у форматований запит або API-запит. Наприклад, вчитель може запитати: \"Хто з учнів виконав останнє завдання?\", і це може викликати функцію з назвою `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Створення структурованих даних**  \n",
    "Користувачі можуть взяти фрагмент тексту або CSV і за допомогою LLM виділити з нього важливу інформацію. Наприклад, студент може перетворити статтю з Вікіпедії про мирні угоди на AI-картки для навчання. Це можна зробити за допомогою функції `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Створення вашого першого виклику функції\n",
    "\n",
    "Процес створення виклику функції складається з трьох основних кроків:\n",
    "1. Викликати Chat Completions API зі списком ваших функцій і повідомленням користувача\n",
    "2. Прочитати відповідь моделі, щоб виконати дію, наприклад, викликати функцію або API\n",
    "3. Зробити ще один виклик до Chat Completions API з відповіддю від вашої функції, щоб використати цю інформацію для створення відповіді користувачу.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Потік виклику функції](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.uk.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Елементи виклику функції\n",
    "\n",
    "#### Введення користувача\n",
    "\n",
    "Перший крок — створити повідомлення користувача. Це можна зробити динамічно, взявши значення з текстового поля, або ж задати значення тут. Якщо ви вперше працюєте з Chat Completions API, потрібно визначити `role` та `content` повідомлення.\n",
    "\n",
    "`role` може бути або `system` (створення правил), або `assistant` (модель), або `user` (кінцевий користувач). Для виклику функції ми встановимо це як `user` і додамо приклад запитання.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Створення функцій.\n",
    "\n",
    "Далі ми визначимо функцію та її параметри. Тут ми використаємо лише одну функцію під назвою `search_courses`, але ви можете створювати кілька функцій.\n",
    "\n",
    "**Важливо**: Функції додаються до системного повідомлення для LLM і враховуються у загальній кількості доступних вам токенів.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Визначення**\n",
    "\n",
    "`name` - Назва функції, яку потрібно викликати.\n",
    "\n",
    "`description` - Опис того, як працює функція. Тут важливо бути конкретним і зрозумілим.\n",
    "\n",
    "`parameters` - Список значень і формат, які ви хочете отримати у відповіді моделі.\n",
    "\n",
    "`type` - Тип даних, у якому будуть зберігатися властивості.\n",
    "\n",
    "`properties` - Список конкретних значень, які модель використає у своїй відповіді.\n",
    "\n",
    "`name` - Назва властивості, яку модель використає у відформатованій відповіді.\n",
    "\n",
    "`type` - Тип даних цієї властивості.\n",
    "\n",
    "`description` - Опис конкретної властивості.\n",
    "\n",
    "**Необов’язково**\n",
    "\n",
    "`required` - обов’язкова властивість для завершення виклику функції\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Виклик функції\n",
    "Після того, як ми визначили функцію, тепер потрібно додати її до виклику Chat Completion API. Для цього ми додаємо `functions` до запиту. У цьому випадку це `functions=functions`.\n",
    "\n",
    "Також є можливість встановити `function_call` у значення `auto`. Це означає, що ми дозволяємо LLM самостійно вирішити, яку функцію викликати на основі повідомлення користувача, замість того щоб призначати її вручну.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер давайте подивимось на відповідь і побачимо, як вона відформатована:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Ви можете побачити, що викликається функція з певною назвою, і з повідомлення користувача LLM зміг знайти дані для заповнення аргументів функції.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Інтеграція викликів функцій у додаток.\n",
    "\n",
    "Після того, як ми протестували відформатовану відповідь від LLM, тепер можемо інтегрувати це у додаток.\n",
    "\n",
    "### Керування потоком\n",
    "\n",
    "Щоб інтегрувати це у наш додаток, виконаймо наступні кроки:\n",
    "\n",
    "Спочатку зробимо запит до сервісів Open AI і збережемо повідомлення у змінній з назвою `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ми визначимо функцію, яка викликатиме API Microsoft Learn для отримання списку курсів:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як найкраща практика, ми спочатку перевіримо, чи модель хоче викликати функцію. Після цього ми створимо одну з доступних функцій і зіставимо її з тією, яку викликає модель.\n",
    "Далі ми візьмемо аргументи функції та зіставимо їх з аргументами від LLM.\n",
    "\n",
    "Нарешті, ми додамо повідомлення про виклик функції та значення, які були повернуті повідомленням `search_courses`. Це дає LLM всю необхідну інформацію,\n",
    "щоб відповісти користувачу природною мовою.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Завдання з коду\n",
    "\n",
    "Чудова робота! Щоб продовжити вивчення Azure Open AI Function Calling, ви можете створити: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Додайте більше параметрів до функції, які можуть допомогти користувачам знаходити більше курсів. Доступні параметри API можна знайти тут:\n",
    " - Створіть ще один виклик функції, який приймає додаткову інформацію від користувача, наприклад, його рідну мову\n",
    " - Додайте обробку помилок на випадок, якщо виклик функції та/або API не повертає жодного відповідного курсу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Відмова від відповідальності**:  \nЦей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, звертаємо вашу увагу, що автоматичний переклад може містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для отримання критично важливої інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильне тлумачення, що виникли внаслідок використання цього перекладу.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-08-25T20:37:21+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "uk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}