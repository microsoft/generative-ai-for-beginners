{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Робота з моделями родини Meta\n",
    "\n",
    "## Вступ\n",
    "\n",
    "У цьому уроці ми розглянемо:\n",
    "\n",
    "- Дослідження двох основних моделей родини Meta — Llama 3.1 та Llama 3.2\n",
    "- Розуміння сценаріїв використання кожної моделі\n",
    "- Приклад коду, який демонструє унікальні можливості кожної моделі\n",
    "\n",
    "## Родина моделей Meta\n",
    "\n",
    "У цьому уроці ми розглянемо 2 моделі з родини Meta, або \"Llama Herd\" — Llama 3.1 та Llama 3.2\n",
    "\n",
    "Ці моделі мають різні варіанти та доступні на маркетплейсі Github Model. Більше інформації про використання Github Models для [прототипування з AI-моделями](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Варіанти моделей:\n",
    "- Llama 3.1 — 70B Instruct\n",
    "- Llama 3.1 — 405B Instruct\n",
    "- Llama 3.2 — 11B Vision Instruct\n",
    "- Llama 3.2 — 90B Vision Instruct\n",
    "\n",
    "*Примітка: Llama 3 також доступна на Github Models, але в цьому уроці вона не розглядається*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "З 405 мільярдами параметрів, Llama 3.1 належить до категорії відкритих LLM.\n",
    "\n",
    "Ця модель є оновленням попередньої версії Llama 3 і пропонує:\n",
    "\n",
    "- Більше контекстне вікно — 128k токенів проти 8k токенів\n",
    "- Більший максимум вихідних токенів — 4096 проти 2048\n",
    "- Краща багатомовна підтримка — завдяки збільшенню кількості тренувальних токенів\n",
    "\n",
    "Ці можливості дозволяють Llama 3.1 вирішувати складніші завдання при створенні GenAI-додатків, зокрема:\n",
    "- Вбудований виклик функцій — можливість викликати зовнішні інструменти та функції поза межами LLM\n",
    "- Краща продуктивність RAG — завдяки більшому контекстному вікну\n",
    "- Генерація синтетичних даних — можливість створювати ефективні дані для задач, таких як донавчання\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Виклик вбудованих функцій\n",
    "\n",
    "Llama 3.1 була донавчена для більш ефективного використання функцій або інструментів. Вона також має два вбудовані інструменти, які модель може розпізнати як необхідні для використання на основі запиту користувача. Ці інструменти:\n",
    "\n",
    "- **Brave Search** — використовується для отримання актуальної інформації, наприклад, про погоду, шляхом веб-пошуку\n",
    "- **Wolfram Alpha** — використовується для складніших математичних обчислень, тому немає потреби писати власні функції.\n",
    "\n",
    "Ви також можете створювати власні інструменти, які LLM зможе викликати.\n",
    "\n",
    "У наведеному нижче прикладі коду:\n",
    "\n",
    "- Ми визначаємо доступні інструменти (brave_search, wolfram_alpha) у системному запиті.\n",
    "- Надсилаємо користувацький запит про погоду в певному місті.\n",
    "- LLM відповість викликом інструменту Brave Search, який виглядатиме так: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Примітка: У цьому прикладі здійснюється лише виклик інструменту. Якщо ви хочете отримати результати, потрібно створити безкоштовний обліковий запис на сторінці Brave API та визначити саму функцію.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Попри те, що Llama 3.1 є LLM, одне з її обмежень — це відсутність мультимодальності. Тобто, вона не може використовувати різні типи вхідних даних, наприклад, зображення як підказки та надавати відповіді. Ця можливість є однією з головних особливостей Llama 3.2. До цих можливостей також належать:\n",
    "\n",
    "- Мультимодальність — здатна аналізувати як текстові, так і візуальні підказки\n",
    "- Варіанти малого та середнього розміру (11B та 90B) — це забезпечує гнучкі варіанти розгортання,\n",
    "- Варіанти лише для тексту (1B та 3B) — це дозволяє запускати модель на пристроях з обмеженими ресурсами або мобільних пристроях і забезпечує низьку затримку\n",
    "\n",
    "Підтримка мультимодальності — це значний крок у світі моделей з відкритим кодом. Наведений нижче приклад коду приймає як зображення, так і текстову підказку, щоб отримати аналіз зображення від Llama 3.2 90B.\n",
    "\n",
    "### Мультимодальна підтримка в Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Навчання не закінчується тут, продовжуйте свій шлях\n",
    "\n",
    "Після завершення цього уроку перегляньте нашу [колекцію навчальних матеріалів з генеративного ШІ](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), щоб і далі поглиблювати свої знання у сфері генеративного ШІ!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Відмова від відповідальності**:  \nЦей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, звертаємо вашу увагу, що автоматичний переклад може містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для отримання критично важливої інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильне тлумачення, що виникли внаслідок використання цього перекладу.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:50:51+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "uk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}