<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df44972d5575ea8cef3c52ee31696d04",
  "translation_date": "2025-12-19T17:46:10+00:00",
  "source_file": "14-the-generative-ai-application-lifecycle/README.md",
  "language_code": "uk"
}
-->
[![Інтеграція з викликом функцій](../../../translated_images/14-lesson-banner.066d74a31727ac12.uk.png)](https://youtu.be/ewtQY_RJrzs?si=dyJ2bjiljH7UUHCh)

# Життєвий цикл застосунку генеративного ШІ

Важливе питання для всіх застосунків ШІ — актуальність функцій ШІ, оскільки ШІ — це швидкозмінна сфера. Щоб ваш застосунок залишався актуальним, надійним і стійким, потрібно постійно його моніторити, оцінювати та вдосконалювати. Саме тут на допомогу приходить життєвий цикл генеративного ШІ.

Життєвий цикл генеративного ШІ — це рамкова структура, яка допомагає вам пройти етапи розробки, розгортання та підтримки застосунку генеративного ШІ. Вона допомагає визначити ваші цілі, вимірювати продуктивність, виявляти проблеми та впроваджувати рішення. Також вона допомагає узгодити ваш застосунок з етичними та правовими стандартами вашої сфери та зацікавлених сторін. Дотримуючись життєвого циклу генеративного ШІ, ви можете гарантувати, що ваш застосунок завжди приносить користь і задовольняє користувачів.

## Вступ

У цьому розділі ви:

- Зрозумієте парадигму переходу від MLOps до LLMOps
- Життєвий цикл LLM
- Інструменти життєвого циклу
- Метрики та оцінка життєвого циклу

## Зрозумійте парадигму переходу від MLOps до LLMOps

LLM — це новий інструмент у арсеналі штучного інтелекту, вони надзвичайно потужні в завданнях аналізу та генерації для застосунків, проте ця потужність має наслідки для того, як ми оптимізуємо завдання ШІ та класичного машинного навчання.

З цим нам потрібна нова парадигма, щоб адаптувати цей інструмент динамічно, з правильними стимулами. Ми можемо класифікувати старіші застосунки ШІ як "ML Apps", а новіші — як "GenAI Apps" або просто "AI Apps", відображаючи основні технології та методи, що використовувалися на той час. Це змінює наш наратив у кількох аспектах, подивіться на наступне порівняння.

![Порівняння LLMOps та MLOps](../../../translated_images/01-llmops-shift.29bc933cb3bb0080.uk.png)

Зверніть увагу, що в LLMOps ми більше зосереджені на розробниках застосунків, використовуючи інтеграції як ключовий момент, застосовуючи "Моделі як сервіс" і розглядаючи наступні показники для метрик.

- Якість: якість відповіді
- Шкода: відповідальний ШІ
- Чесність: обґрунтованість відповіді (Чи має сенс? Чи правильна?)
- Вартість: бюджет рішення
- Затримка: середній час відповіді на токен

## Життєвий цикл LLM

Спершу, щоб зрозуміти життєвий цикл і зміни, зверніть увагу на наступну інфографіку.

![Інфографіка LLMOps](../../../translated_images/02-llmops.70a942ead05a7645.uk.png)

Як ви можете помітити, це відрізняється від звичних життєвих циклів MLOps. LLM мають багато нових вимог, таких як підказки (Prompting), різні техніки для покращення якості (Fine-Tuning, RAG, Meta-Prompts), різна оцінка та відповідальність з урахуванням відповідального ШІ, нарешті, нові метрики оцінки (Якість, Шкода, Чесність, Вартість і Затримка).

Наприклад, подивіться, як ми генеруємо ідеї. Використання інженерії підказок для експериментів з різними LLM, щоб дослідити можливості та перевірити, чи може їхня гіпотеза бути правильною.

Зверніть увагу, що це не лінійний процес, а інтегровані цикли, ітеративні та з загальним циклом.

Як ми можемо дослідити ці кроки? Розглянемо детальніше, як можна побудувати життєвий цикл.

![Робочий процес LLMOps](../../../translated_images/03-llm-stage-flows.3a1e1c401235a6cf.uk.png)

Це може виглядати трохи складно, спочатку зосередьмося на трьох основних кроках.

1. Генерація ідей/Дослідження: Дослідження, тут ми можемо досліджувати відповідно до наших бізнес-потреб. Прототипування, створення [PromptFlow](https://microsoft.github.io/promptflow/index.html?WT.mc_id=academic-105485-koreyst) і перевірка, чи достатньо це ефективно для нашої гіпотези.
1. Побудова/Розширення: Впровадження, тепер ми починаємо оцінювати на більших наборах даних, впроваджуємо техніки, такі як Fine-tuning і RAG, щоб перевірити стійкість нашого рішення. Якщо ні, повторна реалізація, додавання нових кроків у наш потік або реструктуризація даних може допомогти. Після тестування нашого потоку і масштабу, якщо все працює і метрики відповідають, він готовий до наступного кроку.
1. Впровадження в експлуатацію: Інтеграція, тепер додаємо системи моніторингу та оповіщень у нашу систему, розгортання та інтеграцію застосунку.

Потім у нас є загальний цикл управління, зосереджений на безпеці, відповідності та управлінні.

Вітаємо, тепер ваш застосунок ШІ готовий до роботи та експлуатації. Для практичного досвіду ознайомтеся з [демо Contoso Chat.](https://nitya.github.io/contoso-chat/?WT.mc_id=academic-105485-koreys)

Тепер, які інструменти ми можемо використовувати?

## Інструменти життєвого циклу

Для інструментів Microsoft надає [Azure AI Platform](https://azure.microsoft.com/solutions/ai/?WT.mc_id=academic-105485-koreys) та [PromptFlow](https://microsoft.github.io/promptflow/index.html?WT.mc_id=academic-105485-koreyst), які полегшують і роблять ваш цикл простим для впровадження та готовим до роботи.

[Azure AI Platform](https://azure.microsoft.com/solutions/ai/?WT.mc_id=academic-105485-koreys) дозволяє використовувати [AI Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreys). AI Studio — це веб-портал, який дозволяє досліджувати моделі, приклади та інструменти. Керувати вашими ресурсами, UI-потоками розробки та SDK/CLI опціями для розробки з пріоритетом коду.

![Можливості Azure AI](../../../translated_images/04-azure-ai-platform.80203baf03a12fa8.uk.png)

Azure AI дозволяє використовувати різні ресурси для керування операціями, сервісами, проектами, пошуком векторів і базами даних.

![LLMOps з Azure AI](../../../translated_images/05-llm-azure-ai-prompt.a5ce85cdbb494bdf.uk.png)

Створюйте від Proof-of-Concept (POC) до масштабних застосунків з PromptFlow:

- Проєктуйте та створюйте застосунки з VS Code, використовуючи візуальні та функціональні інструменти
- Тестуйте та тонко налаштовуйте ваші застосунки для якісного ШІ з легкістю.
- Використовуйте Azure AI Studio для інтеграції та ітерацій з хмарою, швидкого розгортання та інтеграції.

![LLMOps з PromptFlow](../../../translated_images/06-llm-promptflow.a183eba07a3a7fdf.uk.png)

## Чудово! Продовжуйте навчання!

Дивовижно, тепер дізнайтеся більше про те, як ми структуруємо застосунок, щоб використовувати концепції на прикладі [Contoso Chat App](https://nitya.github.io/contoso-chat/?WT.mc_id=academic-105485-koreyst), щоб побачити, як Cloud Advocacy додає ці концепції в демонстраціях. Для додаткового контенту перегляньте нашу [сесію Ignite!](https://www.youtube.com/watch?v=DdOylyrTOWg)

Тепер перейдіть до уроку 15, щоб зрозуміти, як [Retrieval Augmented Generation та векторні бази даних](../15-rag-and-vector-databases/README.md?WT.mc_id=academic-105485-koreyst) впливають на генеративний ШІ та роблять застосунки більш захопливими!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується звертатися до професійного людського перекладу. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->