<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7f8f4c11f8c1cb6e1794442dead414ea",
  "translation_date": "2025-07-09T08:57:09+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "sv"
}
-->
# Att Anv√§nda Generativ AI Ansvarsfullt

[![Using Generative AI Responsibly](../../../translated_images/03-lesson-banner.1ed56067a452d97709d51f6cc8b6953918b2287132f4909ade2008c936cd4af9.sv.png)](https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst)

> _Klicka p√• bilden ovan f√∂r att se videon till denna lektion_

Det √§r l√§tt att bli fascinerad av AI, och generativ AI i synnerhet, men du beh√∂ver fundera p√• hur du anv√§nder den p√• ett ansvarsfullt s√§tt. Du m√•ste ta h√§nsyn till saker som hur du s√§kerst√§ller att resultatet √§r r√§ttvist, ofarligt och mer. Detta kapitel syftar till att ge dig den n√§mnda kontexten, vad du b√∂r t√§nka p√• och hur du kan ta aktiva steg f√∂r att f√∂rb√§ttra din AI-anv√§ndning.

## Introduktion

Denna lektion kommer att t√§cka:

- Varf√∂r du b√∂r prioritera Responsible AI n√§r du bygger Generative AI-applikationer.
- K√§rnprinciperna f√∂r Responsible AI och hur de relaterar till Generative AI.
- Hur du oms√§tter dessa Responsible AI-principer i praktiken genom strategi och verktyg.

## L√§randem√•l

Efter att ha genomf√∂rt denna lektion kommer du att veta:

- Vikten av Responsible AI n√§r du bygger Generative AI-applikationer.
- N√§r du b√∂r t√§nka p√• och till√§mpa k√§rnprinciperna f√∂r Responsible AI vid utveckling av Generative AI-applikationer.
- Vilka verktyg och strategier som finns tillg√§ngliga f√∂r att oms√§tta Responsible AI i praktiken.

## Principer f√∂r Responsible AI

Intresset f√∂r Generative AI har aldrig varit st√∂rre. Detta intresse har lockat m√•nga nya utvecklare, uppm√§rksamhet och finansiering till omr√•det. √Ñven om detta √§r mycket positivt f√∂r alla som vill bygga produkter och f√∂retag med Generative AI, √§r det ocks√• viktigt att vi g√•r fram p√• ett ansvarsfullt s√§tt.

Under hela kursen fokuserar vi p√• att bygga v√•r startup och v√•r AI-utbildningsprodukt. Vi kommer att anv√§nda principerna f√∂r Responsible AI: R√§ttvisa, Inkludering, Tillf√∂rlitlighet/S√§kerhet, S√§kerhet & Integritet, Transparens och Ansvarstagande. Med dessa principer kommer vi att utforska hur de relaterar till v√•r anv√§ndning av Generative AI i v√•ra produkter.

## Varf√∂r b√∂r du prioritera Responsible AI

N√§r du bygger en produkt leder ett m√§nniskocentrerat angreppss√§tt, d√§r du har anv√§ndarens b√§sta i √•tanke, till de b√§sta resultaten.

Det unika med Generative AI √§r dess f√∂rm√•ga att skapa hj√§lpsamma svar, information, v√§gledning och inneh√•ll f√∂r anv√§ndare. Detta kan g√∂ras utan m√•nga manuella steg, vilket kan ge mycket imponerande resultat. Utan r√§tt planering och strategier kan det tyv√§rr ocks√• leda till skadliga resultat f√∂r dina anv√§ndare, din produkt och samh√§llet i stort.

L√•t oss titta p√• n√•gra (men inte alla) av dessa potentiellt skadliga resultat:

### Hallucinationer

Hallucinationer √§r ett begrepp som anv√§nds f√∂r att beskriva n√§r en LLM producerar inneh√•ll som antingen √§r helt nonsens eller n√•got vi vet √§r faktam√§ssigt felaktigt baserat p√• andra informationsk√§llor.

Ta till exempel att vi bygger en funktion f√∂r v√•r startup som l√•ter studenter st√§lla historiska fr√•gor till en modell. En student fr√•gar: `Who was the sole survivor of Titanic?`

Modellen ger ett svar som det nedan:

![Prompt saying "Who was the sole survivor of the Titanic"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp)

> _(K√§lla: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

Detta √§r ett mycket sj√§lvs√§kert och utf√∂rligt svar. Tyv√§rr √§r det felaktigt. √Ñven med en minimal m√§ngd forskning skulle man uppt√§cka att det fanns fler √§n en √∂verlevande fr√•n Titanic-katastrofen. F√∂r en student som just b√∂rjat unders√∂ka √§mnet kan detta svar vara tillr√§ckligt √∂vertygande f√∂r att inte ifr√•gas√§ttas och behandlas som fakta. Konsekvenserna kan bli att AI-systemet uppfattas som op√•litligt och skada v√•r startups rykte.

Med varje iteration av en given LLM har vi sett f√∂rb√§ttringar i att minimera hallucinationer. Trots denna f√∂rb√§ttring m√•ste vi som applikationsbyggare och anv√§ndare fortfarande vara medvetna om dessa begr√§nsningar.

### Skadligt inneh√•ll

Vi gick igenom i f√∂reg√•ende avsnitt n√§r en LLM producerar felaktiga eller nonsensartade svar. En annan risk vi m√•ste vara medvetna om √§r n√§r en modell svarar med skadligt inneh√•ll.

Skadligt inneh√•ll kan definieras som:

- Att ge instruktioner eller uppmuntra till sj√§lvskada eller skada mot vissa grupper.
- Hatfullt eller f√∂rnedrande inneh√•ll.
- Att v√§gleda planering av attacker eller v√•ldsamma handlingar.
- Att ge instruktioner om hur man hittar olagligt inneh√•ll eller beg√•r olagliga handlingar.
- Att visa sexuellt explicit inneh√•ll.

F√∂r v√•r startup vill vi s√§kerst√§lla att vi har r√§tt verktyg och strategier p√• plats f√∂r att f√∂rhindra att denna typ av inneh√•ll visas f√∂r studenter.

### Brist p√• r√§ttvisa

R√§ttvisa definieras som ‚Äùatt s√§kerst√§lla att ett AI-system √§r fritt fr√•n partiskhet och diskriminering och att det behandlar alla r√§ttvist och lika.‚Äù Inom Generative AI vill vi s√§kerst√§lla att exkluderande v√§rldsuppfattningar om marginaliserade grupper inte f√∂rst√§rks av modellens output.

Denna typ av output √§r inte bara destruktiv f√∂r att skapa positiva produktupplevelser f√∂r v√•ra anv√§ndare, utan orsakar ocks√• ytterligare samh√§llsskada. Som applikationsbyggare b√∂r vi alltid ha en bred och m√•ngfaldig anv√§ndarbas i √•tanke n√§r vi bygger l√∂sningar med Generative AI.

## Hur man anv√§nder Generativ AI ansvarsfullt

Nu n√§r vi har identifierat vikten av Responsible Generative AI, l√•t oss titta p√• 4 steg vi kan ta f√∂r att bygga v√•ra AI-l√∂sningar p√• ett ansvarsfullt s√§tt:

![Mitigate Cycle](../../../translated_images/mitigate-cycle.babcd5a5658e1775d5f2cb47f2ff305cca090400a72d98d0f9e57e9db5637c72.sv.png)

### M√§t potentiella skador

Vid mjukvarutestning testar vi anv√§ndarens f√∂rv√§ntade handlingar i en applikation. P√• samma s√§tt √§r det bra att testa en m√•ngfald av prompts som anv√§ndare sannolikt kommer att anv√§nda f√∂r att m√§ta potentiell skada.

Eftersom v√•r startup bygger en utbildningsprodukt vore det bra att f√∂rbereda en lista med utbildningsrelaterade prompts. Det kan handla om att t√§cka ett visst √§mne, historiska fakta och prompts om studentlivet.

### Minska potentiella skador

Det √§r nu dags att hitta s√§tt att f√∂rhindra eller begr√§nsa den potentiella skada som modellen och dess svar kan orsaka. Vi kan se detta i 4 olika lager:

![Mitigation Layers](../../../translated_images/mitigation-layers.377215120b9a1159a8c3982c6bbcf41b6adf8c8fa04ce35cbaeeb13b4979cdfc.sv.png)

- **Modell**. V√§lja r√§tt modell f√∂r r√§tt anv√§ndningsfall. St√∂rre och mer komplexa modeller som GPT-4 kan inneb√§ra st√∂rre risk f√∂r skadligt inneh√•ll n√§r de anv√§nds i mindre och mer specifika sammanhang. Att anv√§nda din tr√§ningsdata f√∂r finjustering minskar ocks√• risken f√∂r skadligt inneh√•ll.

- **S√§kerhetssystem**. Ett s√§kerhetssystem √§r en upps√§ttning verktyg och konfigurationer p√• plattformen som serverar modellen och hj√§lper till att minska skada. Ett exempel √§r inneh√•llsfiltreringssystemet i Azure OpenAI-tj√§nsten. Systemen b√∂r ocks√• uppt√§cka jailbreak-attacker och o√∂nskad aktivitet som f√∂rfr√•gningar fr√•n bots.

- **Metaprompt**. Metaprompter och grundning √§r s√§tt att styra eller begr√§nsa modellen baserat p√• vissa beteenden och information. Det kan vara att anv√§nda systeminput f√∂r att definiera vissa gr√§nser f√∂r modellen. Dessutom att ge output som √§r mer relevant f√∂r systemets omfattning eller dom√§n.

Det kan ocks√• vara att anv√§nda tekniker som Retrieval Augmented Generation (RAG) f√∂r att l√•ta modellen endast h√§mta information fr√•n ett urval av betrodda k√§llor. Det finns en lektion senare i kursen om [att bygga s√∂kapplikationer](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **Anv√§ndarupplevelse**. Det sista lagret √§r d√§r anv√§ndaren interagerar direkt med modellen via v√•r applikationsgr√§nssnitt p√• n√•got s√§tt. P√• detta s√§tt kan vi designa UI/UX f√∂r att begr√§nsa anv√§ndaren i vilka typer av input de kan skicka till modellen samt text eller bilder som visas f√∂r anv√§ndaren. N√§r vi lanserar AI-applikationen m√•ste vi ocks√• vara transparenta om vad v√•r Generative AI-applikation kan och inte kan g√∂ra.

Vi har en hel lektion dedikerad till [Design av UX f√∂r AI-applikationer](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **Utv√§rdera modellen**. Att arbeta med LLM:er kan vara utmanande eftersom vi inte alltid har kontroll √∂ver den data modellen tr√§nats p√•. Oavsett b√∂r vi alltid utv√§rdera modellens prestanda och output. Det √§r fortfarande viktigt att m√§ta modellens noggrannhet, likhet, grundning och relevans i outputen. Detta hj√§lper till att skapa transparens och f√∂rtroende hos intressenter och anv√§ndare.

### Driva en ansvarsfull Generative AI-l√∂sning

Att bygga en operativ praxis kring dina AI-applikationer √§r det sista steget. Detta inkluderar samarbete med andra delar av v√•r startup som juridik och s√§kerhet f√∂r att s√§kerst√§lla att vi f√∂ljer alla regelverk. Innan lansering vill vi ocks√• skapa planer f√∂r leverans, hantering av incidenter och √•terst√§llning f√∂r att f√∂rhindra att skada p√• v√•ra anv√§ndare v√§xer.

## Verktyg

√Ñven om arbetet med att utveckla Responsible AI-l√∂sningar kan verka omfattande, √§r det v√§l v√§rt insatsen. N√§r omr√•det Generative AI v√§xer kommer fler verktyg som hj√§lper utvecklare att effektivt integrera ansvar i sina arbetsfl√∂den att mogna. Till exempel kan [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) hj√§lpa till att uppt√§cka skadligt inneh√•ll och bilder via en API-f√∂rfr√•gan.

## Kunskapskontroll

Vilka √§r n√•gra saker du beh√∂ver t√§nka p√• f√∂r att s√§kerst√§lla ansvarsfull AI-anv√§ndning?

1. Att svaret √§r korrekt.  
1. Skadlig anv√§ndning, att AI inte anv√§nds f√∂r kriminella √§ndam√•l.  
1. Att s√§kerst√§lla att AI √§r fri fr√•n partiskhet och diskriminering.

Svar: 2 och 3 √§r korrekta. Responsible AI hj√§lper dig att t√§nka p√• hur du kan minska skadliga effekter och bias med mera.

## üöÄ Utmaning

L√§s p√• om [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) och se vad du kan anv√§nda f√∂r din egen anv√§ndning.

## Bra jobbat, forts√§tt ditt l√§rande

Efter att ha genomf√∂rt denna lektion, kolla in v√•r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) f√∂r att forts√§tta utveckla dina kunskaper inom Generative AI!

G√• vidare till Lektion 4 d√§r vi tittar p√• [Grundl√§ggande Prompt Engineering](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, v√§nligen observera att automatiska √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess modersm√•l b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r n√•gra missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.