{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduktion\n",
    "\n",
    "Den här lektionen kommer att ta upp:\n",
    "- Vad funktionsanrop är och när det används\n",
    "- Hur man skapar ett funktionsanrop med Azure OpenAI\n",
    "- Hur man integrerar ett funktionsanrop i en applikation\n",
    "\n",
    "## Lärandemål\n",
    "\n",
    "Efter att ha slutfört den här lektionen kommer du att veta hur man och förstå:\n",
    "\n",
    "- Syftet med att använda funktionsanrop\n",
    "- Konfigurera Funktionsanrop med Azure Open AI-tjänsten\n",
    "- Utforma effektiva funktionsanrop för din applikations användningsområde\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Förstå funktionsanrop\n",
    "\n",
    "I den här lektionen vill vi bygga en funktion för vår utbildningsstartup som låter användare använda en chatbot för att hitta tekniska kurser. Vi kommer att rekommendera kurser som passar deras kunskapsnivå, nuvarande roll och teknikintresse.\n",
    "\n",
    "För att genomföra detta kommer vi att använda en kombination av:\n",
    " - `Azure Open AI` för att skapa en chattupplevelse för användaren\n",
    " - `Microsoft Learn Catalog API` för att hjälpa användare att hitta kurser baserat på deras önskemål\n",
    " - `Function Calling` för att ta användarens fråga och skicka den till en funktion som gör API-anropet.\n",
    "\n",
    "För att komma igång, låt oss titta på varför vi överhuvudtaget vill använda funktionsanrop:\n",
    "\n",
    "print(\"Meddelanden i nästa förfrågan:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # hämta ett nytt svar från GPT där den kan se funktionssvaret\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varför använda Function Calling\n",
    "\n",
    "Om du har gått någon annan lektion i den här kursen, förstår du förmodligen redan styrkan med att använda Large Language Models (LLMs). Förhoppningsvis ser du också några av deras begränsningar.\n",
    "\n",
    "Function Calling är en funktion i Azure Open AI Service som hjälper till att lösa följande begränsningar:\n",
    "1) Konsekvent svarformat\n",
    "2) Möjlighet att använda data från andra källor i en applikation i en chattkontext\n",
    "\n",
    "Innan function calling var svaren från en LLM ostrukturerade och inkonsekventa. Utvecklare var tvungna att skriva komplex valideringskod för att kunna hantera varje variation av ett svar.\n",
    "\n",
    "Användare kunde inte få svar som \"Hur är vädret just nu i Stockholm?\". Detta beror på att modellerna var begränsade till den tidpunkt då datan tränades.\n",
    "\n",
    "Låt oss titta på exemplet nedan som illustrerar detta problem:\n",
    "\n",
    "Säg att vi vill skapa en databas med studentdata så att vi kan föreslå rätt kurs till dem. Nedan har vi två beskrivningar av studenter som är väldigt lika i den data de innehåller.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi vill skicka detta till en LLM för att analysera datan. Detta kan senare användas i vår applikation för att skicka det till ett API eller lagra det i en databas.\n",
    "\n",
    "Låt oss skapa två identiska uppmaningar där vi instruerar LLM:n om vilken information vi är intresserade av:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi vill skicka detta till en LLM för att analysera de delar som är viktiga för vår produkt. Så vi kan skapa två identiska uppmaningar för att instruera LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efter att ha skapat dessa två uppmaningar kommer vi att skicka dem till LLM genom att använda `openai.ChatCompletion`. Vi lagrar uppmaningen i variabeln `messages` och tilldelar rollen till `user`. Detta är för att efterlikna ett meddelande från en användare som skrivs till en chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Även om uppmaningarna är desamma och beskrivningarna liknar varandra, kan vi få olika format på egenskapen `Grades`.\n",
    "\n",
    "Om du kör cellen ovan flera gånger kan formatet vara `3.7` eller `3.7 GPA`.\n",
    "\n",
    "Detta beror på att LLM tar ostrukturerad data i form av den skrivna uppmaningen och även returnerar ostrukturerad data. Vi behöver ha ett strukturerat format så att vi vet vad vi kan förvänta oss när vi lagrar eller använder denna data.\n",
    "\n",
    "Genom att använda funktionella anrop kan vi säkerställa att vi får tillbaka strukturerad data. När vi använder funktionella anrop kör eller anropar LLM faktiskt inga funktioner. Istället skapar vi en struktur som LLM ska följa i sina svar. Vi använder sedan dessa strukturerade svar för att veta vilken funktion vi ska köra i våra applikationer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Funktionsanropsflödesdiagram](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.sv.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Användningsområden för att använda funktionsanrop\n",
    "\n",
    "**Anropa externa verktyg**  \n",
    "Chattbottar är utmärkta på att ge svar på användarfrågor. Genom att använda funktionsanrop kan chattbottar använda meddelanden från användare för att utföra vissa uppgifter. Till exempel kan en student be chattbotten att \"Skicka ett mejl till min lärare och säg att jag behöver mer hjälp med det här ämnet\". Detta kan göra ett funktionsanrop till `send_email(to: string, body: string)`\n",
    "\n",
    "**Skapa API- eller databasfrågor**  \n",
    "Användare kan hitta information genom att använda naturligt språk som omvandlas till en formaterad fråga eller API-förfrågan. Ett exempel på detta kan vara en lärare som frågar \"Vilka elever har gjort klart den senaste uppgiften\", vilket kan anropa en funktion som heter `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Skapa strukturerad data**  \n",
    "Användare kan ta en textmassa eller en CSV-fil och använda LLM för att plocka ut viktig information från den. Till exempel kan en student omvandla en Wikipedia-artikel om fredsavtal för att skapa AI-flashkort. Detta kan göras genom att använda en funktion som heter `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Skapa ditt första funktionsanrop\n",
    "\n",
    "Processen för att skapa ett funktionsanrop består av tre huvudsakliga steg:\n",
    "1. Anropa Chat Completions API med en lista över dina funktioner och ett användarmeddelande\n",
    "2. Läs modellens svar för att utföra en åtgärd, till exempel köra en funktion eller ett API-anrop\n",
    "3. Gör ytterligare ett anrop till Chat Completions API med svaret från din funktion för att använda den informationen till att skapa ett svar till användaren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flöde av ett funktionsanrop](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.sv.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element av ett funktionsanrop\n",
    "\n",
    "#### Användarens inmatning\n",
    "\n",
    "Första steget är att skapa ett användarmeddelande. Detta kan tilldelas dynamiskt genom att ta värdet från ett textfält, eller så kan du ange ett värde här. Om det är första gången du arbetar med Chat Completions API behöver vi definiera `role` och `content` för meddelandet.\n",
    "\n",
    "`role` kan vara antingen `system` (skapar regler), `assistant` (modellen) eller `user` (slutanvändaren). Vid funktionsanrop sätter vi detta till `user` och ger ett exempel på en fråga.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skapa funktioner.\n",
    "\n",
    "Nu ska vi definiera en funktion och dess parametrar. Vi kommer att använda bara en funktion här som heter `search_courses`, men du kan skapa flera funktioner.\n",
    "\n",
    "**Viktigt**: Funktioner inkluderas i systemmeddelandet till LLM och räknas in i det antal tillgängliga tokens du har.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definitioner**\n",
    "\n",
    "`name` - Namnet på funktionen som vi vill ska anropas.\n",
    "\n",
    "`description` - Detta är en beskrivning av hur funktionen fungerar. Här är det viktigt att vara specifik och tydlig.\n",
    "\n",
    "`parameters` - En lista över värden och format som du vill att modellen ska använda i sitt svar.\n",
    "\n",
    "`type` - Datatypen som egenskaperna kommer att lagras i.\n",
    "\n",
    "`properties` - Lista över de specifika värden som modellen kommer att använda i sitt svar.\n",
    "\n",
    "`name` - Namnet på egenskapen som modellen kommer att använda i sitt formaterade svar.\n",
    "\n",
    "`type` - Datatypen för denna egenskap.\n",
    "\n",
    "`description` - Beskrivning av den specifika egenskapen.\n",
    "\n",
    "**Valfritt**\n",
    "\n",
    "`required` - Obligatorisk egenskap för att funktionsanropet ska kunna slutföras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Göra funktionsanropet\n",
    "Efter att ha definierat en funktion behöver vi nu inkludera den i anropet till Chat Completion API:t. Det gör vi genom att lägga till `functions` i förfrågan. I det här fallet `functions=functions`.\n",
    "\n",
    "Det finns också ett alternativ att sätta `function_call` till `auto`. Det innebär att vi låter LLM själv avgöra vilken funktion som ska anropas baserat på användarens meddelande istället för att vi bestämmer det själva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu ska vi titta på svaret och se hur det är formaterat:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Du kan se att namnet på funktionen anropas och från användarens meddelande kunde LLM hitta data för att fylla i argumenten till funktionen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrera funktionsanrop i en applikation.\n",
    "\n",
    "Efter att vi har testat det formaterade svaret från LLM kan vi nu integrera detta i en applikation.\n",
    "\n",
    "### Hantera flödet\n",
    "\n",
    "För att integrera detta i vår applikation, låt oss följa dessa steg:\n",
    "\n",
    "Först gör vi anropet till Open AI-tjänsterna och sparar meddelandet i en variabel som heter `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu ska vi definiera funktionen som kommer att anropa Microsoft Learn API för att hämta en lista över kurser:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som en rekommenderad metod kommer vi sedan att se om modellen vill anropa en funktion. Därefter skapar vi en av de tillgängliga funktionerna och matchar den mot den funktion som anropas.\n",
    "Vi tar sedan argumenten från funktionen och kopplar dem till argumenten från LLM.\n",
    "\n",
    "Slutligen lägger vi till meddelandet om funktionsanropet och de värden som returnerades av meddelandet `search_courses`. Detta ger LLM all information den behöver för att\n",
    "svara användaren med naturligt språk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kodutmaning\n",
    "\n",
    "Bra jobbat! För att fortsätta din inlärning om Azure Open AI Function Calling kan du bygga: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Fler parametrar för funktionen som kan hjälpa användare att hitta fler kurser. Du hittar tillgängliga API-parametrar här:\n",
    " - Skapa ett annat funktionsanrop som tar emot mer information från användaren, till exempel deras modersmål\n",
    " - Skapa felhantering när funktionsanropet och/eller API-anropet inte returnerar några lämpliga kurser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfriskrivning**:  \nDetta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Vi strävar efter noggrannhet, men var medveten om att automatiska översättningar kan innehålla fel eller brister. Det ursprungliga dokumentet på dess originalspråk ska betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi tar inget ansvar för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-08-25T20:17:02+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "sv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}