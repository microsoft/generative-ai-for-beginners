{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bygga med Meta-familjens modeller\n",
    "\n",
    "## Introduktion\n",
    "\n",
    "Den här lektionen kommer att ta upp:\n",
    "\n",
    "- Utforska de två huvudsakliga modellerna i Meta-familjen – Llama 3.1 och Llama 3.2\n",
    "- Förstå användningsområden och scenarier för varje modell\n",
    "- Kodexempel som visar de unika egenskaperna hos varje modell\n",
    "\n",
    "## Meta-familjens modeller\n",
    "\n",
    "I den här lektionen kommer vi att titta på två modeller från Meta-familjen eller \"Llama Herd\" – Llama 3.1 och Llama 3.2\n",
    "\n",
    "Dessa modeller finns i olika varianter och är tillgängliga på Github Model marketplace. Här finns mer information om hur du använder Github Models för att [prototypa med AI-modeller](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Modellvarianter:\n",
    "- Llama 3.1 – 70B Instruct\n",
    "- Llama 3.1 – 405B Instruct\n",
    "- Llama 3.2 – 11B Vision Instruct\n",
    "- Llama 3.2 – 90B Vision Instruct\n",
    "\n",
    "*Obs: Llama 3 finns också på Github Models men kommer inte att tas upp i denna lektion*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Med 405 miljarder parametrar räknas Llama 3.1 som en öppen källkods-LLM.\n",
    "\n",
    "Modellen är en uppgradering av den tidigare versionen Llama 3 och erbjuder:\n",
    "\n",
    "- Större kontextfönster – 128k tokens jämfört med 8k tokens\n",
    "- Större max antal utdata-tokens – 4096 jämfört med 2048\n",
    "- Bättre flerspråkigt stöd – tack vare fler tränings-tokens\n",
    "\n",
    "Detta gör att Llama 3.1 kan hantera mer avancerade användningsområden vid utveckling av GenAI-applikationer, inklusive:\n",
    "- Inbyggd funktionsanropning – möjligheten att använda externa verktyg och funktioner utanför LLM-flödet\n",
    "- Bättre RAG-prestanda – tack vare det större kontextfönstret\n",
    "- Generering av syntetisk data – möjligheten att skapa effektiv data för exempelvis finjustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inbyggd funktionsanrop\n",
    "\n",
    "Llama 3.1 har finjusterats för att vara mer effektiv på att göra funktions- eller verktygsanrop. Den har också två inbyggda verktyg som modellen kan identifiera som nödvändiga att använda baserat på användarens prompt. Dessa verktyg är:\n",
    "\n",
    "- **Brave Search** – Kan användas för att hämta aktuell information som vädret genom att göra en webbsökning\n",
    "- **Wolfram Alpha** – Kan användas för mer avancerade matematiska beräkningar så att du inte behöver skriva egna funktioner.\n",
    "\n",
    "Du kan även skapa egna anpassade verktyg som LLM kan anropa.\n",
    "\n",
    "I kodexemplet nedan:\n",
    "\n",
    "- Vi definierar de tillgängliga verktygen (brave_search, wolfram_alpha) i systemprompten.\n",
    "- Skickar en användarprompt som frågar om vädret i en viss stad.\n",
    "- LLM kommer att svara med ett verktygsanrop till Brave Search-verktyget som kommer att se ut så här: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Observera: Det här exemplet gör endast verktygsanropet. Om du vill få resultatet behöver du skapa ett gratis konto på Brave API-sidan och definiera själva funktionen.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Trots att Llama 3.1 är en LLM, har den en begränsning när det gäller multimodalitet. Det vill säga, att kunna använda olika typer av indata som bilder som prompt och ge svar. Denna förmåga är en av huvudfunktionerna i Llama 3.2. Funktionerna inkluderar även:\n",
    "\n",
    "- Multimodalitet – har möjlighet att utvärdera både text- och bildprompter\n",
    "- Små till medelstora varianter (11B och 90B) – detta ger flexibla alternativ för distribution,\n",
    "- Endast text-varianter (1B och 3B) – detta gör att modellen kan användas på edge- och mobila enheter och ger låg fördröjning\n",
    "\n",
    "Stödet för multimodalitet är ett stort steg framåt inom open source-modeller. Kodexemplet nedan tar både en bild och en textprompt för att få en analys av bilden från Llama 3.2 90B.\n",
    "\n",
    "### Multimodalt stöd med Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lärandet slutar inte här, fortsätt resan\n",
    "\n",
    "När du har slutfört denna lektion, ta en titt på vår [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) för att fortsätta utveckla dina kunskaper inom Generativ AI!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfriskrivning**:  \nDetta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Vi strävar efter noggrannhet, men var medveten om att automatiska översättningar kan innehålla fel eller brister. Det ursprungliga dokumentet på dess originalspråk ska betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi tar inget ansvar för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:43:55+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "sv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}