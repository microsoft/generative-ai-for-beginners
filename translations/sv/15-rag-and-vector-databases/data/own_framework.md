<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:47:15+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "sv"
}
-->
# Introduktion till neurala nÃ¤tverk. Flerlagersperceptron

I fÃ¶regÃ¥ende avsnitt lÃ¤rde du dig om den enklaste modellen fÃ¶r neurala nÃ¤tverk â€“ enlagersperceptron, en linjÃ¤r tvÃ¥klassklassificeringsmodell.

I det hÃ¤r avsnittet kommer vi att utÃ¶ka denna modell till en mer flexibel ram som gÃ¶r det mÃ¶jligt fÃ¶r oss att:

* utfÃ¶ra **flerklassklassificering** utÃ¶ver tvÃ¥klass
* lÃ¶sa **regressionsproblem** utÃ¶ver klassificering
* separera klasser som inte Ã¤r linjÃ¤rt separerbara

Vi kommer ocksÃ¥ att utveckla vÃ¥r egen modulÃ¤ra ram i Python som lÃ¥ter oss konstruera olika arkitekturer fÃ¶r neurala nÃ¤tverk.

## Formalisering av maskininlÃ¤rning

LÃ¥t oss bÃ¶rja med att formalisera problemet inom maskininlÃ¤rning. Anta att vi har en trÃ¤ningsdatamÃ¤ngd **X** med etiketter **Y**, och vi behÃ¶ver bygga en modell *f* som ger sÃ¥ korrekta fÃ¶rutsÃ¤gelser som mÃ¶jligt. Kvaliteten pÃ¥ fÃ¶rutsÃ¤gelserna mÃ¤ts med en **fÃ¶rlustfunktion** â„’. FÃ¶ljande fÃ¶rlustfunktioner anvÃ¤nds ofta:

* FÃ¶r regressionsproblem, nÃ¤r vi behÃ¶ver fÃ¶rutsÃ¤ga ett tal, kan vi anvÃ¤nda **absolut fel** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, eller **kvadratiskt fel** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* FÃ¶r klassificering anvÃ¤nder vi **0-1-fÃ¶rlust** (vilket i princip Ã¤r samma som modellens **noggrannhet**), eller **logistisk fÃ¶rlust**.

FÃ¶r en enlagersperceptron definierades funktionen *f* som en linjÃ¤r funktion *f(x)=wx+b* (hÃ¤r Ã¤r *w* viktmatrisen, *x* Ã¤r vektorn med indatafunktioner, och *b* Ã¤r bias-vektorn). FÃ¶r olika arkitekturer av neurala nÃ¤tverk kan denna funktion anta en mer komplex form.

> Vid klassificering Ã¤r det ofta Ã¶nskvÃ¤rt att fÃ¥ sannolikheter fÃ¶r respektive klasser som nÃ¤tverkets output. FÃ¶r att omvandla godtyckliga tal till sannolikheter (t.ex. fÃ¶r att normalisera output) anvÃ¤nder vi ofta **softmax**-funktionen Ïƒ, och funktionen *f* blir *f(x)=Ïƒ(wx+b)*

I definitionen av *f* ovan kallas *w* och *b* fÃ¶r **parametrar** Î¸=âŸ¨*w,b*âŸ©. Givet datamÃ¤ngden âŸ¨**X**,**Y**âŸ© kan vi berÃ¤kna ett totalt fel Ã¶ver hela datamÃ¤ngden som en funktion av parametrarna Î¸.

> âœ… **MÃ¥let med trÃ¤ningen av neurala nÃ¤tverk Ã¤r att minimera felet genom att variera parametrarna Î¸**

## Optimering med gradientnedstigning

Det finns en vÃ¤lkÃ¤nd metod fÃ¶r funktionsoptimering som kallas **gradientnedstigning**. IdÃ©n Ã¤r att vi kan berÃ¤kna en derivata (i flerdimensionellt fall kallad **gradient**) av fÃ¶rlustfunktionen med avseende pÃ¥ parametrarna, och variera parametrarna pÃ¥ ett sÃ¤tt som minskar felet. Detta kan formaliseras sÃ¥ hÃ¤r:

* Initiera parametrarna med nÃ¥gra slumpmÃ¤ssiga vÃ¤rden w<sup>(0)</sup>, b<sup>(0)</sup>
* Upprepa fÃ¶ljande steg mÃ¥nga gÃ¥nger:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Under trÃ¤ningen ska optimeringsstegen berÃ¤knas med hÃ¤nsyn till hela datamÃ¤ngden (kom ihÃ¥g att fÃ¶rlusten berÃ¤knas som en summa Ã¶ver alla trÃ¤ningsprov). Men i praktiken tar vi smÃ¥ delar av datamÃ¤ngden som kallas **minibatcher**, och berÃ¤knar gradienter baserat pÃ¥ en delmÃ¤ngd av data. Eftersom delmÃ¤ngden tas slumpmÃ¤ssigt varje gÃ¥ng kallas denna metod **stokastisk gradientnedstigning** (SGD).

## Flerlagersperceptroner och backpropagation

Ett enlagers nÃ¤tverk, som vi sett ovan, kan klassificera linjÃ¤rt separerbara klasser. FÃ¶r att bygga en rikare modell kan vi kombinera flera lager i nÃ¤tverket. Matematiskt innebÃ¤r det att funktionen *f* fÃ¥r en mer komplex form och berÃ¤knas i flera steg:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

HÃ¤r Ã¤r Î± en **icke-linjÃ¤r aktiveringsfunktion**, Ïƒ Ã¤r en softmax-funktion, och parametrarna Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradientnedstigningsalgoritmen Ã¤r densamma, men det blir svÃ¥rare att berÃ¤kna gradienterna. Med hjÃ¤lp av kedjeregeln kan vi berÃ¤kna derivator som:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Kedjeregeln anvÃ¤nds fÃ¶r att berÃ¤kna derivator av fÃ¶rlustfunktionen med avseende pÃ¥ parametrarna.

Observera att den vÃ¤nstermest delen i alla dessa uttryck Ã¤r densamma, vilket gÃ¶r att vi effektivt kan berÃ¤kna derivator genom att bÃ¶rja frÃ¥n fÃ¶rlustfunktionen och gÃ¥ "bakÃ¥t" genom berÃ¤kningsgrafen. DÃ¤rfÃ¶r kallas metoden fÃ¶r att trÃ¤na en flerskiktsperceptron fÃ¶r **backpropagation**, eller 'backprop'.



> TODO: bildreferens

> âœ… Vi kommer att gÃ¥ igenom backpropagation i mycket stÃ¶rre detalj i vÃ¥rt notebook-exempel.  

## Slutsats

I denna lektion har vi byggt vÃ¥rt eget bibliotek fÃ¶r neurala nÃ¤tverk och anvÃ¤nt det fÃ¶r en enkel tvÃ¥dimensionell klassificeringsuppgift.

## ğŸš€ Utmaning

I det medfÃ¶ljande notebooket kommer du att implementera din egen ram fÃ¶r att bygga och trÃ¤na flerskiktsperceptroner. Du kommer att kunna se i detalj hur moderna neurala nÃ¤tverk fungerar.

FortsÃ¤tt till OwnFramework-notebooket och arbeta dig igenom det.



## Repetition & SjÃ¤lvstudier

Backpropagation Ã¤r en vanlig algoritm som anvÃ¤nds inom AI och ML, och Ã¤r vÃ¤rd att studera mer ingÃ¥ende.

## Uppgift

I denna labb ska du anvÃ¤nda ramen du byggde i denna lektion fÃ¶r att lÃ¶sa MNIST-klassificering av handskrivna siffror.

* Instruktioner
* Notebook

**Ansvarsfriskrivning**:  
Detta dokument har Ã¶versatts med hjÃ¤lp av AI-Ã¶versÃ¤ttningstjÃ¤nsten [Co-op Translator](https://github.com/Azure/co-op-translator). Ã„ven om vi strÃ¤var efter noggrannhet, vÃ¤nligen observera att automatiska Ã¶versÃ¤ttningar kan innehÃ¥lla fel eller brister. Det ursprungliga dokumentet pÃ¥ dess modersmÃ¥l bÃ¶r betraktas som den auktoritativa kÃ¤llan. FÃ¶r kritisk information rekommenderas professionell mÃ¤nsklig Ã¶versÃ¤ttning. Vi ansvarar inte fÃ¶r nÃ¥gra missfÃ¶rstÃ¥nd eller feltolkningar som uppstÃ¥r vid anvÃ¤ndning av denna Ã¶versÃ¤ttning.