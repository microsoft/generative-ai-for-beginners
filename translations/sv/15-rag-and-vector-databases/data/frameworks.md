<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:32:39+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "sv"
}
-->
# Neural Network Frameworks

Som vi redan har l√§rt oss, f√∂r att kunna tr√§na neurala n√§tverk effektivt beh√∂ver vi g√∂ra tv√• saker:

* Att arbeta med tensorer, t.ex. multiplicera, addera och ber√§kna funktioner som sigmoid eller softmax
* Att ber√§kna gradienter f√∂r alla uttryck, f√∂r att kunna utf√∂ra gradientnedstigningsoptimering

Medan `numpy`-biblioteket kan hantera den f√∂rsta delen, beh√∂ver vi n√•gon mekanism f√∂r att ber√§kna gradienter. I v√•rt ramverk som vi utvecklade i f√∂reg√•ende avsnitt var vi tvungna att manuellt programmera alla derivatfunktioner inuti `backward`-metoden, som utf√∂r backpropagation. Idealiskt sett b√∂r ett ramverk ge oss m√∂jlighet att ber√§kna gradienter f√∂r *vilket uttryck som helst* som vi kan definiera.

En annan viktig sak √§r att kunna utf√∂ra ber√§kningar p√• GPU, eller andra specialiserade ber√§kningsenheter, som TPU. Tr√§ning av djupa neurala n√§tverk kr√§ver *mycket* ber√§kningar, och att kunna parallellisera dessa ber√§kningar p√• GPU:er √§r mycket viktigt.

> ‚úÖ Termen 'parallellisera' betyder att f√∂rdela ber√§kningarna √∂ver flera enheter.

F√∂r n√§rvarande √§r de tv√• mest popul√§ra neurala ramverken: TensorFlow och PyTorch. B√•da erbjuder ett l√•g-niv√• API f√∂r att arbeta med tensorer p√• b√•de CPU och GPU. Ovanp√• l√•g-niv√• API:et finns √§ven ett h√∂g-niv√• API, kallat Keras respektive PyTorch Lightning.

Low-Level API | TensorFlow | PyTorch  
--------------|-------------|---------  
High-level API| Keras       | PyTorch

**L√•g-niv√• API:er** i b√•da ramverken l√•ter dig bygga s√• kallade **ber√§kningsgrafer**. Denna graf definierar hur man ber√§knar output (vanligtvis f√∂rlustfunktionen) med givna indata-parametrar, och kan skickas f√∂r ber√§kning p√• GPU om det finns tillg√§ngligt. Det finns funktioner f√∂r att differentiera denna ber√§kningsgraf och ber√§kna gradienter, som sedan kan anv√§ndas f√∂r att optimera modellparametrar.

**H√∂g-niv√• API:er** betraktar neurala n√§tverk som en **sekvens av lager**, och g√∂r konstruktionen av de flesta neurala n√§tverk mycket enklare. Att tr√§na modellen kr√§ver vanligtvis att man f√∂rbereder data och sedan anropar en `fit`-funktion f√∂r att utf√∂ra tr√§ningen.

H√∂g-niv√• API:et l√•ter dig snabbt bygga typiska neurala n√§tverk utan att beh√∂va oroa dig f√∂r m√•nga detaljer. Samtidigt erbjuder l√•g-niv√• API:et mycket mer kontroll √∂ver tr√§ningsprocessen, och anv√§nds d√§rf√∂r ofta inom forskning n√§r man arbetar med nya neurala n√§tverksarkitekturer.

Det √§r ocks√• viktigt att f√∂rst√• att du kan anv√§nda b√•da API:erna tillsammans, t.ex. kan du utveckla din egen n√§tverkslagerarkitektur med l√•g-niv√• API och sedan anv√§nda den i ett st√∂rre n√§tverk som konstrueras och tr√§nas med h√∂g-niv√• API. Eller s√• kan du definiera ett n√§tverk med h√∂g-niv√• API som en sekvens av lager och sedan anv√§nda din egen l√•g-niv√• tr√§ningsslinga f√∂r optimering. B√•da API:erna anv√§nder samma grundl√§ggande koncept och √§r designade f√∂r att fungera bra tillsammans.

## Learning

I denna kurs erbjuder vi det mesta av inneh√•llet b√•de f√∂r PyTorch och TensorFlow. Du kan v√§lja ditt f√∂redragna ramverk och bara g√• igenom motsvarande notebooks. Om du √§r os√§ker p√• vilket ramverk du ska v√§lja, l√§s n√•gra diskussioner p√• internet om **PyTorch vs. TensorFlow**. Du kan ocks√• titta p√• b√•da ramverken f√∂r att f√• en b√§ttre f√∂rst√•else.

D√§r det √§r m√∂jligt kommer vi att anv√§nda h√∂g-niv√• API f√∂r enkelhetens skull. Men vi anser att det √§r viktigt att f√∂rst√• hur neurala n√§tverk fungerar fr√•n grunden, s√• i b√∂rjan b√∂rjar vi med att arbeta med l√•g-niv√• API och tensorer. Om du d√§remot vill komma ig√•ng snabbt och inte vill l√§gga mycket tid p√• att l√§ra dig dessa detaljer, kan du hoppa √∂ver dem och g√• direkt till h√∂g-niv√• API-notebooks.

## ‚úçÔ∏è √ñvningar: Frameworks

Forts√§tt din inl√§rning i f√∂ljande notebooks:

Low-Level API | TensorFlow+Keras Notebook | PyTorch  
--------------|----------------------------|---------  
High-level API| Keras                      | *PyTorch Lightning*

Efter att ha beh√§rskat ramverken, l√•t oss sammanfatta begreppet √∂veranpassning.

# Overfitting

√ñveranpassning √§r ett extremt viktigt begrepp inom maskininl√§rning, och det √§r mycket viktigt att f√∂rst√• det r√§tt!

T√§nk p√• f√∂ljande problem med att approximera 5 punkter (representerade av `x` p√• graferna nedan):

!linear | overfit  
-------------------------|--------------------------  
**Linj√§r modell, 2 parametrar** | **Icke-linj√§r modell, 7 parametrar**  
Tr√§ningsfel = 5.3 | Tr√§ningsfel = 0  
Valideringsfel = 5.1 | Valideringsfel = 20

* Till v√§nster ser vi en bra rak linje-approximation. Eftersom antalet parametrar √§r l√§mpligt, f√•ngar modellen r√§tt m√∂nster bakom punktf√∂rdelningen.
* Till h√∂ger √§r modellen f√∂r kraftfull. Eftersom vi bara har 5 punkter och modellen har 7 parametrar, kan den anpassa sig s√• att den g√•r igenom alla punkter, vilket g√∂r tr√§ningsfelet till 0. Detta hindrar dock modellen fr√•n att f√∂rst√• det korrekta m√∂nstret bakom datan, vilket g√∂r valideringsfelet mycket h√∂gt.

Det √§r mycket viktigt att hitta en r√§tt balans mellan modellens komplexitet (antal parametrar) och antalet tr√§ningsdata.

## Varf√∂r √∂veranpassning uppst√•r

  * Inte tillr√§ckligt med tr√§ningsdata  
  * F√∂r kraftfull modell  
  * F√∂r mycket brus i indata

## Hur man uppt√§cker √∂veranpassning

Som du kan se i grafen ovan kan √∂veranpassning uppt√§ckas genom mycket l√•gt tr√§ningsfel och h√∂gt valideringsfel. Normalt under tr√§ning ser vi b√•de tr√§nings- och valideringsfel minska, men vid en viss punkt kan valideringsfelet sluta minska och b√∂rja √∂ka. Detta √§r ett tecken p√• √∂veranpassning och en indikation p√• att vi f√∂rmodligen b√∂r stoppa tr√§ningen vid denna punkt (eller √•tminstone ta en snapshot av modellen).

overfitting

## Hur man f√∂rhindrar √∂veranpassning

Om du ser att √∂veranpassning uppst√•r kan du g√∂ra n√•got av f√∂ljande:

 * √ñka m√§ngden tr√§ningsdata  
 * Minska modellens komplexitet  
 * Anv√§nd n√•gon regulariseringsteknik, som Dropout, vilket vi kommer att titta p√• senare.

## √ñveranpassning och Bias-Variance Tradeoff

√ñveranpassning √§r egentligen ett fall av ett mer generellt problem inom statistik som kallas Bias-Variance Tradeoff. Om vi betraktar m√∂jliga felk√§llor i v√•r modell kan vi se tv√• typer av fel:

* **Bias-fel** orsakas av att v√•r algoritm inte kan f√•nga relationen i tr√§ningsdata korrekt. Det kan bero p√• att modellen inte √§r tillr√§ckligt kraftfull (**underanpassning**).
* **Variansfel**, som orsakas av att modellen approximera brus i indata ist√§llet f√∂r meningsfulla samband (**√∂veranpassning**).

Under tr√§ning minskar bias-felet (eftersom modellen l√§r sig att approximera datan), medan variansfelet √∂kar. Det √§r viktigt att stoppa tr√§ningen ‚Äì antingen manuellt (n√§r vi uppt√§cker √∂veranpassning) eller automatiskt (genom att inf√∂ra regularisering) ‚Äì f√∂r att f√∂rhindra √∂veranpassning.

## Slutsats

I denna lektion har du l√§rt dig om skillnaderna mellan de olika API:erna f√∂r de tv√• mest popul√§ra AI-ramverken, TensorFlow och PyTorch. Dessutom har du l√§rt dig om ett mycket viktigt √§mne, √∂veranpassning.

## üöÄ Utmaning

I de medf√∂ljande notebooks hittar du 'uppgifter' l√§ngst ner; arbeta igenom notebooks och slutf√∂r uppgifterna.

## Review & Sj√§lvstudier

G√∂r lite research om f√∂ljande √§mnen:

- TensorFlow  
- PyTorch  
- Overfitting

St√§ll dig sj√§lv f√∂ljande fr√•gor:

- Vad √§r skillnaden mellan TensorFlow och PyTorch?  
- Vad √§r skillnaden mellan √∂veranpassning och underanpassning?

## Uppgift

I detta labb ska du l√∂sa tv√• klassificeringsproblem med hj√§lp av enkla och flerskiktade fullt anslutna n√§tverk med PyTorch eller TensorFlow.

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, v√§nligen observera att automatiska √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess modersm√•l b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r n√•gra missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.