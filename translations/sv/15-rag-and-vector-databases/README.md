<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2210a0466c812d9defc4df2d9a709ff9",
  "translation_date": "2026-01-18T18:04:45+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "sv"
}
-->
# Retrieval Augmented Generation (RAG) och vektordatabaser

[![Retrieval Augmented Generation (RAG) och Vektordatabaser](../../../../../translated_images/sv/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

I lektionen om s√∂kapplikationer l√§rde vi oss kort hur man integrerar egna data i Large Language Models (LLMs). I denna lektion ska vi f√∂rdjupa oss i koncepten f√∂r att f√∂rankra dina data i din LLM-applikation, hur processen fungerar och metoder f√∂r att lagra data, inklusive b√•de embeddings och text.

> **Video kommer snart**

## Introduktion

I denna lektion kommer vi ta upp f√∂ljande:

- En introduktion till RAG, vad det √§r och varf√∂r det anv√§nds inom AI (artificiell intelligens).

- F√∂rst√• vad vektordatabaser √§r och skapa en f√∂r v√•r applikation.

- Ett praktiskt exempel p√• hur man integrerar RAG i en applikation.

## L√§randem√•l

Efter att ha slutf√∂rt denna lektion kommer du att kunna:

- F√∂rklara betydelsen av RAG vid datah√§mtning och bearbetning.

- S√§tta upp en RAG-applikation och f√∂rankra dina data till en LLM.

- Effektiv integration av RAG och vektordatabaser i LLM-applikationer.

## V√•rt scenario: f√∂rb√§ttra v√•ra LLMs med v√•ra egna data

I denna lektion vill vi l√§gga till v√•ra egna anteckningar i utbildningsstartuppen, vilket g√∂r det m√∂jligt f√∂r chatbotten att f√• mer information om de olika √§mnena. Genom att anv√§nda de anteckningar vi har, kan eleverna studera b√§ttre och f√∂rst√• de olika √§mnena, vilket g√∂r det enklare att repetera inf√∂r sina examinationer. F√∂r att skapa v√•rt scenario anv√§nder vi:

- `Azure OpenAI:` den LLM vi ska anv√§nda f√∂r att skapa v√•r chatbot

- `AI for beginners' lesson on Neural Networks:` detta kommer att vara datan vi f√∂rankrar v√•r LLM p√•

- `Azure AI Search` och `Azure Cosmos DB:` vektordatabas f√∂r att lagra v√•ra data och skapa en s√∂kindex

Anv√§ndare kommer att kunna skapa √∂vningsquiz fr√•n sina anteckningar, repetitionsflashkort och sammanfatta dessa till kortfattade √∂versikter. F√∂r att komma ig√•ng, l√•t oss titta p√• vad RAG √§r och hur det fungerar:

## Retrieval Augmented Generation (RAG)

En LLM-driven chatbot bearbetar anv√§ndarfr√•gor f√∂r att generera svar. Den √§r designad f√∂r att vara interaktiv och engagerar sig med anv√§ndare inom m√•nga olika √§mnen. Men dess svar begr√§nsas till det sammanhang som ges och dess grundl√§ggande tr√§ningsdata. Till exempel har GPT-4 kunskapsstopp i september 2021, vilket inneb√§r att den saknar kunskap om h√§ndelser som intr√§ffat efter detta datum. Dessutom inkluderar tr√§ningsdatan f√∂r LLMs inte konfidentiell information som personliga anteckningar eller ett f√∂retags produktmanual.

### Hur RAGs (Retrieval Augmented Generation) fungerar

![ritning som visar hur RAGs fungerar](../../../../../translated_images/sv/how-rag-works.f5d0ff63942bd3a6.webp)

Anta att du vill implementera en chatbot som skapar quiz fr√•n dina anteckningar, d√• beh√∂ver du en koppling till kunskapsbasen. Det √§r h√§r RAG kommer in. RAGs fungerar enligt f√∂ljande:

- **Kunskapsbas:** Innan h√§mtning m√•ste dessa dokument l√§sas in och f√∂rbehandlas, vanligtvis genom att bryta ner stora dokument i mindre delar, omvandla dem till textembeddingar och lagra dem i en databas.

- **Anv√§ndarfr√•ga:** anv√§ndaren st√§ller en fr√•ga

- **H√§mtning:** N√§r anv√§ndaren st√§ller en fr√•ga h√§mtar embeddingmodellen relevant information fr√•n v√•r kunskapsbas f√∂r att ge mer kontext som inkluderas i prompten.

- **Augmented Generation:** LLM f√∂rb√§ttrar sitt svar baserat p√• den h√§mtade datan. Detta till√•ter det genererade svaret att inte bara baseras p√• f√∂rtr√§nad data utan ocks√• p√• relevant information fr√•n den tillagda kontexten. Den h√§mtade datan anv√§nds f√∂r att f√∂rst√§rka LLM:s svar. LLM returnerar d√§refter ett svar p√• anv√§ndarens fr√•ga.

![ritning som visar hur RAGs arkitektur fungerar](../../../../../translated_images/sv/encoder-decode.f2658c25d0eadee2.webp)

Arkitekturen f√∂r RAGs implementeras med transformers best√•ende av tv√• delar: en encoder och en decoder. Till exempel, n√§r en anv√§ndare st√§ller en fr√•ga, "kodas" inmatningstexten till vektorer som f√•ngar ordens betydelse och vektorerna "avkodas" mot v√•rt dokumentindex och genererar ny text baserad p√• anv√§ndarens fr√•ga. LLM anv√§nder b√•de en encoder-decoder-modell f√∂r att generera output.

Tv√• tillv√§gag√•ngss√§tt vid implementering av RAG enligt det f√∂reslagna dokumentet: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) √§r:

- **_RAG-Sequence_** anv√§nder h√§mtade dokument f√∂r att f√∂ruts√§ga b√§sta m√∂jliga svar p√• en anv√§ndarfr√•ga

- **RAG-Token** anv√§nder dokument f√∂r att generera n√§sta token, sedan h√§mtas de f√∂r att svara p√• anv√§ndarens fr√•ga

### Varf√∂r skulle du anv√§nda RAGs?

- **Informationsrikedom:** s√§kerst√§ller att textsvar √§r uppdaterade och aktuella. Detta f√∂rb√§ttrar prestandan p√• dom√§nspecifika uppgifter genom att komma √•t den interna kunskapsbasen.

- Minskar fabricering genom att anv√§nda **verifierbar data** i kunskapsbasen f√∂r att ge kontext till anv√§ndarfr√•gor.

- Det √§r **kostnadseffektivt** eftersom det √§r mer ekonomiskt j√§mf√∂rt med finjustering av en LLM.

## Skapa en kunskapsbas

V√•r applikation baseras p√• v√•ra personliga data, det vill s√§ga lektionen om neurala n√§tverk i AI For Beginners-kursen.

### Vektordatabaser

En vektordatabas √§r, till skillnad fr√•n traditionella databaser, en specialiserad databas designad f√∂r att lagra, hantera och s√∂ka inb√§ddade vektorer. Den lagrar numeriska representationer av dokument. Att bryta ner data till numeriska embeddingar g√∂r det enklare f√∂r v√•rt AI-system att f√∂rst√• och bearbeta datan.

Vi lagrar v√•ra embeddingar i vektordatabaser eftersom LLMs har en gr√§ns f√∂r hur m√•nga tokens de accepterar som input. Eftersom man inte kan skicka hela embeddingarna till en LLM, beh√∂ver vi dela upp dem i bitar och n√§r en anv√§ndare st√§ller en fr√•ga kommer embeddingarna som √§r mest relevanta f√∂r fr√•gan att returneras tillsammans med prompten. Uppdelning minskar ocks√• kostnaderna f√∂r antalet tokens som skickas genom en LLM.

N√•gra popul√§ra vektordatabaser √§r Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant och DeepLake. Du kan skapa en Azure Cosmos DB-modell med Azure CLI med f√∂ljande kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fr√•n text till embeddingar

Innan vi lagrar v√•r data m√•ste vi konvertera den till vektor-embeddingar innan de lagras i databasen. Om du arbetar med stora dokument eller l√•nga texter kan du dela upp dem baserat p√• f√∂rv√§ntade fr√•gor. Uppdelning kan g√∂ras p√• meningsniv√• eller p√• styckesniv√•. Eftersom uppdelning h√§mtar betydelser fr√•n orden runt omkring kan du l√§gga till viss ytterligare kontext till en bit, till exempel genom att l√§gga till dokumentets titel eller inkludera viss text f√∂re eller efter biten. Du kan dela upp datan enligt f√∂ljande:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Om den sista biten inte n√•dde minimil√§ngden, l√§gg till den √§nd√•
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N√§r den v√§l √§r uppdelad kan vi sedan b√§dda in v√•r text med olika embeddingmodeller. N√•gra modeller du kan anv√§nda inkluderar: word2vec, ada-002 fr√•n OpenAI, Azure Computer Vision och m√•nga fler. Valet av modell beror p√• vilka spr√•k du anv√§nder, typen av inneh√•ll som kodas (text/bilder/ljud), storleken p√• input den kan koda och l√§ngden p√• embeddingoutput.

Ett exempel p√• inb√§ddad text med OpenAIs `text-embedding-ada-002` modell √§r:
![en embedding av ordet katt](../../../../../translated_images/sv/cat.74cbd7946bc9ca38.webp)

## H√§mtning och vektors√∂kning

N√§r en anv√§ndare st√§ller en fr√•ga omvandlar retrievern den till en vektor med query-encodern och s√∂ker sedan igenom v√•rt dokumentindex efter relevanta vektorer i dokumentet som relaterar till inmatningen. N√§r detta √§r gjort konverteras b√•de inmatningsvektorn och dokumentvektorer till text och skickas genom LLM.

### H√§mtning

H√§mtning sker n√§r systemet f√∂rs√∂ker snabbt hitta dokument i indexet som uppfyller s√∂kkriterierna. M√•let f√∂r retrievern √§r att f√• fram dokument som kan anv√§ndas f√∂r att ge kontext och f√∂rankra LLM i dina data.

Det finns flera s√§tt att utf√∂ra s√∂kningar i v√•r databas, s√•som:

- **Nyckelordss√∂kning** ‚Äì anv√§nds f√∂r texts√∂kningar

- **Vektors√∂kning** ‚Äì konverterar dokument fr√•n text till vektorrepresentationer med hj√§lp av embeddingmodeller, vilket m√∂jligg√∂r en **semantisk s√∂kning** baserad p√• ordens betydelse. H√§mtning sker genom att fr√•ga de dokument vars vektorrepresentationer √§r n√§rmast anv√§ndarens fr√•ga.

- **Hybrid** ‚Äì en kombination av b√•de nyckelords- och vektors√∂kning.

En utmaning med h√§mtning uppst√•r n√§r det inte finns n√•got liknande svar i databasen; systemet returnerar d√• den b√§sta information det kan finna, men du kan anv√§nda taktiker som att st√§lla in maximal relevansavst√•nd eller anv√§nda hybrid-s√∂kning som kombinerar b√•de nyckelord och vektors√∂kning. I denna lektion anv√§nder vi hybrid-s√∂kning, en kombination av b√•de vektor- och nyckelordss√∂kning. Vi kommer att lagra v√•r data i en dataframe med kolumner som inneh√•ller bitarna samt embeddingarna.

### Vektorsimilaritet

Retrievern s√∂ker igenom kunskapsdatabasen efter embeddingar som ligger n√§ra varandra, n√§rmaste granne, eftersom det handlar om texter som √§r liknande. I scenariot d√§r en anv√§ndare st√§ller en fr√•ga, b√§ddas fr√•gan f√∂rst in och matchas sedan med liknande embeddingar. Det vanliga m√•ttet som anv√§nds f√∂r att hitta hur lika olika vektorer √§r √§r cosinuslikhet, vilket baseras p√• vinkeln mellan tv√• vektorer.

Vi kan m√§ta likhet med andra alternativ som vi kan anv√§nda, till exempel Euklidiskt avst√•nd, vilket √§r den raka linjen mellan vektor√§ndpunkter, och prdotprodukt som m√§ter summan av produkterna f√∂r motsvarande element i tv√• vektorer.

### S√∂kindex

N√§r vi h√§mtar beh√∂ver vi bygga ett s√∂kindex f√∂r v√•r kunskapsbas innan vi utf√∂r s√∂kningen. Ett index lagrar v√•ra embeddingar och kan snabbt h√§mta de mest lika bitarna √§ven i en stor databas. Vi kan skapa v√•rt index lokalt med:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Skapa s√∂kindexet
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# F√∂r att fr√•ga indexet kan du anv√§nda kneighbors-metoden
distances, indices = nbrs.kneighbors(embeddings)
```

### Omrankning

N√§r du har fr√•gat i databasen kan du beh√∂va sortera resultaten fr√•n mest relevanta till minst relevanta. En omrankande LLM anv√§nder maskininl√§rning f√∂r att f√∂rb√§ttra relevansen av s√∂kresultaten genom att ordna dem fr√•n mest relevanta. Med Azure AI Search g√∂rs omrankning automatiskt med en semantisk omrankare. Ett exempel p√• hur omrankning fungerar med n√§rmaste grannar:

```python
# Hitta de mest liknande dokumenten
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Skriv ut de mest liknande dokumenten
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Att f√∂rena allt detta

Det sista steget √§r att l√§gga till v√•r LLM i mixen f√∂r att kunna f√• svar som √§r f√∂rankrade i v√•r data. Vi kan implementera det enligt f√∂ljande:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Konvertera fr√•gan till en f√∂rfr√•gningsvektor
    query_vector = create_embeddings(user_input)

    # Hitta de mest liknande dokumenten
    distances, indices = nbrs.kneighbors([query_vector])

    # l√§gg till dokument till f√∂rfr√•gan f√∂r att ge kontext
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # kombinera historiken och anv√§ndarens input
    history.append(user_input)

    # skapa ett meddelandeobjekt
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # anv√§nd chattkomplettering f√∂r att generera ett svar
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Utv√§rdera v√•r applikation

### Utv√§rderingsm√•tt

- Kvaliteten p√• svaren, att de l√•ter naturliga, flytande och m√§nniskolika

- F√∂rankring av datan: utv√§rdering om svaret kom fr√•n de tillhandah√•llna dokumenten

- Relevans: utv√§rdering att svaret matchar och √§r relaterat till st√§lld fr√•ga

- Flyt ‚Äì om svaret √§r grammatiskt korrekt och begripligt

## Anv√§ndningsomr√•den f√∂r RAG (Retrieval Augmented Generation) och vektordatabaser

Det finns m√•nga olika anv√§ndningsomr√•den d√§r funktionsanrop kan f√∂rb√§ttra din app s√•som:

- Fr√•gor och svar: f√∂rankra din f√∂retagsdata till en chatt som kan anv√§ndas av anst√§llda f√∂r att st√§lla fr√•gor.

- Rekommendationssystem: d√§r du kan skapa ett system som matchar de mest lika v√§rdena, t.ex. filmer, restauranger och mycket mer.

- Chattbottj√§nster: du kan lagra chattloggen och personanpassa konversationen baserat p√• anv√§ndardata.

- Bilds√∂kning baserad p√• vektorembeddingar, anv√§ndbart vid bildigenk√§nning och felavvikelsedetektering.

## Sammanfattning

Vi har t√§ckt grundl√§ggande omr√•den f√∂r RAG fr√•n att l√§gga till v√•r data till applikationen, anv√§ndarfr√•gan och utdata. F√∂r att f√∂renkla skapandet av RAG kan du anv√§nda ramverk som Semantic Kernel, Langchain eller Autogen.

## Uppgift

F√∂r att forts√§tta din l√§rande om Retrieval Augmented Generation (RAG) kan du bygga:

- Skapa ett front-end f√∂r applikationen med valfritt ramverk

- Anv√§nd ett ramverk, antingen LangChain eller Semantic Kernel, och √•terbygg din applikation.

Grattis till att ha slutf√∂rt lektionen üëè.

## L√§randet slutar inte h√§r, forts√§tt resan

Efter att ha slutf√∂rt denna lektion, kolla in v√•r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) f√∂r att forts√§tta h√∂ja din kunskap inom Generative AI!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfriskrivning**:
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). Trots att vi str√§var efter noggrannhet, var v√§nlig observera att automatiska √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r n√•gra missf√∂rst√•nd eller feltolkningar som uppst√•r genom anv√§ndning av denna √∂vers√§ttning.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->