<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b4b0266fbadbba7ded891b6485adc66d",
  "translation_date": "2025-10-17T19:01:05+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "sv"
}
-->
# Retrieval Augmented Generation (RAG) och Vektordatabaser

[![Retrieval Augmented Generation (RAG) och Vektordatabaser](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.sv.png)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

I lektionen om s칬kapplikationer l칛rde vi oss kort hur man integrerar egen data i stora spr친kmodeller (LLMs). I denna lektion kommer vi att f칬rdjupa oss i koncepten kring att grunda din data i din LLM-applikation, mekaniken bakom processen och metoder f칬r att lagra data, inklusive b친de embeddings och text.

> **Video kommer snart**

## Introduktion

I denna lektion kommer vi att t칛cka f칬ljande:

- En introduktion till RAG, vad det 칛r och varf칬r det anv칛nds inom AI (artificiell intelligens).

- F칬rst친 vad vektordatabaser 칛r och skapa en f칬r v친r applikation.

- Ett praktiskt exempel p친 hur man integrerar RAG i en applikation.

## L칛randem친l

Efter att ha avslutat denna lektion kommer du att kunna:

- F칬rklara betydelsen av RAG f칬r datah칛mtning och bearbetning.

- St칛lla in en RAG-applikation och grunda din data till en LLM.

- Effektiv integration av RAG och vektordatabaser i LLM-applikationer.

## V친rt scenario: f칬rb칛ttra v친ra LLMs med v친r egen data

F칬r denna lektion vill vi l칛gga till v친ra egna anteckningar i utbildningsstartupen, vilket g칬r att chatboten kan f친 mer information om olika 칛mnen. Genom att anv칛nda de anteckningar vi har kommer eleverna att kunna studera b칛ttre och f칬rst친 de olika 칛mnena, vilket g칬r det enklare att repetera inf칬r sina prov. F칬r att skapa v친rt scenario kommer vi att anv칛nda:

- `Azure OpenAI:` LLM som vi kommer att anv칛nda f칬r att skapa v친r chatbot.

- `AI f칬r nyb칬rjare-lektion om neurala n칛tverk:` detta kommer att vara den data vi grundar v친r LLM p친.

- `Azure AI Search` och `Azure Cosmos DB:` vektordatabas f칬r att lagra v친r data och skapa ett s칬kindex.

Anv칛ndare kommer att kunna skapa 칬vningsquiz fr친n sina anteckningar, repetitionskort och sammanfatta dem till kortfattade 칬versikter. F칬r att komma ig친ng, l친t oss titta p친 vad RAG 칛r och hur det fungerar:

## Retrieval Augmented Generation (RAG)

En LLM-driven chatbot bearbetar anv칛ndarens fr친gor f칬r att generera svar. Den 칛r designad f칬r att vara interaktiv och engagerar sig med anv칛ndare inom en m칛ngd olika 칛mnen. Men dess svar 칛r begr칛nsade till det sammanhang som tillhandah친lls och dess grundl칛ggande tr칛ningsdata. Till exempel har GPT-4 en kunskapsgr칛ns fr친n september 2021, vilket inneb칛r att den saknar kunskap om h칛ndelser som intr칛ffat efter denna period. Dessutom utesluter den data som anv칛nds f칬r att tr칛na LLMs konfidentiell information s친som personliga anteckningar eller en f칬retags produktmanual.

### Hur RAGs (Retrieval Augmented Generation) fungerar

![ritning som visar hur RAGs fungerar](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.sv.png)

Anta att du vill distribuera en chatbot som skapar quiz fr친n dina anteckningar, d친 beh칬ver du en anslutning till kunskapsbasen. Det 칛r h칛r RAG kommer till unds칛ttning. RAGs fungerar enligt f칬ljande:

- **Kunskapsbas:** Innan h칛mtning m친ste dessa dokument importeras och f칬rbehandlas, vanligtvis genom att bryta ner stora dokument i mindre delar, omvandla dem till textembeddings och lagra dem i en databas.

- **Anv칛ndarfr친ga:** anv칛ndaren st칛ller en fr친ga.

- **H칛mtning:** N칛r en anv칛ndare st칛ller en fr친ga h칛mtar embeddingmodellen relevant information fr친n v친r kunskapsbas f칬r att ge mer sammanhang som kommer att inf칬rlivas i fr친gan.

- **F칬rb칛ttrad generering:** LLM f칬rb칛ttrar sitt svar baserat p친 den data som h칛mtats. Det g칬r att det genererade svaret inte bara baseras p친 f칬rtr칛nad data utan ocks친 relevant information fr친n det tillagda sammanhanget. Den h칛mtade datan anv칛nds f칬r att f칬rb칛ttra LLM:s svar. LLM returnerar sedan ett svar p친 anv칛ndarens fr친ga.

![ritning som visar RAGs arkitektur](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.sv.png)

Arkitekturen f칬r RAGs implementeras med hj칛lp av transformatorer som best친r av tv친 delar: en encoder och en decoder. Till exempel, n칛r en anv칛ndare st칛ller en fr친ga, kodas inputtexten till vektorer som f친ngar inneb칬rden av orden och vektorerna avkodas till v친rt dokumentindex och genererar ny text baserat p친 anv칛ndarens fr친ga. LLM anv칛nder b친de en encoder-decoder-modell f칬r att generera output.

Tv친 tillv칛gag친ngss칛tt vid implementering av RAG enligt det f칬reslagna dokumentet: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) 칛r:

- **_RAG-Sequence_** anv칛nder h칛mtade dokument f칬r att f칬ruts칛ga det b칛sta m칬jliga svaret p친 en anv칛ndarfr친ga.

- **RAG-Token** anv칛nder dokument f칬r att generera n칛sta token och h칛mtar dem sedan f칬r att svara p친 anv칛ndarens fr친ga.

### Varf칬r anv칛nda RAGs?

- **Informationsrikedom:** s칛kerst칛ller att textresponsen 칛r aktuell och uppdaterad. Det f칬rb칛ttrar d칛rf칬r prestandan p친 dom칛nspecifika uppgifter genom att f친 tillg친ng till den interna kunskapsbasen.

- Minskar fabricering genom att anv칛nda **verifierbar data** i kunskapsbasen f칬r att ge sammanhang till anv칛ndarfr친gor.

- Det 칛r **kostnadseffektivt** eftersom de 칛r mer ekonomiska j칛mf칬rt med att finjustera en LLM.

## Skapa en kunskapsbas

V친r applikation 칛r baserad p친 v친r personliga data, dvs. lektionen om neurala n칛tverk fr친n AI f칬r nyb칬rjare-kursen.

### Vektordatabaser

En vektordatabas, till skillnad fr친n traditionella databaser, 칛r en specialiserad databas designad f칬r att lagra, hantera och s칬ka inb칛ddade vektorer. Den lagrar numeriska representationer av dokument. Att bryta ner data till numeriska embeddings g칬r det enklare f칬r v친rt AI-system att f칬rst친 och bearbeta datan.

Vi lagrar v친ra embeddings i vektordatabaser eftersom LLMs har en gr칛ns f칬r antalet tokens de accepterar som input. Eftersom du inte kan skicka hela embeddings till en LLM, m친ste vi dela upp dem i mindre delar och n칛r en anv칛ndare st칛ller en fr친ga kommer de embeddings som 칛r mest lik fr친gan att returneras tillsammans med fr친gan. Att dela upp data i mindre delar minskar ocks친 kostnaderna f칬r antalet tokens som skickas genom en LLM.

N친gra popul칛ra vektordatabaser inkluderar Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant och DeepLake. Du kan skapa en Azure Cosmos DB-modell med Azure CLI med f칬ljande kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fr친n text till embeddings

Innan vi lagrar v친r data m친ste vi konvertera den till vektorembeddings innan den lagras i databasen. Om du arbetar med stora dokument eller l친nga texter kan du dela upp dem baserat p친 de fr친gor du f칬rv칛ntar dig. Uppdelning kan g칬ras p친 meningsniv친 eller p친 styckeniv친. Eftersom uppdelning h칛rleder betydelser fr친n orden runt dem kan du l칛gga till annat sammanhang till en del, till exempel genom att l칛gga till dokumenttiteln eller inkludera viss text f칬re eller efter delen. Du kan dela upp datan enligt f칬ljande:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N칛r den 칛r uppdelad kan vi sedan b칛dda in v친r text med olika embeddingmodeller. N친gra modeller du kan anv칛nda inkluderar: word2vec, ada-002 av OpenAI, Azure Computer Vision och m친nga fler. Valet av modell beror p친 vilka spr친k du anv칛nder, vilken typ av inneh친ll som kodas (text/bilder/ljud), storleken p친 input den kan koda och l칛ngden p친 embeddingoutputen.

Ett exempel p친 inb칛ddad text med OpenAI:s `text-embedding-ada-002`-modell 칛r:
![en embedding av ordet katt](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.sv.png)

## H칛mtning och vektors칬kning

N칛r en anv칛ndare st칛ller en fr친ga omvandlar h칛mtaren den till en vektor med hj칛lp av fr친gekodaren, och s칬ker sedan igenom v친rt dokuments칬kindex efter relevanta vektorer i dokumentet som 칛r relaterade till input. N칛r detta 칛r gjort konverterar den b친de inputvektorn och dokumentvektorerna till text och skickar dem genom LLM.

### H칛mtning

H칛mtning sker n칛r systemet f칬rs칬ker snabbt hitta dokument fr친n indexet som uppfyller s칬kkriterierna. M친let med h칛mtaren 칛r att f친 dokument som kommer att anv칛ndas f칬r att ge sammanhang och grunda LLM p친 din data.

Det finns flera s칛tt att utf칬ra s칬kningar inom v친r databas, s친som:

- **Nyckelordss칬kning** - anv칛nds f칬r texts칬kningar.

- **Semantisk s칬kning** - anv칛nder den semantiska betydelsen av ord.

- **Vektors칬kning** - konverterar dokument fr친n text till vektorrepresentationer med hj칛lp av embeddingmodeller. H칛mtning g칬rs genom att fr친ga dokument vars vektorrepresentationer 칛r n칛rmast anv칛ndarens fr친ga.

- **Hybrid** - en kombination av b친de nyckelordss칬kning och vektors칬kning.

En utmaning med h칛mtning uppst친r n칛r det inte finns n친got liknande svar p친 fr친gan i databasen, systemet kommer d친 att returnera den b칛sta informationen de kan hitta. Du kan dock anv칛nda taktiker som att st칛lla in det maximala avst친ndet f칬r relevans eller anv칛nda hybrids칬kning som kombinerar b친de nyckelord och vektors칬kning. I denna lektion kommer vi att anv칛nda hybrids칬kning, en kombination av b친de vektor- och nyckelordss칬kning. Vi kommer att lagra v친r data i en dataframe med kolumner som inneh친ller delarna samt embeddings.

### Vektorsimilaritet

H칛mtaren kommer att s칬ka igenom kunskapsdatabasen efter embeddings som ligger n칛ra varandra, den n칛rmaste grannen, eftersom de 칛r texter som 칛r liknande. I scenariot d칛r en anv칛ndare st칛ller en fr친ga, b칛ddas den f칬rst in och matchas sedan med liknande embeddings. Det vanliga m친ttet som anv칛nds f칬r att hitta hur lika olika vektorer 칛r, 칛r cosinuslikhet som baseras p친 vinkeln mellan tv친 vektorer.

Vi kan m칛ta likhet med andra alternativ som Euclidean-avst친nd, vilket 칛r den raka linjen mellan vektor칛ndpunkter, och skal칛rprodukt som m칛ter summan av produkterna av motsvarande element i tv친 vektorer.

### S칬kindex

N칛r vi g칬r h칛mtning m친ste vi bygga ett s칬kindex f칬r v친r kunskapsbas innan vi utf칬r s칬kningen. Ett index lagrar v친ra embeddings och kan snabbt h칛mta de mest liknande delarna 칛ven i en stor databas. Vi kan skapa v친rt index lokalt med:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Omrankning

N칛r du har fr친gat databasen kan du beh칬va sortera resultaten fr친n de mest relevanta. En omranknings-LLM anv칛nder maskininl칛rning f칬r att f칬rb칛ttra relevansen av s칬kresultaten genom att ordna dem fr친n de mest relevanta. Med Azure AI Search g칬rs omrankning automatiskt f칬r dig med en semantisk omrankare. Ett exempel p친 hur omrankning fungerar med n칛rmaste grannar:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## S칛tta ihop allt

Det sista steget 칛r att l칛gga till v친r LLM i mixen f칬r att kunna f친 svar som 칛r grundade p친 v친r data. Vi kan implementera det enligt f칬ljande:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Utv칛rdera v친r applikation

### Utv칛rderingsm친tt

- Kvaliteten p친 de svar som ges, s칛kerst칛lla att de l친ter naturliga, flytande och m칛nskliga.

- Grundning av data: utv칛rdera om svaret kom fr친n tillhandah친llna dokument.

- Relevans: utv칛rdera om svaret matchar och 칛r relaterat till den fr친ga som st칛lldes.

- Flyt: om svaret 칛r grammatiskt korrekt.

## Anv칛ndningsomr친den f칬r RAG (Retrieval Augmented Generation) och vektordatabaser

Det finns m친nga olika anv칛ndningsomr친den d칛r funktionsanrop kan f칬rb칛ttra din app, s친som:

- Fr친gor och svar: grunda din f칬retagsdata till en chatt som kan anv칛ndas av anst칛llda f칬r att st칛lla fr친gor.

- Rekommendationssystem: d칛r du kan skapa ett system som matchar de mest liknande v칛rdena, t.ex. filmer, restauranger och mycket mer.

- Chatbot-tj칛nster: du kan lagra chattloggar och anpassa konversationen baserat p친 anv칛ndardata.

- Bilds칬kning baserat p친 vektorembeddings, anv칛ndbart vid bildigenk칛nning och avvikelsedetektering.

## Sammanfattning

Vi har t칛ckt de grundl칛ggande omr친dena f칬r RAG fr친n att l칛gga till v친r data i applikationen, anv칛ndarfr친gan och output. F칬r att f칬renkla skapandet av RAG kan du anv칛nda ramverk som Semantic Kernel, Langchain eller Autogen.

## Uppgift

F칬r att forts칛tta din inl칛rning om Retrieval Augmented Generation (RAG) kan du bygga:

- Bygg en front-end f칬r applikationen med det ramverk du f칬redrar.

- Anv칛nd ett ramverk, antingen LangChain eller Semantic Kernel, och 친terskapa din applikation.

Grattis till att ha avslutat lektionen 游녪.

## L칛randet slutar inte h칛r, forts칛tt resan

Efter att ha avslutat denna lektion, kolla in v친r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) f칬r att forts칛tta utveckla din kunskap om generativ AI!

---

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av AI-칬vers칛ttningstj칛nsten [Co-op Translator](https://github.com/Azure/co-op-translator). 츿ven om vi str칛var efter noggrannhet, b칬r det noteras att automatiserade 칬vers칛ttningar kan inneh친lla fel eller felaktigheter. Det ursprungliga dokumentet p친 dess ursprungliga spr친k b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r eventuella missf칬rst친nd eller feltolkningar som uppst친r vid anv칛ndning av denna 칬vers칛ttning.