<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-05-20T01:20:17+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "sv"
}
-->
# Retrieval Augmented Generation (RAG) och Vektordatabaser

[![Retrieval Augmented Generation (RAG) och Vektordatabaser](../../../translated_images/15-lesson-banner.799d0cd2229970edb365f6667a4c7b3a0f526eb8698baa7d2e05c3bd49a5d83f.sv.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

I lektionen om s칬kapplikationer l칛rde vi oss kort hur man integrerar egen data i Stora Spr친kmodeller (LLMs). I denna lektion kommer vi att f칬rdjupa oss i koncepten kring att grunda din data i din LLM-applikation, processens mekanik och metoder f칬r att lagra data, inklusive b친de inb칛ddningar och text.

> **Video Kommer Snart**

## Introduktion

I denna lektion kommer vi att t칛cka f칬ljande:

- En introduktion till RAG, vad det 칛r och varf칬r det anv칛nds inom AI (artificiell intelligens).

- F칬rst친 vad vektordatabaser 칛r och skapa en f칬r v친r applikation.

- Ett praktiskt exempel p친 hur man integrerar RAG i en applikation.

## L칛randem친l

Efter att ha slutf칬rt denna lektion kommer du att kunna:

- F칬rklara betydelsen av RAG i data친tervinning och bearbetning.

- St칛lla in en RAG-applikation och grunda din data till en LLM

- Effektiv integration av RAG och Vektordatabaser i LLM-applikationer.

## V친rt Scenario: f칬rb칛ttra v친ra LLM:er med v친r egen data

F칬r denna lektion vill vi l칛gga till v친ra egna anteckningar i utbildningsstarten, vilket g칬r att chatboten kan f친 mer information om de olika 칛mnena. Med hj칛lp av de anteckningar vi har kommer eleverna kunna studera b칛ttre och f칬rst친 de olika 칛mnena, vilket g칬r det enklare att repetera inf칬r sina prov. F칬r att skapa v친rt scenario kommer vi att anv칛nda:

- `Azure OpenAI:` LLM som vi kommer att anv칛nda f칬r att skapa v친r chatbot

- `AI for beginners' lesson on Neural Networks`: detta kommer att vara den data vi grundar v친r LLM p친

- `Azure AI Search` och `Azure Cosmos DB:` vektordatabas f칬r att lagra v친r data och skapa ett s칬kindex

Anv칛ndare kommer att kunna skapa 칬vningsquiz fr친n sina anteckningar, repetitionskort och sammanfatta dem till koncisa 칬versikter. F칬r att komma ig친ng, l친t oss titta p친 vad RAG 칛r och hur det fungerar:

## Retrieval Augmented Generation (RAG)

En LLM-driven chatbot bearbetar anv칛ndarens fr친gor f칬r att generera svar. Den 칛r designad f칬r att vara interaktiv och engagerar sig med anv칛ndare inom en m칛ngd olika 칛mnen. Dock 칛r dess svar begr칛nsade till det sammanhang som ges och dess grundl칛ggande tr칛ningsdata. Till exempel 칛r GPT-4:s kunskapsgr칛ns september 2021, vilket inneb칛r att den saknar kunskap om h칛ndelser som intr칛ffat efter denna period. Dessutom utesluter datan som anv칛nds f칬r att tr칛na LLM:er konfidentiell information s친som personliga anteckningar eller en f칬retags produktmanual.

### Hur RAGs (Retrieval Augmented Generation) fungerar

![ritning som visar hur RAGs fungerar](../../../translated_images/how-rag-works.d87a7ed9c30f43126bb9e8e259be5d66e16cd1fef65374e6914746ba9bfb0b2f.sv.png)

Anta att du vill distribuera en chatbot som skapar quiz fr친n dina anteckningar, d친 kommer du beh칬va en koppling till kunskapsbasen. Det 칛r h칛r RAG kommer till unds칛ttning. RAGs fungerar enligt f칬ljande:

- **Kunskapsbas:** Innan 친tervinning beh칬ver dessa dokument intas och f칬rbehandlas, vanligtvis genom att bryta ner stora dokument i mindre delar, omvandla dem till textinb칛ddning och lagra dem i en databas.

- **Anv칛ndarfr친ga:** anv칛ndaren st칛ller en fr친ga

- **칀tervinning:** N칛r en anv칛ndare st칛ller en fr친ga h칛mtar inb칛ddningsmodellen relevant information fr친n v친r kunskapsbas f칬r att ge mer sammanhang som kommer att inf칬rlivas i fr친gan.

- **F칬rst칛rkt generering:** LLM f칬rb칛ttrar sitt svar baserat p친 den data som h칛mtats. Det g칬r att det genererade svaret inte bara baseras p친 f칬rtr칛nad data utan ocks친 relevant information fr친n det tillagda sammanhanget. Den h칛mtade datan anv칛nds f칬r att f칬rst칛rka LLM:s svar. LLM returnerar sedan ett svar p친 anv칛ndarens fr친ga.

![ritning som visar hur RAGs arkitektur](../../../translated_images/encoder-decode.75eebc7093ccefec17568eebc80d3d0b831ecf2ea204566377a04c77a5a57ebb.sv.png)

Arkitekturen f칬r RAGs implementeras med hj칛lp av transformatorer best친ende av tv친 delar: en encoder och en decoder. Till exempel, n칛r en anv칛ndare st칛ller en fr친ga, 'kodas' ing친ngstexten till vektorer som f친ngar ordens betydelse och vektorerna 'avkodas' till v친rt dokumentindex och genererar ny text baserat p친 anv칛ndarens fr친ga. LLM anv칛nder b친de en encoder-decoder-modell f칬r att generera utdata.

Tv친 tillv칛gag친ngss칛tt n칛r man implementerar RAG enligt det f칬reslagna dokumentet: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) 칛r:

- **_RAG-Sequence_** anv칛nder h칛mtade dokument f칬r att f칬ruts칛ga det b칛sta m칬jliga svaret p친 en anv칛ndarfr친ga

- **RAG-Token** anv칛nder dokument f칬r att generera n칛sta token, sedan h칛mta dem f칬r att svara p친 anv칛ndarens fr친ga

### Varf칬r skulle du anv칛nda RAGs?

- **Informationsrikedom:** s칛kerst칛ller att textrespons 칛r aktuella och uppdaterade. Det f칬rb칛ttrar d칛rf칬r prestandan p친 dom칛nspecifika uppgifter genom att f친 tillg친ng till den interna kunskapsbasen.

- Minskar fabricering genom att anv칛nda **verifierbar data** i kunskapsbasen f칬r att ge sammanhang till anv칛ndarfr친gor.

- Det 칛r **kostnadseffektivt** eftersom de 칛r mer ekonomiska j칛mf칬rt med att finjustera en LLM

## Skapa en kunskapsbas

V친r applikation 칛r baserad p친 v친r personliga data, dvs. lektionen om neurala n칛tverk i AI For Beginners-kursplanen.

### Vektordatabaser

En vektordatabas, till skillnad fr친n traditionella databaser, 칛r en specialiserad databas designad f칬r att lagra, hantera och s칬ka inb칛ddade vektorer. Den lagrar numeriska representationer av dokument. Att bryta ner data till numeriska inb칛ddningar g칬r det enklare f칬r v친rt AI-system att f칬rst친 och bearbeta datan.

Vi lagrar v친ra inb칛ddningar i vektordatabaser eftersom LLM:er har en gr칛ns f칬r antalet tokens de accepterar som indata. Eftersom du inte kan skicka hela inb칛ddningarna till en LLM, beh칬ver vi bryta ner dem i delar och n칛r en anv칛ndare st칛ller en fr친ga, kommer de inb칛ddningar som mest liknar fr친gan att returneras tillsammans med fr친gan. Att dela upp i delar minskar ocks친 kostnaderna f칬r antalet tokens som skickas genom en LLM.

N친gra popul칛ra vektordatabaser inkluderar Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant och DeepLake. Du kan skapa en Azure Cosmos DB-modell med hj칛lp av Azure CLI med f칬ljande kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fr친n text till inb칛ddningar

Innan vi lagrar v친r data beh칬ver vi omvandla den till vektor inb칛ddningar innan den lagras i databasen. Om du arbetar med stora dokument eller l친nga texter kan du dela upp dem baserat p친 f칬rv칛ntade fr친gor. Uppdelning kan g칬ras p친 meningsniv친, eller p친 styckeniv친. Eftersom uppdelning h칛rleder betydelser fr친n orden omkring dem kan du l칛gga till annat sammanhang till en del, till exempel genom att l칛gga till dokumentets titel eller inkludera lite text f칬re eller efter delen. Du kan dela upp datan enligt f칬ljande:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N칛r de v칛l 칛r uppdelade kan vi sedan inb칛dda v친r text med olika inb칛ddningsmodeller. N친gra modeller du kan anv칛nda inkluderar: word2vec, ada-002 av OpenAI, Azure Computer Vision och m친nga fler. Att v칛lja en modell att anv칛nda beror p친 de spr친k du anv칛nder, typen av inneh친ll som kodas (text/bilder/ljud), storleken p친 indata den kan koda och l칛ngden p친 inb칛ddningsutdata.

Ett exempel p친 inb칛ddad text med OpenAI:s `text-embedding-ada-002` modell 칛r:
![en inb칛ddning av ordet katt](../../../translated_images/cat.3db013cbca4fd5d90438ea7b312ad0364f7686cf79931ab15cd5922151aea53e.sv.png)

## 칀tervinning och Vektors칬kning

N칛r en anv칛ndare st칛ller en fr친ga omvandlar retrievern den till en vektor med hj칛lp av fr친geencodern, den s칬ker sedan genom v친rt dokuments칬kindex efter relevanta vektorer i dokumentet som 칛r relaterade till indata. N칛r det 칛r klart omvandlar den b친de indatavektorn och dokumentvektorerna till text och skickar det genom LLM.

### 칀tervinning

칀tervinning sker n칛r systemet f칬rs칬ker snabbt hitta dokumenten fr친n indexet som uppfyller s칬kkriterierna. M친let med retrievern 칛r att f친 dokument som kommer att anv칛ndas f칬r att ge sammanhang och grunda LLM p친 din data.

Det finns flera s칛tt att utf칬ra s칬kningar inom v친r databas, s친som:

- **Nyckelordss칬kning** - anv칛nds f칬r texts칬kningar

- **Semantisk s칬kning** - anv칛nder den semantiska betydelsen av ord

- **Vektors칬kning** - omvandlar dokument fr친n text till vektorrepresentationer med hj칛lp av inb칛ddningsmodeller. 칀tervinning kommer att ske genom att fr친ga dokumenten vars vektorrepresentationer 칛r n칛rmast anv칛ndarens fr친ga.

- **Hybrid** - en kombination av b친de nyckelord och vektors칬kning.

En utmaning med 친tervinning uppst친r n칛r det inte finns n친got liknande svar p친 fr친gan i databasen, systemet kommer d친 att returnera den b칛sta informationen de kan f친, dock kan du anv칛nda taktiker som att st칛lla in det maximala avst친ndet f칬r relevans eller anv칛nda hybrid s칬kning som kombinerar b친de nyckelord och vektors칬kning. I denna lektion kommer vi att anv칛nda hybrid s칬kning, en kombination av b친de vektor- och nyckelordss칬kning. Vi kommer att lagra v친r data i en dataframe med kolumner som inneh친ller delarna samt inb칛ddningar.

### Vektorsimilaritet

Retrievern kommer att s칬ka igenom kunskapsdatabasen efter inb칛ddningar som ligger n칛ra varandra, den n칛rmaste grannen, eftersom de 칛r texter som 칛r lika. I scenariot d칛r en anv칛ndare st칛ller en fr친ga, inb칛ddas den f칬rst och matchas sedan med liknande inb칛ddningar. Den vanliga m칛tningen som anv칛nds f칬r att hitta hur lika olika vektorer 칛r, 칛r cosinuslikhet som baseras p친 vinkeln mellan tv친 vektorer.

Vi kan m칛ta likhet med andra alternativ vi kan anv칛nda 칛r euklidiskt avst친nd som 칛r den raka linjen mellan vektor칛ndpunkter och skal칛rprodukt som m칛ter summan av produkterna av motsvarande element av tv친 vektorer.

### S칬kindex

N칛r vi g칬r 친tervinning beh칬ver vi bygga ett s칬kindex f칬r v친r kunskapsbas innan vi utf칬r s칬kning. Ett index kommer att lagra v친ra inb칛ddningar och kan snabbt h칛mta de mest liknande delarna 칛ven i en stor databas. Vi kan skapa v친rt index lokalt med hj칛lp av:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Omrankning

N칛r du har fr친gat databasen kan du beh칬va sortera resultaten fr친n de mest relevanta. En omranknings-LLM anv칛nder Maskininl칛rning f칬r att f칬rb칛ttra relevansen av s칬kresultaten genom att ordna dem fr친n de mest relevanta. Med Azure AI Search g칬rs omrankning automatiskt 친t dig med hj칛lp av en semantisk omrankare. Ett exempel p친 hur omrankning fungerar med n칛rmaste grannar:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Samla allt

Det sista steget 칛r att l칛gga till v친r LLM i mixen f칬r att kunna f친 svar som 칛r grundade p친 v친r data. Vi kan implementera det enligt f칬ljande:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Utv칛rdera v친r applikation

### Utv칛rderingsmetoder

- Kvalitet p친 svaren som tillhandah친lls, s칛kerst칛lla att det l친ter naturligt, flytande och m칛nskligt

- Grundadhet i datan: utv칛rdera om svaret kom fr친n de tillhandah친llna dokumenten

- Relevans: utv칛rdera om svaret matchar och 칛r relaterat till fr친gan som st칛lldes

- Flyt - om svaret 칛r grammatiskt meningsfullt

## Anv칛ndningsfall f칬r att anv칛nda RAG (Retrieval Augmented Generation) och vektordatabaser

Det finns m친nga olika anv칛ndningsfall d칛r funktionsanrop kan f칬rb칛ttra din app, s친som:

- Fr친gor och Svar: grunda din f칬retagsdata till en chatt som kan anv칛ndas av anst칛llda f칬r att st칛lla fr친gor.

- Rekommendationssystem: d칛r du kan skapa ett system som matchar de mest liknande v칛rdena, t.ex. filmer, restauranger och mycket mer.

- Chatbottj칛nster: du kan lagra chattens historia och anpassa konversationen baserat p친 anv칛ndardata.

- Bilds칬kning baserat p친 vektor inb칛ddningar, anv칛ndbart vid bildigenk칛nning och avvikelsedetektering.

## Sammanfattning

Vi har t칛ckt de grundl칛ggande omr친dena av RAG fr친n att l칛gga till v친r data till applikationen, anv칛ndarfr친gan och utdata. F칬r att f칬renkla skapandet av RAG kan du anv칛nda ramverk som Semanti Kernel, Langchain eller Autogen.

## Uppgift

F칬r att forts칛tta din inl칛rning av Retrieval Augmented Generation (RAG) kan du bygga:

- Bygg en front-end f칬r applikationen med hj칛lp av det ramverk du v칛ljer

- Anv칛nd ett ramverk, antingen LangChain eller Semantic Kernel, och 친terskapa din applikation.

Grattis till att ha slutf칬rt lektionen 游녪.

## L칛randet slutar inte h칛r, forts칛tt resan

Efter att ha slutf칬rt denna lektion, kolla in v친r [Generativ AI-l칛rande samling](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) f칬r att forts칛tta f칬rb칛ttra din kunskap om Generativ AI!

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av AI-칬vers칛ttningstj칛nsten [Co-op Translator](https://github.com/Azure/co-op-translator). 츿ven om vi str칛var efter noggrannhet, var medveten om att automatiska 칬vers칛ttningar kan inneh친lla fel eller felaktigheter. Det ursprungliga dokumentet p친 dess modersm친l b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi 칛r inte ansvariga f칬r eventuella missf칬rst친nd eller feltolkningar som uppst친r vid anv칛ndning av denna 칬vers칛ttning.