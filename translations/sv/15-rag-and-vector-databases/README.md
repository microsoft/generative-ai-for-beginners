<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-07-09T16:13:38+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "sv"
}
-->
# Retrieval Augmented Generation (RAG) och vektordatabaser

[![Retrieval Augmented Generation (RAG) och vektordatabaser](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.sv.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

I lektionen om s칬kapplikationer l칛rde vi oss kort hur man integrerar egna data i Large Language Models (LLMs). I den h칛r lektionen g친r vi djupare in p친 konceptet att f칬rankra dina data i din LLM-applikation, hur processen fungerar och metoder f칬r att lagra data, inklusive b친de embeddings och text.

> **Video kommer snart**

## Introduktion

I den h칛r lektionen kommer vi att g친 igenom f칬ljande:

- En introduktion till RAG, vad det 칛r och varf칬r det anv칛nds inom AI (artificiell intelligens).

- F칬rst친 vad vektordatabaser 칛r och hur man skapar en f칬r v친r applikation.

- Ett praktiskt exempel p친 hur man integrerar RAG i en applikation.

## L칛randem친l

Efter att ha genomf칬rt denna lektion kommer du att kunna:

- F칬rklara betydelsen av RAG vid datah칛mtning och bearbetning.

- S칛tta upp en RAG-applikation och f칬rankra dina data i en LLM.

- Effektiv integration av RAG och vektordatabaser i LLM-applikationer.

## V친rt scenario: f칬rb칛ttra v친ra LLMs med v친ra egna data

I denna lektion vill vi l칛gga till v친ra egna anteckningar i utbildningsstartuppen, vilket g칬r att chatboten kan f친 mer information om olika 칛mnen. Med hj칛lp av de anteckningar vi har kan eleverna studera b칛ttre och f칬rst친 olika 칛mnen, vilket g칬r det l칛ttare att repetera inf칬r sina prov. F칬r att skapa v친rt scenario kommer vi att anv칛nda:

- `Azure OpenAI:` den LLM vi anv칛nder f칬r att skapa v친r chatbot

- `AI for beginners' lesson on Neural Networks:` detta blir datan vi f칬rankrar v친r LLM p친

- `Azure AI Search` och `Azure Cosmos DB:` vektordatabas f칬r att lagra v친ra data och skapa ett s칬kindex

Anv칛ndare kommer att kunna skapa 칬vningsquiz fr친n sina anteckningar, repetitionskort och sammanfatta dem till kortfattade 칬versikter. F칬r att komma ig친ng, l친t oss titta p친 vad RAG 칛r och hur det fungerar:

## Retrieval Augmented Generation (RAG)

En LLM-driven chatbot bearbetar anv칛ndarfr친gor f칬r att generera svar. Den 칛r designad f칬r att vara interaktiv och engagerar sig med anv칛ndare i en m칛ngd olika 칛mnen. Dock 칛r dess svar begr칛nsade till den kontext som ges och dess grundl칛ggande tr칛ningsdata. Till exempel har GPT-4 kunskapsavgr칛nsning i september 2021, vilket inneb칛r att den saknar kunskap om h칛ndelser efter detta datum. Dessutom exkluderar data som anv칛nds f칬r att tr칛na LLMs konfidentiell information som personliga anteckningar eller ett f칬retags produktmanual.

### Hur RAGs (Retrieval Augmented Generation) fungerar

![ritning som visar hur RAGs fungerar](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.sv.png)

Anta att du vill lansera en chatbot som skapar quiz fr친n dina anteckningar, d친 beh칬ver du en koppling till kunskapsbasen. Det 칛r h칛r RAG kommer in i bilden. RAGs fungerar p친 f칬ljande s칛tt:

- **Kunskapsbas:** Innan h칛mtning m친ste dessa dokument tas in och f칬rbehandlas, vanligtvis genom att dela upp stora dokument i mindre delar, omvandla dem till text-embedding och lagra dem i en databas.

- **Anv칛ndarfr친ga:** anv칛ndaren st칛ller en fr친ga

- **H칛mtning:** N칛r en anv칛ndare st칛ller en fr친ga h칛mtar embedding-modellen relevant information fr친n v친r kunskapsbas f칬r att ge mer kontext som inkluderas i prompten.

- **F칬rst칛rkt generering:** LLM f칬rb칛ttrar sitt svar baserat p친 den h칛mtade datan. Det g칬r att svaret inte bara baseras p친 f칬rtr칛nad data utan ocks친 p친 relevant information fr친n den tillagda kontexten. Den h칛mtade datan anv칛nds f칬r att f칬rst칛rka LLM:s svar. LLM returnerar sedan ett svar p친 anv칛ndarens fr친ga.

![ritning som visar RAGs arkitektur](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.sv.png)

Arkitekturen f칬r RAGs implementeras med transformers som best친r av tv친 delar: en encoder och en decoder. Till exempel, n칛r en anv칛ndare st칛ller en fr친ga, "kodas" inmatningstexten till vektorer som f친ngar ordens betydelse och vektorerna "avkodas" mot v친rt dokumentindex och genererar ny text baserat p친 anv칛ndarfr친gan. LLM anv칛nder b친de en encoder-decoder-modell f칬r att generera output.

Tv친 tillv칛gag친ngss칛tt vid implementering av RAG enligt den f칬reslagna artikeln: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) 칛r:

- **_RAG-Sequence_** som anv칛nder h칛mtade dokument f칬r att f칬ruts칛ga det b칛sta m칬jliga svaret p친 en anv칛ndarfr친ga

- **RAG-Token** som anv칛nder dokument f칬r att generera n칛sta token, och sedan h칛mtar dem f칬r att svara p친 anv칛ndarens fr친ga

### Varf칬r skulle du anv칛nda RAGs?

- **Informationsrikedom:** s칛kerst칛ller att textsvar 칛r uppdaterade och aktuella. Det f칬rb칛ttrar d칛rmed prestanda p친 dom칛nspecifika uppgifter genom att f친 tillg친ng till den interna kunskapsbasen.

- Minskar fabricering genom att anv칛nda **verifierbar data** i kunskapsbasen f칬r att ge kontext till anv칛ndarfr친gor.

- Det 칛r **kostnadseffektivt** eftersom det 칛r mer ekonomiskt j칛mf칬rt med att finjustera en LLM.

## Skapa en kunskapsbas

V친r applikation baseras p친 v친ra personliga data, dvs. lektionen om Neural Networks i AI For Beginners-kursen.

### Vektordatabaser

En vektordatabas, till skillnad fr친n traditionella databaser, 칛r en specialiserad databas designad f칬r att lagra, hantera och s칬ka inb칛ddade vektorer. Den lagrar numeriska representationer av dokument. Att bryta ner data till numeriska embeddings g칬r det enklare f칬r v친rt AI-system att f칬rst친 och bearbeta datan.

Vi lagrar v친ra embeddings i vektordatabaser eftersom LLMs har en gr칛ns f칬r hur m친nga tokens de kan ta emot som input. Eftersom du inte kan skicka hela embeddingarna till en LLM m친ste vi dela upp dem i delar, och n칛r en anv칛ndare st칛ller en fr친ga returneras de embeddings som mest liknar fr친gan tillsammans med prompten. Uppdelning minskar ocks친 kostnaderna f칬r antalet tokens som skickas genom en LLM.

N친gra popul칛ra vektordatabaser inkluderar Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant och DeepLake. Du kan skapa en Azure Cosmos DB-modell med Azure CLI med f칬ljande kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fr친n text till embeddings

Innan vi lagrar v친r data m친ste vi konvertera den till vektor-embeddings innan den sparas i databasen. Om du arbetar med stora dokument eller l친nga texter kan du dela upp dem baserat p친 f칬rv칛ntade fr친gor. Uppdelning kan g칬ras p친 meningsniv친 eller p친 styckesniv친. Eftersom uppdelning h칛mtar mening fr친n orden runt omkring kan du l칛gga till ytterligare kontext till en del, till exempel genom att l칛gga till dokumentets titel eller inkludera text f칬re eller efter delen. Du kan dela upp datan enligt f칬ljande:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N칛r den 칛r uppdelad kan vi sedan b칛dda in v친r text med olika embedding-modeller. N친gra modeller du kan anv칛nda inkluderar: word2vec, ada-002 fr친n OpenAI, Azure Computer Vision och m친nga fler. Valet av modell beror p친 vilka spr친k du anv칛nder, vilken typ av inneh친ll som kodas (text/bilder/ljud), storleken p친 input den kan koda och l칛ngden p친 embedding-outputen.

Ett exempel p친 inb칛ddad text med OpenAI:s `text-embedding-ada-002`-modell 칛r:
![en embedding av ordet katt](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.sv.png)

## H칛mtning och vektors칬kning

N칛r en anv칛ndare st칛ller en fr친ga omvandlar retrievern den till en vektor med hj칛lp av query-encodern, den s칬ker sedan igenom v친rt dokument-s칬kindex efter relevanta vektorer i dokumentet som 칛r relaterade till inputen. N칛r det 칛r klart konverterar den b친de input-vektorn och dokumentvektorerna till text och skickar det genom LLM.

### H칛mtning

H칛mtning sker n칛r systemet f칬rs칬ker snabbt hitta dokument fr친n indexet som uppfyller s칬kkriterierna. M친let med retrievern 칛r att f친 dokument som kan anv칛ndas f칬r att ge kontext och f칬rankra LLM p친 dina data.

Det finns flera s칛tt att utf칬ra s칬kningar i v친r databas, s친som:

- **Nyckelordss칬kning** - anv칛nds f칬r texts칬kningar

- **Semantisk s칬kning** - anv칛nder ordens semantiska betydelse

- **Vektors칬kning** - konverterar dokument fr친n text till vektorrepresentationer med hj칛lp av embedding-modeller. H칛mtning g칬rs genom att fr친ga efter dokument vars vektorrepresentationer 칛r n칛rmast anv칛ndarens fr친ga.

- **Hybrid** - en kombination av b친de nyckelords- och vektors칬kning.

En utmaning med h칛mtning uppst친r n칛r det inte finns n친got liknande svar p친 fr친gan i databasen, systemet returnerar d친 den b칛sta informationen det kan hitta, men du kan anv칛nda taktiker som att s칛tta en maximal distans f칬r relevans eller anv칛nda hybrid-s칬kning som kombinerar b친de nyckelord och vektors칬kning. I denna lektion anv칛nder vi hybrid-s칬kning, en kombination av b친de vektor- och nyckelordss칬kning. Vi lagrar v친r data i en dataframe med kolumner som inneh친ller b친de delarna och embeddings.

### Vektorsimilaritet

Retrievern s칬ker igenom kunskapsdatabasen efter embeddings som ligger n칛ra varandra, den n칛rmaste grannen, eftersom de 칛r texter som 칛r lika. I scenariot d칛r en anv칛ndare st칛ller en fr친ga b칛ddas den f칬rst in och matchas sedan med liknande embeddings. Det vanliga m친ttet som anv칛nds f칬r att avg칬ra hur lika olika vektorer 칛r 칛r cosinuslikhet, som baseras p친 vinkeln mellan tv친 vektorer.

Vi kan m칛ta likhet med andra alternativ som Euclidean distance, vilket 칛r den raka linjen mellan vektor칛ndpunkter, och dot product som m칛ter summan av produkterna av motsvarande element i tv친 vektorer.

### S칬kindex

N칛r vi g칬r h칛mtning beh칬ver vi bygga ett s칬kindex f칬r v친r kunskapsbas innan vi utf칬r s칬kningen. Ett index lagrar v친ra embeddings och kan snabbt h칛mta de mest liknande delarna 칛ven i en stor databas. Vi kan skapa v친rt index lokalt med:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Omrankning

N칛r du har fr친gat databasen kan du beh칬va sortera resultaten fr친n mest relevanta. En omrankande LLM anv칛nder maskininl칛rning f칬r att f칬rb칛ttra relevansen i s칬kresultaten genom att ordna dem fr친n mest relevanta. Med Azure AI Search g칬rs omrankning automatiskt med en semantisk omrankare. Ett exempel p친 hur omrankning fungerar med n칛rmaste grannar:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Att s칛tta ihop allt

Det sista steget 칛r att l칛gga till v친r LLM i mixen f칬r att kunna f친 svar som 칛r f칬rankrade i v친ra data. Vi kan implementera det s친 h칛r:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Utv칛rdera v친r applikation

### Utv칛rderingsm친tt

- Kvalitet p친 svaren, s칛kerst칛lla att de l친ter naturliga, flytande och m칛nskliga

- F칬rankring av data: utv칛rdera om svaret kommer fr친n de tillhandah친llna dokumenten

- Relevans: utv칛rdera om svaret matchar och 칛r relaterat till den st칛llda fr친gan

- Flyt - om svaret 칛r grammatiskt korrekt och begripligt

## Anv칛ndningsomr친den f칬r RAG (Retrieval Augmented Generation) och vektordatabaser

Det finns m친nga olika anv칛ndningsomr친den d칛r funktionsanrop kan f칬rb칛ttra din app, till exempel:

- Fr친gor och svar: f칬rankra ditt f칬retags data till en chatt som kan anv칛ndas av anst칛llda f칬r att st칛lla fr친gor.

- Rekommendationssystem: d칛r du kan skapa ett system som matchar de mest liknande v칛rdena, t.ex. filmer, restauranger och mycket mer.

- Chattbottj칛nster: du kan lagra chattloggar och anpassa konversationen baserat p친 anv칛ndardata.

- Bilds칬kning baserat p친 vektor-embeddings, anv칛ndbart vid bildigenk칛nning och avvikelsedetektering.

## Sammanfattning

Vi har g친tt igenom grundl칛ggande omr친den f칬r RAG fr친n att l칛gga till v친ra data i applikationen, anv칛ndarfr친gan och output. F칬r att f칬renkla skapandet av RAG kan du anv칛nda ramverk som Semantic Kernel, Langchain eller Autogen.

## Uppgift

F칬r att forts칛tta din inl칛rning av Retrieval Augmented Generation (RAG) kan du bygga:

- Skapa ett frontend f칬r applikationen med det ramverk du f칬redrar

- Anv칛nd ett ramverk, antingen LangChain eller Semantic Kernel, och 친terskapa din applikation.

Grattis till att du har slutf칬rt lektionen 游녪.

## L칛randet slutar inte h칛r, forts칛tt resan

Efter att ha genomf칬rt denna lektion, kolla in v친r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) f칬r att forts칛tta utveckla dina kunskaper inom Generativ AI!

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av AI-칬vers칛ttningstj칛nsten [Co-op Translator](https://github.com/Azure/co-op-translator). 츿ven om vi str칛var efter noggrannhet, v칛nligen observera att automatiska 칬vers칛ttningar kan inneh친lla fel eller brister. Det ursprungliga dokumentet p친 dess modersm친l b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r n친gra missf칬rst친nd eller feltolkningar som uppst친r vid anv칛ndning av denna 칬vers칛ttning.