<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T18:59:44+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "sv"
}
-->
# S칛kerhet f칬r dina generativa AI-applikationer

[![S칛kerhet f칬r dina generativa AI-applikationer](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.sv.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Introduktion

Den h칛r lektionen kommer att behandla:

- S칛kerhet inom AI-system.
- Vanliga risker och hot mot AI-system.
- Metoder och 칬verv칛ganden f칬r att s칛kra AI-system.

## L칛randem친l

Efter att ha genomf칬rt denna lektion kommer du att ha en f칬rst친else f칬r:

- Hot och risker mot AI-system.
- Vanliga metoder och praxis f칬r att s칛kra AI-system.
- Hur implementering av s칛kerhetstester kan f칬rhindra ov칛ntade resultat och f칬rlust av anv칛ndarnas f칬rtroende.

## Vad inneb칛r s칛kerhet inom generativ AI?

N칛r artificiell intelligens (AI) och maskininl칛rning (ML) i allt h칬gre grad p친verkar v친ra liv 칛r det avg칬rande att skydda inte bara kunddata utan 칛ven sj칛lva AI-systemen. AI/ML anv칛nds alltmer f칬r att st칬dja beslutsprocesser med h칬gt v칛rde inom industrier d칛r felaktiga beslut kan f친 allvarliga konsekvenser.

H칛r 칛r viktiga punkter att t칛nka p친:

- **P친verkan av AI/ML**: AI/ML har en betydande inverkan p친 vardagen och d칛rf칬r har det blivit avg칬rande att skydda dem.
- **S칛kerhetsutmaningar**: Denna p친verkan fr친n AI/ML kr칛ver noggrann uppm칛rksamhet f칬r att hantera behovet av att skydda AI-baserade produkter fr친n sofistikerade attacker, vare sig det 칛r fr친n troll eller organiserade grupper.
- **Strategiska problem**: Teknikindustrin m친ste proaktivt hantera strategiska utmaningar f칬r att s칛kerst칛lla l친ngsiktig kunds칛kerhet och dataskydd.

Dessutom har maskininl칛rningsmodeller i stort sett sv친rt att skilja mellan skadlig input och ofarliga avvikande data. En betydande del av tr칛ningsdata h칛mtas fr친n okurerade, omodererade, offentliga dataset som 칛r 칬ppna f칬r bidrag fr친n tredje part. Angripare beh칬ver inte kompromettera dataset n칛r de fritt kan bidra till dem. Med tiden blir l친gkonfident skadlig data h칬gkonfident betrodd data, om datastrukturen/formatet f칬rblir korrekt.

Det 칛r d칛rf칬r avg칬rande att s칛kerst칛lla integriteten och skyddet av de datalager som dina modeller anv칛nder f칬r att fatta beslut.

## F칬rst친 hot och risker f칬r AI

N칛r det g칛ller AI och relaterade system 칛r datagiftning idag det mest betydande s칛kerhetshotet. Datagiftning inneb칛r att n친gon medvetet 칛ndrar informationen som anv칛nds f칬r att tr칛na en AI, vilket leder till att den g칬r misstag. Detta beror p친 avsaknaden av standardiserade metoder f칬r att uppt칛cka och motverka detta, i kombination med v친rt beroende av op친litliga eller okurerade offentliga dataset f칬r tr칛ning. F칬r att uppr칛tth친lla dataintegritet och f칬rhindra en felaktig tr칛ningsprocess 칛r det avg칬rande att sp친ra ursprunget och h칛rkomsten av din data. Annars g칛ller det gamla tales칛ttet "skr칛p in, skr칛p ut", vilket leder till komprometterad modellprestanda.

H칛r 칛r exempel p친 hur datagiftning kan p친verka dina modeller:

1. **Etikettf칬r칛ndring**: Vid en bin칛r klassificeringsuppgift 칛ndrar en angripare medvetet etiketterna p친 en liten del av tr칛ningsdata. Till exempel m칛rks ofarliga prover som skadliga, vilket leder till att modellen l칛r sig felaktiga samband.\
   **Exempel**: Ett spamfilter som felklassificerar legitima e-postmeddelanden som skr칛ppost p친 grund av manipulerade etiketter.
2. **Egenskapsf칬rgiftning**: En angripare 칛ndrar subtilt egenskaper i tr칛ningsdata f칬r att inf칬ra bias eller vilseleda modellen.\
   **Exempel**: L칛gga till irrelevanta nyckelord i produktbeskrivningar f칬r att manipulera rekommendationssystem.
3. **Data-injektion**: Inf칬rande av skadlig data i tr칛ningsupps칛ttningen f칬r att p친verka modellens beteende.\
   **Exempel**: Introducera falska anv칛ndarrecensioner f칬r att snedvrida sentimentanalysresultat.
4. **Bakd칬rrsattacker**: En angripare inf칬r ett dolt m칬nster (bakd칬rr) i tr칛ningsdata. Modellen l칛r sig att k칛nna igen detta m칬nster och beter sig skadligt n칛r det aktiveras.\
   **Exempel**: Ett ansiktsigenk칛nningssystem som tr칛nats med bakd칬rrsbilder som felidentifierar en specifik person.

MITRE Corporation har skapat [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en kunskapsbas med taktiker och tekniker som anv칛nds av angripare i verkliga attacker mot AI-system.

> Det finns ett v칛xande antal s친rbarheter i AI-aktiverade system, eftersom inf칬randet av AI 칬kar attackytan f칬r befintliga system ut칬ver traditionella cyberattacker. Vi utvecklade ATLAS f칬r att 칬ka medvetenheten om dessa unika och utvecklande s친rbarheter, eftersom det globala samh칛llet i allt h칬gre grad inf칬rlivar AI i olika system. ATLAS 칛r modellerat efter MITRE ATT&CK춽-ramverket och dess taktiker, tekniker och procedurer (TTPs) 칛r komplement칛ra till de i ATT&CK.

Precis som MITRE ATT&CK춽-ramverket, som anv칛nds i stor utstr칛ckning inom traditionell cybers칛kerhet f칬r att planera avancerade hotemuleringsscenarier, erbjuder ATLAS en l칛tts칬kt upps칛ttning TTPs som kan hj칛lpa till att b칛ttre f칬rst친 och f칬rbereda sig f칬r att f칬rsvara sig mot framv칛xande attacker.

Dessutom har Open Web Application Security Project (OWASP) skapat en "[Top 10-lista](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" 칬ver de mest kritiska s친rbarheterna som finns i applikationer som anv칛nder LLMs. Listan belyser riskerna med hot som den n칛mnda datagiftningen samt andra som:

- **Prompt Injection**: en teknik d칛r angripare manipulerar en Large Language Model (LLM) genom noggrant utformade inputs, vilket f친r den att bete sig utanf칬r sitt avsedda beteende.
- **S친rbarheter i leveranskedjan**: Komponenterna och mjukvaran som utg칬r applikationerna som anv칛nds av en LLM, s친som Python-moduler eller externa dataset, kan sj칛lva komprometteras vilket leder till ov칛ntade resultat, inf칬rda bias och till och med s친rbarheter i den underliggande infrastrukturen.
- **칐verberoende**: LLMs 칛r felbara och har varit ben칛gna att hallucinera, vilket ger felaktiga eller os칛kra resultat. I flera dokumenterade fall har m칛nniskor tagit resultaten f칬r givna, vilket lett till oavsiktliga negativa konsekvenser i verkligheten.

Microsoft Cloud Advocate Rod Trent har skrivit en gratis e-bok, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), som g친r djupt in p친 dessa och andra framv칛xande AI-hot och ger omfattande v칛gledning om hur man b칛st hanterar dessa scenarier.

## S칛kerhetstestning f칬r AI-system och LLMs

Artificiell intelligens (AI) omvandlar olika omr친den och industrier, och erbjuder nya m칬jligheter och f칬rdelar f칬r samh칛llet. Men AI medf칬r ocks친 betydande utmaningar och risker, s친som dataintegritet, bias, brist p친 f칬rklarbarhet och potentiellt missbruk. D칛rf칬r 칛r det avg칬rande att s칛kerst칛lla att AI-system 칛r s칛kra och ansvarsfulla, vilket inneb칛r att de f칬ljer etiska och juridiska standarder och kan lita p친 av anv칛ndare och intressenter.

S칛kerhetstestning 칛r processen att utv칛rdera s칛kerheten hos ett AI-system eller LLM genom att identifiera och utnyttja dess s친rbarheter. Detta kan utf칬ras av utvecklare, anv칛ndare eller tredje parts granskare, beroende p친 syftet och omfattningen av testningen. N친gra av de vanligaste metoderna f칬r s칛kerhetstestning f칬r AI-system och LLMs 칛r:

- **Datasanering**: Detta 칛r processen att ta bort eller anonymisera k칛nslig eller privat information fr친n tr칛ningsdata eller input till ett AI-system eller LLM. Datasanering kan hj칛lpa till att f칬rhindra datal칛ckage och skadlig manipulation genom att minska exponeringen av konfidentiell eller personlig data.
- **Adversarial testing**: Detta 칛r processen att generera och till칛mpa adversarial exempel p친 input eller output fr친n ett AI-system eller LLM f칬r att utv칛rdera dess robusthet och motst친ndskraft mot adversarial attacker. Adversarial testing kan hj칛lpa till att identifiera och mildra s친rbarheter och svagheter hos ett AI-system eller LLM som kan utnyttjas av angripare.
- **Modellverifiering**: Detta 칛r processen att verifiera korrektheten och fullst칛ndigheten av modellparametrar eller arkitektur hos ett AI-system eller LLM. Modellverifiering kan hj칛lpa till att uppt칛cka och f칬rhindra modellst칬ld genom att s칛kerst칛lla att modellen 칛r skyddad och autentiserad.
- **Outputvalidering**: Detta 칛r processen att validera kvaliteten och tillf칬rlitligheten hos output fr친n ett AI-system eller LLM. Outputvalidering kan hj칛lpa till att uppt칛cka och korrigera skadlig manipulation genom att s칛kerst칛lla att output 칛r konsekvent och korrekt.

OpenAI, en ledare inom AI-system, har uppr칛ttat en serie _s칛kerhetsutv칛rderingar_ som en del av deras red teaming-n칛tverksinitiativ, med syfte att testa output fr친n AI-system i hopp om att bidra till AI-s칛kerhet.

> Utv칛rderingar kan str칛cka sig fr친n enkla Q&A-tester till mer komplexa simuleringar. Som konkreta exempel, h칛r 칛r provutv칛rderingar utvecklade av OpenAI f칬r att utv칛rdera AI-beteenden fr친n olika vinklar:

#### 칐vertalning

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system lura ett annat AI-system att s칛ga ett hemligt ord?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system 칬vertyga ett annat AI-system att donera pengar?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system p친verka ett annat AI-systems st칬d f칬r ett politiskt f칬rslag?

#### Steganografi (dolda meddelanden)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system skicka hemliga meddelanden utan att bli uppt칛ckt av ett annat AI-system?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system komprimera och dekomprimera meddelanden f칬r att m칬jligg칬ra att d칬lja hemliga meddelanden?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system samordna med ett annat AI-system utan direkt kommunikation?

### AI-s칛kerhet

Det 칛r avg칬rande att vi str칛var efter att skydda AI-system fr친n skadliga attacker, missbruk eller oavsiktliga konsekvenser. Detta inkluderar att vidta 친tg칛rder f칬r att s칛kerst칛lla s칛kerheten, tillf칬rlitligheten och f칬rtroendet f칬r AI-system, s친som:

- Skydda data och algoritmer som anv칛nds f칬r att tr칛na och k칬ra AI-modeller
- F칬rhindra obeh칬rig 친tkomst, manipulation eller sabotage av AI-system
- Uppt칛cka och mildra bias, diskriminering eller etiska problem i AI-system
- S칛kerst칛lla ansvar, transparens och f칬rklarbarhet i AI-beslut och handlingar
- Anpassa m친len och v칛rderingarna f칬r AI-system till m칛nniskors och samh칛llets v칛rderingar

AI-s칛kerhet 칛r viktigt f칬r att s칛kerst칛lla integritet, tillg칛nglighet och konfidentialitet f칬r AI-system och data. N친gra av utmaningarna och m칬jligheterna med AI-s칛kerhet 칛r:

- M칬jlighet: Att integrera AI i cybers칛kerhetsstrategier eftersom det kan spela en avg칬rande roll i att identifiera hot och f칬rb칛ttra svarstider. AI kan hj칛lpa till att automatisera och f칬rst칛rka uppt칛ckten och hanteringen av cyberattacker, s친som n칛tfiske, skadlig kod eller ransomware.
- Utmaning: AI kan ocks친 anv칛ndas av angripare f칬r att lansera sofistikerade attacker, s친som att generera falskt eller vilseledande inneh친ll, imitera anv칛ndare eller utnyttja s친rbarheter i AI-system. D칛rf칬r har AI-utvecklare ett unikt ansvar att designa system som 칛r robusta och motst친ndskraftiga mot missbruk.

### Dataskydd

LLMs kan utg칬ra risker f칬r integriteten och s칛kerheten f칬r den data de anv칛nder. Till exempel kan LLMs potentiellt memorera och l칛cka k칛nslig information fr친n sin tr칛ningsdata, s친som personnamn, adresser, l칬senord eller kreditkortsnummer. De kan ocks친 manipuleras eller attackeras av skadliga akt칬rer som vill utnyttja deras s친rbarheter eller bias. D칛rf칬r 칛r det viktigt att vara medveten om dessa risker och vidta l칛mpliga 친tg칛rder f칬r att skydda den data som anv칛nds med LLMs. Det finns flera steg du kan ta f칬r att skydda den data som anv칛nds med LLMs. Dessa steg inkluderar:

- **Begr칛nsa m칛ngden och typen av data som delas med LLMs**: Dela endast den data som 칛r n칬dv칛ndig och relevant f칬r de avsedda 칛ndam친len, och undvik att dela data som 칛r k칛nslig, konfidentiell eller personlig. Anv칛ndare b칬r ocks친 anonymisera eller kryptera den data de delar med LLMs, s친som genom att ta bort eller maskera identifierande information eller anv칛nda s칛kra kommunikationskanaler.
- **Verifiera den data som LLMs genererar**: Kontrollera alltid noggrant noggrannheten och kvaliteten p친 den output som genereras av LLMs f칬r att s칛kerst칛lla att den inte inneh친ller o칬nskad eller ol칛mplig information.
- **Rapportera och varna vid dataintr친ng eller incidenter**: Var vaksam p친 misst칛nkta eller onormala aktiviteter eller beteenden fr친n LLMs, s친som att generera texter som 칛r irrelevanta, felaktiga, st칬tande eller skadliga. Detta kan vara en indikation p친 ett dataintr친ng eller en s칛kerhetsincident.

Datas칛kerhet, styrning och efterlevnad 칛r avg칬rande f칬r alla organisationer som vill utnyttja kraften i data och AI i en multi-cloud-milj칬. Att s칛kra och styra all din data 칛r en komplex och m친ngfacetterad uppgift. Du beh칬ver s칛kra och styra olika typer av data (strukturerad, ostrukturerad och data genererad av AI) p친 olika platser 칬ver flera moln, och du beh칬ver ta h칛nsyn till befintliga och framtida regler f칬r datas칛kerhet, styrning och AI. F칬r att skydda din data beh칬ver du anta n친gra b칛sta praxis och f칬rsiktighets친tg칛rder, s친som:

- Anv칛nd molntj칛nster eller plattformar som erbjuder dataskydd och integritetsfunktioner.
- Anv칛nd verktyg f칬r datakvalitet och validering f칬r att kontrollera din data f칬r fel, inkonsekvenser eller avvikelser.
- Anv칛nd ramverk f칬r datastyrning och etik f칬r att s칛kerst칛lla att din data anv칛nds p친 ett ansvarsfullt och transparent s칛tt.

### Emulera verkliga hot - AI red teaming
Att simulera verkliga hot anses nu vara en standardpraxis f칬r att bygga motst친ndskraftiga AI-system genom att anv칛nda liknande verktyg, taktiker och procedurer f칬r att identifiera risker f칬r systemen och testa f칬rsvararnas respons.

> Praktiken med AI-red teaming har utvecklats till att f친 en mer omfattande betydelse: den t칛cker inte bara att identifiera s칛kerhetsbrister, utan inkluderar 칛ven att unders칬ka andra systemfel, s친som generering av potentiellt skadligt inneh친ll. AI-system medf칬r nya risker, och red teaming 칛r centralt f칬r att f칬rst친 dessa nya risker, s친som promptinjektion och produktion av ogrundat inneh친ll. - [Microsoft AI Red Team bygger framtidens s칛krare AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![V칛gledning och resurser f칬r red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.sv.png)]()

Nedan f칬ljer viktiga insikter som har format Microsofts AI Red Team-program.

1. **Omfattande r칛ckvidd f칬r AI-red teaming:**
   AI-red teaming omfattar nu b친de s칛kerhet och ansvarsfull AI (RAI). Traditionellt har red teaming fokuserat p친 s칛kerhetsaspekter och behandlat modellen som en vektor (t.ex. att stj칛la den underliggande modellen). Men AI-system introducerar nya s칛kerhetsbrister (t.ex. promptinjektion, f칬rgiftning), vilket kr칛ver s칛rskild uppm칛rksamhet. Ut칬ver s칛kerhet unders칬ker AI-red teaming ocks친 r칛ttvisefr친gor (t.ex. stereotyper) och skadligt inneh친ll (t.ex. glorifiering av v친ld). Tidig identifiering av dessa problem m칬jligg칬r prioritering av f칬rsvarsinvesteringar.
2. **Skadliga och ofarliga fel:**
   AI-red teaming tar h칛nsyn till fel fr친n b친de skadliga och ofarliga perspektiv. Till exempel, n칛r vi red teamar nya Bing, unders칬ker vi inte bara hur skadliga akt칬rer kan manipulera systemet, utan ocks친 hur vanliga anv칛ndare kan st칬ta p친 problematiskt eller skadligt inneh친ll. Till skillnad fr친n traditionell s칛kerhetsred teaming, som fr칛mst fokuserar p친 skadliga akt칬rer, tar AI-red teaming h칛nsyn till ett bredare spektrum av personas och potentiella fel.
3. **Dynamisk natur hos AI-system:**
   AI-applikationer utvecklas st칛ndigt. I applikationer med stora spr친kmodeller anpassar utvecklare sig till f칬r칛ndrade krav. Kontinuerlig red teaming s칛kerst칛ller st칛ndig vaksamhet och anpassning till f칬r칛ndrade risker.

AI-red teaming 칛r inte helt칛ckande och b칬r betraktas som ett komplement till ytterligare kontroller s친som [rollbaserad 친tkomstkontroll (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) och omfattande datahanteringsl칬sningar. Det 칛r avsett att komplettera en s칛kerhetsstrategi som fokuserar p친 att anv칛nda s칛kra och ansvarsfulla AI-l칬sningar som tar h칛nsyn till integritet och s칛kerhet samtidigt som man str칛var efter att minimera f칬rdomar, skadligt inneh친ll och desinformation som kan minska anv칛ndarnas f칬rtroende.

H칛r 칛r en lista med ytterligare l칛sning som kan hj칛lpa dig att b칛ttre f칬rst친 hur red teaming kan hj칛lpa till att identifiera och mildra risker i dina AI-system:

- [Planera red teaming f칬r stora spr친kmodeller (LLMs) och deras applikationer](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Vad 칛r OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - En nyckelpraxis f칬r att bygga s칛krare och mer ansvarsfulla AI-l칬sningar](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en kunskapsbas med taktiker och tekniker som anv칛nds av angripare i verkliga attacker p친 AI-system.

## Kunskapskontroll

Vad kan vara en bra metod f칬r att uppr칛tth친lla dataintegritet och f칬rhindra missbruk?

1. Ha starka rollbaserade kontroller f칬r data친tkomst och datahantering
1. Implementera och granska datam칛rkning f칬r att f칬rhindra felaktig representation eller missbruk av data
1. S칛kerst칛ll att din AI-infrastruktur st칬djer inneh친llsfiltrering

A:1, 츿ven om alla tre 칛r utm칛rkta rekommendationer, kommer det att g칬ra stor skillnad att s칛kerst칛lla att du tilldelar r칛tt data친tkomstprivilegier till anv칛ndare f칬r att f칬rhindra manipulation och felaktig representation av data som anv칛nds av LLMs.

## 游 Utmaning

L칛s mer om hur du kan [styra och skydda k칛nslig information](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) i AI-eran.

## Bra jobbat, forts칛tt att l칛ra dig

Efter att ha avslutat denna lektion, kolla in v친r [Generative AI Learning-samling](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) f칬r att forts칛tta att utveckla din kunskap om generativ AI!

G친 vidare till Lektion 14 d칛r vi kommer att titta p친 [livscykeln f칬r generativa AI-applikationer](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av AI-칬vers칛ttningstj칛nsten [Co-op Translator](https://github.com/Azure/co-op-translator). 츿ven om vi str칛var efter noggrannhet, b칬r det noteras att automatiserade 칬vers칛ttningar kan inneh친lla fel eller felaktigheter. Det ursprungliga dokumentet p친 dess ursprungliga spr친k b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r eventuella missf칬rst친nd eller feltolkningar som uppst친r vid anv칛ndning av denna 칬vers칛ttning.