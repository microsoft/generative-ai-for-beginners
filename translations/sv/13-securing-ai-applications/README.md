<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:28:47+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "sv"
}
-->
# S√§kerst√§ll dina generativa AI-applikationer

[![S√§kerst√§ll dina generativa AI-applikationer](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.sv.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Introduktion

Den h√§r lektionen t√§cker:

- S√§kerhet inom AI-systemens kontext.
- Vanliga risker och hot mot AI-system.
- Metoder och √∂verv√§ganden f√∂r att s√§kra AI-system.

## L√§randem√•l

Efter att ha genomf√∂rt denna lektion kommer du att ha f√∂rst√•else f√∂r:

- Hot och risker mot AI-system.
- Vanliga metoder och praxis f√∂r att s√§kra AI-system.
- Hur implementering av s√§kerhetstestning kan f√∂rhindra ov√§ntade resultat och f√∂rtroendef√∂rlust hos anv√§ndare.

## Vad inneb√§r s√§kerhet inom generativ AI?

N√§r artificiell intelligens (AI) och maskininl√§rning (ML) i allt st√∂rre utstr√§ckning formar v√•ra liv √§r det avg√∂rande att skydda inte bara kunddata utan √§ven AI-systemen sj√§lva. AI/ML anv√§nds allt mer f√∂r att st√∂dja beslut med h√∂gt v√§rde i branscher d√§r fel beslut kan f√• allvarliga konsekvenser.

H√§r √§r viktiga punkter att t√§nka p√•:

- **AI/ML:s p√•verkan**: AI/ML har stor p√•verkan p√• vardagen och d√§rf√∂r √§r det n√∂dv√§ndigt att skydda dem.
- **S√§kerhetsutmaningar**: Den p√•verkan AI/ML har kr√§ver s√§rskild uppm√§rksamhet f√∂r att skydda AI-baserade produkter fr√•n avancerade attacker, oavsett om de kommer fr√•n troll eller organiserade grupper.
- **Strategiska problem**: Teknikindustrin m√•ste proaktivt hantera strategiska utmaningar f√∂r att s√§kerst√§lla l√•ngsiktig kunds√§kerhet och datas√§kerhet.

Dessutom har maskininl√§rningsmodeller i stor utstr√§ckning sv√•rt att skilja mellan illvillig input och ofarlig anomal data. En stor del av tr√§ningsdata kommer fr√•n okurerade, omodererade offentliga dataset som √§r √∂ppna f√∂r bidrag fr√•n tredje part. Angripare beh√∂ver inte kompromettera dataset n√§r de fritt kan bidra till dem. Med tiden blir l√•gkonfidens illvillig data till h√∂gkonfidens betrodd data, s√• l√§nge datastrukturen/formateringen √§r korrekt.

D√§rf√∂r √§r det avg√∂rande att s√§kerst√§lla integriteten och skyddet av de datalager som dina modeller anv√§nder f√∂r att fatta beslut.

## F√∂rst√• hot och risker med AI

N√§r det g√§ller AI och relaterade system √§r dataf√∂rgiftning det mest betydande s√§kerhetshotet idag. Datapoisning inneb√§r att n√•gon medvetet √§ndrar informationen som anv√§nds f√∂r att tr√§na en AI, vilket f√•r den att g√∂ra fel. Detta beror p√• avsaknaden av standardiserade metoder f√∂r uppt√§ckt och √•tg√§rd, tillsammans med v√•rt beroende av op√•litliga eller okurerade offentliga dataset f√∂r tr√§ning. F√∂r att uppr√§tth√•lla dataintegritet och f√∂rhindra en bristf√§llig tr√§ningsprocess √§r det viktigt att sp√•ra dataursprung och h√§rkomst. Annars g√§ller det gamla tales√§ttet ‚Äùgarbage in, garbage out‚Äù, vilket leder till f√∂rs√§mrad modellprestanda.

H√§r √§r exempel p√• hur dataf√∂rgiftning kan p√•verka dina modeller:

1. **Label Flipping**: Vid en bin√§r klassificeringsuppgift v√§nder en angripare medvetet p√• etiketterna f√∂r en liten del av tr√§ningsdata. Till exempel m√§rks ofarliga exempel som skadliga, vilket f√•r modellen att l√§ra sig felaktiga samband.\
   **Exempel**: Ett spamfilter som felaktigt klassificerar legitima mejl som skr√§ppost p√• grund av manipulerade etiketter.
2. **Feature Poisoning**: En angripare modifierar subtilt egenskaper i tr√§ningsdata f√∂r att inf√∂ra bias eller vilseleda modellen.\
   **Exempel**: L√§gga till irrelevanta nyckelord i produktbeskrivningar f√∂r att manipulera rekommendationssystem.
3. **Data Injection**: Injicera skadlig data i tr√§ningsupps√§ttningen f√∂r att p√•verka modellens beteende.\
   **Exempel**: Inf√∂ra falska anv√§ndarrecensioner f√∂r att snedvrida sentimentanalys.
4. **Backdoor Attacks**: En angripare l√§gger in ett dolt m√∂nster (bakd√∂rr) i tr√§ningsdata. Modellen l√§r sig k√§nna igen detta m√∂nster och beter sig skadligt n√§r det aktiveras.\
   **Exempel**: Ett ansiktsigenk√§nningssystem tr√§nat med bakd√∂rrsbilder som felidentifierar en specifik person.

MITRE Corporation har skapat [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en kunskapsbas √∂ver taktiker och tekniker som anv√§nds av angripare i verkliga attacker mot AI-system.

> Det finns ett v√§xande antal s√•rbarheter i AI-drivna system, eftersom inf√∂randet av AI √∂kar attackytan f√∂r befintliga system ut√∂ver traditionella cyberattacker. Vi utvecklade ATLAS f√∂r att √∂ka medvetenheten om dessa unika och f√∂r√§nderliga s√•rbarheter, eftersom det globala samh√§llet i allt st√∂rre utstr√§ckning integrerar AI i olika system. ATLAS √§r modellerat efter MITRE ATT&CK¬Æ-ramverket och dess taktiker, tekniker och procedurer (TTPs) kompletterar de i ATT&CK.

Precis som MITRE ATT&CK¬Æ-ramverket, som anv√§nds flitigt inom traditionell cybers√§kerhet f√∂r att planera avancerade hotemuleringsscenarier, erbjuder ATLAS en l√§ttillg√§nglig upps√§ttning TTPs som kan hj√§lpa till att b√§ttre f√∂rst√• och f√∂rbereda f√∂rsvar mot nya attacker.

Dessutom har Open Web Application Security Project (OWASP) skapat en "[Top 10-lista](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" √∂ver de mest kritiska s√•rbarheterna i applikationer som anv√§nder LLMs. Listan lyfter fram risker som datapoisoning och andra hot som:

- **Prompt Injection**: en teknik d√§r angripare manipulerar en Large Language Model (LLM) genom noggrant utformade indata, vilket f√•r modellen att agera utanf√∂r sitt avsedda beteende.
- **Supply Chain Vulnerabilities**: Komponenter och mjukvara som utg√∂r applikationer som anv√§nds av en LLM, s√•som Python-moduler eller externa dataset, kan sj√§lva bli komprometterade vilket leder till ov√§ntade resultat, inf√∂rda bias och till och med s√•rbarheter i underliggande infrastruktur.
- **√ñverberoende**: LLMs √§r felbara och har visat sig hallucinera, vilket ger felaktiga eller os√§kra resultat. I flera dokumenterade fall har m√§nniskor tagit resultaten f√∂r givna, vilket lett till oavsiktliga negativa konsekvenser i verkliga livet.

Microsoft Cloud Advocate Rod Trent har skrivit en gratis e-bok, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), som g√•r p√• djupet med dessa och andra framv√§xande AI-hot och ger omfattande v√§gledning om hur man b√§st hanterar dessa scenarier.

## S√§kerhetstestning f√∂r AI-system och LLMs

Artificiell intelligens (AI) f√∂r√§ndrar m√•nga omr√•den och branscher och erbjuder nya m√∂jligheter och f√∂rdelar f√∂r samh√§llet. Samtidigt medf√∂r AI betydande utmaningar och risker, s√•som dataskydd, bias, brist p√• f√∂rklarbarhet och potentiellt missbruk. D√§rf√∂r √§r det avg√∂rande att s√§kerst√§lla att AI-system √§r s√§kra och ansvarsfulla, vilket inneb√§r att de f√∂ljer etiska och juridiska standarder och kan litas p√• av anv√§ndare och intressenter.

S√§kerhetstestning √§r processen att utv√§rdera s√§kerheten i ett AI-system eller LLM genom att identifiera och utnyttja dess s√•rbarheter. Detta kan utf√∂ras av utvecklare, anv√§ndare eller tredjepartsrevisorer, beroende p√• syfte och omfattning av testningen. N√•gra av de vanligaste metoderna f√∂r s√§kerhetstestning av AI-system och LLMs √§r:

- **Datasanering**: Processen att ta bort eller anonymisera k√§nslig eller privat information fr√•n tr√§ningsdata eller input till ett AI-system eller LLM. Datasanering kan hj√§lpa till att f√∂rhindra datal√§ckage och illvillig manipulation genom att minska exponeringen av konfidentiell eller personlig data.
- **Adversarial testing**: Processen att generera och applicera adversariala exempel p√• input eller output fr√•n ett AI-system eller LLM f√∂r att utv√§rdera dess robusthet och motst√•ndskraft mot attacker. Adversarial testing kan hj√§lpa till att identifiera och mildra s√•rbarheter och svagheter som angripare kan utnyttja.
- **Modellverifiering**: Processen att verifiera korrektheten och fullst√§ndigheten av modellparametrar eller arkitektur i ett AI-system eller LLM. Modellverifiering kan hj√§lpa till att uppt√§cka och f√∂rhindra modellst√∂ld genom att s√§kerst√§lla att modellen √§r skyddad och autentiserad.
- **Outputvalidering**: Processen att validera kvaliteten och tillf√∂rlitligheten i output fr√•n ett AI-system eller LLM. Outputvalidering kan hj√§lpa till att uppt√§cka och korrigera illvillig manipulation genom att s√§kerst√§lla att output √§r konsekvent och korrekt.

OpenAI, en ledare inom AI-system, har etablerat en serie _s√§kerhetsutv√§rderingar_ som en del av deras red teaming-n√§tverksinitiativ, med m√•let att testa AI-systemens output f√∂r att bidra till AI-s√§kerhet.

> Utv√§rderingar kan variera fr√•n enkla fr√•gor och svar till mer komplexa simuleringar. H√§r √§r n√•gra exempel p√• utv√§rderingar utvecklade av OpenAI f√∂r att bed√∂ma AI-beteenden fr√•n olika perspektiv:

#### √ñvertalning

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system lura ett annat AI-system att s√§ga ett hemligt ord?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system √∂vertyga ett annat AI-system att donera pengar?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system p√•verka ett annat AI-systems st√∂d f√∂r ett politiskt f√∂rslag?

#### Steganografi (dold kommunikation)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system skicka hemliga meddelanden utan att uppt√§ckas av ett annat AI-system?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system komprimera och dekomprimera meddelanden f√∂r att m√∂jligg√∂ra dold kommunikation?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Hur bra kan ett AI-system koordinera med ett annat AI-system utan direkt kommunikation?

### AI-s√§kerhet

Det √§r avg√∂rande att vi skyddar AI-system fr√•n illvilliga attacker, missbruk eller oavsiktliga konsekvenser. Detta inkluderar att vidta √•tg√§rder f√∂r att s√§kerst√§lla AI-systemens s√§kerhet, tillf√∂rlitlighet och trov√§rdighet, s√•som:

- S√§kerst√§lla data och algoritmer som anv√§nds f√∂r att tr√§na och k√∂ra AI-modeller
- F√∂rhindra obeh√∂rig √•tkomst, manipulation eller sabotage av AI-system
- Uppt√§cka och mildra bias, diskriminering eller etiska problem i AI-system
- S√§kerst√§lla ansvarstagande, transparens och f√∂rklarbarhet i AI-beslut och handlingar
- Anpassa AI-systemens m√•l och v√§rderingar med m√§nniskors och samh√§llets

AI-s√§kerhet √§r viktigt f√∂r att garantera integritet, tillg√§nglighet och konfidentialitet f√∂r AI-system och data. N√•gra av utmaningarna och m√∂jligheterna inom AI-s√§kerhet √§r:

- M√∂jlighet: Att integrera AI i cybers√§kerhetsstrategier eftersom AI kan spela en avg√∂rande roll i att identifiera hot och f√∂rb√§ttra responstider. AI kan hj√§lpa till att automatisera och f√∂rst√§rka uppt√§ckt och √•tg√§rd av cyberattacker, s√•som phishing, malware eller ransomware.
- Utmaning: AI kan ocks√• anv√§ndas av angripare f√∂r att genomf√∂ra avancerade attacker, som att generera falskt eller vilseledande inneh√•ll, utge sig f√∂r att vara anv√§ndare eller utnyttja s√•rbarheter i AI-system. D√§rf√∂r har AI-utvecklare ett unikt ansvar att designa system som √§r robusta och motst√•ndskraftiga mot missbruk.

### Dataskydd

LLMs kan inneb√§ra risker f√∂r integriteten och s√§kerheten f√∂r den data de anv√§nder. Till exempel kan LLMs potentiellt memorera och l√§cka k√§nslig information fr√•n sin tr√§ningsdata, s√•som personnamn, adresser, l√∂senord eller kreditkortsnummer. De kan ocks√• manipuleras eller attackeras av illvilliga akt√∂rer som vill utnyttja deras s√•rbarheter eller bias. D√§rf√∂r √§r det viktigt att vara medveten om dessa risker och vidta l√§mpliga √•tg√§rder f√∂r att skydda data som anv√§nds med LLMs. N√•gra steg du kan ta f√∂r att skydda data som anv√§nds med LLMs √§r:

- **Begr√§nsa m√§ngden och typen av data som delas med LLMs**: Dela endast den data som √§r n√∂dv√§ndig och relevant f√∂r avsedda √§ndam√•l, och undvik att dela k√§nslig, konfidentiell eller personlig data. Anv√§ndare b√∂r ocks√• anonymisera eller kryptera data som delas med LLMs, till exempel genom att ta bort eller maskera identifierande information eller anv√§nda s√§kra kommunikationskanaler.
- **Verifiera data som LLMs genererar**: Kontrollera alltid noggrannheten och kvaliteten p√• output fr√•n LLMs f√∂r att s√§kerst√§lla att den inte inneh√•ller o√∂nskad eller ol√§mplig information.
- **Rapportera och larma vid dataintr√•ng eller incidenter**: Var vaksam p√• misst√§nkt eller onormalt beteende fr√•n LLMs, s√•som att generera irrelevanta, felaktiga, st√∂tande eller skadliga texter. Detta kan vara en indikation p√• dataintr√•ng eller s√§kerhetsincident.

Datas√§kerhet, styrning och efterlevnad √§r avg√∂rande f√∂r alla organisationer som vill utnyttja kraften i data och AI i en multicloud-milj√∂. Att s√§kra och styra all din data √§r en komplex och m√•ngfacetterad uppgift. Du beh√∂ver s√§kra och styra olika typer av data (strukturerad, ostrukturerad och AI-genererad data) p√• olika platser √∂ver flera moln, och du m√•ste ta h√§nsyn till befintliga och framtida regler f√∂r datas√§kerhet, styrning och AI. F√∂r att skydda din data beh√∂ver du anta n√•gra b√§sta praxis och f√∂rsiktighets√•tg√§rder, s√•som:

- Anv√§nda molntj√§nster eller plattformar som erbjuder dataskydd och integritetsfunktioner.
- Anv√§nda verktyg f√∂r datakvalitet och validering f√∂r att kontrollera din data f√∂r fel, inkonsekvenser eller anomalier.
- Anv√§nda ramverk f√∂r datastyrning och etik f√∂r att s√§kerst√§lla att din data anv√§nds p√• ett ansvarsfullt och transparent s√§tt.

### Emulera verkliga hot ‚Äì AI red teaming

Att emulera verkliga hot anses nu vara en standardpraxis f√∂r att bygga motst√•ndskraftiga AI-system genom att anv√§nda liknande verktyg, taktiker och procedurer f√∂r att identifiera risker f√∂r system och testa f√∂rsvararnas respons.
> Praktiken med AI red teaming har utvecklats till att f√• en bredare betydelse: det handlar inte bara om att unders√∂ka s√§kerhetss√•rbarheter, utan inkluderar √§ven att testa f√∂r andra systemfel, s√•som generering av potentiellt skadligt inneh√•ll. AI-system medf√∂r nya risker, och red teaming √§r centralt f√∂r att f√∂rst√• dessa nya risker, som promptinjektion och produktion av ogrundat inneh√•ll. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.sv.png)]()

Nedan f√∂ljer viktiga insikter som har format Microsofts AI Red Team-program.

1. **Omfattande omfattning av AI Red Teaming:**  
   AI red teaming omfattar nu b√•de s√§kerhets- och Responsible AI (RAI)-resultat. Traditionellt har red teaming fokuserat p√• s√§kerhetsaspekter, d√§r modellen betraktas som en attackvektor (t.ex. st√∂ld av den underliggande modellen). AI-system introducerar dock nya s√§kerhetss√•rbarheter (t.ex. promptinjektion, f√∂rgiftning), vilket kr√§ver s√§rskild uppm√§rksamhet. Ut√∂ver s√§kerhet unders√∂ker AI red teaming √§ven r√§ttviseaspekter (t.ex. stereotyper) och skadligt inneh√•ll (t.ex. glorifiering av v√•ld). Tidig identifiering av dessa problem m√∂jligg√∂r prioritering av f√∂rsvarsinsatser.  
2. **Skadliga och ofarliga fel:**  
   AI red teaming tar h√§nsyn till fel b√•de fr√•n skadliga och ofarliga perspektiv. Till exempel, n√§r vi red teamar nya Bing, unders√∂ker vi inte bara hur illvilliga akt√∂rer kan manipulera systemet utan ocks√• hur vanliga anv√§ndare kan st√∂ta p√• problematiskt eller skadligt inneh√•ll. Till skillnad fr√•n traditionell s√§kerhetsred teaming, som fr√§mst fokuserar p√• skadliga akt√∂rer, beaktar AI red teaming en bredare upps√§ttning anv√§ndartyper och potentiella fel.  
3. **AI-systemens dynamiska natur:**  
   AI-applikationer utvecklas st√§ndigt. I applikationer med stora spr√•kmodeller anpassar utvecklare sig till f√∂r√§ndrade krav. Kontinuerlig red teaming s√§kerst√§ller st√§ndig vaksamhet och anpassning till nya risker.

AI red teaming √§r inte helt√§ckande och b√∂r ses som ett komplement till andra kontroller som [rollbaserad √•tkomstkontroll (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) och omfattande datastyrningsl√∂sningar. Det √§r t√§nkt att komplettera en s√§kerhetsstrategi som fokuserar p√• att anv√§nda s√§kra och ansvarsfulla AI-l√∂sningar som tar h√§nsyn till integritet och s√§kerhet samtidigt som man str√§var efter att minimera bias, skadligt inneh√•ll och desinformation som kan undergr√§va anv√§ndarnas f√∂rtroende.

H√§r √§r en lista med ytterligare l√§sning som kan hj√§lpa dig att b√§ttre f√∂rst√• hur red teaming kan hj√§lpa till att identifiera och mildra risker i dina AI-system:

- [Planering av red teaming f√∂r stora spr√•kmodeller (LLMs) och deras applikationer](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [Vad √§r OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming ‚Äì en nyckelmetod f√∂r att bygga s√§krare och mer ansvarsfulla AI-l√∂sningar](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en kunskapsbas √∂ver taktiker och tekniker som anv√§nds av angripare i verkliga attacker mot AI-system.

## Kunskapskontroll

Vad kan vara ett bra tillv√§gag√•ngss√§tt f√∂r att uppr√§tth√•lla dataintegritet och f√∂rhindra missbruk?

1. Ha starka rollbaserade kontroller f√∂r data√•tkomst och datastyrning  
1. Implementera och granska datam√§rkning f√∂r att f√∂rhindra felaktig representation eller missbruk av data  
1. S√§kerst√§ll att din AI-infrastruktur st√∂djer inneh√•llsfiltrering

A:1, √Ñven om alla tre √§r bra rekommendationer, kommer det att g√∂ra stor skillnad att se till att r√§tt data√•tkomstr√§ttigheter tilldelas anv√§ndare f√∂r att f√∂rhindra manipulation och felaktig representation av data som anv√§nds av LLMs.

## üöÄ Utmaning

L√§s mer om hur du kan [styra och skydda k√§nslig information](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) i AI-eran.

## Bra jobbat, forts√§tt din l√§rande

Efter att ha slutf√∂rt denna lektion, kolla in v√•r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) f√∂r att forts√§tta utveckla dina kunskaper inom Generative AI!

G√• vidare till Lektion 14 d√§r vi tittar p√• [Generative AI Application Lifecycle](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, v√§nligen observera att automatiska √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess modersm√•l b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r n√•gra missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.