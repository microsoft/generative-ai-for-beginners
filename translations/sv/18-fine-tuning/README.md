<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-05-20T07:49:35+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "sv"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.8487555c3e3225eefc1dc84e72c8e00bce1ee76db867a080628fb0fbb04aa0d2.sv.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Finjustera din LLM

Att anv칛nda stora spr친kmodeller f칬r att bygga generativa AI-applikationer medf칬r nya utmaningar. En viktig fr친ga 칛r att s칛kerst칛lla svarskvaliteten (noggrannhet och relevans) i inneh친ll som genereras av modellen f칬r en given anv칛ndarf칬rfr친gan. I tidigare lektioner diskuterade vi tekniker som promptteknik och retrieval-augmented generation som f칬rs칬ker l칬sa problemet genom att _modifiera promptinmatningen_ till den befintliga modellen.

I dagens lektion diskuterar vi en tredje teknik, **finjustering**, som f칬rs칬ker l칬sa utmaningen genom att _tr칛na om modellen sj칛lv_ med ytterligare data. L친t oss dyka in i detaljerna.

## Inl칛rningsm친l

Denna lektion introducerar konceptet finjustering f칬r f칬rtr칛nade spr친kmodeller, utforskar f칬rdelarna och utmaningarna med detta tillv칛gag친ngss칛tt, och ger v칛gledning om n칛r och hur man anv칛nder finjustering f칬r att f칬rb칛ttra prestandan hos dina generativa AI-modeller.

I slutet av denna lektion b칬r du kunna svara p친 f칬ljande fr친gor:

- Vad 칛r finjustering f칬r spr친kmodeller?
- N칛r, och varf칬r, 칛r finjustering anv칛ndbar?
- Hur kan jag finjustera en f칬rtr칛nad modell?
- Vilka 칛r begr칛nsningarna med finjustering?

Redo? L친t oss b칬rja.

## Illustrerad guide

Vill du f친 en 칬verblick 칬ver vad vi kommer att t칛cka innan vi dyker in? Kolla in denna illustrerade guide som beskriver l칛randeprocessen f칬r denna lektion - fr친n att l칛ra sig k칛rnkoncepten och motivationen f칬r finjustering, till att f칬rst친 processen och b칛sta praxis f칬r att utf칬ra finjusteringsuppgiften. Detta 칛r ett fascinerande 칛mne f칬r utforskning, s친 gl칬m inte att kolla in [Resurser](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) sidan f칬r ytterligare l칛nkar f칬r att st칬dja din sj칛lvstyrda inl칛rningsresa!

![Illustrerad guide till finjustering av spr친kmodeller](../../../translated_images/18-fine-tuning-sketchnote.92733966235199dd260184b1aae3a84b877c7496bc872d8e63ad6fa2dd96bafc.sv.png)

## Vad 칛r finjustering f칬r spr친kmodeller?

Per definition 칛r stora spr친kmodeller _f칬rtr칛nade_ p친 stora m칛ngder text h칛mtad fr친n olika k칛llor, inklusive internet. Som vi har l칛rt oss i tidigare lektioner beh칬ver vi tekniker som _promptteknik_ och _retrieval-augmented generation_ f칬r att f칬rb칛ttra kvaliteten p친 modellens svar p친 anv칛ndarens fr친gor ("prompter").

En popul칛r promptteknik inneb칛r att ge modellen mer v칛gledning om vad som f칬rv칛ntas i svaret antingen genom att ge _instruktioner_ (explicit v칛gledning) eller _ge den n친gra exempel_ (implicit v칛gledning). Detta kallas _few-shot learning_ men det har tv친 begr칛nsningar:

- Modellens tokenbegr칛nsningar kan begr칛nsa antalet exempel du kan ge och begr칛nsa effektiviteten.
- Modellens tokenkostnader kan g칬ra det dyrt att l칛gga till exempel till varje prompt och begr칛nsa flexibiliteten.

Finjustering 칛r en vanlig praxis i maskininl칛rningssystem d칛r vi tar en f칬rtr칛nad modell och tr칛nar om den med ny data f칬r att f칬rb칛ttra dess prestanda p친 en specifik uppgift. I sammanhanget av spr친kmodeller kan vi finjustera den f칬rtr칛nade modellen _med en noggrant utvald upps칛ttning exempel f칬r en given uppgift eller applikationsdom칛n_ f칬r att skapa en **anpassad modell** som kan vara mer exakt och relevant f칬r den specifika uppgiften eller dom칛nen. En bieffekt av finjustering 칛r att den ocks친 kan minska antalet exempel som beh칬vs f칬r few-shot learning - vilket minskar tokenanv칛ndning och relaterade kostnader.

## N칛r och varf칬r b칬r vi finjustera modeller?

I _detta_ sammanhang, n칛r vi talar om finjustering, h칛nvisar vi till **칬vervakad** finjustering d칛r omtr칛ningen g칬rs genom att **l칛gga till ny data** som inte var en del av den ursprungliga tr칛ningsdatam칛ngden. Detta skiljer sig fr친n ett o칬vervakat finjusteringstillv칛gag친ngss칛tt d칛r modellen tr칛nas om p친 den ursprungliga datan, men med olika hyperparametrar.

Det viktigaste att komma ih친g 칛r att finjustering 칛r en avancerad teknik som kr칛ver en viss niv친 av expertis f칬r att f친 칬nskade resultat. Om det g칬rs felaktigt kan det inte ge de f칬rv칛ntade f칬rb칛ttringarna och kan till och med f칬rs칛mra modellens prestanda f칬r din m친ldom칛n.

S친 innan du l칛r dig "hur" du finjusterar spr친kmodeller, beh칬ver du veta "varf칬r" du b칬r ta denna v칛g och "n칛r" du ska b칬rja processen med finjustering. B칬rja med att st칛lla dig sj칛lv dessa fr친gor:

- **Anv칛ndningsfall**: Vad 칛r ditt _anv칛ndningsfall_ f칬r finjustering? Vilken aspekt av den nuvarande f칬rtr칛nade modellen vill du f칬rb칛ttra?
- **Alternativ**: Har du provat _andra tekniker_ f칬r att uppn친 de 칬nskade resultaten? Anv칛nd dem f칬r att skapa en baslinje f칬r j칛mf칬relse.
  - Promptteknik: Prova tekniker som f친-shot-prompting med exempel p친 relevanta prompt-svar. Utv칛rdera svarens kvalitet.
  - Retrieval Augmented Generation: F칬rs칬k att f칬rst칛rka prompter med s칬kresultat h칛mtade genom att s칬ka i din data. Utv칛rdera svarens kvalitet.
- **Kostnader**: Har du identifierat kostnaderna f칬r finjustering?
  - Justerbarhet - 칛r den f칬rtr칛nade modellen tillg칛nglig f칬r finjustering?
  - Insats - f칬r att f칬rbereda tr칛ningsdata, utv칛rdera och f칬rfina modellen.
  - Ber칛kning - f칬r att k칬ra finjusteringsjobb och distribuera finjusterad modell.
  - Data - tillg친ng till tillr칛ckligt m친nga kvalitativa exempel f칬r finjusteringsp친verkan.
- **F칬rdelar**: Har du bekr칛ftat f칬rdelarna med finjustering?
  - Kvalitet - 칬vertr칛ffade den finjusterade modellen baslinjen?
  - Kostnad - minskar den tokenanv칛ndningen genom att f칬renkla prompter?
  - Utbyggbarhet - kan du 친teranv칛nda basmodellen f칬r nya dom칛ner?

Genom att svara p친 dessa fr친gor b칬r du kunna avg칬ra om finjustering 칛r r칛tt tillv칛gag친ngss칛tt f칬r ditt anv칛ndningsfall. Idealiskt 칛r tillv칛gag친ngss칛ttet giltigt endast om f칬rdelarna 칬verv칛ger kostnaderna. N칛r du har best칛mt dig f칬r att g친 vidare 칛r det dags att t칛nka p친 _hur_ du kan finjustera den f칬rtr칛nade modellen.

Vill du f친 mer insikt i beslutsprocessen? Titta p친 [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Hur kan vi finjustera en f칬rtr칛nad modell?

F칬r att finjustera en f칬rtr칛nad modell beh칬ver du ha:

- en f칬rtr칛nad modell att finjustera
- en datam칛ngd att anv칛nda f칬r finjustering
- en tr칛ningsmilj칬 f칬r att k칬ra finjusteringsjobbet
- en v칛rdmilj칬 f칬r att distribuera den finjusterade modellen

## Finjustering i praktiken

F칬ljande resurser ger steg-f칬r-steg-handledningar f칬r att guida dig genom ett verkligt exempel med en utvald modell och en noggrant utvald datam칛ngd. F칬r att arbeta igenom dessa handledningar beh칬ver du ett konto hos den specifika leverant칬ren, tillsammans med tillg친ng till den relevanta modellen och datam칛ngderna.

| Leverant칬r   | Handledning                                                                                                                                                                      | Beskrivning                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                | L칛r dig att finjustera en `gpt-35-turbo` f칬r en specifik dom칛n ("receptassistent") genom att f칬rbereda tr칛ningsdata, k칬ra finjusteringsjobbet och anv칛nda den finjusterade modellen f칬r inferens.                                                                                                                                                                                                                                    |
| Azure OpenAI | [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | L칛r dig att finjustera en `gpt-35-turbo-0613` modell **p친 Azure** genom att ta steg f칬r att skapa och ladda upp tr칛ningsdata, k칬ra finjusteringsjobbet. Distribuera och anv칛nd den nya modellen.                                                                                                                                                                                                                                          |
| Hugging Face | [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Detta blogginl칛gg guidar dig genom finjustering av en _칬ppen LLM_ (ex: `CodeLlama 7B`) med hj칛lp av [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteket & [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) med 칬ppna [datasets](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) p친 Hugging Face. |
|              |                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| 游뱅 AutoTrain | [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                         | AutoTrain (eller AutoTrain Advanced) 칛r ett python-bibliotek utvecklat av Hugging Face som m칬jligg칬r finjustering f칬r m친nga olika uppgifter inklusive LLM finjustering. AutoTrain 칛r en kodfri l칬sning och finjustering kan g칬ras i din egen moln, p친 Hugging Face Spaces eller lokalt. Det st칬der b친de en webbaserad GUI, CLI och tr칛ning via yaml-konfigurationsfiler.                                                                 |

## Uppgift

V칛lj en av handledningarna ovan och g친 igenom dem. _Vi kan replikera en version av dessa handledningar i Jupyter Notebooks i detta repo endast som referens. Anv칛nd de ursprungliga k칛llorna direkt f칬r att f친 de senaste versionerna_.

## Bra jobbat! Forts칛tt ditt l칛rande.

Efter att ha slutf칬rt denna lektion, kolla in v친r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) f칬r att forts칛tta f칬rb칛ttra din kunskap om Generativ AI!

Grattis!! Du har slutf칬rt den sista lektionen fr친n v2-serien f칬r denna kurs! Sluta inte l칛ra och bygga. **Kolla in [RESURSER](RESOURCES.md?WT.mc_id=academic-105485-koreyst) sidan f칬r en lista 칬ver ytterligare f칬rslag f칬r just detta 칛mne.

V친r v1-serie av lektioner har ocks친 uppdaterats med fler uppgifter och koncept. S친 ta en minut f칬r att fr칛scha upp din kunskap - och v칛nligen [dela dina fr친gor och feedback](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) f칬r att hj칛lpa oss att f칬rb칛ttra dessa lektioner f칬r samh칛llet.

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av AI-칬vers칛ttningstj칛nsten [Co-op Translator](https://github.com/Azure/co-op-translator). 츿ven om vi str칛var efter noggrannhet, var medveten om att automatiserade 칬vers칛ttningar kan inneh친lla fel eller felaktigheter. Det ursprungliga dokumentet p친 sitt modersm친l b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r eventuella missf칬rst친nd eller feltolkningar som uppst친r vid anv칛ndningen av denna 칬vers칛ttning.