<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3772dcd23a98e2010f53ce8b9c583631",
  "translation_date": "2026-01-18T18:02:12+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "sv"
}
-->
[![Open Source Models](../../../../../translated_images/sv/18-lesson-banner.f30176815b1a5074.webp)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# Finjustera din LLM

Att anv√§nda stora spr√•kmodeller f√∂r att bygga generativa AI-applikationer inneb√§r nya utmaningar. En nyckelfr√•ga √§r att s√§kerst√§lla svarskvalitet (noggrannhet och relevans) i inneh√•ll som genereras av modellen f√∂r en given anv√§ndarf√∂rfr√•gan. I tidigare lektioner har vi diskuterat tekniker som prompt-engineering och retrieval-augmented generation som f√∂rs√∂ker l√∂sa problemet genom att _modifiera prompt-inmatningen_ till den befintliga modellen.

I dagens lektion diskuterar vi en tredje teknik, **finjustering**, som f√∂rs√∂ker ta itu med utmaningen genom att _omtr√§na sj√§lva modellen_ med ytterligare data. L√•t oss g√• in p√• detaljerna.

## L√§randem√•l

Denna lektion introducerar begreppet finjustering f√∂r f√∂rtr√§nade spr√•kmodeller, utforskar f√∂rdelar och utmaningar med detta tillv√§gag√•ngss√§tt och ger v√§gledning om n√§r och hur man anv√§nder finjustering f√∂r att f√∂rb√§ttra prestandan hos dina generativa AI-modeller.

I slutet av denna lektion b√∂r du kunna svara p√• f√∂ljande fr√•gor:

- Vad √§r finjustering f√∂r spr√•kmodeller?
- N√§r och varf√∂r √§r finjustering anv√§ndbart?
- Hur kan jag finjustera en f√∂rtr√§nad modell?
- Vilka √§r begr√§nsningarna med finjustering?

Redo? L√•t oss s√§tta ig√•ng.

## Illustrerad guide

Vill du f√• en √∂vergripande bild av vad vi ska t√§cka innan vi dyker in? Titta p√• denna illustrerade guide som beskriver l√§randeresan f√∂r denna lektion ‚Äì fr√•n att l√§ra sig k√§rnbegrepp och motivation f√∂r finjustering till att f√∂rst√• processen och b√§sta praxis f√∂r att utf√∂ra finjusteringsuppgiften. Detta √§r ett fascinerande √§mne att utforska, s√• gl√∂m inte att kolla in sidan [Resurser](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) f√∂r ytterligare l√§nkar som st√∂djer din sj√§lvstyrda inl√§rningsresa!

![Illustrated Guide to Fine Tuning Language Models](../../../../../translated_images/sv/18-fine-tuning-sketchnote.11b21f9ec8a70346.webp)

## Vad √§r finjustering f√∂r spr√•kmodeller?

Enligt definition √§r stora spr√•kmodeller _f√∂rtr√§nade_ p√• stora m√§ngder text h√§mtad fr√•n olika k√§llor inklusive internet. Som vi l√§rt oss i tidigare lektioner beh√∂ver vi tekniker som _prompt-engineering_ och _retrieval-augmented generation_ f√∂r att f√∂rb√§ttra kvaliteten p√• modellens svar p√• anv√§ndarens fr√•gor ("prompts").

En popul√§r prompt-engineering teknik inneb√§r att ge modellen mer v√§gledning om vad som f√∂rv√§ntas i svaret, antingen genom att ge _instruktioner_ (explicit v√§gledning) eller _ge den n√•gra exempel_ (implicit v√§gledning). Detta kallas _few-shot learning_ men har tv√• begr√§nsningar:

- Modellens tokenbegr√§nsningar kan begr√§nsa antalet exempel du kan ge och d√§rmed effektiviteten.
- Kostnaden f√∂r modellens tokens kan g√∂ra det dyrt att l√§gga till exempel till varje prompt och begr√§nsa flexibiliteten.

Finjustering √§r en vanlig praxis inom maskininl√§rningssystem d√§r vi tar en f√∂rtr√§nad modell och tr√§nar om den med ny data f√∂r att f√∂rb√§ttra dess prestanda p√• en specifik uppgift. I kontexten av spr√•kmodeller kan vi finjustera den f√∂rtr√§nade modellen _med en kuraterad upps√§ttning exempel f√∂r en given uppgift eller applikationsdom√§n_ f√∂r att skapa en **anpassad modell** som kan vara mer exakt och relevant f√∂r just den uppgiften eller dom√§nen. En ytterligare f√∂rdel med finjustering √§r att det ocks√• kan minska antalet exempel som beh√∂vs f√∂r few-shot learning ‚Äì vilket minskar tokenanv√§ndning och relaterade kostnader.

## N√§r och varf√∂r b√∂r vi finjustera modeller?

I _detta_ sammanhang, n√§r vi pratar om finjustering, avser vi **√∂vervakad** finjustering d√§r omtr√§ningen g√∂rs genom att **l√§gga till ny data** som inte var del av den ursprungliga tr√§ningsdatam√§ngden. Detta skiljer sig fr√•n en o√∂vervakad finjustering d√§r modellen tr√§nas om p√• ursprungsdata, men med olika hyperparametrar.

Det viktigaste att komma ih√•g √§r att finjustering √§r en avancerad teknik som kr√§ver en viss niv√• av expertis f√∂r att uppn√• √∂nskade resultat. Om det g√∂rs felaktigt kan det kanske inte ge de f√∂rv√§ntade f√∂rb√§ttringarna och kan till och med f√∂rs√§mra modellens prestanda f√∂r din m√•lade dom√§n.

S√• innan du l√§r dig "hur" du finjusterar spr√•kmodeller, beh√∂ver du veta "varf√∂r" du ska ta denna v√§g och "n√§r" du ska b√∂rja finjusteringsprocessen. B√∂rja med att st√§lla dig sj√§lv dessa fr√•gor:

- **Anv√§ndningsfall**: Vad √§r ditt _anv√§ndningsfall_ f√∂r finjustering? Vilken aspekt av den nuvarande f√∂rtr√§nade modellen vill du f√∂rb√§ttra?
- **Alternativ**: Har du provat _andra tekniker_ f√∂r att uppn√• √∂nskade resultat? Anv√§nd dem f√∂r att skapa en baslinje f√∂r j√§mf√∂relse.
  - Prompt-engineering: Testa tekniker som few-shot prompting med exempel p√• relevanta prompt-svar. Utv√§rdera svarens kvalitet.
  - Retrieval Augmented Generation: F√∂rs√∂k att f√∂rst√§rka prompts med s√∂kresultat fr√•n dina data. Utv√§rdera svarens kvalitet.
- **Kostnader**: Har du identifierat kostnaderna f√∂r finjustering?
  - Tunbarhet ‚Äì √§r den f√∂rtr√§nade modellen tillg√§nglig f√∂r finjustering?
  - Insats ‚Äì f√∂r att f√∂rbereda tr√§ningsdata, utv√§rdera och f√∂rfina modellen.
  - Ber√§kning ‚Äì f√∂r att k√∂ra finjusteringsjobb och distribuera den finjusterade modellen.
  - Data ‚Äì tillg√•ng till kvalitativa exempel i tillr√§cklig omfattning f√∂r finjusteringsp√•verkan.
- **F√∂rdelar**: Har du bekr√§ftat f√∂rdelarna med finjustering?
  - Kvalitet ‚Äì presterade den finjusterade modellen b√§ttre √§n baslinjen?
  - Kostnad ‚Äì minskar det tokenanv√§ndningen genom att f√∂renkla prompts?
  - Utbyggbarhet ‚Äì kan du √•teranv√§nda basmodellen f√∂r nya dom√§ner?

Genom att svara p√• dessa fr√•gor b√∂r du kunna avg√∂ra om finjustering √§r r√§tt tillv√§gag√•ngss√§tt f√∂r ditt anv√§ndningsfall. Idealiskt √§r tillv√§gag√•ngss√§ttet giltigt endast om f√∂rdelarna √∂verv√§ger kostnaderna. N√§r du best√§mt dig f√∂r att g√• vidare √§r det dags att t√§nka p√• _hur_ du kan finjustera den f√∂rtr√§nade modellen.

Vill du ha fler insikter om beslutsprocessen? Titta p√• [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Hur kan vi finjustera en f√∂rtr√§nad modell?

F√∂r att finjustera en f√∂rtr√§nad modell beh√∂ver du:

- en f√∂rtr√§nad modell att finjustera
- en dataset att anv√§nda f√∂r finjusteringen
- en tr√§ningsmilj√∂ f√∂r att k√∂ra finjusteringsjobbet
- en hosting-milj√∂ f√∂r att distribuera den finjusterade modellen

## Finjustering i praktiken

F√∂ljande resurser erbjuder steg-f√∂r-steg tutorials som guidar dig genom ett verkligt exempel med en utvald modell och kuraterad dataset. F√∂r att arbeta med dessa tutorials beh√∂ver du ett konto hos respektive leverant√∂r samt tillg√•ng till relevanta modeller och datasets.

| Leverant√∂r  | Tutorial                                                                                                                                                                         | Beskrivning                                                                                                                                                                                                                                                                                                                                                                                                                      |
| ----------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI      | [Hur man finjusterar chattmodeller](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)             | L√§r dig att finjustera en `gpt-35-turbo` f√∂r en specifik dom√§n ("receptassistent") genom att f√∂rbereda tr√§ningsdata, k√∂ra finjusteringsjobbet och anv√§nda den finjusterade modellen f√∂r inferens.                                                                                                                                                                                                                               |
| Azure OpenAI| [GPT 3.5 Turbo finjusteringsguide](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst)      | L√§r dig att finjustera en `gpt-35-turbo-0613` modell **p√• Azure** genom att utf√∂ra steg f√∂r att skapa och ladda upp tr√§ningsdata, k√∂ra finjusteringsjobbet. Distribuera och anv√§nd den nya modellen.                                                                                                                                                                                                                               |
| Hugging Face| [Finjustera LLMs med Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                                 | Denna bloggpost visar hur du finjusterar en _√∂ppen LLM_ (exempel: `CodeLlama 7B`) med hj√§lp av [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteket & [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) med √∂ppna [datasets](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) p√• Hugging Face. |
|             |                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ü§ó AutoTrain| [Finjustera LLMs med AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                             | AutoTrain (eller AutoTrain Advanced) √§r ett python-bibliotek utvecklat av Hugging Face som till√•ter finjustering f√∂r m√•nga olika uppgifter inklusive LLM finjustering. AutoTrain √§r en l√∂sning utan kod och finjustering kan g√∂ras i din egen moln, p√• Hugging Face Spaces eller lokalt. Den st√∂der b√•de webbaserat GUI, CLI och tr√§ning via yaml-konfigurationsfiler.                                                                                          |
|             |                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ü¶• Unsloth  | [Finjustera LLMs med Unsloth](https://github.com/unslothai/unsloth)                                                                                                             | Unsloth √§r ett open-source-ramverk som st√∂djer finjustering av LLM och f√∂rst√§rkningsinl√§rning (RL). Unsloth f√∂renklar lokal tr√§ning, utv√§rdering och distribution med f√§rdiga [notebooks](https://github.com/unslothai/notebooks). Det st√∂der √§ven text-till-tal (TTS), BERT och multimodala modeller. F√∂r att komma ig√•ng, l√§s deras steg-f√∂r-steg [Finjusteringsguide f√∂r LLMs](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide).                                   |
|             |                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
## Uppgift

V√§lj en av tutorials ovan och g√• igenom den. _Vi kan komma att √•terskapa en version av dessa tutorials i Jupyter Notebooks i detta repo f√∂r referens endast. V√§nligen anv√§nd originalk√§llorna direkt f√∂r att f√• de senaste versionerna_.

## Bra jobbat! Forts√§tt din inl√§rning.

Efter att ha slutf√∂rt denna lektion, kika p√• v√•r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) f√∂r att forts√§tta utveckla din kunskap om generativ AI!

Grattis!! Du har slutf√∂rt den sista lektionen i v2-serien f√∂r denna kurs! Sluta inte l√§ra och bygga. \*\*Kolla in sidan [RESURSER](RESOURCES.md?WT.mc_id=academic-105485-koreyst) f√∂r en lista p√• ytterligare f√∂rslag f√∂r just detta √§mne.

V√•r v1-serie av lektioner har ocks√• uppdaterats med fler uppgifter och koncept. S√• ta en minut f√∂r att fr√§scha upp din kunskap ‚Äì och v√§nligen [dela dina fr√•gor och feedback](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) f√∂r att hj√§lpa oss f√∂rb√§ttra dessa lektioner f√∂r communityn.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfriskrivning**:
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). Trots att vi str√§var efter noggrannhet kan automatiska √∂vers√§ttningar inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess modersm√•l ska anses vara den auktoritativa k√§llan. F√∂r viktig information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r n√•gra missf√∂rst√•nd eller feltolkningar som uppst√•r till f√∂ljd av anv√§ndningen av denna √∂vers√§ttning.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->