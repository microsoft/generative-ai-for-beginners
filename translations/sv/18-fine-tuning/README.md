<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "807f0d9fc1747e796433534e1be6a98a",
  "translation_date": "2025-10-17T19:03:31+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "sv"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.sv.png)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# Finjustering av din LLM

Att anv칛nda stora spr친kmodeller f칬r att bygga generativa AI-applikationer medf칬r nya utmaningar. En viktig fr친ga 칛r att s칛kerst칛lla svarskvalitet (noggrannhet och relevans) i inneh친llet som genereras av modellen f칬r en given anv칛ndarf칬rfr친gan. I tidigare lektioner diskuterade vi tekniker som promptdesign och retrieval-augmented generation som f칬rs칬ker l칬sa problemet genom att _modifiera promptinmatningen_ till den befintliga modellen.

I dagens lektion diskuterar vi en tredje teknik, **finjustering**, som f칬rs칬ker hantera utmaningen genom att _tr칛na om sj칛lva modellen_ med ytterligare data. L친t oss dyka in i detaljerna.

## L칛randem친l

Denna lektion introducerar konceptet finjustering f칬r f칬rtr칛nade spr친kmodeller, utforskar f칬rdelarna och utmaningarna med detta tillv칛gag친ngss칛tt, och ger v칛gledning om n칛r och hur man kan anv칛nda finjustering f칬r att f칬rb칛ttra prestandan hos dina generativa AI-modeller.

I slutet av denna lektion b칬r du kunna svara p친 f칬ljande fr친gor:

- Vad 칛r finjustering f칬r spr친kmodeller?
- N칛r och varf칬r 칛r finjustering anv칛ndbart?
- Hur kan jag finjustera en f칬rtr칛nad modell?
- Vilka 칛r begr칛nsningarna med finjustering?

Redo? L친t oss b칬rja.

## Illustrerad guide

Vill du f친 en 칬verblick 칬ver vad vi kommer att t칛cka innan vi dyker in? Kolla in denna illustrerade guide som beskriver l칛randets resa f칬r denna lektion - fr친n att l칛ra sig k칛rnkoncepten och motivationen f칬r finjustering, till att f칬rst친 processen och b칛sta praxis f칬r att utf칬ra finjusteringsuppgiften. Detta 칛r ett fascinerande 칛mne att utforska, s친 gl칬m inte att kolla in [Resurser](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) f칬r ytterligare l칛nkar som st칬djer din sj칛lvstyrda l칛randeresa!

![Illustrerad guide till finjustering av spr친kmodeller](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.sv.png)

## Vad 칛r finjustering f칬r spr친kmodeller?

Per definition 칛r stora spr친kmodeller _f칬rtr칛nade_ p친 stora m칛ngder text fr친n olika k칛llor, inklusive internet. Som vi har l칛rt oss i tidigare lektioner beh칬ver vi tekniker som _promptdesign_ och _retrieval-augmented generation_ f칬r att f칬rb칛ttra kvaliteten p친 modellens svar p친 anv칛ndarens fr친gor ("prompter").

En popul칛r teknik inom promptdesign inneb칛r att ge modellen mer v칛gledning om vad som f칬rv칛ntas i svaret, antingen genom att ge _instruktioner_ (explicit v칛gledning) eller _ge den n친gra exempel_ (implicit v칛gledning). Detta kallas _few-shot learning_, men det har tv친 begr칛nsningar:

- Modellens tokenbegr칛nsningar kan begr칛nsa antalet exempel du kan ge och minska effektiviteten.
- Modellens tokenkostnader kan g칬ra det dyrt att l칛gga till exempel till varje prompt och begr칛nsa flexibiliteten.

Finjustering 칛r en vanlig praxis inom maskininl칛rningssystem d칛r vi tar en f칬rtr칛nad modell och tr칛nar om den med ny data f칬r att f칬rb칛ttra dess prestanda f칬r en specifik uppgift. N칛r det g칛ller spr친kmodeller kan vi finjustera den f칬rtr칛nade modellen _med en noggrant utvald upps칛ttning exempel f칬r en given uppgift eller applikationsdom칛n_ f칬r att skapa en **anpassad modell** som kan vara mer exakt och relevant f칬r just den uppgiften eller dom칛nen. En sidoeffekt av finjustering 칛r att det ocks친 kan minska antalet exempel som beh칬vs f칬r few-shot learning - vilket minskar tokenanv칛ndning och relaterade kostnader.

## N칛r och varf칬r ska vi finjustera modeller?

I _detta_ sammanhang, n칛r vi pratar om finjustering, h칛nvisar vi till **칬vervakad** finjustering d칛r omtr칛ningen g칬rs genom att **l칛gga till ny data** som inte var en del av den ursprungliga tr칛ningsdatam칛ngden. Detta skiljer sig fr친n en o칬vervakad finjusteringsmetod d칛r modellen tr칛nas om p친 den ursprungliga datan, men med olika hyperparametrar.

Det viktiga att komma ih친g 칛r att finjustering 칛r en avancerad teknik som kr칛ver en viss niv친 av expertis f칬r att uppn친 칬nskade resultat. Om det g칬rs felaktigt kan det h칛nda att det inte ger de f칬rv칛ntade f칬rb칛ttringarna och till och med f칬rs칛mrar modellens prestanda f칬r din m친lgrupp.

S친 innan du l칛r dig "hur" man finjusterar spr친kmodeller, beh칬ver du veta "varf칬r" du b칬r ta denna v칛g och "n칛r" du ska b칬rja processen med finjustering. B칬rja med att st칛lla dig sj칛lv dessa fr친gor:

- **Anv칛ndningsfall**: Vad 칛r ditt _anv칛ndningsfall_ f칬r finjustering? Vilken aspekt av den nuvarande f칬rtr칛nade modellen vill du f칬rb칛ttra?
- **Alternativ**: Har du provat _andra tekniker_ f칬r att uppn친 칬nskade resultat? Anv칛nd dem f칬r att skapa en baslinje f칬r j칛mf칬relse.
  - Promptdesign: Prova tekniker som few-shot prompting med exempel p친 relevanta promptresponser. Utv칛rdera svarens kvalitet.
  - Retrieval Augmented Generation: Prova att f칬rst칛rka prompts med s칬kresultat fr친n din data. Utv칛rdera svarens kvalitet.
- **Kostnader**: Har du identifierat kostnaderna f칬r finjustering?
  - Anpassningsbarhet - 칛r den f칬rtr칛nade modellen tillg칛nglig f칬r finjustering?
  - Arbete - f칬r att f칬rbereda tr칛ningsdata, utv칛rdera och f칬rb칛ttra modellen.
  - Ber칛kningskraft - f칬r att k칬ra finjusteringsjobb och distribuera den finjusterade modellen.
  - Data - tillg친ng till tillr칛ckligt m친nga kvalitativa exempel f칬r att p친verka finjusteringen.
- **F칬rdelar**: Har du bekr칛ftat f칬rdelarna med finjustering?
  - Kvalitet - 칬vertr칛ffade den finjusterade modellen baslinjen?
  - Kostnad - minskar det tokenanv칛ndningen genom att f칬renkla prompts?
  - Utbyggbarhet - kan du 친teranv칛nda basmodellen f칬r nya dom칛ner?

Genom att besvara dessa fr친gor b칬r du kunna avg칬ra om finjustering 칛r r칛tt tillv칛gag친ngss칛tt f칬r ditt anv칛ndningsfall. Idealiskt sett 칛r tillv칛gag친ngss칛ttet giltigt endast om f칬rdelarna 칬verv칛ger kostnaderna. N칛r du har best칛mt dig f칬r att g친 vidare 칛r det dags att t칛nka p친 _hur_ du kan finjustera den f칬rtr칛nade modellen.

Vill du f친 fler insikter om beslutsprocessen? Titta p친 [Att finjustera eller inte finjustera](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Hur kan vi finjustera en f칬rtr칛nad modell?

F칬r att finjustera en f칬rtr칛nad modell beh칬ver du ha:

- en f칬rtr칛nad modell att finjustera
- en datam칛ngd att anv칛nda f칬r finjustering
- en tr칛ningsmilj칬 f칬r att k칬ra finjusteringsjobbet
- en hostingmilj칬 f칬r att distribuera den finjusterade modellen

## Finjustering i praktiken

F칬ljande resurser erbjuder steg-f칬r-steg handledningar f칬r att guida dig genom ett verkligt exempel med en utvald modell och en noggrant utvald datam칛ngd. F칬r att arbeta igenom dessa handledningar beh칬ver du ett konto hos den specifika leverant칬ren, tillsammans med tillg친ng till relevant modell och datam칛ngder.

| Leverant칬r   | Handledning                                                                                                                                                                   | Beskrivning                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [Hur man finjusterar chattmodeller](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)          | L칛r dig att finjustera en `gpt-35-turbo` f칬r en specifik dom칛n ("receptassistent") genom att f칬rbereda tr칛ningsdata, k칬ra finjusteringsjobbet och anv칛nda den finjusterade modellen f칬r inferens.                                                                                                                                                                                                                                   |
| Azure OpenAI | [GPT 3.5 Turbo finjusteringshandledning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | L칛r dig att finjustera en `gpt-35-turbo-0613` modell **p친 Azure** genom att ta steg f칬r att skapa och ladda upp tr칛ningsdata, k칬ra finjusteringsjobbet. Distribuera och anv칛nd den nya modellen.                                                                                                                                                                                                                                   |
| Hugging Face | [Finjustering av LLMs med Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                           | Denna bloggpost guidar dig genom finjustering av en _칬ppen LLM_ (ex: `CodeLlama 7B`) med hj칛lp av [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteket & [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) med 칬ppna [datam칛ngder](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) p친 Hugging Face. |
|              |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 游뱅 AutoTrain | [Finjustering av LLMs med AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                     | AutoTrain (eller AutoTrain Advanced) 칛r ett Python-bibliotek utvecklat av Hugging Face som m칬jligg칬r finjustering f칬r m친nga olika uppgifter inklusive finjustering av LLM. AutoTrain 칛r en kodfri l칬sning och finjustering kan g칬ras i din egen molnmilj칬, p친 Hugging Face Spaces eller lokalt. Det st칬der b친de ett webbaserat GUI, CLI och tr칛ning via yaml-konfigurationsfiler.                                                                 |

## Uppgift

V칛lj en av handledningarna ovan och g친 igenom den. _Vi kan replikera en version av dessa handledningar i Jupyter Notebooks i detta repo endast som referens. Anv칛nd de ursprungliga k칛llorna direkt f칬r att f친 de senaste versionerna_.

## Bra jobbat! Forts칛tt ditt l칛rande.

Efter att ha avslutat denna lektion, kolla in v친r [Generative AI Learning-samling](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) f칬r att forts칛tta utveckla din kunskap om generativ AI!

Grattis!! Du har avslutat den sista lektionen fr친n v2-serien f칬r denna kurs! Sluta inte l칛ra dig och bygga. \*\*Kolla in [RESURSER](RESOURCES.md?WT.mc_id=academic-105485-koreyst) f칬r en lista 칬ver ytterligare f칬rslag f칬r just detta 칛mne.

V친r v1-serie av lektioner har ocks친 uppdaterats med fler uppgifter och koncept. S친 ta en minut f칬r att fr칛scha upp din kunskap - och v칛nligen [dela dina fr친gor och feedback](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) f칬r att hj칛lpa oss att f칬rb칛ttra dessa lektioner f칬r gemenskapen.

---

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av AI-칬vers칛ttningstj칛nsten [Co-op Translator](https://github.com/Azure/co-op-translator). 츿ven om vi str칛var efter noggrannhet, b칬r du vara medveten om att automatiserade 칬vers칛ttningar kan inneh친lla fel eller felaktigheter. Det ursprungliga dokumentet p친 dess modersm친l b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r eventuella missf칬rst친nd eller feltolkningar som uppst친r vid anv칛ndning av denna 칬vers칛ttning.