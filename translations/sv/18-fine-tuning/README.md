<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-07-09T17:44:01+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "sv"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.sv.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Finjustera din LLM

Att anv√§nda stora spr√•kmodeller f√∂r att bygga generativa AI-applikationer inneb√§r nya utmaningar. En viktig fr√•ga √§r att s√§kerst√§lla svarskvalitet (noggrannhet och relevans) i det inneh√•ll som modellen genererar f√∂r en given anv√§ndarf√∂rfr√•gan. I tidigare lektioner har vi diskuterat tekniker som prompt engineering och retrieval-augmented generation som f√∂rs√∂ker l√∂sa problemet genom att _modifiera promptinmatningen_ till den befintliga modellen.

I dagens lektion tar vi upp en tredje teknik, **finjustering**, som f√∂rs√∂ker hantera utmaningen genom att _tr√§na om modellen sj√§lv_ med ytterligare data. L√•t oss g√• in p√• detaljerna.

## L√§randem√•l

Den h√§r lektionen introducerar begreppet finjustering f√∂r f√∂rtr√§nade spr√•kmodeller, utforskar f√∂rdelar och utmaningar med detta tillv√§gag√•ngss√§tt och ger v√§gledning om n√§r och hur man anv√§nder finjustering f√∂r att f√∂rb√§ttra prestandan hos dina generativa AI-modeller.

I slutet av lektionen ska du kunna svara p√• f√∂ljande fr√•gor:

- Vad √§r finjustering f√∂r spr√•kmodeller?
- N√§r och varf√∂r √§r finjustering anv√§ndbart?
- Hur kan jag finjustera en f√∂rtr√§nad modell?
- Vilka √§r begr√§nsningarna med finjustering?

Redo? D√• k√∂r vi.

## Illustrerad guide

Vill du f√• en √∂verblick √∂ver vad vi kommer att g√• igenom innan vi dyker ner i detaljerna? Kolla in denna illustrerade guide som beskriver l√§randeresan f√∂r denna lektion ‚Äì fr√•n att l√§ra sig k√§rnkoncepten och motivationen f√∂r finjustering, till att f√∂rst√• processen och b√§sta praxis f√∂r att genomf√∂ra finjusteringsuppgiften. Det h√§r √§r ett fascinerande √§mne att utforska, s√• gl√∂m inte att titta p√• [Resurser](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) f√∂r fler l√§nkar som st√∂djer din sj√§lvstyrda l√§randeresa!

![Illustrerad guide till finjustering av spr√•kmodeller](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.sv.png)

## Vad √§r finjustering f√∂r spr√•kmodeller?

Per definition √§r stora spr√•kmodeller _f√∂rtr√§nade_ p√• stora m√§ngder text fr√•n olika k√§llor, inklusive internet. Som vi l√§rt oss i tidigare lektioner beh√∂ver vi tekniker som _prompt engineering_ och _retrieval-augmented generation_ f√∂r att f√∂rb√§ttra kvaliteten p√• modellens svar p√• anv√§ndarens fr√•gor ("prompter").

En popul√§r prompt-engineering-teknik inneb√§r att ge modellen mer v√§gledning om vad som f√∂rv√§ntas i svaret, antingen genom att ge _instruktioner_ (explicit v√§gledning) eller _ge n√•gra exempel_ (implicit v√§gledning). Detta kallas _few-shot learning_ men har tv√• begr√§nsningar:

- Modellens tokenbegr√§nsningar kan begr√§nsa antalet exempel du kan ge och minska effektiviteten.
- Kostnaden f√∂r tokens kan g√∂ra det dyrt att l√§gga till exempel i varje prompt och begr√§nsa flexibiliteten.

Finjustering √§r en vanlig metod inom maskininl√§rningssystem d√§r man tar en f√∂rtr√§nad modell och tr√§nar om den med ny data f√∂r att f√∂rb√§ttra dess prestanda p√• en specifik uppgift. I spr√•kmodellernas sammanhang kan vi finjustera den f√∂rtr√§nade modellen _med en noggrant utvald upps√§ttning exempel f√∂r en viss uppgift eller applikationsdom√§n_ f√∂r att skapa en **anpassad modell** som kan vara mer exakt och relevant f√∂r just den uppgiften eller dom√§nen. En extra f√∂rdel med finjustering √§r att det ocks√• kan minska antalet exempel som beh√∂vs f√∂r few-shot learning ‚Äì vilket minskar tokenanv√§ndning och relaterade kostnader.

## N√§r och varf√∂r b√∂r vi finjustera modeller?

I _detta_ sammanhang, n√§r vi pratar om finjustering, syftar vi p√• **√∂vervakad** finjustering d√§r omtr√§ningen g√∂rs genom att **l√§gga till ny data** som inte ingick i den ursprungliga tr√§ningsdatan. Detta skiljer sig fr√•n en o√∂vervakad finjusteringsmetod d√§r modellen tr√§nas om p√• originaldatan, men med andra hyperparametrar.

Det viktigaste att komma ih√•g √§r att finjustering √§r en avancerad teknik som kr√§ver en viss niv√• av expertis f√∂r att uppn√• √∂nskade resultat. Om det g√∂rs felaktigt kan det leda till att f√∂rb√§ttringarna uteblir, eller till och med f√∂rs√§mra modellens prestanda f√∂r din specifika dom√§n.

S√• innan du l√§r dig "hur" man finjusterar spr√•kmodeller, beh√∂ver du veta "varf√∂r" du ska ta denna v√§g och "n√§r" du ska starta finjusteringsprocessen. B√∂rja med att st√§lla dig sj√§lv dessa fr√•gor:

- **Anv√§ndningsfall**: Vad √§r ditt _anv√§ndningsfall_ f√∂r finjustering? Vilken aspekt av den nuvarande f√∂rtr√§nade modellen vill du f√∂rb√§ttra?
- **Alternativ**: Har du provat _andra tekniker_ f√∂r att uppn√• √∂nskat resultat? Anv√§nd dem f√∂r att skapa en baslinje att j√§mf√∂ra med.
  - Prompt engineering: Prova tekniker som few-shot prompting med exempel p√• relevanta prompt-svar. Utv√§rdera svarens kvalitet.
  - Retrieval Augmented Generation: Prova att f√∂rst√§rka prompts med s√∂kresultat fr√•n din data. Utv√§rdera svarens kvalitet.
- **Kostnader**: Har du identifierat kostnaderna f√∂r finjustering?
  - M√∂jlighet till justering ‚Äì √§r den f√∂rtr√§nade modellen tillg√§nglig f√∂r finjustering?
  - Arbetsinsats ‚Äì f√∂r att f√∂rbereda tr√§ningsdata, utv√§rdera och f√∂rfina modellen.
  - Ber√§kningsresurser ‚Äì f√∂r att k√∂ra finjusteringsjobb och distribuera den finjusterade modellen.
  - Data ‚Äì tillg√•ng till tillr√§ckligt m√•nga kvalitativa exempel f√∂r att finjusteringen ska f√• effekt.
- **F√∂rdelar**: Har du bekr√§ftat f√∂rdelarna med finjustering?
  - Kvalitet ‚Äì presterade den finjusterade modellen b√§ttre √§n baslinjen?
  - Kostnad ‚Äì minskar det tokenanv√§ndningen genom att f√∂renkla prompts?
  - Utbyggbarhet ‚Äì kan du √•teranv√§nda basmodellen f√∂r nya dom√§ner?

Genom att svara p√• dessa fr√•gor b√∂r du kunna avg√∂ra om finjustering √§r r√§tt metod f√∂r ditt anv√§ndningsfall. Idealiskt √§r metoden bara giltig om f√∂rdelarna √∂verv√§ger kostnaderna. N√§r du best√§mt dig f√∂r att g√• vidare √§r det dags att fundera p√• _hur_ du kan finjustera den f√∂rtr√§nade modellen.

Vill du f√• fler insikter om beslutsprocessen? Titta p√• [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Hur kan vi finjustera en f√∂rtr√§nad modell?

F√∂r att finjustera en f√∂rtr√§nad modell beh√∂ver du:

- en f√∂rtr√§nad modell att finjustera
- en dataset att anv√§nda f√∂r finjustering
- en tr√§ningsmilj√∂ f√∂r att k√∂ra finjusteringsjobbet
- en hostingmilj√∂ f√∂r att distribuera den finjusterade modellen

## Finjustering i praktiken

F√∂ljande resurser erbjuder steg-f√∂r-steg-handledning som tar dig igenom ett verkligt exempel med en utvald modell och en noggrant utvald dataset. F√∂r att arbeta med dessa handledningar beh√∂ver du ett konto hos respektive leverant√∂r, samt tillg√•ng till relevant modell och dataset.

| Leverant√∂r  | Handledning                                                                                                                                                                  | Beskrivning                                                                                                                                                                                                                                                                                                                                                                                                                      |
| ----------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI      | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)               | L√§r dig finjustera en `gpt-35-turbo` f√∂r en specifik dom√§n ("receptassistent") genom att f√∂rbereda tr√§ningsdata, k√∂ra finjusteringsjobbet och anv√§nda den finjusterade modellen f√∂r inferens.                                                                                                                                                                                                                                   |
| Azure OpenAI| [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | L√§r dig finjustera en `gpt-35-turbo-0613` modell **p√• Azure** genom att skapa och ladda upp tr√§ningsdata, k√∂ra finjusteringsjobbet samt distribuera och anv√§nda den nya modellen.                                                                                                                                                                                                                                                |
| Hugging Face| [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                            | Denna bloggpost visar hur du finjusterar en _√∂ppen LLM_ (t.ex. `CodeLlama 7B`) med hj√§lp av [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteket och [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) med √∂ppna [datasets](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) p√• Hugging Face. |
|             |                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ü§ó AutoTrain| [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                      | AutoTrain (eller AutoTrain Advanced) √§r ett pythonbibliotek utvecklat av Hugging Face som m√∂jligg√∂r finjustering f√∂r m√•nga olika uppgifter, inklusive LLM-finjustering. AutoTrain √§r en kodfri l√∂sning och finjustering kan g√∂ras i din egen molnmilj√∂, p√• Hugging Face Spaces eller lokalt. Det st√∂der b√•de webbaserat GUI, CLI och tr√§ning via yaml-konfigurationsfiler.                                                                                 |
|             |                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                  |

## Uppgift

V√§lj en av handledningarna ovan och g√• igenom den. _Vi kan komma att replikera en version av dessa handledningar i Jupyter Notebooks i detta repo f√∂r referens. Anv√§nd g√§rna originalk√§llorna direkt f√∂r att f√• de senaste versionerna_.

## Bra jobbat! Forts√§tt din l√§rande.

Efter att ha slutf√∂rt denna lektion, kolla in v√•r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) f√∂r att forts√§tta utveckla din kunskap inom Generativ AI!

Grattis!! Du har avslutat den sista lektionen i v2-serien f√∂r denna kurs! Sluta inte att l√§ra och bygga. \*\*Kolla in [RESOURCES](RESOURCES.md?WT.mc_id=academic-105485-koreyst) sidan f√∂r en lista med fler f√∂rslag just f√∂r detta √§mne.

V√•r v1-serie av lektioner har ocks√• uppdaterats med fler uppgifter och koncept. Ta en minut att fr√§scha upp dina kunskaper ‚Äì och v√§nligen [dela dina fr√•gor och feedback](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) f√∂r att hj√§lpa oss f√∂rb√§ttra dessa lektioner f√∂r communityn.

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, v√§nligen observera att automatiska √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess modersm√•l b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r n√•gra missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.