<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-12-19T19:10:23+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "ml"
}
-->
# നിങ്ങളുടെ ജനറേറ്റീവ് AI ആപ്ലിക്കേഷനുകൾ സുരക്ഷിതമാക്കൽ

[![നിങ്ങളുടെ ജനറേറ്റീവ് AI ആപ്ലിക്കേഷനുകൾ സുരക്ഷിതമാക്കൽ](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.ml.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## പരിചയം

ഈ പാഠം ഉൾക്കൊള്ളുന്നതാണ്:

- AI സിസ്റ്റങ്ങളിലെ സുരക്ഷ.
- AI സിസ്റ്റങ്ങളിലെ സാധാരണ അപകടങ്ങളും ഭീഷണികളും.
- AI സിസ്റ്റങ്ങൾ സുരക്ഷിതമാക്കുന്നതിനുള്ള മാർഗങ്ങളും പരിഗണനകളും.

## പഠന ലക്ഷ്യങ്ങൾ

ഈ പാഠം പൂർത്തിയാക്കിയശേഷം, നിങ്ങൾക്ക് മനസ്സിലാകും:

- AI സിസ്റ്റങ്ങളിലെ ഭീഷണികളും അപകടങ്ങളും.
- AI സിസ്റ്റങ്ങൾ സുരക്ഷിതമാക്കുന്നതിനുള്ള സാധാരണ മാർഗങ്ങളും പ്രാക്ടീസുകളും.
- സുരക്ഷാ പരിശോധന നടപ്പിലാക്കുന്നത് എങ്ങനെ അനിയന്ത്രിത ഫലങ്ങളും ഉപയോക്തൃ വിശ്വാസത്തിന്റെ നഷ്ടവും തടയാമെന്ന്.

## ജനറേറ്റീവ് AI യുടെ സാന്ദർഭ്യത്തിൽ സുരക്ഷ എന്താണ് അർത്ഥം?

കൃത്രിമ ബുദ്ധിമുട്ട് (AI)യും മെഷീൻ ലേണിങ്ങും (ML) നമ്മുടെ ജീവിതത്തെ കൂടുതൽ രൂപപ്പെടുത്തുന്നതിനാൽ, ഉപഭോക്തൃ ഡാറ്റ മാത്രമല്ല, AI സിസ്റ്റങ്ങളും സംരക്ഷിക്കുന്നത് അത്യന്താപേക്ഷിതമാണ്. തെറ്റായ തീരുമാനങ്ങൾ ഗുരുതര ഫലങ്ങൾ ഉണ്ടാക്കുന്ന വ്യവസായങ്ങളിൽ AI/ML ഉയർന്ന മൂല്യമുള്ള തീരുമാനമെടുക്കൽ പ്രക്രിയകളെ പിന്തുണയ്ക്കാൻ ഉപയോഗിക്കുന്നു.

ഇവിടെ പരിഗണിക്കേണ്ട പ്രധാന കാര്യങ്ങൾ:

- **AI/ML ന്റെ സ്വാധീനം**: AI/ML ദൈനംദിന ജീവിതത്തിൽ വലിയ സ്വാധീനം ചെലുത്തുന്നു, അതിനാൽ അവ സംരക്ഷിക്കുന്നത് അനിവാര്യമാണ്.
- **സുരക്ഷാ വെല്ലുവിളികൾ**: AI/ML ന്റെ സ്വാധീനം കൃത്യമായി പരിഗണിച്ച്, ട്രോളുകൾ അല്ലെങ്കിൽ സംഘാടിത സംഘങ്ങൾ നടത്തുന്ന സങ്കീർണ്ണമായ ആക്രമണങ്ങളിൽ നിന്ന് AI അടിസ്ഥാനമാക്കിയ ഉൽപ്പന്നങ്ങളെ സംരക്ഷിക്കേണ്ടതുണ്ട്.
- **ยุทธศาสตร์ പ്രശ്നങ്ങൾ**: ടെക് ഇൻഡസ്ട്രി ദീർഘകാല ഉപഭോക്തൃ സുരക്ഷയും ഡാറ്റ സുരക്ഷയും ഉറപ്പാക്കാൻ മുൻകൈയെടുക്കണം.

കൂടാതെ, മെഷീൻ ലേണിങ്ങ് മോഡലുകൾ ദുഷ്പ്രവർത്തന ഇൻപുട്ടും സാധാരണ അനോമലസ് ഡാറ്റയും വേർതിരിക്കാൻ സാധിക്കാറില്ല. പരിശീലന ഡാറ്റയുടെ വലിയൊരു ഭാഗം നിയന്ത്രിക്കപ്പെടാത്ത, പബ്ലിക് ഡാറ്റാസെറ്റുകളിൽ നിന്നാണ്, അവ 3-ആം കക്ഷി സംഭാവനകൾക്ക് തുറന്നതാണ്. ആക്രമകർ ഡാറ്റാസെറ്റുകൾ തകർക്കേണ്ടതില്ല, അവയിൽ സംഭാവന ചെയ്യാം. കാലക്രമേണ, കുറഞ്ഞ വിശ്വാസമുള്ള ദുഷ്പ്രവർത്തന ഡാറ്റ ഉയർന്ന വിശ്വാസമുള്ള വിശ്വസനീയ ഡാറ്റയായി മാറും, ഡാറ്റയുടെ ഘടന/ഫോർമാറ്റ് ശരിയായിരിക്കുകയാണെങ്കിൽ.

അതിനാൽ, നിങ്ങളുടെ മോഡലുകൾ തീരുമാനങ്ങൾ എടുക്കാൻ ഉപയോഗിക്കുന്ന ഡാറ്റാ സ്റ്റോറുകളുടെ അഖണ്ഡതയും സംരക്ഷണവും ഉറപ്പാക്കുന്നത് അത്യന്താപേക്ഷിതമാണ്.

## AI യുടെ ഭീഷണികളും അപകടങ്ങളും മനസ്സിലാക്കൽ

AI യും ബന്ധപ്പെട്ട സിസ്റ്റങ്ങളും സംബന്ധിച്ച്, ഡാറ്റാ വിഷബാധ (data poisoning) ഇന്ന് ഏറ്റവും പ്രധാനപ്പെട്ട സുരക്ഷാ ഭീഷണിയാണ്. ഡാറ്റാ വിഷബാധ എന്നത് ആരോ ഉദ്ദേശപൂർവ്വം AI പരിശീലനത്തിനായി ഉപയോഗിക്കുന്ന വിവരങ്ങൾ മാറ്റിമറിച്ച് മോഡൽ തെറ്റായ തീരുമാനങ്ങൾ എടുക്കാൻ കാരണമാകുന്നത് ആണ്. ഇത് സ്റ്റാൻഡേർഡൈസ്ഡ് കണ്ടെത്തൽ, പരിഹാര മാർഗങ്ങൾ ഇല്ലാത്തതും, വിശ്വസനീയമല്ലാത്ത പബ്ലിക് ഡാറ്റാസെറ്റുകളിൽ ആശ്രയിക്കുന്നതും മൂലമാണ്. ഡാറ്റയുടെ അഖണ്ഡത നിലനിർത്താനും തെറ്റായ പരിശീലനം തടയാനും, നിങ്ങളുടെ ഡാറ്റയുടെ ഉറവിടവും വംശപരമ്പരയും ട്രാക്ക് ചെയ്യുന്നത് അത്യന്താപേക്ഷിതമാണ്. അല്ലെങ്കിൽ, പഴയ പ്രയോഗം "കുഴപ്പമുള്ളത് കുഴപ്പമായാണ് പുറത്തുവരുന്നത്" ശരിയാകും, മോഡൽ പ്രകടനം ബാധിക്കും.

ഡാറ്റാ വിഷബാധ നിങ്ങളുടെ മോഡലുകളെ എങ്ങനെ ബാധിക്കാമെന്ന് ഉദാഹരണങ്ങൾ:

1. **ലേബൽ ഫ്ലിപ്പിംഗ്**: ബൈനറി ക്ലാസിഫിക്കേഷൻ ടാസ്കിൽ, ഒരു ആക്രമകൻ ചെറിയ ഒരു പരിശീലന ഡാറ്റ സെറ്റിന്റെ ലേബലുകൾ ഉദ്ദേശപൂർവ്വം മറിക്കുന്നു. ഉദാഹരണത്തിന്, സാധാരണ സാമ്പിളുകൾ ദുഷ്പ്രവർത്തനമായി ലേബൽ ചെയ്യുന്നു, മോഡൽ തെറ്റായ ബന്ധങ്ങൾ പഠിക്കുന്നു.\
   **ഉദാഹരണം**: സ്പാം ഫിൽട്ടർ സാധുവായ ഇമെയിലുകൾ സ്പാമായി തെറ്റായി തിരിച്ചറിയുന്നു.
2. **ഫീച്ചർ വിഷബാധ**: ഒരു ആക്രമകൻ പരിശീലന ഡാറ്റയിലെ ഫീച്ചറുകൾ സൂക്ഷ്മമായി മാറ്റി മോഡലിനെ തെറ്റിദ്ധരിപ്പിക്കുന്നു.\
   **ഉദാഹരണം**: ഉൽപ്പന്ന വിവരണങ്ങളിൽ അനാവശ്യ കീവേഡുകൾ ചേർത്ത് ശുപാർശാ സിസ്റ്റങ്ങൾ നിയന്ത്രിക്കുന്നു.
3. **ഡാറ്റ ഇൻജക്ഷൻ**: മോഡലിന്റെ പെരുമാറ്റം സ്വാധീനിക്കാൻ ദുഷ്പ്രവർത്തന ഡാറ്റ പരിശീലന സെറ്റിൽ ചേർക്കുന്നു.\
   **ഉദാഹരണം**: വ്യാജ ഉപയോക്തൃ അവലോകനങ്ങൾ ചേർത്ത് സന്റിമെന്റ് അനാലിസിസ് ഫലങ്ങൾ വക്രമാക്കുന്നു.
4. **ബാക്ക്‌ഡോർ ആക്രമണങ്ങൾ**: ഒരു ആക്രമകൻ പരിശീലന ഡാറ്റയിൽ മറഞ്ഞ പാറ്റേൺ (ബാക്ക്‌ഡോർ) ചേർക്കുന്നു. മോഡൽ ഈ പാറ്റേൺ തിരിച്ചറിയാൻ പഠിച്ച്, ട്രിഗർ ചെയ്താൽ ദുഷ്പ്രവർത്തനം നടത്തുന്നു.\
   **ഉദാഹരണം**: ബാക്ക്‌ഡോർ ചിത്രങ്ങളോടെ പരിശീലിപ്പിച്ച മുഖം തിരിച്ചറിയൽ സിസ്റ്റം ഒരു വ്യക്തിയെ തെറ്റായി തിരിച്ചറിയുന്നു.

MITRE കോർപ്പറേഷൻ [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) എന്നത് സൃഷ്ടിച്ചിട്ടുണ്ട്, AI സിസ്റ്റങ്ങളിലെ യഥാർത്ഥ ലോക ആക്രമണങ്ങളിൽ എതിരാളികൾ ഉപയോഗിക്കുന്ന തന്ത്രങ്ങളും സാങ്കേതിക വിദ്യകളും ഉൾക്കൊള്ളുന്ന അറിവ് അടിസ്ഥാനമാണ്.

> AI-സജ്ജമായ സിസ്റ്റങ്ങളിൽ വളരുന്ന സുരക്ഷാ ദുർബലതകൾ ഉണ്ട്, കാരണം AI ഉൾപ്പെടുത്തൽ നിലവിലുള്ള സിസ്റ്റങ്ങളുടെ ആക്രമണ വിസ്തീർണ്ണം പരമ്പരാഗത സൈബർ ആക്രമണങ്ങളെക്കാൾ കൂടുതലാക്കുന്നു. ഈ പ്രത്യേകവും വളരുന്ന ദുർബലതകളെക്കുറിച്ച് ബോധവൽക്കരണം ഉയർത്താൻ ATLAS വികസിപ്പിച്ചു, കാരണം ആഗോള സമൂഹം വിവിധ സിസ്റ്റങ്ങളിൽ AI ഉൾപ്പെടുത്തുന്നു. ATLAS MITRE ATT&CK® ഫ്രെയിംവർക്ക് അനുകരിച്ചാണ് രൂപകൽപ്പന ചെയ്തിരിക്കുന്നത്, അതിന്റെ തന്ത്രങ്ങളും സാങ്കേതിക വിദ്യകളും ATT&CK-ലെവിടെ ഉള്ളവയ്ക്ക് അനുബന്ധമാണ്.

പരമ്പരാഗത സൈബർസുരക്ഷയിൽ വികസിത ഭീഷണി അനുകരണ സന്നാഹങ്ങൾ രൂപകൽപ്പന ചെയ്യാൻ വ്യാപകമായി ഉപയോഗിക്കുന്ന MITRE ATT&CK® ഫ്രെയിംവർക്ക് പോലെയാണ് ATLAS, ഉയർന്ന ഭീഷണികളെ മനസ്സിലാക്കാനും പ്രതിരോധിക്കാൻ സഹായിക്കുന്ന TTPs (തന്ത്രങ്ങൾ, സാങ്കേതിക വിദ്യകൾ, പ്രക്രിയകൾ) എളുപ്പത്തിൽ തിരയാവുന്നതും നൽകുന്നു.

കൂടാതെ, Open Web Application Security Project (OWASP) LLMs ഉപയോഗിക്കുന്ന ആപ്ലിക്കേഷനുകളിൽ കണ്ടെത്തിയ ഏറ്റവും ഗുരുതരമായ ദുർബലതകളുടെ "[ടോപ്പ് 10 ലിസ്റ്റ്](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" സൃഷ്ടിച്ചിട്ടുണ്ട്. ഈ ലിസ്റ്റ് മുൻപിൽ പറഞ്ഞ ഡാറ്റാ വിഷബാധ പോലുള്ള ഭീഷണികളുടെ അപകടങ്ങൾ ഉൾപ്പെടെ താഴെപ്പറയുന്നവയെക്കുറിച്ചും ശ്രദ്ധ കേന്ദ്രീകരിക്കുന്നു:

- **പ്രോംപ്റ്റ് ഇൻജക്ഷൻ**: ആക്രമകർ സൂക്ഷ്മമായി രൂപകൽപ്പന ചെയ്ത ഇൻപുട്ടുകൾ വഴി ഒരു വലിയ ഭാഷാ മോഡലിനെ (LLM) നിയന്ത്രിച്ച്, അതിന്റെ ഉദ്ദേശിച്ച പെരുമാറ്റത്തിന് പുറത്തേക്ക് പ്രവർത്തിപ്പിക്കുന്ന സാങ്കേതിക വിദ്യ.
- **സപ്ലൈ ചെയിൻ ദുർബലതകൾ**: LLM ഉപയോഗിക്കുന്ന ആപ്ലിക്കേഷനുകളുടെ ഘടകങ്ങളും സോഫ്റ്റ്‌വെയറുകളും, ഉദാഹരണത്തിന് Python മോഡ്യൂളുകൾ അല്ലെങ്കിൽ ബാഹ്യ ഡാറ്റാസെറ്റുകൾ, തന്നെ തകർക്കപ്പെടാം, ഇത് അനിയന്ത്രിത ഫലങ്ങൾ, പരിച്ഛേദങ്ങൾ, അടിസ്ഥാന ഘടനയിലെ ദുർബലതകൾ എന്നിവക്ക് കാരണമാകാം.
- **അധിക ആശ്രയം**: LLMs പിഴവുകൾ വരുത്താനും ഹല്യൂസിനേഷൻ നടത്താനും സാധ്യതയുള്ളവയാണ്, തെറ്റായ അല്ലെങ്കിൽ അപകടകരമായ ഫലങ്ങൾ നൽകാറുണ്ട്. രേഖപ്പെടുത്തിയ പല സാഹചര്യങ്ങളിലും, ആളുകൾ ഫലങ്ങളെ യഥാർത്ഥമായി സ്വീകരിച്ച് അനിഷ്ടമായ ഫലങ്ങൾ ഉണ്ടാക്കിയിട്ടുണ്ട്.

Microsoft ക്ലൗഡ് അഡ്വക്കേറ്റ് റോഡ് ട്രെന്റ് എഴുതിയ [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst) എന്ന സൗജന്യ ഇബുക്ക് ഈ ഭീഷണികളും മറ്റ് ഉയർന്ന ഭീഷണികളും വിശദമായി പരിശോധിച്ച്, അവയെ എങ്ങനെ നേരിടാമെന്ന് വ്യാപക മാർഗ്ഗനിർദ്ദേശം നൽകുന്നു.

## AI സിസ്റ്റങ്ങൾക്കും LLMs-ക്കും സുരക്ഷാ പരിശോധന

കൃത്രിമ ബുദ്ധിമുട്ട് (AI) വ്യത്യസ്ത മേഖലകളിലും വ്യവസായങ്ങളിലും മാറ്റങ്ങൾ കൊണ്ടുവരുന്നു, സമൂഹത്തിന് പുതിയ സാധ്യതകളും ലാഭങ്ങളും നൽകുന്നു. എന്നാൽ, AI ഡാറ്റാ സ്വകാര്യത, പക്ഷപാതം, വിശദീകരണക്ഷമതയുടെ അഭാവം, ദുരുപയോഗ സാധ്യത തുടങ്ങിയ വലിയ വെല്ലുവിളികളും അപകടങ്ങളും ഉണ്ട്. അതിനാൽ, AI സിസ്റ്റങ്ങൾ സുരക്ഷിതവും ഉത്തരവാദിത്വമുള്ളതുമായിരിക്കണം, അതായത് നൈതികവും നിയമപരവുമായ മാനദണ്ഡങ്ങൾ പാലിക്കുകയും ഉപയോക്താക്കളും പങ്കാളികളും വിശ്വസിക്കാവുന്നതുമായിരിക്കണം.

സുരക്ഷാ പരിശോധന AI സിസ്റ്റം അല്ലെങ്കിൽ LLM-ന്റെ സുരക്ഷ വിലയിരുത്തുന്ന പ്രക്രിയയാണ്, അവയുടെ ദുർബലതകൾ കണ്ടെത്തുകയും ഉപയോഗപ്പെടുത്തുകയും ചെയ്യുന്നു. ഇത് വികസിപ്പിച്ചവരും, ഉപയോക്താക്കളും, മൂന്നാം കക്ഷി ഓഡിറ്റർമാരും നടത്താം, പരിശോധനയുടെ ഉദ്ദേശ്യവും പരിധിയും അനുസരിച്ച്. AI സിസ്റ്റങ്ങൾക്കും LLMs-ക്കും സാധാരണയായി ഉപയോഗിക്കുന്ന ചില സുരക്ഷാ പരിശോധനാ മാർഗങ്ങൾ:

- **ഡാറ്റാ ശുദ്ധീകരണം**: AI സിസ്റ്റം അല്ലെങ്കിൽ LLM-ന്റെ പരിശീലന ഡാറ്റയിലും ഇൻപുട്ടിലും നിന്നുള്ള സങ്കീർണ്ണമായ അല്ലെങ്കിൽ സ്വകാര്യ വിവരങ്ങൾ നീക്കം ചെയ്യുകയോ അനാമകരീകരിക്കുകയോ ചെയ്യുന്നത്. ഡാറ്റാ ശുദ്ധീകരണം ഡാറ്റാ ചോർച്ചയും ദുഷ്പ്രവർത്തന നിയന്ത്രണവും തടയാൻ സഹായിക്കുന്നു.
- **എതിരാളി പരിശോധന**: AI സിസ്റ്റം അല്ലെങ്കിൽ LLM-ന്റെ ഇൻപുട്ടിലും ഔട്ട്പുട്ടിലും എതിരാളി ഉദാഹരണങ്ങൾ സൃഷ്ടിച്ച് പ്രയോഗിച്ച് അതിന്റെ ദൃഢതയും പ്രതിരോധ ശേഷിയും വിലയിരുത്തൽ. ഇത് ആക്രമണങ്ങളിൽ ഉപയോഗിക്കാവുന്ന ദുർബലതകൾ കണ്ടെത്താനും പരിഹരിക്കാനും സഹായിക്കുന്നു.
- **മോഡൽ പരിശോധന**: AI സിസ്റ്റം അല്ലെങ്കിൽ LLM-ന്റെ മോഡൽ പാരാമീറ്ററുകളും ഘടനയും ശരിയാണോ എന്ന് പരിശോധിക്കൽ. മോഡൽ മോഷണം തടയാനും മോഡൽ സംരക്ഷിക്കാനും ഇത് സഹായിക്കുന്നു.
- **ഔട്ട്പുട്ട് പരിശോധന**: AI സിസ്റ്റം അല്ലെങ്കിൽ LLM-ന്റെ ഔട്ട്പുട്ടിന്റെ ഗുണനിലവാരവും വിശ്വാസ്യതയും പരിശോധിക്കൽ. ദുഷ്പ്രവർത്തന നിയന്ത്രണത്തിനും തെറ്റായ ഫലങ്ങൾ തിരുത്തുന്നതിനും ഇത് സഹായിക്കുന്നു.

AI സിസ്റ്റങ്ങളിൽ മുൻനിരയിൽ ഉള്ള OpenAI അവരുടെ റെഡ് ടീമിംഗ് നെറ്റ്‌വർക്ക് സംരംഭത്തിന്റെ ഭാഗമായുള്ള _സുരക്ഷാ മൂല്യനിർണയങ്ങൾ_ സജ്ജമാക്കിയിട്ടുണ്ട്, AI സുരക്ഷയ്ക്ക് സംഭാവന നൽകാൻ AI സിസ്റ്റങ്ങളുടെ ഔട്ട്പുട്ട് പരിശോധിക്കുന്നതിനായി.

> മൂല്യനിർണയങ്ങൾ ലളിതമായ Q&A പരീക്ഷണങ്ങളിൽ നിന്നു കൂടുതൽ സങ്കീർണ്ണമായ സിമുലേഷനുകളിലേക്കും വ്യാപിച്ചിരിക്കുന്നു. ഉദാഹരണമായി, OpenAI വികസിപ്പിച്ച ചില AI പെരുമാറ്റങ്ങൾ വിലയിരുത്താനുള്ള സാമ്പിൾ മൂല്യനിർണയങ്ങൾ:

#### പ്രേരണ

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): ഒരു AI സിസ്റ്റം മറ്റൊരു AI സിസ്റ്റത്തിനെ രഹസ്യവാക്ക് പറയാൻ എത്രത്തോളം തന്ത്രം പ്രയോഗിക്കാം?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): ഒരു AI സിസ്റ്റം മറ്റൊരു AI സിസ്റ്റത്തിനെ പണം സംഭാവന ചെയ്യാൻ എത്രത്തോളം പ്രേരിപ്പിക്കാം?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): ഒരു AI സിസ്റ്റം മറ്റൊരു AI സിസ്റ്റത്തിന്റെ രാഷ്ട്രീയ പ്രമേയ പിന്തുണയെ എത്രത്തോളം സ്വാധീനിക്കാം?

#### സ്റ്റെഗനോഗ്രഫി (മറഞ്ഞ സന്ദേശങ്ങൾ)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): ഒരു AI സിസ്റ്റം മറ്റൊരു AI സിസ്റ്റത്തിനെ പിടിക്കപ്പെടാതെ രഹസ്യ സന്ദേശങ്ങൾ കൈമാറാൻ എത്രത്തോളം കഴിയും?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): രഹസ്യ സന്ദേശങ്ങൾ മറയ്ക്കാൻ സന്ദേശങ്ങൾ സംക്ഷിപ്തമാക്കാനും പുനഃസംയോജിപ്പിക്കാനും ഒരു AI സിസ്റ്റം എത്രത്തോളം കഴിയും?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): നേരിട്ട് ആശയവിനിമയം ഇല്ലാതെ ഒരു AI സിസ്റ്റം മറ്റൊരു AI സിസ്റ്റത്തോടൊപ്പം എത്രത്തോളം ഏകോപനം നടത്താം?

### AI സുരക്ഷ

AI സിസ്റ്റങ്ങളെ ദുഷ്പ്രവർത്തന ആക്രമണങ്ങളിൽ നിന്ന്, ദുരുപയോഗത്തിൽ നിന്ന്, അല്ലെങ്കിൽ അനിഷ്ട ഫലങ്ങളിൽ നിന്ന് സംരക്ഷിക്കേണ്ടത് അത്യന്താപേക്ഷിതമാണ്. ഇതിൽ ഉൾപ്പെടുന്നത് AI സിസ്റ്റങ്ങളുടെ സുരക്ഷ, വിശ്വാസ്യത, വിശ്വാസ്യത ഉറപ്പാക്കാനുള്ള നടപടികൾ ആണ്, ഉദാഹരണത്തിന്:

- AI മോഡലുകൾ പരിശീലിപ്പിക്കാനും പ്രവർത്തിപ്പിക്കാനും ഉപയോഗിക്കുന്ന ഡാറ്റയും ആൽഗോരിതങ്ങളും സുരക്ഷിതമാക്കൽ
- അനധികൃത പ്രവേശനം, നിയന്ത്രണം, നാശം തടയൽ
- AI സിസ്റ്റങ്ങളിലെ പക്ഷപാതം, വിവേചനം, നൈതിക പ്രശ്നങ്ങൾ കണ്ടെത്തുകയും പരിഹരിക്കുകയും ചെയ്യൽ
- AI തീരുമാനങ്ങളും പ്രവർത്തനങ്ങളും ഉത്തരവാദിത്വം, പാരദർശിത്വം, വിശദീകരണക്ഷമത ഉറപ്പാക്കൽ
- AI സിസ്റ്റങ്ങളുടെ ലക്ഷ്യങ്ങളും മൂല്യങ്ങളും മനുഷ്യരും സമൂഹവും ഉള്ളവയുമായി പൊരുത്തപ്പെടുത്തൽ

AI സുരക്ഷ AI സിസ്റ്റങ്ങളുടെയും ഡാറ്റയുടെയും അഖണ്ഡത, ലഭ്യത, രഹസ്യത ഉറപ്പാക്കാൻ പ്രധാനമാണ്. AI സുരക്ഷയുടെ ചില വെല്ലുവിളികളും അവസരങ്ങളും:

- അവസരം: ഭീഷണികൾ തിരിച്ചറിയാനും പ്രതികരണ സമയം മെച്ചപ്പെടുത്താനും AI സൈബർസുരക്ഷാ തന്ത്രങ്ങളിൽ ഉൾപ്പെടുത്താം. ഫിഷിംഗ്, മാൽവെയർ, റാൻസംവെയർ പോലുള്ള സൈബർ ആക്രമണങ്ങൾ കണ്ടെത്താനും തടയാനും AI സഹായിക്കും.
- വെല്ലുവിളി: AI ആക്രമകർക്ക് വ്യാജ അല്ലെങ്കിൽ തെറ്റിദ്ധരിപ്പിക്കുന്ന ഉള്ളടക്കം സൃഷ്ടിക്കാൻ, ഉപയോക്താക്കളായി വേഷം മാറാൻ, AI സിസ്റ്റങ്ങളിലെ ദുർബലതകൾ ഉപയോഗപ്പെടുത്താൻ സഹായിക്കും. അതിനാൽ, AI വികസിപ്പിക്കുന്നവർക്ക് ദുരുപയോഗത്തിനെതിരെ ദൃഢവും പ്രതിരോധശേഷിയുള്ളവുമായ സിസ്റ്റങ്ങൾ രൂപകൽപ്പന ചെയ്യേണ്ടത് പ്രത്യേക ഉത്തരവാദിത്വമാണ്.

### ഡാറ്റ സംരക്ഷണം

LLMs ഉപയോഗിക്കുന്ന ഡാറ്റയുടെ സ്വകാര്യതക്കും സുരക്ഷക്കും അപകടങ്ങൾ ഉണ്ടാകാം. ഉദാഹരണത്തിന്, LLMs പരിശീലന ഡാറ്റയിൽ നിന്നുള്ള വ്യക്തിഗത പേരുകൾ, വിലാസങ്ങൾ, പാസ്‌വേഡുകൾ, ക്രെഡിറ്റ് കാർഡ് നമ്പറുകൾ പോലുള്ള സങ്കീർണ്ണ വിവരങ്ങൾ ഓർമ്മിച്ച് ചോർത്തേക്കാം. അവ ദുഷ്പ്രവർത്തകരാൽ നിയന്ത്രിക്കപ്പെടാനും ആക്രമിക്കപ്പെടാനും സാധ്യതയുണ്ട്. അതിനാൽ, ഈ അപകടങ്ങളെക്കുറിച്ച് ജാഗ്രത പുലർത്തുകയും LLMs ഉപയോഗിക്കുന്ന ഡാറ്റ സംരക്ഷിക്കാൻ അനുയോജ്യമായ നടപടികൾ സ്വീകരിക്കേണ്ടതാണ്. LLMs ഉപയോഗിക്കുന്ന ഡാറ്റ സംരക്ഷിക്കാൻ നിങ്ങൾ സ്വീകരിക്കാവുന്ന ചില നടപടികൾ:

- **LLMs-നോട് പങ്കുവെക്കുന്ന ഡാറ്റയുടെ അളവും തരം നിയന്ത്രിക്കുക**: ആവശ്യമായതും പ്രസക്തവുമായ ഡാറ്റ മാത്രമേ പങ്കുവെക്കരുത്, സങ്കീർണ്ണമായ, രഹസ്യമായ, വ്യക്തിഗതമായ ഡാറ്റ പങ്കുവെക്കുന്നത് ഒഴിവാക്കുക. ഉപയോക്താക്കൾ പങ്കുവെക്കുന്ന ഡാറ്റ അനാമകരീകരിക്കുകയോ എൻക്രിപ്റ്റ് ചെയ്യുകയോ ചെയ്യണം, ഉദാഹരണത്തിന് തിരിച്ചറിയൽ വിവരങ്ങൾ നീക്കം ചെയ്യുകയോ മറയ്ക്കുകയോ ചെയ്യുക, സുരക്ഷിത ആശയവിനിമയ ചാനലുകൾ ഉപയോഗിക്കുക.
- **LLMs സൃഷ്ടിക്കുന്ന ഡാറ്റ പരിശോധിക്കുക**: LLMs സൃഷ്ടിക്കുന്ന ഔട്ട്പുട്ടിന്റെ കൃത്യതയും ഗുണനിലവാരവും പരിശോധിച്ച് അവ അനാവശ്യമായ അല്ലെങ്കിൽ അപ്രാസംഗികമായ വിവരങ്ങൾ ഉൾക്കൊള്ളുന്നില്ലെന്ന് ഉറപ്പാക്കുക.
- **ഡാറ്റാ ലംഘനങ്ങളോ സംഭവങ്ങളോ റിപ്പോർട്ട് ചെയ്യുകയും അലർട്ട് നൽകുകയും ചെയ്യുക**: LLMs-ൽ നിന്ന് സംശയാസ്പദമായ അല്ലെങ്കിൽ അസാധാരണ പ്രവർത്തനങ്ങൾ, ഉദാഹരണത്തിന് പ്രസക്തമല്ലാത്ത, തെറ്റായ, അപമാനകരമായ, ഹാനികരമായ വാചകങ്ങൾ സൃഷ്ടിക്കുന്നത് ശ്രദ്ധിക്കുക. ഇത് ഡാറ്റാ ലംഘനമോ സുരക്ഷാ സംഭവമോ ആകാം.

ഡാറ്റാ സുരക്ഷ, ഗവർണൻസ്, പാലനങ്ങൾ മൾട്ടി-ക്ലൗഡ് പരിസ്ഥിതിയിൽ ഡാറ്റയും AI യും പ്രയോജനപ്പെടുത്താൻ ആഗ്രഹിക്കുന്ന ഏത് സംഘടനയ്ക്കും അത്യന്താപേക്ഷിതമാണ്. നിങ്ങളുടെ എല്ലാ ഡാറ്റയും സുരക്ഷിതവും നിയന്ത്രിതവുമാക്കുന്നത് സങ്കീർണ്ണവും ബഹുമുഖവുമാണ്. വിവിധ തരത്തിലുള്ള ഡാറ്റ (സംഘടിതം, അസംഘടിതം, AI സൃഷ്ടിച്ച ഡാറ്റ) വിവിധ സ്ഥലങ്ങളിൽ മൾട്ടി-ക്ലൗഡുകളിൽ സുരക്ഷിതവും നിയന്ത്രിതവുമാക്കണം, നിലവിലുള്ളതും ഭാവിയിലെ ഡാറ്റാ സുരക്ഷ, ഗവർണൻസ്, AI നിയമങ്ങളും പരിഗണിക്കണം. നിങ്ങളുടെ ഡാറ്റ സംരക്ഷിക്കാൻ ചില മികച്ച പ്രാക്ടീസുകളും മുൻകരുതലുകളും സ്വീകരിക്കണം, ഉദാഹരണത്തിന്:

- ഡാറ്റ സംരക്ഷണവും സ്വകാര്യതാ സവിശേഷതകളും നൽകുന്ന ക്ലൗഡ് സേവനങ്ങൾ അല്ലെങ്കിൽ പ്ലാറ്റ്ഫോമുകൾ ഉപയോഗിക്കുക.
- നിങ്ങളുടെ ഡാറ്റയിൽ പിശകുകൾ, അസംഘടിതത്വം, അനോമലികൾ പരിശോധിക്കാൻ ഡാറ്റ ഗുണനിലവാരവും പരിശോധനാ ഉപകരണങ്ങളും ഉപയോഗിക്കുക.
- നിങ്ങളുടെ ഡാറ്റ ഉത്തരവാദിത്വപരവും പാരദർശകവുമായ രീതിയിൽ ഉപയോഗിക്കുന്നതിന് ഡാറ്റ ഗവർണൻസ്, നൈതിക ചട്ടക്കൂടുകൾ പാലിക്കുക.

### യഥാർത്ഥ ലോക ഭീഷണികൾ അനുകരിക്കൽ - AI റെഡ് ടീമിംഗ്
Translation for chunk 2 of 'README.md' skipped due to timeout.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**അസൂയാപത്രം**:  
ഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖയാണ് പ്രാമാണികമായ ഉറവിടം എന്ന് പരിഗണിക്കേണ്ടതാണ്. നിർണായക വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനത്തിന്റെ ഉപയോഗത്തിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ തെറ്റായ വ്യാഖ്യാനങ്ങൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->