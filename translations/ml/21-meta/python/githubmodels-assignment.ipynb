{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# മെറ്റാ ഫാമിലി മോഡലുകളുമായി നിർമ്മാണം\n",
    "\n",
    "## പരിചയം\n",
    "\n",
    "ഈ പാഠത്തിൽ ഉൾപ്പെടുന്നത്:\n",
    "\n",
    "- രണ്ട് പ്രധാന മെറ്റാ ഫാമിലി മോഡലുകൾ - ല്ലാമ 3.1, ല്ലാമ 3.2 എന്നിവയുടെ അന്വേഷണവും\n",
    "- ഓരോ മോഡലിന്റെയും ഉപയോഗ കേസുകളും സാഹചര്യങ്ങളും മനസ്സിലാക്കൽ\n",
    "- ഓരോ മോഡലിന്റെയും പ്രത്യേകതകൾ കാണിക്കുന്ന കോഡ് സാമ്പിൾ\n",
    "\n",
    "## മെറ്റാ ഫാമിലി മോഡലുകൾ\n",
    "\n",
    "ഈ പാഠത്തിൽ, മെറ്റാ ഫാമിലി അല്ലെങ്കിൽ \"ല്ലാമ ഹെർഡ്\" എന്നറിയപ്പെടുന്ന 2 മോഡലുകൾ - ല്ലാമ 3.1, ല്ലാമ 3.2 എന്നിവയെക്കുറിച്ച് അന്വേഷിക്കും\n",
    "\n",
    "ഈ മോഡലുകൾ വ്യത്യസ്ത വകഭേദങ്ങളിലായി ലഭ്യമാണ്, കൂടാതെ ഗിത്തബ് മോഡൽ മാർക്കറ്റ്പ്ലേസിൽ ലഭ്യമാണ്. ഗിത്തബ് മോഡലുകൾ ഉപയോഗിച്ച് [AI മോഡലുകളുമായി പ്രോട്ടോടൈപ്പ് ചെയ്യുന്നതിനെക്കുറിച്ച്](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) കൂടുതൽ വിവരങ്ങൾ ഇവിടെ കാണാം.\n",
    "\n",
    "മോഡൽ വകഭേദങ്ങൾ:\n",
    "- ല്ലാമ 3.1 - 70B ഇൻസ്ട്രക്റ്റ്\n",
    "- ല്ലാമ 3.1 - 405B ഇൻസ്ട്രക്റ്റ്\n",
    "- ല്ലാമ 3.2 - 11B വിഷൻ ഇൻസ്ട്രക്റ്റ്\n",
    "- ല്ലാമ 3.2 - 90B വിഷൻ ഇൻസ്ട്രക്റ്റ്\n",
    "\n",
    "*കുറിപ്പ്: ല്ലാമ 3 ഗിത്തബ് മോഡലുകളിൽ ലഭ്യമാണ്, പക്ഷേ ഈ പാഠത്തിൽ ഉൾപ്പെടുത്തുന്നില്ല*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "405 ബില്യൺ പാരാമീറ്ററുകളോടെ, Llama 3.1 ഓപ്പൺ സോഴ്‌സ് LLM വിഭാഗത്തിൽപ്പെടുന്നു.\n",
    "\n",
    "മോഡ് മുൻപ് പുറത്തിറങ്ങിയ Llama 3 ന്റെ അപ്ഗ്രേഡാണ്, ഇത് നൽകുന്നത്:\n",
    "\n",
    "- വലിയ കോൺടെക്സ്റ്റ് വിൻഡോ - 8k ടോക്കൺസിനേക്കാൾ 128k ടോക്കൺസ്\n",
    "- വലിയ മാക്സ് ഔട്ട്പുട്ട് ടോക്കൺസ് - 2048 നേക്കാൾ 4096\n",
    "- മെച്ചപ്പെട്ട ബഹുഭാഷാ പിന്തുണ - പരിശീലന ടോക്കൺസിന്റെ വർദ്ധനവിന്റെ ഫലമായി\n",
    "\n",
    "ഇവ Llama 3.1 നു GenAI ആപ്ലിക്കേഷനുകൾ നിർമ്മിക്കുമ്പോൾ കൂടുതൽ സങ്കീർണ്ണമായ ഉപയോഗകേസുകൾ കൈകാര്യം ചെയ്യാൻ സഹായിക്കുന്നു, അതിൽ ഉൾപ്പെടുന്നു:\n",
    "- നേറ്റീവ് ഫംഗ്ഷൻ കോൾ ചെയ്യൽ - LLM പ്രവാഹത്തിന് പുറത്തുള്ള ബാഹ്യ ടൂളുകളും ഫംഗ്ഷനുകളും കോൾ ചെയ്യാനുള്ള കഴിവ്\n",
    "- മെച്ചപ്പെട്ട RAG പ്രകടനം - ഉയർന്ന കോൺടെക്സ്റ്റ് വിൻഡോയുടെ ഫലമായി\n",
    "- സിന്തറ്റിക് ഡാറ്റ ജനറേഷൻ - ഫൈൻ-ട്യൂണിങ്ങ് പോലുള്ള ജോലികൾക്കായി ഫലപ്രദമായ ഡാറ്റ സൃഷ്ടിക്കാനുള്ള കഴിവ്\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### നേറ്റീവ് ഫംഗ്ഷൻ കോളിംഗ്\n",
    "\n",
    "Llama 3.1 ഫംഗ്ഷൻ അല്ലെങ്കിൽ ടൂൾ കോളുകൾ കൂടുതൽ ഫലപ്രദമായി നടത്താൻ ഫൈൻ-ട്യൂൺ ചെയ്തിട്ടുണ്ട്. ഉപയോക്താവിന്റെ പ്രോംപ്റ്റ് അടിസ്ഥാനമാക്കി മോഡൽ ഉപയോഗിക്കേണ്ടതായ ടൂളുകൾ തിരിച്ചറിയാൻ രണ്ട് ഇൻബിൽറ്റ് ടൂളുകളും ഇതിൽ ഉൾപ്പെടുത്തിയിട്ടുണ്ട്. ഈ ടൂളുകൾ:\n",
    "\n",
    "- **Brave Search** - വെബ് സെർച്ച് നടത്തിക്കൊണ്ട് കാലാവസ്ഥ പോലുള്ള പുതുക്കിയ വിവരങ്ങൾ ലഭിക്കാൻ ഉപയോഗിക്കാം\n",
    "- **Wolfram Alpha** - കൂടുതൽ സങ്കീർണ്ണമായ ഗണിത കണക്കുകൾക്കായി ഉപയോഗിക്കാം, അതിനാൽ നിങ്ങളുടെ സ്വന്തം ഫംഗ്ഷനുകൾ എഴുതേണ്ടതില്ല.\n",
    "\n",
    "നിങ്ങൾക്ക് LLM കോളുചെയ്യാൻ കഴിയുന്ന നിങ്ങളുടെ സ്വന്തം കസ്റ്റം ടൂളുകളും സൃഷ്ടിക്കാം.\n",
    "\n",
    "താഴെ കൊടുത്തിരിക്കുന്ന കോഡ് ഉദാഹരണത്തിൽ:\n",
    "\n",
    "- സിസ്റ്റം പ്രോംപ്റ്റിൽ ലഭ്യമായ ടൂളുകൾ (brave_search, wolfram_alpha) നിർവചിക്കുന്നു.\n",
    "- ഒരു നഗരത്തിലെ കാലാവസ്ഥയെക്കുറിച്ച് ചോദിക്കുന്ന ഉപയോക്തൃ പ്രോംപ്റ്റ് അയയ്ക്കുന്നു.\n",
    "- LLM Brave Search ടൂൾ കോളുമായി പ്രതികരിക്കും, ഇത് ഇങ്ങനെ കാണപ്പെടും `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*കുറിപ്പ്: ഈ ഉദാഹരണം ടൂൾ കോളു മാത്രമാണ് ചെയ്യുന്നത്, ഫലങ്ങൾ ലഭിക്കാൻ നിങ്ങൾക്ക് Brave API പേജിൽ ഒരു സൗജന്യ അക്കൗണ്ട് സൃഷ്ടിച്ച് ഫംഗ്ഷൻ തന്നെ നിർവചിക്കേണ്ടതുണ്ട്*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2 \n",
    "\n",
    "LLM ആയിരുന്നാലും, Llama 3.1-ന് ഉള്ള ഒരു പരിമിതിയാണ് മൾട്ടിമോഡാലിറ്റി. അതായത്, ചിത്രങ്ങൾ പോലുള്ള വ്യത്യസ്ത തരത്തിലുള്ള ഇൻപുട്ടുകൾ പ്രോംപ്റ്റുകളായി ഉപയോഗിച്ച് പ്രതികരണങ്ങൾ നൽകാൻ കഴിയുക. ഈ കഴിവാണ് Llama 3.2-യുടെ പ്രധാന സവിശേഷതകളിൽ ഒന്നായി. ഈ സവിശേഷതകളിൽ ഉൾപ്പെടുന്നു: \n",
    "\n",
    "- മൾട്ടിമോഡാലിറ്റി - ടെക്സ്റ്റും ചിത്ര പ്രോംപ്റ്റുകളും വിലയിരുത്താനുള്ള കഴിവ് \n",
    "- ചെറിയ മുതൽ മധ്യമതിലവരെയുള്ള വേരിയേഷനുകൾ (11B, 90B) - ഇത് ലളിതമായ വിന്യാസ ഓപ്ഷനുകൾ നൽകുന്നു, \n",
    "- ടെക്സ്റ്റ് മാത്രം വേരിയേഷനുകൾ (1B, 3B) - ഇത് മോഡൽ എഡ്ജ് / മൊബൈൽ ഉപകരണങ്ങളിൽ വിന്യസിക്കാൻ അനുവദിക്കുകയും കുറഞ്ഞ ലാറ്റൻസി നൽകുകയും ചെയ്യുന്നു \n",
    "\n",
    "മൾട്ടിമോഡൽ പിന്തുണ തുറന്ന ഉറവിട മോഡലുകളുടെ ലോകത്ത് വലിയ ഒരു മുന്നേറ്റമാണ് പ്രതിനിധാനം ചെയ്യുന്നത്. താഴെയുള്ള കോഡ് ഉദാഹരണം ഒരു ചിത്രം കൂടാതെ ടെക്സ്റ്റ് പ്രോംപ്റ്റും സ്വീകരിച്ച് Llama 3.2 90B-ൽ നിന്നുള്ള ചിത്രത്തിന്റെ വിശകലനം നേടുന്നു. \n",
    "\n",
    "### Llama 3.2-യുമായി മൾട്ടിമോഡൽ പിന്തുണ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## പഠനം ഇവിടെ അവസാനിക്കുന്നില്ല, യാത്ര തുടരുക\n",
    "\n",
    "ഈ പാഠം പൂർത്തിയാക്കിയ ശേഷം, നിങ്ങളുടെ ജനറേറ്റീവ് AI അറിവ് മെച്ചപ്പെടുത്താൻ ഞങ്ങളുടെ [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) പരിശോധിക്കുക!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**അസൂയാ**:  \nഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖയാണ് പ്രാമാണികമായ ഉറവിടം എന്ന് പരിഗണിക്കേണ്ടതാണ്. നിർണായകമായ വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനം ഉപയോഗിക്കുന്നതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ തെറ്റായ വ്യാഖ്യാനങ്ങൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-12-19T20:58:23+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}