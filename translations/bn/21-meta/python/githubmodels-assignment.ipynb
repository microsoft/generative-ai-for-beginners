{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# মেটা ফ্যামিলি মডেল নিয়ে কাজ করা\n",
    "\n",
    "## পরিচিতি\n",
    "\n",
    "এই পাঠে আলোচনা করা হবে:\n",
    "\n",
    "- মেটা ফ্যামিলির দুটি প্রধান মডেল - Llama 3.1 এবং Llama 3.2 নিয়ে জানবো\n",
    "- প্রতিটি মডেলের ব্যবহার ক্ষেত্র ও উপযোগিতা বোঝা\n",
    "- প্রতিটি মডেলের বিশেষ বৈশিষ্ট্য দেখানোর জন্য কোডের নমুনা\n",
    "\n",
    "## মেটা ফ্যামিলির মডেলসমূহ\n",
    "\n",
    "এই পাঠে আমরা মেটা ফ্যামিলি বা \"Llama Herd\" থেকে ২টি মডেল - Llama 3.1 এবং Llama 3.2 নিয়ে আলোচনা করবো\n",
    "\n",
    "এই মডেলগুলো বিভিন্ন ভ্যারিয়েন্টে আসে এবং Github Model মার্কেটপ্লেসে পাওয়া যায়। Github Models ব্যবহার করে [AI মডেল দিয়ে প্রোটোটাইপ তৈরি](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) করার বিষয়ে আরও জানতে এখানে দেখুন।\n",
    "\n",
    "মডেল ভ্যারিয়েন্টসমূহ:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*নোট: Llama 3-ও Github Models-এ পাওয়া যায়, তবে এই পাঠে তা আলোচনা করা হবে না*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "৪০৫ বিলিয়ন প্যারামিটারে, Llama 3.1 ওপেন সোর্স LLM ক্যাটাগরিতে পড়ে।\n",
    "\n",
    "এই মডেলটি আগের ভার্সন Llama 3 থেকে আপগ্রেড, যা দেয়:\n",
    "\n",
    "- বড় কনটেক্সট উইন্ডো - ১২৮কে টোকেন বনাম ৮কে টোকেন\n",
    "- বড় ম্যাক্স আউটপুট টোকেন - ৪০৯৬ বনাম ২০৪৮\n",
    "- আরও ভালো বহু-ভাষার সাপোর্ট - ট্রেনিং টোকেন বাড়ার কারণে\n",
    "\n",
    "এসবের ফলে Llama 3.1 আরও জটিল ব্যবহারিক ক্ষেত্র সামলাতে পারে, যেমন GenAI অ্যাপ্লিকেশন তৈরিতে:\n",
    "\n",
    "- নেটিভ ফাংশন কলিং - LLM ওয়ার্কফ্লোর বাইরে এক্সটার্নাল টুল ও ফাংশন কল করার ক্ষমতা\n",
    "- আরও ভালো RAG পারফরম্যান্স - বড় কনটেক্সট উইন্ডোর কারণে\n",
    "- সিনথেটিক ডেটা জেনারেশন - ফাইন-টিউনিংয়ের মতো কাজে কার্যকর ডেটা তৈরি করার ক্ষমতা\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### নেটিভ ফাংশন কলিং\n",
    "\n",
    "Llama 3.1-কে আরও কার্যকরভাবে ফাংশন বা টুল কল করার জন্য ফাইন-টিউন করা হয়েছে। এতে দুটি বিল্ট-ইন টুল রয়েছে, যেগুলো মডেল ব্যবহারকারীর প্রম্পট অনুযায়ী ব্যবহার করা দরকার কিনা তা নিজে চিনতে পারে। এই টুলগুলো হলো:\n",
    "\n",
    "- **Brave Search** - ওয়েব সার্চের মাধ্যমে আবহাওয়ার মতো আপডেটেড তথ্য পেতে ব্যবহার করা যায়\n",
    "- **Wolfram Alpha** - আরও জটিল গাণিতিক হিসাবের জন্য ব্যবহার করা যায়, ফলে নিজে ফাংশন লেখার দরকার হয় না।\n",
    "\n",
    "আপনি চাইলে নিজের কাস্টম টুলও তৈরি করতে পারেন, যেগুলো LLM কল করতে পারবে।\n",
    "\n",
    "নিচের কোড উদাহরণে:\n",
    "\n",
    "- আমরা সিস্টেম প্রম্পটে (brave_search, wolfram_alpha) টুলগুলো ডিফাইন করি।\n",
    "- ব্যবহারকারীর প্রম্পট পাঠানো হয়, যেখানে নির্দিষ্ট একটি শহরের আবহাওয়া জানতে চাওয়া হয়েছে।\n",
    "- LLM Brave Search টুলে একটি টুল কল করবে, যা এভাবে দেখাবে `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*নোট: এই উদাহরণে শুধু টুল কল করা হয়েছে, যদি আপনি ফলাফল পেতে চান, তাহলে আপনাকে Brave API পেজে একটি ফ্রি অ্যাকাউন্ট খুলতে হবে এবং ফাংশনটি নিজে ডিফাইন করতে হবে*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "LLM হলেও, Llama 3.1-এর একটি সীমাবদ্ধতা হলো মাল্টিমোডালিটি। অর্থাৎ, বিভিন্ন ধরনের ইনপুট যেমন ছবি ব্যবহার করে প্রম্পট দেওয়া এবং তার উত্তর দেওয়া। এই ক্ষমতাই Llama 3.2-এর অন্যতম প্রধান বৈশিষ্ট্য। এই ফিচারগুলোর মধ্যে রয়েছে:\n",
    "\n",
    "- মাল্টিমোডালিটি - টেক্সট এবং ছবি, দুই ধরনের প্রম্পট বিশ্লেষণ করতে পারে\n",
    "- ছোট থেকে মাঝারি সাইজের ভ্যারিয়েশন (11B এবং 90B) - এতে ডিপ্লয়মেন্টের জন্য নমনীয়তা পাওয়া যায়,\n",
    "- শুধু টেক্সট-ভিত্তিক ভ্যারিয়েশন (1B এবং 3B) - এতে মডেলটি এজ বা মোবাইল ডিভাইসে ডিপ্লয় করা যায় এবং কম লেটেন্সি পাওয়া যায়\n",
    "\n",
    "মাল্টিমোডাল সাপোর্ট ওপেন সোর্স মডেলের জগতে একটি বড় অগ্রগতি। নিচের কোড উদাহরণে একটি ছবি এবং টেক্সট প্রম্পট একসাথে ব্যবহার করে Llama 3.2 90B থেকে ছবিটির বিশ্লেষণ পাওয়া যায়।\n",
    "\n",
    "### Llama 3.2-এ মাল্টিমোডাল সাপোর্ট\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## শেখা এখানেই শেষ নয়, যাত্রা চালিয়ে যান\n",
    "\n",
    "এই পাঠটি শেষ করার পর, আমাদের [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) দেখুন, যাতে আপনি আপনার Generative AI জ্ঞানের স্তর আরও বাড়াতে পারেন!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**দায়িত্ব অস্বীকার**:\nএই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব নির্ভুল অনুবাদের চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন, স্বয়ংক্রিয় অনুবাদে ভুল বা অসঙ্গতি থাকতে পারে। মূল ভাষায় রচিত নথিটিকেই চূড়ান্ত ও নির্ভরযোগ্য উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য পেশাদার মানব অনুবাদ গ্রহণ করার পরামর্শ দেওয়া হচ্ছে। এই অনুবাদের ব্যবহারে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী নই।\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:40:16+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "bn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}