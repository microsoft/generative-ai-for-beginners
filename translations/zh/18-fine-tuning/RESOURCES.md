# 自主学习资源

本课程使用了来自 OpenAI 和 Azure OpenAI 的核心资源作为术语和教程的参考。以下是一个非详尽的列表，供您在自主学习旅程中使用。

## 1. 主要资源

| 标题/链接                                                                                                                                                                                                                   | 描述                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [使用 OpenAI 模型进行微调](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | 微调通过在比提示中能容纳的更多示例上进行训练来改进少样本学习，从而节省成本，提高响应质量，并实现低延迟请求。**从 OpenAI 获取微调概述。**                                                                                    |
| [Azure OpenAI 的微调是什么？](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | 了解 **微调是什么（概念）**，为什么你应该关注它（动机问题），使用什么数据（训练）以及如何衡量质量                                                                                                                                                                           |
| [通过微调自定义模型](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Azure OpenAI 服务让您可以使用微调根据个人数据集定制我们的模型。学习如何使用 Azure AI Studio、Python SDK 或 REST API **微调（过程）**选择模型。                                                                                                                                |
| [LLM 微调建议](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLM 在特定领域、任务或数据集上的表现可能不佳，或者可能产生不准确或误导性的输出。**何时应该考虑微调**作为对此的可能解决方案？                                                                                                                                  |
| [持续微调](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | 持续微调是一个迭代过程，它选择一个已经微调的模型作为基础模型，并在新的训练示例集上**进一步微调**。                                                                                                                                                     |
| [微调和函数调用](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | 使用函数调用示例对模型进行微调可以通过获得更准确和一致的输出来改进模型输出——具有类似格式的响应和成本节约                                                                                                                                        |
| [微调模型：Azure OpenAI 指导](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | 查阅此表以了解 **哪些模型可以在 Azure OpenAI 中进行微调**，以及这些模型在哪些地区可用。如有需要，查阅它们的令牌限制和训练数据过期日期。                                                                                                                            |
| [微调还是不微调？这是个问题](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | 这个 **2023 年 10 月** 的 AI Show 讨论了帮助您做出决策的优点、缺点和实用见解。                                                                                                                                                                                        |
| [LLM 微调入门](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning?WT.mc_id=academic-105485-koreyst)                                             | 这个 **AI Playbook** 资源引导您了解数据要求、格式化、超参数微调以及您应该知道的挑战/限制。                                                                                                                                                                         |
| **教程**: [Azure OpenAI GPT3.5 Turbo 微调](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | 学习创建一个示例微调数据集，为微调做好准备，创建一个微调任务，并在 Azure 上部署微调后的模型。                                                                                                                                                                                    |
| **教程**: [在 Azure AI Studio 中微调 Llama 2 模型](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio 让您可以使用适合低代码开发人员的 UI 工作流，根据个人数据集定制大型语言模型。查看此示例。                                                                                                                                                               |
| **教程**:[在 Azure 上为单个 GPU 微调 Hugging Face 模型](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | 本文描述了如何使用 Hugging Face transformers 库在单个 GPU 上使用 Azure DataBricks + Hugging Face Trainer 库微调 Hugging Face 模型。                                                                                                                                                |
| **训练**：[使用 Azure 机器学习微调基础模型](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Azure 机器学习中的模型目录提供了许多开源模型，您可以针对特定任务进行微调。尝试此模块，该模块来自 [AzureML 生成式 AI 学习路径](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **教程**: [Azure OpenAI 微调](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | 在 Microsoft Azure 上使用 W&B 微调 GPT-3.5 或 GPT-4 模型，可以详细跟踪和分析模型性能。本指南扩展了 OpenAI 微调指南中的概念，并提供了适用于 Azure OpenAI 的具体步骤和功能。                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. 次要资源

本节包含了一些值得探索的附加资源，但我们在本课程中没有时间涵盖这些资源。它们可能会在将来的课程中涉及，或作为次要作业选项。现在，您可以使用它们来构建您在此主题上的专业知识和知识。

| 标题/链接                                                                                                                                                                                                            | 描述                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [聊天模型微调的数据准备和分析](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | 这个笔记本作为一个工具来预处理和分析用于微调聊天模型的聊天数据集。它检查格式错误，提供基本统计数据，并估算微调成本的令牌数量。请参阅：[gpt-3.5-turbo 的微调方法](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)。                                                                                                                                                                   |
| **OpenAI Cookbook**: [使用 Qdrant 进行检索增强生成（RAG）的微调](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | 本笔记本的目的是通过一个全面的示例来演示如何为检索增强生成（RAG）微调 OpenAI 模型。我们还将集成 Qdrant 和少样本学习，以提高模型性能并减少虚构内容。                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [使用 Weights & Biases 微调 GPT](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) 是一个 AI 开发者平台，提供训练模型、微调模型和利用基础模型的工具。首先阅读他们的 [OpenAI 微调](https://docs.wandb.ai/guides/integrations/openai?WT.mc_id=academic-105485-koreyst) 指南，然后尝试 Cookbook 练习。                                                                                                                                                                                                                  |
| **社区教程** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - 小型语言模型的微调                                                   | 了解 [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst)，这是 Microsoft 的新小型模型，功能强大且紧凑。本教程将指导您如何微调 Phi-2，展示如何构建一个独特的数据集并使用 QLoRA 微调模型。                                                                                                                                                                       |
| **Hugging Face 教程** [如何在 2024 年使用 Hugging Face 微调 LLMs](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | 这篇博客文章引导您如何使用 Hugging Face TRL、Transformers 和数据集在 2024 年微调开放的 LLMs。您需要定义一个用例，设置开发环境，准备数据集，微调模型，测试评估，然后将其部署到生产环境。                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | 提供更快和更简单的[最先进的机器学习模型](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst) 的训练和部署。该仓库有适合 Colab 的教程，并提供 YouTube 视频指导，用于微调。**反映最近的 [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst) 更新**。阅读 [AutoTrain 文档](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**免责声明**：  
本文件使用机器翻译服务进行了翻译。尽管我们力求准确，但请注意，自动翻译可能包含错误或不准确之处。应将原文档视为权威来源。对于关键信息，建议进行专业人工翻译。对于因使用本翻译而引起的任何误解或误读，我们概不负责。