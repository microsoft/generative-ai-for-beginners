<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3772dcd23a98e2010f53ce8b9c583631",
  "translation_date": "2026-01-18T17:02:41+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "zh"
}
-->
[![开源模型](../../../../../translated_images/zh/18-lesson-banner.f30176815b1a5074.webp)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# 微调您的大型语言模型

使用大型语言模型构建生成式 AI 应用带来了新的挑战。一个关键问题是确保模型针对给定用户请求生成的内容的响应质量（准确性和相关性）。在之前的课程中，我们讨论了诸如提示工程和检索增强生成等技术，这些技术试图通过_修改模型的提示输入_来解决该问题。

在今天的课程中，我们讨论第三种技术——**微调**，它试图通过_用额外数据重新训练模型本身_来解决这一挑战。让我们深入了解细节。

## 学习目标

本课程介绍了针对预训练语言模型的微调概念，探讨了这种方法的优点和挑战，并提供了何时以及如何使用微调来提升生成式 AI 模型性能的指导。

通过本节课结束，您应该能够回答以下问题：

- 什么是语言模型的微调？
- 微调在什么时候、为何有用？
- 如何微调预训练模型？
- 微调有哪些限制？

准备好了吗？让我们开始吧。

## 图解指南

想在深入学习之前了解我们将要覆盖的整体内容吗？请查看此图解指南，介绍本课程的学习旅程——从学习微调的核心概念和动机，到理解微调过程和最佳实践。这是一个引人入胜的探索主题，别忘了查看[资源](./RESOURCES.md?WT.mc_id=academic-105485-koreyst)页面，获取支持您自主学习旅程的更多链接！

![微调语言模型图解指南](../../../../../translated_images/zh/18-fine-tuning-sketchnote.11b21f9ec8a70346.webp)

## 什么是语言模型的微调？

根据定义，大型语言模型是在从互联网等多样化来源收集的大量文本上进行_预训练_的。正如我们在之前的课程中所学，我们需要诸如_提示工程_和_检索增强生成_等技术来提升模型对用户问题（“提示”）的响应质量。

一种流行的提示工程技术是给模型提供更多关于预期响应的指导，或者通过提供_指令_（明确指导）或者_给出几个示例_（隐式指导）。这称为_少样本学习_，但它有两个限制：

- 模型的令牌限制会限制提供示例的数量，从而限制效果。
- 模型令牌成本可能使得为每个提示添加示例变得昂贵，从而限制灵活性。

微调是在机器学习系统中常见的做法，我们会拿一个预训练模型，使用新数据对其进行重新训练，以提升其在特定任务上的性能。在语言模型领域中，我们可以用_针对特定任务或应用领域精心准备的示例集_对预训练模型进行微调，创建一个**定制模型**，使其在该特定任务或领域上可能更加准确和相关。微调的一个附带好处是，它还可以减少少样本学习所需的示例数量，从而降低令牌使用和相关成本。

## 什么时候以及为何需要微调模型？

在_此处_，当我们谈论微调时，指的是**有监督**微调，即通过**添加原始训练数据集中未包含的新数据**来进行重新训练。这不同于无监督微调，后者是在相同原始数据上以不同超参数进行重训。

需要记住的关键是，微调是一种高级技巧，需要一定专业知识才能获得预期效果。如果操作不当，可能无法带来预期改善，甚至会降低模型在目标领域的性能。

因此，在学习“如何”微调语言模型之前，您需要知道“为什么”选择这条路线，以及“何时”启动微调过程。先问自己以下问题：

- **用例**：您的微调_用例_是什么？您想提升当前预训练模型的哪些方面？
- **替代方案**：您是否尝试过_其他技术_来达到预期效果？用它们建立基准进行比较。
  - 提示工程：尝试用相关提示响应示例进行少样本提示。评估响应质量。
  - 检索增强生成：尝试用查询结果增强提示，通过搜索您的数据来获得。评估响应质量。
- **成本**：您是否已评估微调的成本？
  - 可调节性——预训练模型是否支持微调？
  - 工作量——准备训练数据、评估和改进模型的工作量
  - 计算资源——运行微调作业和部署微调模型所需算力
  - 数据——用于微调的高质量示例数据的可获取性
- **收益**：您是否确认了微调带来的好处？
  - 质量——微调模型的表现是否优于基线模型？
  - 成本——是否通过简化提示减少了令牌使用？
  - 扩展性——是否可以将基础模型重新用于新的领域？

通过回答这些问题，您应该能决定微调是否适合您的用例。理想情况下，只有当收益大于成本时此方法才有效。确定继续后，就该考虑_如何_微调预训练模型了。

想要获得更多关于决策流程的见解？观看[微调还是不微调](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## 如何微调预训练模型？

微调预训练模型，您需要：

- 一个预训练模型可供微调
- 用于微调的数据集
- 运行微调作业的训练环境
- 部署微调模型的托管环境

## 微调实操

以下资源提供逐步教程，带您通过精心挑选的数据集，使用选定模型进行实际操作。要完成这些教程，您需要在相应服务商注册账号，并获得相关模型和数据集访问权限。

| 提供商         | 教程链接                                                                                                                                                                     | 描述                                                                                                                                                                                                                                                                                                                                                                                                                              |
| -------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI         | [如何微调聊天模型](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                          | 学习如何微调 `gpt-35-turbo` 用于特定领域（“食谱助手”），包括准备训练数据、运行微调作业以及使用微调后的模型进行推理。                                                                                                                                                                                                                                                                                                       |
| Azure OpenAI   | [GPT 3.5 Turbo 微调教程](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst)            | 学习如何在**Azure 上**微调 `gpt-35-turbo-0613` 模型，涵盖创建并上传训练数据，运行微调任务，部署并使用新模型。                                                                                                                                                                                                                                                                                                             |
| Hugging Face   | [使用 Hugging Face 微调大型语言模型](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                           | 本博客指导您怎样用 [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) 库和 [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) ，结合 Hugging Face 上的开源 [数据集](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) ，对开源 LLM（例如 `CodeLlama 7B`）微调。    |
|                |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| 🤗 AutoTrain   | [使用 AutoTrain 微调大型语言模型](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                       | AutoTrain（或 AutoTrain Advanced）是 Hugging Face 开发的 Python 库，支持多种任务的微调，包括大型语言模型的微调。AutoTrain 是无代码解决方案，支持在您自己的云环境、Hugging Face Spaces 或本地执行微调。支持基于网页 GUI、命令行（CLI）及通过 yaml 配置文件的训练。                                                                                                                                                   |
|                |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| 🦥 Unsloth     | [使用 Unsloth 微调大型语言模型](https://github.com/unslothai/unsloth)                                                                                                       | Unsloth 是一个支持大型语言模型微调和强化学习（RL）的开源框架。Unsloth 精简了本地训练、评估和部署流程，并提供可以直接使用的 [notebooks](https://github.com/unslothai/notebooks)。它还支持文本转语音（TTS）、BERT 和多模态模型。入门请阅读其逐步[微调大型语言模型指南](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide)。                                                                                     |
|                |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                   |
## 练习任务

从以上教程中选择一个并完成学习。_我们可能会在本仓库中以 Jupyter Notebook 版本复现这些教程，仅供参考。请直接使用原始来源，以获取最新版教程_。

## 干得好！继续学习。

完成本课程后，访问我们的[生成式 AI 学习集](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，继续提升您的生成式 AI 知识水平！

恭喜！！您已完成本课程 v2 系列的最终课程！请继续学习和构建。**请查看[资源](RESOURCES.md?WT.mc_id=academic-105485-koreyst)页面，获取仅针对本主题的更多建议列表。

我们的 v1 系列课程也已更新，增加了更多作业和概念。花点时间刷新您的知识，并请[分享您的问题和反馈](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst)，帮助我们为社区改进这些课程。

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**免责声明**：
本文件使用AI翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。尽管我们力求准确，但请注意自动翻译可能包含错误或不准确之处。原始语言版本的文件应被视为权威来源。对于重要信息，建议使用专业人工翻译。因使用本翻译而产生的任何误解或误读，我们不承担任何责任。
<!-- CO-OP TRANSLATOR DISCLAIMER END -->