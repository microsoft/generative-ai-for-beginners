# 保护你的生成式AI应用程序

[![保护你的生成式AI应用程序](../../../translated_images/13-lesson-banner.png?WT.028697a53f1c3c0ea07dafd10617ce0380ac2b809bb145d7171be69e83daac89.zh.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## 介绍

本课将涵盖：

- AI系统背景下的安全性。
- AI系统常见的风险和威胁。
- 保护AI系统的方法和考虑因素。

## 学习目标

完成本课后，你将了解：

- AI系统面临的威胁和风险。
- 保护AI系统的常见方法和实践。
- 如何通过实施安全测试来防止意外结果和用户信任的流失。

## 在生成式AI背景下，安全意味着什么？

随着人工智能（AI）和机器学习（ML）技术越来越多地影响我们的生活，保护客户数据和AI系统本身变得至关重要。AI/ML越来越多地被用于支持高价值决策的过程中，在某些行业中错误的决策可能导致严重后果。

以下是需要考虑的关键点：

- **AI/ML的影响**：AI/ML对日常生活有着显著影响，因此保护它们已成为必需。
- **安全挑战**：AI/ML的影响需要适当关注，以应对保护基于AI的产品免受复杂攻击（无论是恶意者还是有组织团体）的需求。
- **战略问题**：科技行业必须主动解决战略性挑战，以确保长期的客户安全和数据安全。

此外，机器学习模型大多无法区分恶意输入和良性异常数据。训练数据的一个重要来源来自未经过滤、未经过审核的公共数据集，这些数据集开放给第三方贡献者。当攻击者可以自由贡献时，他们不需要破坏数据集。随着时间的推移，如果数据结构/格式保持正确，低可信度的恶意数据会变成高可信度的信任数据。

这就是为什么确保模型用于决策的数据存储的完整性和保护至关重要的原因。

## 了解AI的威胁和风险

在AI及相关系统方面，数据中毒是当今最显著的安全威胁。数据中毒是指有人故意更改用于训练AI的信息，导致其出错。这是由于缺乏标准化的检测和缓解方法，加上我们依赖不可信或未经过滤的公共数据集进行训练。为了维护数据完整性并防止有缺陷的训练过程，跟踪数据的来源和沿袭至关重要。否则，老话“垃圾进，垃圾出”就会应验，导致模型性能受损。

以下是数据中毒如何影响模型的例子：

1. **标签翻转**：在二分类任务中，对手故意翻转一小部分训练数据的标签。例如，将良性样本标记为恶意，导致模型学习错误的关联。\
   **示例**：垃圾邮件过滤器由于标签被操纵而错误地将合法邮件分类为垃圾邮件。
2. **特征中毒**：攻击者微妙地修改训练数据中的特征以引入偏见或误导模型。\
   **示例**：在产品描述中添加无关的关键词以操纵推荐系统。
3. **数据注入**：将恶意数据注入训练集中以影响模型的行为。\
   **示例**：引入虚假用户评论以偏斜情感分析结果。
4. **后门攻击**：对手在训练数据中插入隐藏模式（后门）。模型学习识别此模式，并在被触发时表现出恶意行为。\
   **示例**：面部识别系统用后门图像训练，错误识别特定人员。

MITRE公司创建了[ATLAS（人工智能系统的对抗性威胁格局）](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)，这是一个关于对手在现实世界中对AI系统攻击中使用的策略和技术的知识库。

> 随着AI的整合增加，AI启用的系统中的漏洞数量不断增加，这扩大了现有系统的攻击面，超出了传统网络攻击的范围。我们开发ATLAS是为了提高对这些独特和不断演变的漏洞的认识，因为全球社区越来越多地将AI整合到各种系统中。ATLAS是以MITRE ATT&CK®框架为模型，其策略、技术和程序（TTPs）与ATT&CK中的互补。

就像在传统网络安全中广泛使用的MITRE ATT&CK®框架一样，用于规划高级威胁模拟场景，ATLAS提供了一组易于搜索的TTPs，可以帮助更好地理解和准备抵御新兴攻击。

此外，开放Web应用程序安全项目（OWASP）创建了一个"[十大列表](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)"，列出了使用LLM的应用程序中发现的最关键漏洞。该列表强调了诸如上述数据中毒等威胁的风险以及其他威胁，如：

- **提示注入**：一种攻击者通过精心设计的输入操纵大型语言模型（LLM）的方法，使其行为超出预期。
- **供应链漏洞**：构成LLM应用程序的组件和软件，如Python模块或外部数据集，本身可能被破坏，导致意外结果、引入偏见甚至基础设施中的漏洞。
- **过度依赖**：LLM是容易出错的，容易产生幻觉，提供不准确或不安全的结果。在几个已记录的情况下，人们将结果视为理所当然，导致意想不到的现实世界负面后果。

微软云倡导者Rod Trent撰写了一本免费的电子书，[必须学习AI安全](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst)，深入探讨这些和其他新兴的AI威胁，并提供广泛的指导，帮助更好地应对这些场景。

## AI系统和LLM的安全测试

人工智能（AI）正在改变各个领域和行业，为社会提供新的可能性和利益。然而，AI也带来了显著的挑战和风险，如数据隐私、偏见、缺乏可解释性和潜在的滥用。因此，确保AI系统是安全和负责任的是至关重要的，这意味着它们遵循伦理和法律标准，并且可以被用户和利益相关者信任。

安全测试是评估AI系统或LLM安全性的过程，通过识别和利用其漏洞。这可以由开发人员、用户或第三方审计员执行，具体取决于测试的目的和范围。AI系统和LLM最常见的一些安全测试方法是：

- **数据净化**：这是从AI系统或LLM的训练数据或输入中删除或匿名化敏感或私人信息的过程。数据净化可以通过减少机密或个人数据的暴露来防止数据泄露和恶意操纵。
- **对抗性测试**：这是生成和应用对抗性示例到AI系统或LLM的输入或输出的过程，以评估其对抗对抗性攻击的鲁棒性和弹性。对抗性测试可以帮助识别和减轻AI系统或LLM的漏洞和弱点，这些可能被攻击者利用。
- **模型验证**：这是验证AI系统或LLM的模型参数或架构的正确性和完整性的过程。模型验证可以通过确保模型被保护和认证来检测和防止模型窃取。
- **输出验证**：这是验证AI系统或LLM输出的质量和可靠性的过程。输出验证可以通过确保输出的一致性和准确性来检测和纠正恶意操纵。

OpenAI，AI系统的领导者，已经设置了一系列_安全评估_作为其红队网络计划的一部分，旨在测试输出AI系统，以期为AI安全做出贡献。

> 评估可以从简单的问答测试到更复杂的模拟。例如，以下是OpenAI开发的用于从多个角度评估AI行为的示例评估：

#### 说服

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地诱骗另一个AI系统说出一个秘密词？
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地说服另一个AI系统捐款？
- [投票提案](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地影响另一个AI系统对政治提案的支持？

#### 隐写术（隐藏信息）

- [隐写术](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地在不被另一个AI系统发现的情况下传递秘密信息？
- [文本压缩](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地压缩和解压缩消息，以实现隐藏秘密信息？
- [谢林点](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst)：AI系统能多好地与另一个AI系统协调，而无需直接通信？

### AI安全

我们必须保护AI系统免受恶意攻击、滥用或意外后果。这包括采取措施确保AI系统的安全性、可靠性和可信性，例如：

- 保护用于训练和运行AI模型的数据和算法
- 防止未经授权的访问、操纵或破坏AI系统
- 检测和减轻AI系统中的偏见、歧视或伦理问题
- 确保AI决策和行动的问责制、透明性和可解释性
- 使AI系统的目标和价值观与人类和社会的目标和价值观保持一致

AI安全对于确保AI系统和数据的完整性、可用性和机密性至关重要。AI安全的一些挑战和机会包括：

- 机会：将AI纳入网络安全策略中，因为它可以在识别威胁和提高响应时间方面发挥关键作用。AI可以帮助自动化和增强网络攻击（如网络钓鱼、恶意软件或勒索软件）的检测和缓解。
- 挑战：对手也可以利用AI发动复杂的攻击，如生成虚假或误导性内容、冒充用户或利用AI系统中的漏洞。因此，AI开发者有责任设计出对滥用行为具有鲁棒性和弹性的系统。

### 数据保护

LLM可能对其使用的数据的隐私和安全构成风险。例如，LLM可能会记住并泄露其训练数据中的敏感信息，如个人姓名、地址、密码或信用卡号。它们也可能被恶意行为者操纵或攻击，后者希望利用其漏洞或偏见。因此，了解这些风险并采取适当措施保护与LLM使用的数据是很重要的。你可以采取以下几个步骤来保护与LLM使用的数据。这些步骤包括：

- **限制与LLM共享的数据的数量和类型**：仅共享为实现预期目的所必需和相关的数据，避免共享任何敏感、机密或个人数据。用户还应对与LLM共享的数据进行匿名化或加密处理，例如删除或屏蔽任何识别信息，或使用安全通信渠道。
- **验证LLM生成的数据**：始终检查LLM生成的输出的准确性和质量，以确保它们不包含任何不需要或不当的信息。
- **报告和警告任何数据泄露或事件**：警惕LLM的任何可疑或异常活动或行为，例如生成不相关、不准确、冒犯或有害的文本。这可能是数据泄露或安全事件的迹象。

数据安全、治理和合规对于任何希望在多云环境中利用数据和AI力量的组织来说都是至关重要的。保护和治理所有数据是一项复杂而多方面的任务。你需要在多个云中保护和治理不同类型的数据（结构化、非结构化和AI生成的数据），并需要考虑现有和未来的数据安全、治理和AI法规。为了保护你的数据，你需要采用一些最佳实践和预防措施，例如：

- 使用提供数据保护和隐私功能的云服务或平台。
- 使用数据质量和验证工具检查数据中的错误、不一致或异常。
- 使用数据治理和伦理框架确保数据以负责任和透明的方式使用。

### 模拟现实威胁 - AI红队

模拟现实威胁现在被认为是构建弹性AI系统的标准实践，通过使用类似的工具、战术和程序来识别系统风险并测试防御者的响应。

> AI红队的实践已经演变为具有更广泛的意义：它不仅涵盖了对安全漏洞的探测，还包括对其他系统故障的探测，例如生成潜在有害内容。AI系统带来了新的风险，红队是理解这些新风险的核心，例如提示注入和生成无根据的内容。 - [微软AI红队构建更安全的AI未来](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![红队的指导和资源](../../../translated_images/13-AI-red-team.png?WT.5a1ed56fe6f4caf0ada6509bb7aacc47c7da784e5747e1a5373539d9f05bede2.zh.mc_id=academic-105485-koreyst)]()

以下是塑造微软AI红队计划的关键见解。

1. **AI红队的广泛范围**：
   AI红队现在涵盖了安全和责任AI（RAI）结果。传统上，红队专注于安全方面，将模型视为一个向量（例如，窃取基础模型）。然而，AI系统引入了新的安全漏洞（例如，提示注入、中毒），需要特别注意。除了安全之外，AI红队还探讨公平性问题（例如，刻板印象）和有害内容（例如，美化暴力）。早期识别这些问题可以优先考虑防御投资。
2. **恶意和良性故障**：
   AI红队考虑来自恶意和良性视角的故障。例如，在红队新的Bing时，我们不仅探索恶意对手如何颠覆系统，还探索普通用户可能遇到的问题或有害内容。与传统的安全红队主要关注恶意行为者不同，AI红队考虑了更广泛的人物角色和潜在故障。
3. **AI系统的动态特性**：
   AI应用程序不断发展。在大型语言模型应用程序中，开发人员适应不断变化的需求。持续的红队确保对不断演变的风险保持持续警惕和适应。

AI红队并不是包罗万象的，应该被视为其他控制措施的补充动作，例如[基于角色的访问控制（RBAC）](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst)和全面的数据管理解决方案。它旨在补充一个安全策略，专注于使用安全和负责任的AI解决方案，考虑隐私和安全，同时努力最大限度地减少偏见、有害内容和可能侵蚀用户信心的错误信息。

以下是一些额外的阅读材料，可以帮助你更好地了解红队如何帮助识别和减轻AI系统中的风险：

- [规划大型语言模型（LLM）及其应用程序的红队](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [什么是OpenAI红队网络？](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI红队 - 构建更安全和负责任AI解决方案的关键实践](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS（人工智能系统的对抗性威胁格局）](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)，这是一个关于对手在现实世界中对AI系统攻击中使用的策略和技术的知识库。

## 知识检查

保持数据完整性和防止滥用的好方法是什么？

1. 对数据访问和数据管理进行强有力的基于角色的控制
1. 实施和审核数据标注以防止数据误表示或滥用
1. 确保你的AI基础设施支持内容过滤

A:1，虽然这三个建议都很棒，但确保你为用户分配了适当的数据访问权限将大大有助于防止对LLM使用的数据的操纵和误表示。

## 🚀 挑战

阅读更多关于如何在AI时代[管理和保护敏感信息](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst)的内容。

## 很棒的工作，继续学习

完成本课后，查看我们的[生成式AI学习集合](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)以继续提升你的生成式AI知识！

前往第14课，我们将探讨[生成式AI应用程序生命周期](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)！

**免责声明**：
本文件已使用基于机器的人工智能翻译服务进行翻译。尽管我们努力确保准确性，但请注意，自动翻译可能包含错误或不准确之处。应将原始语言的文件视为权威来源。对于关键信息，建议使用专业的人类翻译。对于因使用本翻译而产生的任何误解或误释，我们概不负责。