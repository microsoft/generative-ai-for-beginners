{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 Mistral 模型进行开发\n",
    "\n",
    "## 简介\n",
    "\n",
    "本课将涵盖：\n",
    "- 探索不同的 Mistral 模型\n",
    "- 了解每种模型的使用场景和适用情境\n",
    "- 代码示例展示每个模型的独特功能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral 模型\n",
    "\n",
    "在本课中，我们将探索三种不同的 Mistral 模型：**Mistral Large**、**Mistral Small** 和 **Mistral Nemo**。\n",
    "\n",
    "这些模型都可以在 Github Model 市场免费获取。本笔记本中的代码将使用这些模型来运行代码。关于如何使用 Github Models 进行 [AI 模型原型开发](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst)，可以参考这里的详细说明。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Large 2 (2407)\n",
    "Mistral Large 2 目前是 Mistral 的旗舰模型，专为企业级应用设计。\n",
    "\n",
    "该模型是在原有 Mistral Large 的基础上升级而来，提供了\n",
    "- 更大的上下文窗口——128k 对比 32k\n",
    "- 更强的数学和编程任务表现——平均准确率 76.9%，而原来为 60.4%\n",
    "- 多语言能力提升——支持的语言包括：英语、法语、德语、西班牙语、意大利语、葡萄牙语、荷兰语、俄语、中文、日语、韩语、阿拉伯语和印地语。\n",
    "\n",
    "凭借这些特性，Mistral Large 在以下方面表现突出：\n",
    "- *检索增强生成（RAG）*——得益于更大的上下文窗口\n",
    "- *函数调用*——该模型原生支持函数调用，可以与外部工具和 API 集成。这些调用既可以并行执行，也可以按顺序依次执行。\n",
    "- *代码生成*——该模型在 Python、Java、TypeScript 和 C++ 代码生成方面表现优异。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 Mistral Large 2 的 RAG 示例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，我们使用 Mistral Large 2 对一个文本文档运行 RAG 模式。问题是用韩语写的，询问作者在上大学之前的活动。\n",
    "\n",
    "它使用 Cohere Embeddings Model 为文本文档和问题生成嵌入。在这个示例中，使用 faiss Python 包作为向量存储。\n",
    "\n",
    "发送给 Mistral 模型的提示包括问题和与问题相似的检索片段。模型随后会给出自然语言的回答。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /home/codespace/.python/current/lib/python3.12/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author primarily engaged in two activities before college: writing and programming. In terms of writing, they wrote short stories, albeit not very good ones, with minimal plot and characters expressing strong feelings. For programming, they started writing programs on the IBM 1401 used for data processing during their 9th grade, at the age of 13 or 14. They used an early version of Fortran and typed programs on punch cards, later loading them into the card reader to run the program.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference import EmbeddingsClient\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-large\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text\n",
    "\n",
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "len(chunks)\n",
    "\n",
    "embed_model_name = \"cohere-embed-v3-multilingual\" \n",
    "\n",
    "embed_client = EmbeddingsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(token)\n",
    ")\n",
    "\n",
    "embed_response = embed_client.embed(\n",
    "    input=chunks,\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "text_embeddings = []\n",
    "for item in embed_response.data:\n",
    "    length = len(item.embedding)\n",
    "    text_embeddings.append(item.embedding)\n",
    "text_embeddings = np.array(text_embeddings)\n",
    "\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)\n",
    "\n",
    "question = \"저자가 대학에 오기 전에 주로 했던 두 가지 일은 무엇이었나요?？\"\n",
    "\n",
    "question_embedding = embed_client.embed(\n",
    "    input=[question],\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "question_embeddings = np.array(question_embedding.data[0].embedding)\n",
    "\n",
    "\n",
    "D, I = index.search(question_embeddings.reshape(1, -1), k=2) # distance, index\n",
    "retrieved_chunks = [chunks[i] for i in I.tolist()[0]]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunks}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=prompt),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Small\n",
    "Mistral Small 是 Mistral 系列中属于高端/企业级别的另一款模型。顾名思义，这是一款小型语言模型（SLM）。使用 Mistral Small 的优势包括：\n",
    "- 相比 Mistral Large 和 NeMo 等 Mistral 大型语言模型，成本更低——价格下降了 80%\n",
    "- 延迟低——响应速度比 Mistral 的大型语言模型更快\n",
    "- 灵活性高——可以在不同环境中部署，对所需资源的限制更少\n",
    "\n",
    "Mistral Small 非常适合以下场景：\n",
    "- 基于文本的任务，比如摘要、情感分析和翻译\n",
    "- 由于成本效益高，适用于需要频繁请求的应用\n",
    "- 低延迟的代码相关任务，如代码审查和代码建议\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比 Mistral Small 和 Mistral Large\n",
    "\n",
    "为了展示 Mistral Small 和 Large 在延迟上的差异，请运行下面的代码单元。\n",
    "\n",
    "你应该能看到两者的响应时间相差大约 3-5 秒。同时也可以注意在相同提示下，回复的长度和风格也会有所不同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-small\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-large\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral NeMo\n",
    "\n",
    "与本课讨论的另外两个模型相比，Mistral NeMo 是唯一一个拥有 Apache2 许可证的免费模型。\n",
    "\n",
    "它被认为是 Mistral 早期开源大语言模型 Mistral 7B 的升级版。\n",
    "\n",
    "NeMo 模型还有一些其他特点：\n",
    "\n",
    "- *更高效的分词方式：* 该模型采用 Tekken 分词器，而不是更常用的 tiktoken。这让它在多语言和代码处理方面表现更好。\n",
    "\n",
    "- *可微调：* 基础模型支持微调。这为需要微调的应用场景提供了更大的灵活性。\n",
    "\n",
    "- *原生函数调用* —— 和 Mistral Large 一样，这个模型也经过了函数调用训练。这使它成为首批支持该功能的开源模型之一。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral NeMo\n",
    "\n",
    "与本课讨论的另外两个模型相比，Mistral NeMo 是唯一一个拥有 Apache2 许可证的免费模型。\n",
    "\n",
    "它被认为是 Mistral 早期开源大语言模型 Mistral 7B 的升级版。\n",
    "\n",
    "NeMo 模型还有一些其他特点：\n",
    "\n",
    "- *更高效的分词方式：* 该模型采用了 Tekken 分词器，而不是更常用的 tiktoken。这让它在多种语言和代码上的表现更加出色。\n",
    "\n",
    "- *可微调：* 基础模型支持微调。这为需要微调的应用场景提供了更大的灵活性。\n",
    "\n",
    "- *原生函数调用* —— 和 Mistral Large 一样，这个模型也经过了函数调用的训练。作为首批支持该功能的开源模型之一，这让它非常独特。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对比分词器\n",
    "\n",
    "在本示例中，我们将看看 Mistral NeMo 在分词处理上与 Mistral Large 有什么不同。\n",
    "\n",
    "这两个示例使用相同的提示，但你会发现 NeMo 返回的分词数量比 Mistral Large 少。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistral-common\n",
      "  Downloading mistral_common-1.4.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (4.23.0)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from mistral-common) (2.9.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (2.32.3)\n",
      "Collecting sentencepiece==0.2.0 (from mistral-common)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from mistral-common)\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from mistral-common) (4.12.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (0.20.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.1->mistral-common) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.1->mistral-common) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (2024.8.30)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<0.8.0,>=0.7.0->mistral-common)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Downloading mistral_common-1.4.4-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, regex, tiktoken, mistral-common\n",
      "Successfully installed mistral-common-1.4.4 regex-2024.9.11 sentencepiece-0.2.0 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mistral-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"open-mistral-nemo\t\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"mistral-large-latest\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习不止于此，继续你的旅程\n",
    "\n",
    "完成本课后，欢迎访问我们的[生成式 AI 学习合集](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，持续提升你的生成式 AI 知识！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**免责声明**：  \n本文件由 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 翻译。我们力求准确，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文件应被视为权威来源。对于关键信息，建议使用专业人工翻译。因使用本翻译而产生的任何误解或曲解，我们概不负责。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "coopTranslator": {
   "original_hash": "03bc2561665a8b411acfc24a8aa0d442",
   "translation_date": "2025-08-25T22:14:08+00:00",
   "source_file": "20-mistral/python/githubmodels-assignment.ipynb",
   "language_code": "zh"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}