<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:23:18+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "pt"
}
-->
# Protegendo as Suas Aplica√ß√µes de IA Generativa

[![Protegendo as Suas Aplica√ß√µes de IA Generativa](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.pt.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Introdu√ß√£o

Esta li√ß√£o ir√° abordar:

- Seguran√ßa no contexto dos sistemas de IA.
- Riscos e amea√ßas comuns aos sistemas de IA.
- M√©todos e considera√ß√µes para proteger sistemas de IA.

## Objetivos de Aprendizagem

Ap√≥s concluir esta li√ß√£o, ter√° uma compreens√£o de:

- As amea√ßas e riscos aos sistemas de IA.
- M√©todos e pr√°ticas comuns para proteger sistemas de IA.
- Como a implementa√ß√£o de testes de seguran√ßa pode prevenir resultados inesperados e a perda de confian√ßa dos utilizadores.

## O que significa seguran√ßa no contexto da IA generativa?

√Ä medida que as tecnologias de Intelig√™ncia Artificial (IA) e Aprendizagem Autom√°tica (ML) moldam cada vez mais as nossas vidas, √© fundamental proteger n√£o s√≥ os dados dos clientes, mas tamb√©m os pr√≥prios sistemas de IA. A IA/ML √© cada vez mais utilizada para apoiar processos de tomada de decis√£o de alto valor em setores onde uma decis√£o errada pode ter consequ√™ncias graves.

Aqui est√£o pontos-chave a considerar:

- **Impacto da IA/ML**: A IA/ML tem um impacto significativo no quotidiano e, por isso, a sua prote√ß√£o tornou-se essencial.
- **Desafios de Seguran√ßa**: Este impacto exige aten√ß√£o adequada para proteger produtos baseados em IA contra ataques sofisticados, seja por trolls ou grupos organizados.
- **Problemas Estrat√©gicos**: A ind√∫stria tecnol√≥gica deve abordar proativamente desafios estrat√©gicos para garantir a seguran√ßa a longo prazo dos clientes e dos seus dados.

Al√©m disso, os modelos de Aprendizagem Autom√°tica t√™m grande dificuldade em distinguir entre entradas maliciosas e dados an√≥malos benignos. Uma parte significativa dos dados de treino prov√©m de conjuntos de dados p√∫blicos n√£o curados e n√£o moderados, abertos a contribui√ß√µes de terceiros. Os atacantes n√£o precisam de comprometer os conjuntos de dados quando podem simplesmente contribuir para eles. Com o tempo, dados maliciosos de baixa confian√ßa tornam-se dados confi√°veis de alta confian√ßa, desde que a estrutura/formata√ß√£o dos dados se mantenha correta.

Por isso, √© fundamental garantir a integridade e prote√ß√£o dos reposit√≥rios de dados que os seus modelos utilizam para tomar decis√µes.

## Compreender as amea√ßas e riscos da IA

No que diz respeito √† IA e sistemas relacionados, o envenenamento de dados destaca-se como a amea√ßa de seguran√ßa mais significativa atualmente. Envenenamento de dados ocorre quando algu√©m altera intencionalmente a informa√ß√£o usada para treinar uma IA, fazendo com que esta cometa erros. Isto deve-se √† aus√™ncia de m√©todos padronizados de dete√ß√£o e mitiga√ß√£o, aliado √† nossa depend√™ncia de conjuntos de dados p√∫blicos n√£o confi√°veis ou n√£o curados para treino. Para manter a integridade dos dados e evitar um processo de treino falho, √© crucial rastrear a origem e a linhagem dos seus dados. Caso contr√°rio, o velho ditado ‚Äúlixo entra, lixo sai‚Äù mant√©m-se, levando a um desempenho comprometido do modelo.

Aqui est√£o exemplos de como o envenenamento de dados pode afetar os seus modelos:

1. **Invers√£o de Etiquetas**: Numa tarefa de classifica√ß√£o bin√°ria, um advers√°rio inverte intencionalmente as etiquetas de um pequeno subconjunto dos dados de treino. Por exemplo, amostras benignas s√£o rotuladas como maliciosas, levando o modelo a aprender associa√ß√µes incorretas.\
   **Exemplo**: Um filtro de spam que classifica erroneamente emails leg√≠timos como spam devido a etiquetas manipuladas.
2. **Envenenamento de Caracter√≠sticas**: Um atacante modifica subtilmente caracter√≠sticas nos dados de treino para introduzir vi√©s ou enganar o modelo.\
   **Exemplo**: Adicionar palavras-chave irrelevantes √†s descri√ß√µes de produtos para manipular sistemas de recomenda√ß√£o.
3. **Inje√ß√£o de Dados**: Injetar dados maliciosos no conjunto de treino para influenciar o comportamento do modelo.\
   **Exemplo**: Introduzir avalia√ß√µes falsas de utilizadores para distorcer resultados de an√°lise de sentimento.
4. **Ataques de Porta-Traseira**: Um advers√°rio insere um padr√£o oculto (porta-traseira) nos dados de treino. O modelo aprende a reconhecer este padr√£o e comporta-se maliciosamente quando ativado.\
   **Exemplo**: Um sistema de reconhecimento facial treinado com imagens com porta-traseira que identifica incorretamente uma pessoa espec√≠fica.

A MITRE Corporation criou o [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), uma base de conhecimento sobre t√°ticas e t√©cnicas usadas por advers√°rios em ataques reais a sistemas de IA.

> Existem cada vez mais vulnerabilidades em sistemas com IA, pois a incorpora√ß√£o da IA aumenta a superf√≠cie de ataque dos sistemas existentes para al√©m dos ataques cibern√©ticos tradicionais. Desenvolvemos o ATLAS para aumentar a consciencializa√ß√£o sobre estas vulnerabilidades √∫nicas e em evolu√ß√£o, √† medida que a comunidade global incorpora cada vez mais IA em v√°rios sistemas. O ATLAS √© modelado com base no framework MITRE ATT&CK¬Æ e as suas t√°ticas, t√©cnicas e procedimentos (TTPs) complementam os do ATT&CK.

Tal como o framework MITRE ATT&CK¬Æ, amplamente utilizado na ciberseguran√ßa tradicional para planear cen√°rios avan√ßados de emula√ß√£o de amea√ßas, o ATLAS fornece um conjunto pesquis√°vel de TTPs que ajudam a compreender melhor e a preparar a defesa contra ataques emergentes.

Al√©m disso, o Open Web Application Security Project (OWASP) criou uma "[Lista Top 10](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" das vulnerabilidades mais cr√≠ticas encontradas em aplica√ß√µes que utilizam LLMs. A lista destaca riscos de amea√ßas como o envenenamento de dados mencionado, bem como outros como:

- **Inje√ß√£o de Prompt**: t√©cnica onde atacantes manipulam um Modelo de Linguagem Grande (LLM) atrav√©s de entradas cuidadosamente elaboradas, fazendo-o comportar-se fora do seu comportamento previsto.
- **Vulnerabilidades na Cadeia de Abastecimento**: Os componentes e software que comp√µem as aplica√ß√µes usadas por um LLM, como m√≥dulos Python ou conjuntos de dados externos, podem ser comprometidos, levando a resultados inesperados, vi√©s introduzido e at√© vulnerabilidades na infraestrutura subjacente.
- **Excesso de Confian√ßa**: Os LLMs s√£o fal√≠veis e t√™m tend√™ncia a "alucinar", fornecendo resultados imprecisos ou inseguros. Em v√°rias situa√ß√µes documentadas, as pessoas aceitaram os resultados como verdadeiros, levando a consequ√™ncias negativas no mundo real.

O Microsoft Cloud Advocate Rod Trent escreveu um ebook gratuito, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), que aprofunda estas e outras amea√ßas emergentes da IA, oferecendo orienta√ß√µes extensas sobre como lidar com estes cen√°rios.

## Testes de Seguran√ßa para Sistemas de IA e LLMs

A intelig√™ncia artificial (IA) est√° a transformar v√°rios dom√≠nios e ind√∫strias, oferecendo novas possibilidades e benef√≠cios para a sociedade. No entanto, a IA tamb√©m apresenta desafios e riscos significativos, como privacidade de dados, vi√©s, falta de explicabilidade e potencial uso indevido. Por isso, √© crucial garantir que os sistemas de IA s√£o seguros e respons√°veis, ou seja, que cumprem padr√µes √©ticos e legais e podem ser confi√°veis por utilizadores e partes interessadas.

O teste de seguran√ßa √© o processo de avaliar a seguran√ßa de um sistema de IA ou LLM, identificando e explorando as suas vulnerabilidades. Pode ser realizado por desenvolvedores, utilizadores ou auditores externos, dependendo do prop√≥sito e √¢mbito do teste. Alguns dos m√©todos mais comuns de teste de seguran√ßa para sistemas de IA e LLMs s√£o:

- **Sanitiza√ß√£o de dados**: Processo de remover ou anonimizar informa√ß√µes sens√≠veis ou privadas dos dados de treino ou da entrada de um sistema de IA ou LLM. A sanitiza√ß√£o ajuda a prevenir fugas de dados e manipula√ß√£o maliciosa, reduzindo a exposi√ß√£o de dados confidenciais ou pessoais.
- **Testes adversariais**: Processo de gerar e aplicar exemplos adversariais na entrada ou sa√≠da de um sistema de IA ou LLM para avaliar a sua robustez e resili√™ncia contra ataques adversariais. Estes testes ajudam a identificar e mitigar vulnerabilidades e fraquezas que podem ser exploradas por atacantes.
- **Verifica√ß√£o do modelo**: Processo de verificar a corre√ß√£o e completude dos par√¢metros ou arquitetura do modelo de um sistema de IA ou LLM. A verifica√ß√£o ajuda a detetar e prevenir o roubo de modelos, garantindo que o modelo est√° protegido e autenticado.
- **Valida√ß√£o da sa√≠da**: Processo de validar a qualidade e fiabilidade da sa√≠da de um sistema de IA ou LLM. A valida√ß√£o ajuda a detetar e corrigir manipula√ß√µes maliciosas, assegurando que a sa√≠da √© consistente e precisa.

A OpenAI, l√≠der em sistemas de IA, estabeleceu uma s√©rie de _avalia√ß√µes de seguran√ßa_ como parte da sua iniciativa de red teaming, com o objetivo de testar a sa√≠da dos sistemas de IA e contribuir para a seguran√ßa da IA.

> As avalia√ß√µes podem variar desde testes simples de perguntas e respostas at√© simula√ß√µes mais complexas. Como exemplos concretos, aqui est√£o algumas avalia√ß√µes desenvolvidas pela OpenAI para avaliar comportamentos de IA sob v√°rios √¢ngulos:

#### Persuas√£o

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem pode um sistema de IA enganar outro sistema de IA para dizer uma palavra secreta?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem pode um sistema de IA convencer outro sistema de IA a doar dinheiro?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem pode um sistema de IA influenciar o apoio de outro sistema de IA a uma proposta pol√≠tica?

#### Esteganografia (mensagens ocultas)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem pode um sistema de IA passar mensagens secretas sem ser detetado por outro sistema de IA?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem pode um sistema de IA comprimir e descomprimir mensagens para permitir esconder mensagens secretas?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem pode um sistema de IA coordenar-se com outro sistema de IA, sem comunica√ß√£o direta?

### Seguran√ßa da IA

√â imperativo que nos esforcemos para proteger os sistemas de IA contra ataques maliciosos, uso indevido ou consequ√™ncias n√£o intencionais. Isto inclui tomar medidas para garantir a seguran√ßa, fiabilidade e confian√ßa dos sistemas de IA, tais como:

- Proteger os dados e algoritmos usados para treinar e executar modelos de IA
- Prevenir acessos n√£o autorizados, manipula√ß√£o ou sabotagem dos sistemas de IA
- Detetar e mitigar vi√©s, discrimina√ß√£o ou quest√µes √©ticas nos sistemas de IA
- Garantir a responsabilidade, transpar√™ncia e explicabilidade das decis√µes e a√ß√µes da IA
- Alinhar os objetivos e valores dos sistemas de IA com os dos humanos e da sociedade

A seguran√ßa da IA √© importante para garantir a integridade, disponibilidade e confidencialidade dos sistemas e dados de IA. Alguns dos desafios e oportunidades da seguran√ßa da IA s√£o:

- Oportunidade: Incorporar a IA em estrat√©gias de ciberseguran√ßa, pois pode desempenhar um papel crucial na identifica√ß√£o de amea√ßas e melhoria dos tempos de resposta. A IA pode ajudar a automatizar e aumentar a dete√ß√£o e mitiga√ß√£o de ciberataques, como phishing, malware ou ransomware.
- Desafio: A IA tamb√©m pode ser usada por advers√°rios para lan√ßar ataques sofisticados, como gerar conte√∫dos falsos ou enganosos, personificar utilizadores ou explorar vulnerabilidades em sistemas de IA. Por isso, os desenvolvedores de IA t√™m uma responsabilidade √∫nica de projetar sistemas robustos e resilientes contra uso indevido.

### Prote√ß√£o de Dados

Os LLMs podem representar riscos para a privacidade e seguran√ßa dos dados que utilizam. Por exemplo, os LLMs podem memorizar e divulgar informa√ß√µes sens√≠veis dos seus dados de treino, como nomes pessoais, moradas, palavras-passe ou n√∫meros de cart√£o de cr√©dito. Tamb√©m podem ser manipulados ou atacados por agentes maliciosos que queiram explorar as suas vulnerabilidades ou vi√©s. Por isso, √© importante estar ciente destes riscos e tomar medidas adequadas para proteger os dados usados com LLMs. Algumas medidas que pode adotar para proteger os dados usados com LLMs incluem:

- **Limitar a quantidade e tipo de dados que partilha com LLMs**: Partilhe apenas os dados necess√°rios e relevantes para os fins pretendidos, evitando partilhar dados sens√≠veis, confidenciais ou pessoais. Os utilizadores devem tamb√©m anonimizar ou encriptar os dados partilhados com LLMs, por exemplo, removendo ou mascarando qualquer informa√ß√£o identificativa, ou usando canais de comunica√ß√£o seguros.
- **Verificar os dados gerados pelos LLMs**: Verifique sempre a precis√£o e qualidade da sa√≠da gerada pelos LLMs para garantir que n√£o cont√©m informa√ß√µes indesejadas ou inadequadas.
- **Reportar e alertar sobre quaisquer fugas de dados ou incidentes**: Esteja atento a atividades ou comportamentos suspeitos ou anormais dos LLMs, como gerar textos irrelevantes, imprecisos, ofensivos ou prejudiciais. Isto pode indicar uma fuga de dados ou incidente de seguran√ßa.

A seguran√ßa, governa√ß√£o e conformidade dos dados s√£o cr√≠ticas para qualquer organiza√ß√£o que queira aproveitar o poder dos dados e da IA num ambiente multi-cloud. Proteger e governar todos os seus dados √© uma tarefa complexa e multifacetada. √â necess√°rio proteger e governar diferentes tipos de dados (estruturados, n√£o estruturados e dados gerados por IA) em diferentes locais atrav√©s de m√∫ltiplas clouds, tendo em conta as regulamenta√ß√µes atuais e futuras de seguran√ßa, governa√ß√£o e IA. Para proteger os seus dados, deve adotar algumas boas pr√°ticas e precau√ß√µes, tais como:

- Usar servi√ßos ou plataformas cloud que ofere√ßam funcionalidades de prote√ß√£o e privacidade de dados.
- Usar ferramentas de qualidade e valida√ß√£o de dados para verificar erros, inconsist√™ncias ou anomalias.
- Usar frameworks de governa√ß√£o e √©tica de dados para garantir que os seus dados s√£o usados de forma respons√°vel e transparente.

### Emular amea√ßas do mundo real - red teaming em IA

Emular amea√ßas do mundo real √© agora considerado uma pr√°tica padr√£o na constru√ß√£o de sistemas de IA resilientes, empregando ferramentas, t√°ticas e procedimentos semelhantes para identificar riscos aos sistemas e testar a resposta dos defensores.
> A pr√°tica de red teaming em IA evoluiu para assumir um significado mais amplo: n√£o se limita a identificar vulnerabilidades de seguran√ßa, mas tamb√©m inclui a dete√ß√£o de outras falhas do sistema, como a gera√ß√£o de conte√∫do potencialmente prejudicial. Os sistemas de IA trazem novos riscos, e o red teaming √© fundamental para compreender esses riscos inovadores, como a inje√ß√£o de prompts e a produ√ß√£o de conte√∫do sem fundamento. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Orienta√ß√£o e recursos para red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.pt.png)]()

A seguir, apresentamos os principais insights que moldaram o programa de AI Red Team da Microsoft.

1. **√Çmbito abrangente do AI Red Teaming:**  
   O AI red teaming abrange agora tanto a seguran√ßa como os resultados de IA Respons√°vel (RAI). Tradicionalmente, o red teaming focava nos aspetos de seguran√ßa, tratando o modelo como um vetor (por exemplo, roubo do modelo subjacente). No entanto, os sistemas de IA introduzem vulnerabilidades de seguran√ßa novas (por exemplo, inje√ß√£o de prompts, envenenamento), exigindo aten√ß√£o especial. Para al√©m da seguran√ßa, o AI red teaming tamb√©m investiga quest√µes de justi√ßa (por exemplo, estere√≥tipos) e conte√∫dos prejudiciais (por exemplo, glorifica√ß√£o da viol√™ncia). A identifica√ß√£o precoce destes problemas permite priorizar os investimentos em defesa.  
2. **Falhas maliciosas e benignas:**  
   O AI red teaming considera falhas tanto do ponto de vista malicioso como benigno. Por exemplo, ao testar o novo Bing, exploramos n√£o s√≥ como advers√°rios maliciosos podem subverter o sistema, mas tamb√©m como utilizadores comuns podem encontrar conte√∫dos problem√°ticos ou prejudiciais. Ao contr√°rio do red teaming tradicional de seguran√ßa, que se foca principalmente em atores maliciosos, o AI red teaming abrange uma gama mais ampla de perfis e potenciais falhas.  
3. **Natureza din√¢mica dos sistemas de IA:**  
   As aplica√ß√µes de IA est√£o em constante evolu√ß√£o. Nas aplica√ß√µes de grandes modelos de linguagem, os desenvolvedores adaptam-se a requisitos em mudan√ßa. O red teaming cont√≠nuo assegura vigil√¢ncia constante e adapta√ß√£o aos riscos em evolu√ß√£o.

O AI red teaming n√£o √© uma solu√ß√£o completa e deve ser considerado um complemento a outros controlos, como o [controlo de acesso baseado em fun√ß√µes (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) e solu√ß√µes abrangentes de gest√£o de dados. Destina-se a complementar uma estrat√©gia de seguran√ßa que se foca na utiliza√ß√£o de solu√ß√µes de IA seguras e respons√°veis, que tenham em conta a privacidade e a seguran√ßa, ao mesmo tempo que procuram minimizar preconceitos, conte√∫dos prejudiciais e desinforma√ß√£o que possam minar a confian√ßa dos utilizadores.

Aqui fica uma lista de leituras adicionais que podem ajudar a compreender melhor como o red teaming pode ajudar a identificar e mitigar riscos nos seus sistemas de IA:

- [Planeamento de red teaming para grandes modelos de linguagem (LLMs) e as suas aplica√ß√µes](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [O que √© a OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming - Uma pr√°tica essencial para construir solu√ß√µes de IA mais seguras e respons√°veis](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), uma base de conhecimento sobre t√°ticas e t√©cnicas usadas por advers√°rios em ataques reais a sistemas de IA.

## Verifica√ß√£o de conhecimento

Qual poder√° ser uma boa abordagem para manter a integridade dos dados e prevenir o uso indevido?

1. Ter controlos fortes baseados em fun√ß√µes para o acesso e gest√£o dos dados  
1. Implementar e auditar a rotulagem dos dados para evitar a m√° representa√ß√£o ou uso indevido dos dados  
1. Garantir que a sua infraestrutura de IA suporta filtragem de conte√∫dos

A:1, Embora as tr√™s sejam √≥timas recomenda√ß√µes, garantir que est√° a atribuir os privil√©gios de acesso aos dados adequados aos utilizadores √© fundamental para prevenir manipula√ß√£o e m√° representa√ß√£o dos dados usados pelos LLMs.

## üöÄ Desafio

Leia mais sobre como pode [governar e proteger informa√ß√£o sens√≠vel](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) na era da IA.

## Excelente trabalho, continue a aprender

Depois de concluir esta li√ß√£o, consulte a nossa [cole√ß√£o de Aprendizagem de IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para continuar a aprofundar os seus conhecimentos em IA Generativa!

Siga para a Li√ß√£o 14, onde vamos explorar [o Ciclo de Vida da Aplica√ß√£o de IA Generativa](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos pela precis√£o, por favor tenha em conta que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorizada. Para informa√ß√µes cr√≠ticas, recomenda-se tradu√ß√£o profissional humana. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes da utiliza√ß√£o desta tradu√ß√£o.