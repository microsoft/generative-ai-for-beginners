<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2210a0466c812d9defc4df2d9a709ff9",
  "translation_date": "2026-01-18T17:47:04+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "pt"
}
-->
# Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) e Bases de Dados Vetoriais

[![Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) e Bases de Dados Vetoriais](../../../../../translated_images/pt/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

Na li√ß√£o de aplica√ß√µes de pesquisa, aprendemos brevemente como integrar os seus pr√≥prios dados em Grandes Modelos de Linguagem (LLMs). Nesta li√ß√£o, vamos aprofundar os conceitos de fundamentar os seus dados na sua aplica√ß√£o LLM, os mecanismos do processo e os m√©todos para armazenar dados, incluindo tanto embeddings como texto.

> **V√≠deo em Breve**

## Introdu√ß√£o

Nesta li√ß√£o iremos abordar o seguinte:

- Uma introdu√ß√£o ao RAG, o que √© e porque √© usado em IA (intelig√™ncia artificial).

- Compreender o que s√£o bases de dados vetoriais e criar uma para a nossa aplica√ß√£o.

- Um exemplo pr√°tico de como integrar RAG numa aplica√ß√£o.

## Objetivos de Aprendizagem

Ap√≥s completar esta li√ß√£o, ser√° capaz de:

- Explicar a import√¢ncia do RAG na recupera√ß√£o e processamento de dados.

- Configurar uma aplica√ß√£o RAG e fundamentar os seus dados num LLM.

- Integra√ß√£o eficaz do RAG e Bases de Dados Vetoriais em Aplica√ß√µes LLM.

## O nosso Cen√°rio: melhorar os nossos LLMs com os nossos pr√≥prios dados

Para esta li√ß√£o, queremos adicionar as nossas pr√≥prias notas √† startup educativa, o que permite ao chatbot obter mais informa√ß√µes sobre as diferentes disciplinas. Utilizando as notas que temos, os alunos poder√£o estudar melhor e compreender os diferentes temas, tornando mais f√°cil rever para os seus exames. Para criar o nosso cen√°rio, iremos usar:

- `Azure OpenAI:` o LLM que iremos usar para criar o nosso chatbot

- `Li√ß√£o AI para iniciantes em Redes Neuronais:` estes ser√£o os dados nos quais fundamentamos o nosso LLM

- `Azure AI Search` e `Azure Cosmos DB:` base de dados vetorial para armazenar os nossos dados e criar um √≠ndice de pesquisa

Os utilizadores poder√£o criar quizzes de pr√°tica a partir das suas notas, cart√µes de revis√£o e resumir para vis√µes gerais concisas. Para come√ßar, vejamos o que √© RAG e como funciona:

## Gera√ß√£o Aumentada por Recupera√ß√£o (RAG)

Um chatbot alimentado por um LLM processa os prompts do utilizador para gerar respostas. Foi concebido para ser interativo e envolver-se com os utilizadores numa ampla variedade de t√≥picos. No entanto, as suas respostas est√£o limitadas ao contexto fornecido e aos seus dados de treino fundamentais. Por exemplo, o corte de conhecimento do GPT-4 √© setembro de 2021, o que significa que n√£o tem conhecimento de eventos que ocorreram ap√≥s esse per√≠odo. Al√©m disso, os dados usados para treinar os LLMs excluem informa√ß√£o confidencial como notas pessoais ou o manual do produto de uma empresa.

### Como funcionam os RAG (Gera√ß√£o Aumentada por Recupera√ß√£o)

![desenho a mostrar como funcionam os RAGs](../../../../../translated_images/pt/how-rag-works.f5d0ff63942bd3a6.webp)

Suponha que quer implementar um chatbot que cria quizzes a partir das suas notas, ir√° precisar de uma liga√ß√£o √† base de conhecimento. √â aqui que o RAG entra em a√ß√£o. Os RAGs operam da seguinte forma:

- **Base de conhecimento:** Antes da recupera√ß√£o, estes documentos precisam de ser ingeridos e pr√©-processados, normalmente dividindo documentos grandes em peda√ßos menores, transformando-os em embeddings de texto e armazenando-os numa base de dados.

- **Consulta do Utilizador:** o utilizador faz uma pergunta

- **Recupera√ß√£o:** Quando um utilizador faz uma pergunta, o modelo de embedding recupera informa√ß√£o relevante da nossa base de conhecimento para fornecer mais contexto que ser√° incorporado no prompt.

- **Gera√ß√£o Aumentada:** o LLM melhora a sua resposta com base nos dados recuperados. Isto permite que a resposta gerada n√£o seja apenas baseada em dados pr√©-treinados, mas tamb√©m em informa√ß√£o relevante do contexto adicionado. Os dados recuperados s√£o usados para aumentar as respostas do LLM. O LLM retorna ent√£o uma resposta √† pergunta do utilizador.

![desenho a mostrar como √© a arquitetura dos RAGs](../../../../../translated_images/pt/encoder-decode.f2658c25d0eadee2.webp)

A arquitetura para os RAGs √© implementada usando transformers consistindo de duas partes: um codificador (encoder) e um descodificador (decoder). Por exemplo, quando um utilizador faz uma pergunta, o texto de entrada √© ‚Äòcodificado‚Äô em vetores que capturam o significado das palavras, e os vetores s√£o ‚Äòdescodificados‚Äô no nosso √≠ndice de documentos e geram novo texto baseado na consulta do utilizador. O LLM usa um modelo codificador-descodificador para gerar a sa√≠da.

Duas abordagens quando se implementa RAG segundo o artigo proposto: [Retrieval-Augmented Generation for Knowledge intensive NLP (tarefas de processamento de linguagem natural)](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) s√£o:

- **_RAG-Sequence_** usando documentos recuperados para prever a melhor resposta poss√≠vel a uma consulta do utilizador

- **RAG-Token** usando documentos para gerar o pr√≥ximo token, depois recuper√°-los para responder √† consulta do utilizador

### Por que raz√£o usar RAGs?

- **Riqueza de informa√ß√£o:** garante que as respostas de texto est√£o atualizadas e atuais. Por isso, melhora o desempenho em tarefas espec√≠ficas de dom√≠nio ao aceder √† base de conhecimento interna.

- Reduz fabricacÃß√£o ao utilizar **dados verific√°veis** na base de conhecimento para fornecer contexto √†s quest√µes dos utilizadores.

- √â **custo-efetivo** pois s√£o mais econ√≥micos em compara√ß√£o com o ajuste fino de um LLM

## Criar uma base de conhecimento

A nossa aplica√ß√£o baseia-se nos nossos dados pessoais, ou seja, a li√ß√£o sobre Redes Neuronais no curr√≠culo AI para iniciantes.

### Bases de Dados Vetoriais

Uma base de dados vetorial, ao contr√°rio das bases de dados tradicionais, √© uma base especializada para armazenar, gerir e pesquisar vetores embutidos. Armazena representa√ß√µes num√©ricas de documentos. Dividir os dados em embeddings num√©ricos torna mais f√°cil para o nosso sistema de IA compreender e processar os dados.

Armazenamos os nossos embeddings em bases de dados vetoriais pois os LLMs t√™m um limite no n√∫mero de tokens que aceitam como entrada. Como n√£o pode passar todos os embeddings para um LLM, vamos precisar de os dividir em peda√ßos, e quando um utilizador faz uma pergunta, os embeddings mais semelhantes ser√£o devolvidos juntamente com o prompt. A divis√£o em peda√ßos tamb√©m reduz os custos no n√∫mero de tokens passados por um LLM.

Algumas bases de dados vetoriais populares incluem Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant e DeepLake. Pode criar um modelo Azure Cosmos DB usando Azure CLI com o seguinte comando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### De texto a embeddings

Antes de armazenar os nossos dados, precisamos converter em embeddings vetoriais antes de serem armazenados na base de dados. Se estiver a trabalhar com documentos grandes ou textos longos, pode dividir em peda√ßos baseados nas perguntas que espera receber. A divis√£o pode ser feita a n√≠vel de frase ou par√°grafo. Como a divis√£o deriva significados das palavras √† sua volta, pode adicionar algum outro contexto a um peda√ßo, por exemplo, adicionando o t√≠tulo do documento ou incluindo algum texto antes ou depois do peda√ßo. Pode dividir os dados da seguinte forma:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Se o √∫ltimo peda√ßo n√£o atingir o comprimento m√≠nimo, adiciona-o na mesma
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Uma vez divididos, podemos depois incorporar o nosso texto usando diferentes modelos de embedding. Alguns modelos que pode usar incluem: word2vec, ada-002 da OpenAI, Azure Computer Vision e muitos mais. A sele√ß√£o do modelo depende das l√≠nguas que est√° a usar, o tipo de conte√∫do codificado (texto/imagens/√°udio), o tamanho da entrada que pode codificar e o comprimento da sa√≠da de embedding.

Um exemplo de texto embutido usando o modelo `text-embedding-ada-002` da OpenAI √©:
![um embedding da palavra gato](../../../../../translated_images/pt/cat.74cbd7946bc9ca38.webp)

## Recupera√ß√£o e Pesquisa Vetorial

Quando um utilizador faz uma pergunta, o motor de recupera√ß√£o transforma-a num vetor usando o codificador de consulta, depois pesquisa no nosso √≠ndice de documento os vetores relevantes relacionados com a entrada. Depois de feito, converte tanto o vetor de entrada como os vetores do documento em texto e envia para o LLM.

### Recupera√ß√£o

A recupera√ß√£o acontece quando o sistema tenta encontrar rapidamente os documentos do √≠ndice que satisfazem os crit√©rios de pesquisa. O objetivo do motor de recupera√ß√£o √© obter documentos que ser√£o usados para fornecer contexto e fundamentar o LLM nos seus dados.

Existem v√°rias formas de fazer pesquisa na nossa base de dados, como:

- **Pesquisa por palavra-chave** - usada para pesquisas de texto

- **Pesquisa vetorial** - converte documentos de texto em representa√ß√µes vetoriais usando modelos de embedding, permitindo uma **pesquisa sem√¢ntica** usando o significado das palavras. A recupera√ß√£o ser√° feita consultando os documentos cujas representa√ß√µes vetoriais sejam mais pr√≥ximas da pergunta do utilizador.

- **H√≠brido** - uma combina√ß√£o de pesquisa por palavra-chave e vetorial.

Um desafio na recupera√ß√£o surge quando n√£o existe uma resposta similar √† pergunta na base de dados; o sistema retorna ent√£o a melhor informa√ß√£o poss√≠vel, contudo, pode usar t√°ticas como definir a dist√¢ncia m√°xima para relev√¢ncia ou usar pesquisa h√≠brida que combina palavras-chave e pesquisa vetorial. Nesta li√ß√£o vamos usar pesquisa h√≠brida, uma combina√ß√£o de ambas. Iremos armazenar os dados num dataframe com colunas que cont√™m os peda√ßos assim como os embeddings.

### Similaridade Vetorial

O motor de recupera√ß√£o procura na base de conhecimento por embeddings que est√£o pr√≥ximos, o vizinho mais pr√≥ximo, pois s√£o textos semelhantes. No cen√°rio em que um utilizador faz uma consulta, esta √© primeiro embutida e depois comparada com embeddings semelhantes. A m√©trica comum utilizada para medir a semelhan√ßa entre diferentes vetores √© a similaridade do cosseno baseada no √¢ngulo entre dois vetores.

Podemos medir similaridade usando outras alternativas como a dist√¢ncia Euclidiana, que √© a linha reta entre as extremidades dos vetores, e o produto escalar que mede a soma dos produtos dos elementos correspondentes de dois vetores.

### √çndice de pesquisa

Ao realizar a recupera√ß√£o, precisaremos construir um √≠ndice de pesquisa para a nossa base de conhecimento antes de realizar buscas. Um √≠ndice armazenar√° os nossos embeddings e pode rapidamente recuperar os peda√ßos mais semelhantes mesmo numa base de dados grande. Podemos criar o nosso √≠ndice localmente usando:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Criar o √≠ndice de pesquisa
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Para consultar o √≠ndice, pode usar o m√©todo kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```

### Reordenamento

Depois de consultar a base de dados, pode ser preciso ordenar os resultados do mais relevante para o menos relevante. Um LLM de reordenamento utiliza Aprendizagem Autom√°tica para melhorar a relev√¢ncia dos resultados da pesquisa, ordenando-os do mais relevante. Usando Azure AI Search, o reordenamento √© feito automaticamente para si usando um reranker sem√¢ntico. Um exemplo de como funciona o reordenamento com os vizinhos mais pr√≥ximos:

```python
# Encontre os documentos mais semelhantes
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Imprima os documentos mais semelhantes
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Unindo tudo

A √∫ltima etapa √© adicionar o nosso LLM para conseguir obter respostas que estejam fundamentadas nos nossos dados. Podemos implement√°-lo da seguinte forma:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Converter a pergunta num vetor de consulta
    query_vector = create_embeddings(user_input)

    # Encontrar os documentos mais semelhantes
    distances, indices = nbrs.kneighbors([query_vector])

    # adicionar documentos √† consulta para fornecer contexto
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combinar o hist√≥rico e a entrada do utilizador
    history.append(user_input)

    # criar um objeto de mensagem
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # usar a conclus√£o de chat para gerar uma resposta
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Avalia√ß√£o da nossa aplica√ß√£o

### M√©tricas de Avalia√ß√£o

- Qualidade das respostas fornecidas, garantindo que soam naturais, fluentes e semelhantes √†s humanas

- Fundamenta√ß√£o dos dados: avaliar se a resposta veio dos documentos fornecidos

- Relev√¢ncia: avaliar se a resposta corresponde e est√° relacionada com a pergunta feita

- Flu√™ncia - se a resposta faz sentido gramaticalmente

## Casos de Uso para usar RAG (Gera√ß√£o Aumentada por Recupera√ß√£o) e bases de dados vetoriais

Existem muitos casos de uso onde chamadas de fun√ß√£o podem melhorar a sua app, tais como:

- Perguntas e Respostas: fundamentar os dados da sua empresa para um chat que possa ser usado por colaboradores para fazer perguntas.

- Sistemas de Recomenda√ß√£o: onde pode criar um sistema que corresponde aos valores mais semelhantes, ex.: filmes, restaurantes e muitos mais.

- Servi√ßos de chatbot: pode armazenar hist√≥rico de conversa e personalizar a conversa com base nos dados do utilizador.

- Pesquisa de imagens baseada em embeddings vetoriais, √∫til para reconhecimento de imagem e dete√ß√£o de anomalias.

## Resumo

Cobriram-se as √°reas fundamentais do RAG desde adicionar os nossos dados √† aplica√ß√£o, a consulta do utilizador e a sa√≠da. Para simplificar a cria√ß√£o de RAG, pode usar frameworks como Semanti Kernel, Langchain ou Autogen.

## Tarefa

Para continuar a sua aprendizagem sobre Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) pode construir:

- Construir uma interface front-end para a aplica√ß√£o usando o framework da sua escolha

- Utilizar um framework, seja LangChain ou Semantic Kernel, e recriar a sua aplica√ß√£o.

Parab√©ns por completar a li√ß√£o üëè.

## A aprendizagem n√£o para aqui, continue a Jornada

Ap√≥s completar esta li√ß√£o, veja a nossa [cole√ß√£o de Aprendizagem em IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para continuar a aprimorar os seus conhecimentos em IA Generativa!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Aviso legal**:
Este documento foi traduzido usando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, por favor tenha em conta que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se tradu√ß√£o profissional humana. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas que possam resultar do uso desta tradu√ß√£o.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->