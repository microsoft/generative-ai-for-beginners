<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:45:10+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "pt"
}
-->
# Introdu√ß√£o √†s Redes Neurais. Perceptr√£o Multi-Camada

Na sec√ß√£o anterior, aprendeu sobre o modelo mais simples de rede neural - o perceptr√£o de uma camada, um modelo linear de classifica√ß√£o bin√°ria.

Nesta sec√ß√£o, vamos expandir este modelo para um quadro mais flex√≠vel, permitindo-nos:

* realizar **classifica√ß√£o multiclasse** al√©m da classifica√ß√£o bin√°ria
* resolver **problemas de regress√£o** al√©m da classifica√ß√£o
* separar classes que n√£o s√£o linearmente separ√°veis

Tamb√©m iremos desenvolver o nosso pr√≥prio framework modular em Python que nos permitir√° construir diferentes arquiteturas de redes neurais.

## Formaliza√ß√£o do Aprendizado de M√°quina

Vamos come√ßar por formalizar o problema do Aprendizado de M√°quina. Suponha que temos um conjunto de dados de treino **X** com etiquetas **Y**, e precisamos construir um modelo *f* que fa√ßa as previs√µes mais precisas poss√≠veis. A qualidade das previs√µes √© medida pela **fun√ß√£o de perda** ‚Ñí. As seguintes fun√ß√µes de perda s√£o frequentemente usadas:

* Para problemas de regress√£o, quando precisamos prever um n√∫mero, podemos usar o **erro absoluto** ‚àë<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, ou o **erro quadr√°tico** ‚àë<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Para classifica√ß√£o, usamos a **perda 0-1** (que √© essencialmente o mesmo que a **acur√°cia** do modelo), ou a **perda log√≠stica**.

Para o perceptr√£o de um n√≠vel, a fun√ß√£o *f* foi definida como uma fun√ß√£o linear *f(x)=wx+b* (aqui *w* √© a matriz de pesos, *x* √© o vetor de caracter√≠sticas de entrada, e *b* √© o vetor de bias). Para diferentes arquiteturas de redes neurais, esta fun√ß√£o pode assumir formas mais complexas.

> No caso da classifica√ß√£o, √© frequentemente desej√°vel obter probabilidades das classes correspondentes como sa√≠da da rede. Para converter n√∫meros arbitr√°rios em probabilidades (por exemplo, para normalizar a sa√≠da), usamos frequentemente a fun√ß√£o **softmax** œÉ, e a fun√ß√£o *f* torna-se *f(x)=œÉ(wx+b)*

Na defini√ß√£o de *f* acima, *w* e *b* s√£o chamados de **par√¢metros** Œ∏=‚ü®*w,b*‚ü©. Dado o conjunto de dados ‚ü®**X**,**Y**‚ü©, podemos calcular o erro total no conjunto de dados como uma fun√ß√£o dos par√¢metros Œ∏.

> ‚úÖ **O objetivo do treino da rede neural √© minimizar o erro variando os par√¢metros Œ∏**

## Otimiza√ß√£o por Gradiente Descendente

Existe um m√©todo bem conhecido de otimiza√ß√£o de fun√ß√µes chamado **gradiente descendente**. A ideia √© que podemos calcular a derivada (no caso multidimensional chamada **gradiente**) da fun√ß√£o de perda em rela√ß√£o aos par√¢metros, e variar os par√¢metros de forma a que o erro diminua. Isto pode ser formalizado da seguinte forma:

* Inicializar os par√¢metros com alguns valores aleat√≥rios w<sup>(0)</sup>, b<sup>(0)</sup>
* Repetir o seguinte passo v√°rias vezes:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇw
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇb

Durante o treino, os passos de otimiza√ß√£o devem ser calculados considerando o conjunto de dados completo (lembre-se que a perda √© calculada como uma soma por todas as amostras de treino). No entanto, na pr√°tica, usamos pequenas por√ß√µes do conjunto de dados chamadas **minibatches**, e calculamos os gradientes com base num subconjunto dos dados. Como o subconjunto √© escolhido aleatoriamente a cada vez, este m√©todo √© chamado **gradiente descendente estoc√°stico** (SGD).

## Perceptr√µes Multi-Camada e Backpropagation

A rede de uma camada, como vimos acima, √© capaz de classificar classes linearmente separ√°veis. Para construir um modelo mais rico, podemos combinar v√°rias camadas da rede. Matematicamente, isto significa que a fun√ß√£o *f* ter√° uma forma mais complexa, e ser√° calculada em v√°rios passos:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Œ±(z<sub>1</sub>)+b<sub>2</sub>
* f = œÉ(z<sub>2</sub>)

Aqui, Œ± √© uma **fun√ß√£o de ativa√ß√£o n√£o linear**, œÉ √© a fun√ß√£o softmax, e os par√¢metros Œ∏=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

O algoritmo de gradiente descendente mant√©m-se o mesmo, mas o c√°lculo dos gradientes torna-se mais complexo. Dada a regra da diferencia√ß√£o em cadeia, podemos calcular as derivadas como:

* ‚àÇ‚Ñí/‚àÇw<sub>2</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇw<sub>2</sub>)
* ‚àÇ‚Ñí/‚àÇw<sub>1</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇŒ±)(‚àÇŒ±/‚àÇz<sub>1</sub>)(‚àÇz<sub>1</sub>/‚àÇw<sub>1</sub>)

> ‚úÖ A regra da diferencia√ß√£o em cadeia √© usada para calcular as derivadas da fun√ß√£o de perda em rela√ß√£o aos par√¢metros.

Note que a parte mais √† esquerda de todas estas express√µes √© a mesma, e assim podemos calcular as derivadas de forma eficiente come√ßando pela fun√ß√£o de perda e indo "para tr√°s" atrav√©s do grafo computacional. Por isso, o m√©todo de treino de um perceptr√£o multi-camada √© chamado **backpropagation**, ou simplesmente 'backprop'.

> TODO: cita√ß√£o da imagem

> ‚úÖ Iremos abordar o backprop com muito mais detalhe no nosso exemplo no notebook.

## Conclus√£o

Nesta li√ß√£o, constru√≠mos a nossa pr√≥pria biblioteca de redes neurais, e usamos-a para uma tarefa simples de classifica√ß√£o bidimensional.

## üöÄ Desafio

No notebook que acompanha, ir√° implementar o seu pr√≥prio framework para construir e treinar perceptr√µes multi-camada. Poder√° ver em detalhe como funcionam as redes neurais modernas.

Prossiga para o notebook OwnFramework e trabalhe nele.

## Revis√£o & Estudo Aut√≥nomo

Backpropagation √© um algoritmo comum usado em IA e ML, vale a pena estud√°-lo com mais detalhe.

## Tarefa

Neste laborat√≥rio, √© pedido que use o framework que construiu nesta li√ß√£o para resolver a classifica√ß√£o de d√≠gitos manuscritos MNIST.

* Instru√ß√µes
* Notebook

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, por favor tenha em conta que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorizada. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional humana. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes erradas decorrentes da utiliza√ß√£o desta tradu√ß√£o.