{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução \n",
    "\n",
    "Esta lição irá cobrir: \n",
    "- O que é a chamada de função e os seus casos de uso \n",
    "- Como criar uma chamada de função usando OpenAI \n",
    "- Como integrar uma chamada de função numa aplicação \n",
    "\n",
    "## Objetivos de Aprendizagem \n",
    "\n",
    "Após completar esta lição, saberá como e compreenderá: \n",
    "\n",
    "- O propósito de usar chamadas de função \n",
    "- Configurar a Chamada de Função usando o Serviço OpenAI \n",
    "- Projetar chamadas de função eficazes para o caso de uso da sua aplicação \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compreender Chamadas de Funções\n",
    "\n",
    "Para esta lição, queremos construir uma funcionalidade para a nossa startup de educação que permita aos utilizadores usar um chatbot para encontrar cursos técnicos. Vamos recomendar cursos que se adequem ao seu nível de competência, função atual e tecnologia de interesse.\n",
    "\n",
    "Para completar isto, vamos usar uma combinação de:\n",
    " - `OpenAI` para criar uma experiência de chat para o utilizador\n",
    " - `Microsoft Learn Catalog API` para ajudar os utilizadores a encontrar cursos com base no pedido do utilizador\n",
    " - `Function Calling` para pegar na consulta do utilizador e enviá-la para uma função para fazer o pedido à API.\n",
    "\n",
    "Para começar, vejamos por que razão quereríamos usar chamadas de função em primeiro lugar:\n",
    "\n",
    "print(\"Mensagens no próximo pedido:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # obter uma nova resposta do GPT onde pode ver a resposta da função\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porquê a Chamada de Funções\n",
    "\n",
    "Se completou alguma outra lição deste curso, provavelmente já compreende o poder de usar Modelos de Linguagem Grande (LLMs). Esperamos que também consiga ver algumas das suas limitações.\n",
    "\n",
    "A Chamada de Funções é uma funcionalidade do Serviço OpenAI concebida para resolver os seguintes desafios:\n",
    "\n",
    "Formatação Inconsistente das Respostas:\n",
    "- Antes da chamada de funções, as respostas de um modelo de linguagem grande eram não estruturadas e inconsistentes. Os programadores tinham de escrever código complexo de validação para lidar com cada variação na saída.\n",
    "\n",
    "Integração Limitada com Dados Externos:\n",
    "- Antes desta funcionalidade, era difícil incorporar dados de outras partes de uma aplicação num contexto de chat.\n",
    "\n",
    "Ao padronizar os formatos de resposta e permitir uma integração fluida com dados externos, a chamada de funções simplifica o desenvolvimento e reduz a necessidade de lógica adicional de validação.\n",
    "\n",
    "Os utilizadores não conseguiam obter respostas como \"Qual é o tempo atual em Estocolmo?\". Isto porque os modelos estavam limitados ao momento em que os dados foram treinados.\n",
    "\n",
    "Vamos ver o exemplo abaixo que ilustra este problema:\n",
    "\n",
    "Suponha que queremos criar uma base de dados de dados de estudantes para podermos sugerir o curso certo para eles. Abaixo temos duas descrições de estudantes que são muito semelhantes nos dados que contêm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos enviar isto para um LLM para analisar os dados. Isto pode depois ser usado na nossa aplicação para enviar isto para uma API ou armazenar numa base de dados.\n",
    "\n",
    "Vamos criar dois prompts idênticos onde instruímos o LLM sobre a informação que nos interessa:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos enviar isto para um LLM para analisar as partes que são importantes para o nosso produto. Assim, podemos criar dois prompts idênticos para instruir o LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de criar estes dois prompts, iremos enviá-los para o LLM utilizando `openai.ChatCompletion`. Armazenamos o prompt na variável `messages` e atribuímos o papel de `user`. Isto é para imitar uma mensagem de um utilizador a ser escrita para um chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos enviar ambos os pedidos para o LLM e examinar a resposta que recebemos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mesmo que os prompts sejam os mesmos e as descrições sejam semelhantes, podemos obter formatos diferentes da propriedade `Grades`.\n",
    "\n",
    "Se executar a célula acima várias vezes, o formato pode ser `3.7` ou `3.7 GPA`.\n",
    "\n",
    "Isto acontece porque o LLM recebe dados não estruturados na forma do prompt escrito e também devolve dados não estruturados. Precisamos de ter um formato estruturado para sabermos o que esperar ao armazenar ou usar estes dados.\n",
    "\n",
    "Ao usar chamadas funcionais, podemos garantir que recebemos dados estruturados de volta. Ao usar chamadas funcionais, o LLM não chama nem executa realmente quaisquer funções. Em vez disso, criamos uma estrutura para o LLM seguir nas suas respostas. Depois usamos essas respostas estruturadas para saber que função executar nas nossas aplicações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama de Fluxo de Chamada de Função](../../../../translated_images/Function-Flow.083875364af4f4bb.pt.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos então pegar o que é retornado pela função e enviar isso de volta para o LLM. O LLM responderá então usando linguagem natural para responder à consulta do utilizador.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casos de Uso para utilizar chamadas de função\n",
    "\n",
    "**Chamar Ferramentas Externas**  \n",
    "Os chatbots são ótimos para fornecer respostas a perguntas dos utilizadores. Ao usar chamadas de função, os chatbots podem usar mensagens dos utilizadores para completar certas tarefas. Por exemplo, um estudante pode pedir ao chatbot para \"Enviar um email ao meu professor a dizer que preciso de mais ajuda com esta matéria\". Isto pode fazer uma chamada de função para `send_email(to: string, body: string)`\n",
    "\n",
    "**Criar Consultas API ou Base de Dados**  \n",
    "Os utilizadores podem encontrar informação usando linguagem natural que é convertida numa consulta formatada ou pedido API. Um exemplo disto pode ser um professor que pede \"Quem são os estudantes que completaram o último trabalho\" que poderia chamar uma função chamada `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Criar Dados Estruturados**  \n",
    "Os utilizadores podem pegar num bloco de texto ou CSV e usar o LLM para extrair informação importante dele. Por exemplo, um estudante pode converter um artigo da Wikipedia sobre acordos de paz para criar flash cards de IA. Isto pode ser feito usando uma função chamada `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criar a Sua Primeira Chamada de Função\n",
    "\n",
    "O processo de criar uma chamada de função inclui 3 passos principais:  \n",
    "1. Chamar a API de Completações de Chat com uma lista das suas funções e uma mensagem do utilizador  \n",
    "2. Ler a resposta do modelo para executar uma ação, ou seja, executar uma função ou chamada de API  \n",
    "3. Fazer outra chamada à API de Completações de Chat com a resposta da sua função para usar essa informação para criar uma resposta para o utilizador.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fluxo de uma Chamada de Função](../../../../translated_images/LLM-Flow.3285ed8caf4796d7.pt.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementos de uma chamada de função \n",
    "\n",
    "#### Entrada dos Utilizadores \n",
    "\n",
    "O primeiro passo é criar uma mensagem do utilizador. Esta pode ser atribuída dinamicamente ao obter o valor de uma entrada de texto ou pode atribuir um valor aqui. Se esta for a sua primeira vez a trabalhar com a API de Chat Completions, precisamos de definir o `role` e o `content` da mensagem. \n",
    "\n",
    "O `role` pode ser `system` (criação de regras), `assistant` (o modelo) ou `user` (o utilizador final). Para a chamada de função, iremos atribuir isto como `user` e uma pergunta de exemplo. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar funções.\n",
    "\n",
    "De seguida, vamos definir uma função e os parâmetros dessa função. Vamos usar apenas uma função aqui chamada `search_courses`, mas pode criar múltiplas funções.\n",
    "\n",
    "**Importante** : As funções são incluídas na mensagem do sistema para o LLM e serão incluídas na quantidade de tokens disponíveis que tem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definições** \n",
    "\n",
    "A estrutura de definição da função tem múltiplos níveis, cada um com as suas próprias propriedades. Aqui está uma descrição da estrutura aninhada:\n",
    "\n",
    "**Propriedades da Função ao Nível Superior:**\n",
    "\n",
    "`name` - O nome da função que queremos que seja chamada. \n",
    "\n",
    "`description` - Esta é a descrição de como a função funciona. Aqui é importante ser específico e claro \n",
    "\n",
    "`parameters` - Uma lista de valores e formato que se pretende que o modelo produza na sua resposta \n",
    "\n",
    "**Propriedades do Objeto Parameters:**\n",
    "\n",
    "`type` - O tipo de dados do objeto parameters (normalmente \"object\")\n",
    "\n",
    "`properties` - Lista dos valores específicos que o modelo irá usar para a sua resposta \n",
    "\n",
    "**Propriedades Individuais dos Parâmetros:**\n",
    "\n",
    "`name` - Definido implicitamente pela chave da propriedade (ex., \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - O tipo de dados deste parâmetro específico (ex., \"string\", \"number\", \"boolean\") \n",
    "\n",
    "`description` - Descrição do parâmetro específico \n",
    "\n",
    "**Propriedades Opcionais:**\n",
    "\n",
    "`required` - Um array que lista quais parâmetros são necessários para que a chamada da função seja completada \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazer a chamada da função  \n",
    "Depois de definir uma função, agora precisamos incluí-la na chamada para a API de Conclusão de Chat. Fazemos isso adicionando `functions` ao pedido. Neste caso, `functions=functions`. \n",
    "\n",
    "Existe também a opção de definir `function_call` para `auto`. Isto significa que deixaremos o LLM decidir qual função deve ser chamada com base na mensagem do utilizador, em vez de a atribuirmos nós próprios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos analisar a resposta e ver como está formatada:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Pode ver que o nome da função é chamado e, a partir da mensagem do utilizador, o LLM conseguiu encontrar os dados para preencher os argumentos da função.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Integrar Chamadas de Funções numa Aplicação. \n",
    "\n",
    "\n",
    "Depois de termos testado a resposta formatada do LLM, agora podemos integrá-la numa aplicação. \n",
    "\n",
    "### Gerir o fluxo \n",
    "\n",
    "Para integrar isto na nossa aplicação, vamos seguir os seguintes passos: \n",
    "\n",
    "Primeiro, vamos fazer a chamada aos serviços da OpenAI e armazenar a mensagem numa variável chamada `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos definir a função que irá chamar a API do Microsoft Learn para obter uma lista de cursos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como boa prática, iremos então verificar se o modelo pretende chamar uma função. Depois disso, iremos criar uma das funções disponíveis e associá-la à função que está a ser chamada.  \n",
    "Em seguida, iremos pegar nos argumentos da função e mapeá-los para os argumentos do LLM.\n",
    "\n",
    "Por fim, iremos anexar a mensagem de chamada da função e os valores que foram retornados pela mensagem `search_courses`. Isto dá ao LLM toda a informação de que necessita para  \n",
    "responder ao utilizador usando linguagem natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos enviar a mensagem atualizada para o LLM para que possamos receber uma resposta em linguagem natural em vez de uma resposta formatada em JSON da API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafio de Código \n",
    "\n",
    "Ótimo trabalho! Para continuar a sua aprendizagem sobre OpenAI Function Calling pode construir: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - Mais parâmetros da função que podem ajudar os aprendizes a encontrar mais cursos. Pode encontrar os parâmetros disponíveis da API aqui: \n",
    " - Criar outra chamada de função que recolha mais informações do aprendiz, como a sua língua nativa \n",
    " - Criar tratamento de erros quando a chamada da função e/ou chamada da API não retornar cursos adequados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Aviso Legal**:\nEste documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, por favor tenha em conta que traduções automáticas podem conter erros ou imprecisões. O documento original na sua língua nativa deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se a tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações erradas decorrentes do uso desta tradução.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T09:59:44+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "pt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}