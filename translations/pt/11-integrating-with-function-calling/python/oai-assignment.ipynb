{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Esta lição irá abordar:\n",
    "- O que é a chamada de função e em que situações pode ser utilizada\n",
    "- Como criar uma chamada de função usando a OpenAI\n",
    "- Como integrar uma chamada de função numa aplicação\n",
    "\n",
    "## Objetivos de Aprendizagem\n",
    "\n",
    "Depois de concluir esta lição, saberá como e compreenderá:\n",
    "\n",
    "- O objetivo de utilizar chamadas de função\n",
    "- Configurar uma chamada de função utilizando o Serviço OpenAI\n",
    "- Conceber chamadas de função eficazes para o caso de uso da sua aplicação\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compreender as Chamadas de Funções\n",
    "\n",
    "Nesta lição, queremos criar uma funcionalidade para a nossa startup de educação que permita aos utilizadores usar um chatbot para encontrar cursos técnicos. Vamos recomendar cursos que se adequem ao seu nível de competências, função atual e tecnologia de interesse.\n",
    "\n",
    "Para concretizar isto, vamos usar uma combinação de:\n",
    " - `OpenAI` para criar uma experiência de chat para o utilizador\n",
    " - `Microsoft Learn Catalog API` para ajudar os utilizadores a encontrar cursos com base no pedido do utilizador\n",
    " - `Function Calling` para pegar no pedido do utilizador e enviá-lo para uma função que faz o pedido à API.\n",
    "\n",
    "Para começar, vejamos porque é que queremos usar chamadas de funções:\n",
    "\n",
    "print(\"Mensagens no próximo pedido:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # obter uma nova resposta do GPT onde pode ver a resposta da função\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porque Usar Function Calling\n",
    "\n",
    "Se já completaste alguma outra lição deste curso, provavelmente já percebeste o poder de utilizar Large Language Models (LLMs). Esperamos também que consigas identificar algumas das suas limitações.\n",
    "\n",
    "Function Calling é uma funcionalidade do OpenAI Service criada para resolver os seguintes desafios:\n",
    "\n",
    "Formatação Inconsistente das Respostas:\n",
    "- Antes do function calling, as respostas de um large language model eram pouco estruturadas e inconsistentes. Os programadores tinham de escrever código de validação complexo para lidar com cada variação no output.\n",
    "\n",
    "Integração Limitada com Dados Externos:\n",
    "- Antes desta funcionalidade, era difícil incorporar dados de outras partes de uma aplicação num contexto de chat.\n",
    "\n",
    "Ao normalizar os formatos das respostas e permitir uma integração fluida com dados externos, o function calling simplifica o desenvolvimento e reduz a necessidade de lógica adicional de validação.\n",
    "\n",
    "Os utilizadores não conseguiam obter respostas como \"Qual é o tempo atual em Estocolmo?\". Isto porque os modelos estavam limitados à data em que foram treinados.\n",
    "\n",
    "Vamos analisar o exemplo abaixo que ilustra este problema:\n",
    "\n",
    "Imagina que queremos criar uma base de dados com informações de alunos para podermos sugerir o curso mais adequado a cada um. Em baixo temos duas descrições de alunos que são muito semelhantes nos dados que apresentam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos enviar isto para um LLM para analisar os dados. Mais tarde, isto pode ser usado na nossa aplicação para enviar para uma API ou guardar numa base de dados.\n",
    "\n",
    "Vamos criar dois prompts idênticos onde damos instruções ao LLM sobre que informação nos interessa:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos enviar isto para um LLM para analisar as partes que são importantes para o nosso produto. Assim, podemos criar dois prompts idênticos para instruir o LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de criar estes dois prompts, vamos enviá-los para o LLM usando `openai.ChatCompletion`. Guardamos o prompt na variável `messages` e atribuímos o papel de `user`. Isto serve para simular uma mensagem de um utilizador a ser escrita para um chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar de os prompts serem iguais e as descrições semelhantes, podemos obter formatos diferentes da propriedade `Grades`.\n",
    "\n",
    "Se executares a célula acima várias vezes, o formato pode ser `3.7` ou `3.7 GPA`.\n",
    "\n",
    "Isto acontece porque o LLM recebe dados não estruturados sob a forma do prompt escrito e também devolve dados não estruturados. Precisamos de um formato estruturado para sabermos o que esperar ao armazenar ou utilizar estes dados.\n",
    "\n",
    "Ao usar chamadas funcionais, podemos garantir que recebemos dados estruturados de volta. Ao utilizar chamadas de função, o LLM não chama nem executa realmente nenhuma função. Em vez disso, criamos uma estrutura para o LLM seguir nas suas respostas. Depois, usamos essas respostas estruturadas para saber que função executar nas nossas aplicações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama de Fluxo de Chamadas de Função](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.pt.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casos de Utilização para chamadas de funções\n",
    "\n",
    "**Chamar Ferramentas Externas**  \n",
    "Os chatbots são ótimos a responder a perguntas dos utilizadores. Ao usar chamadas de funções, os chatbots podem utilizar as mensagens dos utilizadores para realizar certas tarefas. Por exemplo, um estudante pode pedir ao chatbot: \"Envia um email ao meu professor a dizer que preciso de mais ajuda nesta matéria\". Isto pode fazer uma chamada à função `send_email(to: string, body: string)`\n",
    "\n",
    "**Criar Pedidos à API ou Consultas à Base de Dados**  \n",
    "Os utilizadores podem encontrar informação usando linguagem natural, que é convertida num pedido formatado ou numa consulta à API. Um exemplo disto pode ser um professor que pergunta \"Quem são os alunos que completaram o último trabalho\", o que pode chamar uma função chamada `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Criar Dados Estruturados**  \n",
    "Os utilizadores podem pegar num bloco de texto ou num ficheiro CSV e usar o LLM para extrair informação importante. Por exemplo, um estudante pode converter um artigo da Wikipédia sobre acordos de paz para criar cartões de estudo com IA. Isto pode ser feito usando uma função chamada `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criar a Sua Primeira Chamada de Função\n",
    "\n",
    "O processo de criar uma chamada de função inclui 3 passos principais:\n",
    "1. Chamar a API de Chat Completions com uma lista das suas funções e uma mensagem do utilizador\n",
    "2. Ler a resposta do modelo para realizar uma ação, ou seja, executar uma função ou chamada de API\n",
    "3. Fazer outra chamada à API de Chat Completions com a resposta da sua função para usar essa informação e criar uma resposta para o utilizador.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fluxo de uma Chamada de Função](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.pt.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementos de uma chamada de função\n",
    "\n",
    "#### Entrada do Utilizador\n",
    "\n",
    "O primeiro passo é criar uma mensagem do utilizador. Isto pode ser atribuído dinamicamente ao obter o valor de um campo de texto ou pode atribuir um valor aqui. Se for a primeira vez que trabalha com a API de Chat Completions, é necessário definir o `role` e o `content` da mensagem.\n",
    "\n",
    "O `role` pode ser `system` (criação de regras), `assistant` (o modelo) ou `user` (o utilizador final). Para chamadas de função, vamos definir isto como `user` e dar um exemplo de pergunta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar funções.\n",
    "\n",
    "De seguida, vamos definir uma função e os parâmetros dessa função. Aqui vamos usar apenas uma função chamada `search_courses`, mas podes criar várias funções.\n",
    "\n",
    "**Importante**: As funções são incluídas na mensagem de sistema para o LLM e vão contar para o número de tokens disponíveis que tens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definições**\n",
    "\n",
    "A estrutura de definição de funções tem vários níveis, cada um com as suas próprias propriedades. Aqui está uma explicação da estrutura aninhada:\n",
    "\n",
    "**Propriedades de Função ao Nível Superior:**\n",
    "\n",
    "`name` - O nome da função que queremos que seja chamada.\n",
    "\n",
    "`description` - Esta é a descrição de como a função funciona. Aqui é importante ser específico e claro.\n",
    "\n",
    "`parameters` - Uma lista de valores e o formato que pretende que o modelo produza na sua resposta.\n",
    "\n",
    "**Propriedades do Objeto Parameters:**\n",
    "\n",
    "`type` - O tipo de dados do objeto parameters (normalmente \"object\")\n",
    "\n",
    "`properties` - Lista dos valores específicos que o modelo irá usar na sua resposta\n",
    "\n",
    "**Propriedades de Cada Parâmetro:**\n",
    "\n",
    "`name` - Definido implicitamente pela chave da propriedade (por exemplo, \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - O tipo de dados deste parâmetro específico (por exemplo, \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - Descrição do parâmetro específico\n",
    "\n",
    "**Propriedades Opcionais:**\n",
    "\n",
    "`required` - Um array que indica quais os parâmetros que são obrigatórios para que a chamada da função seja concluída\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazer a chamada da função\n",
    "Depois de definir uma função, precisamos agora incluí-la na chamada à API de Chat Completion. Fazemos isto adicionando `functions` ao pedido. Neste caso, `functions=functions`.\n",
    "\n",
    "Existe também a opção de definir `function_call` como `auto`. Isto significa que deixamos que o LLM decida qual função deve ser chamada com base na mensagem do utilizador, em vez de a atribuirmos manualmente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos analisar a resposta e ver como está formatada:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Pode ver que o nome da função é chamado e, a partir da mensagem do utilizador, o LLM conseguiu encontrar os dados para preencher os argumentos da função.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrar Chamadas de Funções numa Aplicação.\n",
    "\n",
    "Depois de testarmos a resposta formatada do LLM, podemos agora integrá-la numa aplicação.\n",
    "\n",
    "### Gerir o fluxo\n",
    "\n",
    "Para integrar isto na nossa aplicação, vamos seguir os seguintes passos:\n",
    "\n",
    "Primeiro, vamos fazer a chamada aos serviços da OpenAI e guardar a mensagem numa variável chamada `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos definir a função que irá chamar a API do Microsoft Learn para obter uma lista de cursos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como boa prática, vamos verificar se o modelo pretende chamar uma função. Depois disso, vamos criar uma das funções disponíveis e associá-la à função que está a ser chamada.\n",
    "De seguida, vamos pegar nos argumentos da função e mapeá-los para os argumentos provenientes do LLM.\n",
    "\n",
    "Por fim, vamos adicionar a mensagem de chamada da função e os valores que foram devolvidos pela mensagem `search_courses`. Isto dá ao LLM toda a informação de que precisa para\n",
    "responder ao utilizador em linguagem natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafio de Código\n",
    "\n",
    "Bom trabalho! Para continuares a aprender sobre OpenAI Function Calling, podes construir: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Mais parâmetros da função que possam ajudar os utilizadores a encontrar mais cursos. Podes encontrar os parâmetros disponíveis da API aqui:\n",
    " - Criar outra chamada de função que recolha mais informações do utilizador, como a sua língua materna\n",
    " - Implementar tratamento de erros para quando a chamada da função e/ou da API não devolve cursos adequados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Aviso Legal**:  \nEste documento foi traduzido utilizando o serviço de tradução automática [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos pela precisão, tenha em atenção que traduções automáticas podem conter erros ou imprecisões. O documento original, na sua língua nativa, deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se a tradução profissional por um humano. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas resultantes da utilização desta tradução.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T20:58:49+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "pt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}