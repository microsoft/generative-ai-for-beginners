{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Capítulo 7: Construir Aplicações de Chat\n",
    "## Introdução Rápida à API OpenAI\n",
    "\n",
    "Este notebook foi adaptado do [Repositório de Exemplos Azure OpenAI](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst), que inclui notebooks que acedem aos serviços [Azure OpenAI](notebook-azure-openai.ipynb).\n",
    "\n",
    "A API Python da OpenAI funciona também com os Modelos Azure OpenAI, com algumas modificações. Saiba mais sobre as diferenças aqui: [Como alternar entre os endpoints OpenAI e Azure OpenAI com Python](https://learn.microsoft.com/azure/ai-services/openai/how-to/switching-endpoints?WT.mc_id=academic-109527-jasmineg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Visão Geral  \n",
    "\"Os grandes modelos de linguagem são funções que transformam texto em texto. Dado um texto de entrada, um grande modelo de linguagem tenta prever o texto que virá a seguir\"(1). Este notebook de \"início rápido\" vai apresentar aos utilizadores conceitos fundamentais sobre LLM, os requisitos principais do pacote para começar a usar o AML, uma breve introdução ao design de prompts e vários exemplos curtos de diferentes casos de utilização.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Índice  \n",
    "\n",
    "[Visão Geral](../../../../07-building-chat-applications/python)  \n",
    "[Como usar o Serviço OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[1. Criar o seu Serviço OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[2. Instalação](../../../../07-building-chat-applications/python)    \n",
    "[3. Credenciais](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Casos de Utilização](../../../../07-building-chat-applications/python)    \n",
    "[1. Resumir Texto](../../../../07-building-chat-applications/python)  \n",
    "[2. Classificar Texto](../../../../07-building-chat-applications/python)  \n",
    "[3. Gerar Novos Nomes de Produtos](../../../../07-building-chat-applications/python)  \n",
    "[4. Afinar um Classificador](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Referências](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Crie o seu primeiro prompt  \n",
    "Este breve exercício vai dar-lhe uma introdução básica sobre como enviar prompts para um modelo da OpenAI para uma tarefa simples de \"resumo\".\n",
    "\n",
    "\n",
    "**Passos**:  \n",
    "1. Instale a biblioteca OpenAI no seu ambiente python  \n",
    "2. Carregue as bibliotecas auxiliares padrão e defina as suas credenciais de segurança habituais para o Serviço OpenAI que criou  \n",
    "3. Escolha um modelo para a sua tarefa  \n",
    "4. Crie um prompt simples para o modelo  \n",
    "5. Envie o seu pedido para a API do modelo!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\")\n",
    "assert API_KEY, \"ERROR: OpenAI Key is missing\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Encontrar o modelo certo  \n",
    "Os modelos GPT-3.5-turbo ou GPT-4 conseguem compreender e gerar linguagem natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Design de Prompts  \n",
    "\n",
    "\"A magia dos grandes modelos de linguagem é que, ao serem treinados para minimizar este erro de previsão em enormes quantidades de texto, acabam por aprender conceitos úteis para estas previsões. Por exemplo, aprendem conceitos como\"(1):\n",
    "\n",
    "* como se escreve\n",
    "* como funciona a gramática\n",
    "* como parafrasear\n",
    "* como responder a perguntas\n",
    "* como manter uma conversa\n",
    "* como escrever em várias línguas\n",
    "* como programar\n",
    "* etc.\n",
    "\n",
    "#### Como controlar um grande modelo de linguagem  \n",
    "\"De todos os inputs para um grande modelo de linguagem, o mais influente é, de longe, o prompt de texto(1).\n",
    "\n",
    "Os grandes modelos de linguagem podem ser orientados a produzir resultados de várias formas:\n",
    "\n",
    "Instrução: Dizer ao modelo o que pretende\n",
    "Completação: Induzir o modelo a completar o início do que deseja\n",
    "Demonstração: Mostrar ao modelo o que pretende, com:\n",
    "Alguns exemplos no prompt\n",
    "Centenas ou milhares de exemplos num conjunto de dados de treino para afinação\"\n",
    "\n",
    "\n",
    "\n",
    "#### Existem três orientações básicas para criar prompts:\n",
    "\n",
    "**Mostre e explique**. Seja claro sobre o que pretende, seja através de instruções, exemplos ou uma combinação dos dois. Se quiser que o modelo ordene uma lista de itens por ordem alfabética ou que classifique um parágrafo por sentimento, mostre-lhe que é isso que pretende.\n",
    "\n",
    "**Forneça dados de qualidade**. Se está a tentar criar um classificador ou fazer com que o modelo siga um padrão, certifique-se de que há exemplos suficientes. Não se esqueça de rever os seus exemplos — o modelo normalmente é suficientemente inteligente para perceber erros ortográficos básicos e dar-lhe uma resposta, mas também pode assumir que foi intencional e isso pode afetar a resposta.\n",
    "\n",
    "**Verifique as suas definições.** As definições de temperature e top_p controlam o quão determinístico é o modelo ao gerar uma resposta. Se pedir uma resposta em que só há uma solução correta, deve definir estes valores mais baixos. Se procura respostas mais variadas, pode querer defini-los mais altos. O erro mais comum nestas definições é assumir que são controlos de \"inteligência\" ou \"criatividade\".\n",
    "\n",
    "\n",
    "Fonte: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Resumir Texto  \n",
    "#### Desafio  \n",
    "Resume texto adicionando um 'tl;dr:' no final de uma passagem. Repara como o modelo consegue realizar várias tarefas sem instruções adicionais. Podes experimentar com prompts mais descritivos do que tl;dr para alterar o comportamento do modelo e personalizar o resumo que recebes(3).  \n",
    "\n",
    "Trabalhos recentes demonstraram ganhos significativos em várias tarefas e benchmarks de PLN ao pré-treinar com um grande corpus de texto, seguido de afinação para uma tarefa específica. Embora normalmente a arquitetura seja independente da tarefa, este método ainda exige conjuntos de dados de afinação específicos com milhares ou dezenas de milhares de exemplos. Em contraste, os humanos conseguem geralmente realizar uma nova tarefa linguística apenas com alguns exemplos ou instruções simples – algo com que os sistemas de PLN atuais ainda têm bastante dificuldade. Aqui mostramos que aumentar a escala dos modelos de linguagem melhora bastante o desempenho independente da tarefa, mesmo com poucos exemplos, chegando por vezes a ser competitivo com abordagens anteriores de afinação de última geração. \n",
    "\n",
    "\n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Exercícios para vários casos de uso  \n",
    "1. Resumir Texto  \n",
    "2. Classificar Texto  \n",
    "3. Gerar Novos Nomes de Produtos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Classificar Texto  \n",
    "#### Desafio  \n",
    "Classificar itens em categorias fornecidas no momento da inferência. No exemplo seguinte, fornecemos tanto as categorias como o texto a classificar no prompt (*playground_reference).\n",
    "\n",
    "Pedido do cliente: Olá, uma das teclas do teclado do meu portátil partiu-se recentemente e vou precisar de uma substituição:\n",
    "\n",
    "Categoria classificada:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Gerar Novos Nomes de Produtos\n",
    "#### Desafio\n",
    "Cria nomes de produtos a partir de palavras de exemplo. Aqui incluímos na instrução informações sobre o produto para o qual vamos gerar nomes. Também fornecemos um exemplo semelhante para mostrar o padrão que pretendemos receber. Definimos ainda o valor de temperatura alto para aumentar a aleatoriedade e obter respostas mais inovadoras.\n",
    "\n",
    "Descrição do produto: Uma máquina de fazer batidos em casa\n",
    "Palavras-chave: rápido, saudável, compacto.\n",
    "Nomes de produtos: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Descrição do produto: Um par de sapatos que se adapta a qualquer tamanho de pé.\n",
    "Palavras-chave: adaptável, ajuste, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Referências  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Exemplos do OpenAI Studio](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Boas práticas para afinar o GPT-3 para classificar texto](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Para Mais Ajuda  \n",
    "[Equipa de Comercialização da OpenAI](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Contribuidores\n",
    "* Louis Li\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Aviso Legal**:  \nEste documento foi traduzido utilizando o serviço de tradução automática [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos pela precisão, tenha em atenção que traduções automáticas podem conter erros ou imprecisões. O documento original, na sua língua nativa, deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se a tradução profissional por um humano. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas resultantes da utilização desta tradução.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "1854bdde1dd56b366f1f6c9122f7d304",
   "translation_date": "2025-08-25T18:12:59+00:00",
   "source_file": "07-building-chat-applications/python/oai-assignment.ipynb",
   "language_code": "pt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}