<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b2651fb16bcfbc62b8e518751ed90fdb",
  "translation_date": "2025-10-18T00:36:50+00:00",
  "source_file": "05-advanced-prompts/README.md",
  "language_code": "pt"
}
-->
# Criando Prompts Avan√ßados

[![Criando Prompts Avan√ßados](../../../translated_images/05-lesson-banner.522610fd4a2cd82dbed66bb7e6fe104ed6da172e085dbb4d9100b28dc73ed435.pt.png)](https://youtu.be/BAjzkaCdRok?si=NmUIyRf7-cDgbjtt)

Vamos recapitular alguns aprendizados do cap√≠tulo anterior:

> A _engenharia de prompts_ √© o processo pelo qual **orientamos o modelo para respostas mais relevantes** fornecendo instru√ß√µes ou contexto mais √∫teis.

Existem tamb√©m dois passos para escrever prompts: construir o prompt, fornecendo contexto relevante, e _otimiza√ß√£o_, que √© como melhorar gradualmente o prompt.

Neste ponto, temos uma compreens√£o b√°sica de como escrever prompts, mas precisamos ir mais fundo. Neste cap√≠tulo, voc√™ passar√° de experimentar v√°rios prompts para entender por que um prompt √© melhor que outro. Voc√™ aprender√° a construir prompts seguindo algumas t√©cnicas b√°sicas que podem ser aplicadas a qualquer LLM.

## Introdu√ß√£o

Neste cap√≠tulo, abordaremos os seguintes t√≥picos:

- Ampliar seu conhecimento sobre engenharia de prompts aplicando diferentes t√©cnicas aos seus prompts.
- Configurar seus prompts para variar o resultado.

## Objetivos de aprendizagem

Ap√≥s concluir esta li√ß√£o, voc√™ ser√° capaz de:

- Aplicar t√©cnicas de engenharia de prompts que melhoram o resultado dos seus prompts.
- Realizar prompts que sejam variados ou determin√≠sticos.

## Engenharia de prompts

Engenharia de prompts √© o processo de criar prompts que produzam o resultado desejado. H√° mais na engenharia de prompts do que apenas escrever um texto. Engenharia de prompts n√£o √© uma disciplina de engenharia, mas sim um conjunto de t√©cnicas que voc√™ pode aplicar para obter o resultado desejado.

### Um exemplo de prompt

Vamos pegar um prompt b√°sico como este:

> Gere 10 perguntas sobre geografia.

Neste prompt, voc√™ est√° aplicando, na verdade, um conjunto de diferentes t√©cnicas de prompts.

Vamos analisar isso.

- **Contexto**, voc√™ especifica que deve ser sobre "geografia".
- **Limita√ß√£o do resultado**, voc√™ quer no m√°ximo 10 perguntas.

### Limita√ß√µes de prompts simples

Voc√™ pode ou n√£o obter o resultado desejado. Suas perguntas ser√£o geradas, mas geografia √© um tema amplo e voc√™ pode n√£o obter o que deseja devido aos seguintes motivos:

- **Tema amplo**, voc√™ n√£o sabe se ser√° sobre pa√≠ses, capitais, rios e assim por diante.
- **Formato**, e se voc√™ quisesse que as perguntas fossem formatadas de uma determinada maneira?

Como pode ver, h√° muito a considerar ao criar prompts.

At√© agora, vimos um exemplo de prompt simples, mas a IA generativa √© capaz de muito mais para ajudar pessoas em uma variedade de fun√ß√µes e ind√∫strias. Vamos explorar algumas t√©cnicas b√°sicas a seguir.

### T√©cnicas para criar prompts

Primeiro, precisamos entender que criar prompts √© uma propriedade _emergente_ de um LLM, o que significa que isso n√£o √© uma funcionalidade embutida no modelo, mas algo que descobrimos ao usar o modelo.

Existem algumas t√©cnicas b√°sicas que podemos usar para criar prompts em um LLM. Vamos explor√°-las.

- **Prompt sem exemplos (Zero-shot prompting)**, esta √© a forma mais b√°sica de criar prompts. √â um √∫nico prompt solicitando uma resposta do LLM com base apenas nos seus dados de treinamento.
- **Prompt com poucos exemplos (Few-shot prompting)**, este tipo de prompt orienta o LLM fornecendo 1 ou mais exemplos nos quais ele pode se basear para gerar sua resposta.
- **Cadeia de racioc√≠nio (Chain-of-thought)**, este tipo de prompt instrui o LLM sobre como dividir um problema em etapas.
- **Conhecimento gerado**, para melhorar a resposta de um prompt, voc√™ pode fornecer fatos ou conhecimentos gerados adicionalmente ao seu prompt.
- **Do mais simples ao mais complexo (Least to most)**, como a cadeia de racioc√≠nio, esta t√©cnica trata de dividir um problema em uma s√©rie de etapas e pedir que essas etapas sejam realizadas em ordem.
- **Auto-refinamento**, esta t√©cnica trata de criticar a sa√≠da do LLM e depois pedir que ele melhore.
- **Prompt mai√™utico (Maieutic prompting)**. Aqui, voc√™ quer garantir que a resposta do LLM esteja correta e pede que ele explique v√°rias partes da resposta. Esta √© uma forma de auto-refinamento.

### Prompt sem exemplos (Zero-shot prompting)

Este estilo de prompt √© muito simples, consiste em um √∫nico prompt. Esta t√©cnica √© provavelmente o que voc√™ est√° usando ao come√ßar a aprender sobre LLMs. Aqui est√° um exemplo:

- Prompt: "O que √© √Ålgebra?"
- Resposta: "√Ålgebra √© um ramo da matem√°tica que estuda s√≠mbolos matem√°ticos e as regras para manipul√°-los."

### Prompt com poucos exemplos (Few-shot prompting)

Este estilo de prompt ajuda o modelo fornecendo alguns exemplos junto com a solicita√ß√£o. Consiste em um √∫nico prompt com dados adicionais espec√≠ficos da tarefa. Aqui est√° um exemplo:

- Prompt: "Escreva um poema no estilo de Shakespeare. Aqui est√£o alguns exemplos de sonetos de Shakespeare:
  Soneto 18: 'Devo comparar-te a um dia de ver√£o? √âs mais belo e mais temperado...'
  Soneto 116: 'N√£o deixemos que a uni√£o de mentes verdadeiras Admita impedimentos. O amor n√£o √© amor Que muda quando encontra mudan√ßa...'
  Soneto 132: 'Teus olhos eu amo, e eles, como se tivessem pena de mim, Sabendo que teu cora√ß√£o me atormenta com desd√©m,...'
  Agora, escreva um soneto sobre a beleza da lua."
- Resposta: "No c√©u, a lua suavemente brilha, Em luz prateada que lan√ßa sua gra√ßa gentil,..."

Os exemplos fornecem ao LLM o contexto, formato ou estilo do resultado desejado. Eles ajudam o modelo a entender a tarefa espec√≠fica e gerar respostas mais precisas e relevantes.

### Cadeia de racioc√≠nio (Chain-of-thought)

Cadeia de racioc√≠nio √© uma t√©cnica muito interessante, pois trata de levar o LLM por uma s√©rie de etapas. A ideia √© instruir o LLM de forma que ele entenda como fazer algo. Considere o seguinte exemplo, com e sem cadeia de racioc√≠nio:

    - Prompt: "Alice tem 5 ma√ß√£s, joga fora 3 ma√ß√£s, d√° 2 para Bob e Bob devolve uma, quantas ma√ß√£s Alice tem?"
    - Resposta: 5

O LLM responde com 5, o que est√° incorreto. A resposta correta √© 1 ma√ß√£, dado o c√°lculo (5 - 3 - 2 + 1 = 1).

Ent√£o, como podemos ensinar o LLM a fazer isso corretamente?

Vamos tentar a cadeia de racioc√≠nio. Aplicar a cadeia de racioc√≠nio significa:

1. Dar ao LLM um exemplo semelhante.
1. Mostrar o c√°lculo e como calcul√°-lo corretamente.
1. Fornecer o prompt original.

Veja como:

- Prompt: "Lisa tem 7 ma√ß√£s, joga fora 1 ma√ß√£, d√° 4 ma√ß√£s para Bart e Bart devolve uma:
  7 - 1 = 6
  6 - 4 = 2
  2 + 1 = 3  
  Alice tem 5 ma√ß√£s, joga fora 3 ma√ß√£s, d√° 2 para Bob e Bob devolve uma, quantas ma√ß√£s Alice tem?"
  Resposta: 1

Note como escrevemos prompts substancialmente mais longos com outro exemplo, um c√°lculo e depois o prompt original, e chegamos √† resposta correta, 1.

Como pode ver, a cadeia de racioc√≠nio √© uma t√©cnica muito poderosa.

### Conhecimento gerado

Muitas vezes, quando voc√™ quer construir um prompt, deseja faz√™-lo usando os dados da sua pr√≥pria empresa. Voc√™ quer que parte do prompt venha da empresa e a outra parte seja o prompt que realmente lhe interessa.

Como exemplo, este √© o aspecto que seu prompt pode ter se voc√™ estiver no setor de seguros:

```text
{{company}}: {{company_name}}
{{products}}:
{{products_list}}
Please suggest an insurance given the following budget and requirements:
Budget: {{budget}}
Requirements: {{requirements}}
```

Acima, voc√™ v√™ como o prompt √© constru√≠do usando um modelo. No modelo, h√° v√°rias vari√°veis, denotadas por `{{variable}}`, que ser√£o substitu√≠das por valores reais de uma API da empresa.

Aqui est√° um exemplo de como o prompt pode parecer uma vez que as vari√°veis tenham sido substitu√≠das por conte√∫do da sua empresa:

```text
Insurance company: ACME Insurance
Insurance products (cost per month):
- Car, cheap, 500 USD
- Car, expensive, 1100 USD
- Home, cheap, 600 USD
- Home, expensive, 1200 USD
- Life, cheap, 100 USD

Please suggest an insurance given the following budget and requirements:
Budget: $1000
Requirements: Car, Home, and Life insurance
```

Executar este prompt em um LLM produzir√° uma resposta como esta:

```output
Given the budget and requirements, we suggest the following insurance package from ACME Insurance:
- Car, cheap, 500 USD
- Home, cheap, 600 USD
- Life, cheap, 100 USD
Total cost: $1,200 USD
```

Como pode ver, ele tamb√©m sugere o seguro de vida, o que n√£o deveria. Este resultado √© uma indica√ß√£o de que precisamos otimizar o prompt, tornando-o mais claro sobre o que ele pode permitir. Ap√≥s alguns _testes e ajustes_, chegamos ao seguinte prompt:

```text
Insurance company: ACME Insurance
Insurance products (cost per month):
- type: Car, cheap, cost: 500 USD
- type: Car, expensive, cost: 1100 USD
- type: Home, cheap, cost: 600 USD
- type: Home, expensive, cost: 1200 USD
- type: Life, cheap, cost: 100 USD

Please suggest an insurance given the following budget and requirements:
Budget: $1000 restrict choice to types: Car, Home
```

Note como adicionar _tipo_ e _custo_ e tamb√©m usar a palavra-chave _restringir_ ajuda o LLM a entender o que queremos.

Agora obtemos a seguinte resposta:

```output
Given the budget and requirements, we suggest the Car, Cheap insurance product which costs 500 USD per month.
```

O objetivo deste exemplo foi mostrar que, mesmo quando usamos uma t√©cnica b√°sica como _conhecimento gerado_, ainda precisamos otimizar o prompt na maioria dos casos para obter o resultado desejado.

### Do mais simples ao mais complexo (Least-to-most)

A ideia do prompt do mais simples ao mais complexo √© dividir um problema maior em subproblemas. Dessa forma, voc√™ ajuda a guiar o LLM sobre como "conquistar" o problema maior. Um bom exemplo pode ser na ci√™ncia de dados, onde voc√™ pode pedir ao LLM para dividir um problema assim:

> Prompt: Como realizar ci√™ncia de dados em 5 etapas?

Com seu assistente de IA respondendo com:

1. Coletar dados
1. Limpar dados
1. Analisar dados
1. Plotar dados
1. Apresentar dados

### Auto-refinamento, criticar os resultados

Com IAs generativas e LLMs, voc√™ n√£o pode confiar cegamente no resultado. √â necess√°rio verificar. Afinal, o LLM est√° apenas apresentando o que √© mais prov√°vel dizer em seguida, n√£o o que √© correto. Portanto, uma boa ideia √© pedir ao LLM para criticar a si mesmo, o que nos leva √† t√©cnica de auto-refinamento.

Como funciona √© que voc√™ segue os seguintes passos:

1. Prompt inicial pedindo ao LLM para resolver um problema
1. O LLM responde
1. Voc√™ critica a resposta e pede √† IA para melhorar
1. O LLM responde novamente, desta vez considerando a cr√≠tica e sugerindo solu√ß√µes que ele elaborou

Voc√™ pode repetir este processo quantas vezes quiser.

Aqui est√° um exemplo usando esta t√©cnica:

> Prompt: "Crie uma API Web em Python com rotas para produtos e clientes"

Resposta da IA:

```python
import flask
app = Flask(__name__)

@app.route('/products')
def products():
    return 'Products'

@app.route('/customers')
def customers():
    return 'Customers'
```

> Prompt: sugira 3 melhorias para o c√≥digo acima

Resposta da IA:

```text
Sure, here are three suggestions to improve the code:

Import the correct module: The first line should import Flask instead of flask. The correct import statement should be from flask import Flask.

Add a main function: Add a if __name__ == '__main__': block to the end of the file to run the application when the file is executed.

Use f-strings: Use f-strings to format the return statements in the route functions. For example, return 'Products' can be changed to return f'This is the {request.path} page'.
```

```python
from flask import Flask, request

app = Flask(__name__)

@app.route('/products')
def products():
    return f'This is the {request.path} page'

@app.route('/customers')
def customers():
    return f'This is the {request.path} page'

if __name__ == '__main__':
    app.run()
```

Como pode ver, a resposta acima da IA est√° melhorando o primeiro c√≥digo sugerido gra√ßas √† cr√≠tica feita √† primeira resposta.

### Prompt mai√™utico (Maieutic prompting)

O prompt mai√™utico √© uma t√©cnica semelhante ao auto-refinamento, mas trata mais de pedir ao LLM para se explicar. O objetivo √© reduzir inconsist√™ncias na sa√≠da do LLM para garantir que ele chegue √† resposta correta. O fluxo de trabalho a seguir √©:

1. Pedir ao LLM para responder a uma pergunta
1. Para cada parte da resposta, pedir ao LLM para explic√°-la mais detalhadamente.
1. Se houver inconsist√™ncias, descartar as partes que s√£o inconsistentes.

Repita os passos 2 e 3 at√© que tenha analisado todas as partes e esteja satisfeito com a resposta.

Aqui est√° um exemplo de prompt:

> Prompt: Como posso criar um plano de crise para mitigar uma pandemia em 5 etapas?
> Resposta do LLM:

```text
1. Identify the risks
1. Identify the stakeholders
1. Identify the resources
1. Identify the actions
1. Identify the communication channels
```

Ele identificou 5 etapas, mas podemos determinar se isso est√° correto? Vamos pedir ao LLM para explicar cada etapa:

> Prompt: Explique a primeira etapa em mais detalhes, quais s√£o os riscos detalhados de uma pandemia?

```text
The risks are the following:

- Loss of life
- Loss of business
- Loss of reputation
- Loss of customers
- Loss of employees
```

Neste ponto, queremos ter certeza de que o LLM est√° correto, ent√£o pedimos que ele explique o primeiro risco em mais detalhes e esperamos que seja consistente com a resposta acima:

> Prompt: Em uma pandemia, qual √© o maior risco e por qu√™?

```text
The biggest risk is loss of life because it's the most important thing to protect.
```

> Quais s√£o os dois maiores riscos em uma pandemia?

```text
The two biggest risks are loss of life and loss of business.
```

Neste ponto, o LLM √© consistente e menciona "vida" e "neg√≥cios" como os dois maiores riscos. Agora podemos continuar para a pr√≥xima etapa e nos sentir razoavelmente confiantes. No entanto, n√£o devemos confiar cegamente no LLM, sempre devemos verificar o resultado.

## Variar o seu resultado

LLMs s√£o naturalmente n√£o determin√≠sticos, o que significa que voc√™ obter√° resultados diferentes cada vez que executar o mesmo prompt. Experimente o seguinte prompt, por exemplo:

> "Gere c√≥digo para uma API Web em Python"

```python
# Import necessary modules
import flask
from flask import request, jsonify

# Create a Flask app
app = flask.Flask(__name__)

# Create a route for the API
@app.route('/api', methods=['GET'])
def api():
    # Get the data from the request
    data = request.args

    # Process the data
    result = process_data(data)

    # Return the result as JSON
    return jsonify(result)

# Function to process the data
def process_data(data):
    # Do something with the data
    result = data + 1

    # Return the result
    return result

# Run the app
if __name__ == '__main__':
    app.run()

```

Executar o mesmo prompt novamente gera uma resposta ligeiramente diferente:

```python
#import necessary packages
import flask
from flask import request, jsonify

#create the Flask application
app = flask.Flask(__name__)
app.config["DEBUG"] = True

#create a list of books
books = [
    {'id': 0,
     'title': 'A Fire Upon the Deep',
     'author': 'Vernor Vinge',
     'first_sentence': 'The coldsleep itself was dreamless.',
     'year_published': '1992'},
    {'id': 1,
     'title': 'The Ones Who Walk Away From Omelas',
     'author': 'Ursula K. Le Guin',
     'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.',
     'published': '1973'},
    {'id': 2,
     'title': 'Dhalgren',
     'author': 'Samuel R. Delany',
     'first_sentence': 'to wound the autumnal city.',
     'published': '1975'}
]

#create an endpoint for the API
@app.route('/', methods=['GET'])
def home():
    return '''<h1>Books API</h1>
<p>A prototype API for retrieving books.</p>'''

#create an endpoint to return all books
@app.route('/api/v1/resources/books/all', methods=['GET'])
def api_all():
    return jsonify(books)

#create an endpoint to return a single book
@app.route('/api/v1/resources/books', methods=['GET'])
def api_id():
    #check if an ID was provided as part of the URL
    #if ID is provided, assign it to a variable
    #if no ID is provided, display an error
    if 'id' in request.args:
        id = int(request.args['id'])
    else:
        return "Error: No id field provided. Please specify an id."

    #create an empty list for our results
    results = []

    #loop through the data and match results that fit the requested ID
    #IDs are unique, but other fields might return many results
    for book in books:
        if book['id'] == id:
            results.append(book)

    #use the jsonify function from Flask to convert our list of
    #Python dictionaries to the JSON format
    return jsonify(results)

app.run()
```

> Ent√£o, o resultado variado √© um problema?

Depende do que voc√™ est√° tentando fazer. Se voc√™ quer uma resposta espec√≠fica, ent√£o √© um problema. Se voc√™ est√° bem com um resultado variado, como "Gere 3 perguntas sobre geografia", ent√£o n√£o √© um problema.

### Usando temperatura para variar o seu resultado

Ok, ent√£o decidimos que queremos limitar o resultado para ser mais previs√≠vel, ou seja, mais determin√≠stico. Como fazemos isso?

Temperatura √© um valor entre 0 e 1, onde 0 √© o mais determin√≠stico e 1 √© o mais variado. O valor padr√£o √© 0.7. Vamos ver o que acontece com duas execu√ß√µes do mesmo prompt com a temperatura definida em 0.1:

> "Gere c√≥digo para uma API Web em Python"

```python
#import necessary libraries
import flask
from flask import request, jsonify

#create an instance of the Flask class
app = flask.Flask(__name__)

#create an endpoint for the API
@app.route('/api/v1/endpoint', methods=['GET'])
def api_endpoint():
    #get the request data
    data = request.get_json()

    #process the data
    result = process_data(data)

    #return the result
    return jsonify(result)

#function to process the data
def process_data(data):
    #process the data
    result = {'result': 'success'}

    #return the result
    return result

#run the Flask app
if __name__ == '__main__':
    app.run()

```

Executar o prompt novamente nos d√° este resultado:

```python
#import necessary libraries
import flask
from flask import request, jsonify

#create a Flask app
app = flask.Flask(__name__)

#create a route for the API
@app.route('/api', methods=['GET'])
def api_call():
    #get the data from the request
    data = request.get_json()

    #process the data
    result = process_data(data)

    #return the result
    return jsonify(result)

#define the process_data function
def process_data(data):
    #do something with the data
    result = data + 1

    #return the result
    return result

#run the app
if __name__ == '__main__':
    app.run()

```

H√° apenas uma pequena diferen√ßa entre esses dois resultados. Vamos fazer o oposto desta vez, definir a temperatura para 0.9:

```python
# Import necessary libraries
import flask
from flask import request, jsonify

# Create a Flask app
app = flask.Flask(__name__)

# Create a route for the API
@app.route('/api', methods=['GET'])
def api_call():
    # Get the data from the request
    data = request.args

    # Process the data
    result = process_data(data)

    # Return the result
    return jsonify(result)

# Function to process the data
def process_data(data):
    # Do something with the data
    result = data + 1

    # Return the result
    return result

# Run the app
if __name__ == '__main__':
    app.run()

```

e a segunda tentativa com o valor de temperatura definido em 0.9:

```python
import flask
from flask import request, jsonify

# create the Flask app
app = flask.Flask(__name__)
app.config['DEBUG'] = True

# create some test data
books = [
    {'id': 0, 'title': 'A Fire Upon The Deep', 'author': 'Vernor Vinge', 'first_sentence': 'The coldsleep itself was dreamless.', 'year_published': '1992'},
    {'id': 1, 'title': 'The Ones Who Walk Away From Omelas', 'author': 'Ursula K. Le Guin', 'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.', 'published': '1973'},
    {'id': 2, 'title': 'Dhalgren', 'author': 'Samuel R. Delany', 'first_sentence': 'to wound the autumnal city.', 'published': '1975'}
]

# create an endpoint
@app.route('/', methods=['GET'])
def home():
    return '''<h1>Welcome to our book API!</h1>'''

@app.route('/api/v1/resources/books

```

Como pode ver, os resultados n√£o poderiam ser mais variados.

> Note que existem mais par√¢metros que pode alterar para variar o resultado, como top-k, top-p, penaliza√ß√£o de repeti√ß√£o, penaliza√ß√£o de comprimento e penaliza√ß√£o de diversidade, mas estes est√£o fora do √¢mbito deste curr√≠culo.

## Boas pr√°ticas

Existem muitas pr√°ticas que pode aplicar para tentar obter o que deseja. Vai encontrar o seu pr√≥prio estilo √† medida que utiliza mais e mais o prompting.

Al√©m das t√©cnicas que abord√°mos, h√° algumas boas pr√°ticas a considerar ao criar prompts para um LLM.

Aqui est√£o algumas boas pr√°ticas a considerar:

- **Especificar o contexto**. O contexto √© importante; quanto mais puder especificar, como dom√≠nio, t√≥pico, etc., melhor.
- Limitar o resultado. Se quiser um n√∫mero espec√≠fico de itens ou um comprimento espec√≠fico, especifique-o.
- **Especificar o qu√™ e como**. Lembre-se de mencionar tanto o que quer como como quer, por exemplo, "Crie uma API Web em Python com rotas para produtos e clientes, dividida em 3 ficheiros".
- **Utilizar templates**. Muitas vezes, vai querer enriquecer os seus prompts com dados da sua empresa. Utilize templates para fazer isso. Os templates podem ter vari√°veis que substitui por dados reais.
- **Escrever corretamente**. Os LLMs podem fornecer-lhe uma resposta correta, mas se escrever corretamente, obter√° uma resposta melhor.

## Exerc√≠cio

Aqui est√° um c√≥digo em Python que mostra como construir uma API simples usando Flask:

```python
from flask import Flask, request

app = Flask(__name__)

@app.route('/')
def hello():
    name = request.args.get('name', 'World')
    return f'Hello, {name}!'

if __name__ == '__main__':
    app.run()
```

Utilize um assistente de IA como o GitHub Copilot ou o ChatGPT e aplique a t√©cnica de "auto-refinamento" para melhorar o c√≥digo.

## Solu√ß√£o

Por favor, tente resolver o exerc√≠cio adicionando prompts adequados ao c√≥digo.

> [!TIP]
> Formule um prompt para pedir melhorias; √© uma boa ideia limitar o n√∫mero de melhorias. Tamb√©m pode pedir para melhorar de uma forma espec√≠fica, por exemplo, arquitetura, desempenho, seguran√ßa, etc.

[Solu√ß√£o](../../../05-advanced-prompts/python/aoai-solution.py)

## Verifica√ß√£o de conhecimento

Por que usaria o prompting de cadeia de pensamento? Mostre-me 1 resposta correta e 2 respostas incorretas.

1. Para ensinar o LLM a resolver um problema.
1. B, Para ensinar o LLM a encontrar erros no c√≥digo.
1. C, Para instruir o LLM a propor diferentes solu√ß√µes.

A: 1, porque o prompting de cadeia de pensamento consiste em mostrar ao LLM como resolver um problema fornecendo-lhe uma s√©rie de passos, e problemas semelhantes e como foram resolvidos.

## üöÄ Desafio

Acabou de utilizar a t√©cnica de auto-refinamento no exerc√≠cio. Pegue em qualquer programa que tenha criado e considere que melhorias gostaria de aplicar. Agora utilize a t√©cnica de auto-refinamento para aplicar as altera√ß√µes propostas. O que achou do resultado, melhor ou pior?

## Excelente trabalho! Continue a aprender

Depois de completar esta li√ß√£o, consulte a nossa [cole√ß√£o de aprendizagem de IA generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para continuar a aprofundar os seus conhecimentos sobre IA generativa!

Avance para a Li√ß√£o 6, onde aplicaremos os nossos conhecimentos de Engenharia de Prompts [criando aplica√ß√µes de gera√ß√£o de texto](../06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se uma tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.