[![Open Source Models](../../../translated_images/18-lesson-banner.png?WT.73626ba24f59a39704c5137a18c9de8b23179ea0e1ace42c97e02f0310adcee0.ja.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# LLMのファインチューニング

大規模言語モデルを使用して生成AIアプリケーションを構築することには、新しい課題が伴います。主な問題は、特定のユーザーリクエストに対してモデルが生成するコンテンツの品質（正確性と関連性）を確保することです。前回のレッスンでは、既存のモデルへのプロンプト入力を変更することでこの問題を解決しようとするプロンプトエンジニアリングやリトリーバル拡張生成といった技術について説明しました。

今日のレッスンでは、モデル自体を追加データで再訓練することでこの課題に対処しようとする**ファインチューニング**という第三の技術について説明します。それでは、詳細を見ていきましょう。

## 学習目標

このレッスンでは、事前訓練された言語モデルのファインチューニングの概念を紹介し、このアプローチの利点と課題を探り、生成AIモデルのパフォーマンスを向上させるためにいつどのようにファインチューニングを使用するかについてのガイダンスを提供します。

このレッスンを終えると、次の質問に答えられるようになります：

- 言語モデルのファインチューニングとは何ですか？
- ファインチューニングはいつ、なぜ有用なのですか？
- 事前訓練されたモデルをどのようにファインチューニングできますか？
- ファインチューニングの限界は何ですか？

準備はいいですか？始めましょう。

## イラストガイド

本題に入る前にカバーする内容の全体像を知りたいですか？このレッスンの学習の旅を説明するイラストガイドをチェックしてください - ファインチューニングの核心概念と動機を学ぶことから、プロセスと実行のベストプラクティスを理解することまで。このトピックは探求にとても面白いので、自分で学ぶ旅をサポートするための追加リンクを提供する[リソース](./RESOURCES.md?WT.mc_id=academic-105485-koreyst)ページを忘れずにチェックしてください！

![Illustrated Guide to Fine Tuning Language Models](../../../translated_images/18-fine-tuning-sketchnote.png?WT.6cca0798e805b67b1f22beaba7478f40066f3a2879380a0e27dbc70ac1dc7832.ja.mc_id=academic-105485-koreyst)

## 言語モデルのファインチューニングとは何ですか？

定義上、大規模言語モデルはインターネットを含む多様なソースからの大量のテキストで事前訓練されています。前回のレッスンで学んだように、ユーザーの質問（「プロンプト」）に対するモデルの応答の品質を向上させるために、_プロンプトエンジニアリング_や_リトリーバル拡張生成_のような技術が必要です。

一般的なプロンプトエンジニアリング技術は、_指示_（明示的なガイダンス）を提供するか、_いくつかの例を示す_（暗黙のガイダンス）ことで応答に期待されるものについてモデルにより多くのガイダンスを与えることを含みます。これは_少数ショット学習_と呼ばれますが、2つの制限があります：

- モデルのトークン制限が与えられる例の数を制限し、その効果を制限する可能性があります。
- モデルのトークンコストが、すべてのプロンプトに例を追加するのを高価にし、柔軟性を制限する可能性があります。

ファインチューニングは、事前訓練されたモデルを取り、新しいデータで再訓練して特定のタスクのパフォーマンスを向上させる一般的な機械学習システムの実践です。言語モデルの文脈では、特定のタスクやアプリケーションドメインのためにキュレートされた一連の例を使って事前訓練されたモデルをファインチューニングし、その特定のタスクやドメインに対してより正確で関連性のある**カスタムモデル**を作成できます。ファインチューニングの副次的な利点は、少数ショット学習に必要な例の数を減らし、トークンの使用と関連コストを削減できることです。

## モデルをいつ、なぜファインチューニングするべきですか？

_この_コンテキストでファインチューニングについて話すとき、私たちは**監督された**ファインチューニングを指しており、再訓練は元のトレーニングデータセットの一部ではなかった**新しいデータを追加すること**によって行われます。これは、元のデータで異なるハイパーパラメータを使用してモデルを再訓練する非監督ファインチューニングアプローチとは異なります。

覚えておくべき重要なことは、ファインチューニングは望ましい結果を得るためにある程度の専門知識を必要とする高度な技術であるということです。誤って行うと、期待される改善が得られないか、対象ドメインのモデルのパフォーマンスを悪化させる可能性があります。

したがって、言語モデルを「どのように」ファインチューニングするかを学ぶ前に、この方法を選ぶべき「理由」とファインチューニングのプロセスを開始する「タイミング」を知る必要があります。まず次の質問を自問してください：

- **ユースケース**: ファインチューニングのための_ユースケース_は何ですか？現在の事前訓練されたモデルのどの側面を改善したいですか？
- **代替案**: 望ましい結果を達成するために_他の技術_を試しましたか？それらを使用して比較のためのベースラインを作成してください。
  - プロンプトエンジニアリング: 関連するプロンプト応答の例を用いた少数ショットプロンプトなどの技術を試してください。応答の品質を評価します。
  - リトリーバル拡張生成: データを検索して取得したクエリ結果でプロンプトを補強することを試みてください。応答の品質を評価します。
- **コスト**: ファインチューニングのコストを特定しましたか？
  - チューニング可能性 - 事前訓練されたモデルがファインチューニング可能ですか？
  - 努力 - トレーニングデータの準備、モデルの評価と改善のための努力。
  - コンピュート - ファインチューニングジョブを実行し、ファインチューニングされたモデルをデプロイするためのコンピュート。
  - データ - ファインチューニングの影響を与えるのに十分な品質の例へのアクセス。
- **利益**: ファインチューニングの利益を確認しましたか？
  - 品質 - ファインチューニングされたモデルがベースラインを上回りましたか？
  - コスト - プロンプトを簡素化することでトークン使用量を削減しますか？
  - 拡張性 - 新しいドメインにベースモデルを再利用できますか？

これらの質問に答えることで、ファインチューニングがあなたのユースケースにとって正しいアプローチかどうかを決定することができます。理想的には、利益がコストを上回る場合にのみアプローチが有効です。進めることを決定したら、事前訓練されたモデルをどのようにファインチューニングできるかを考える時です。

意思決定プロセスについてさらに洞察を得たいですか？[ファインチューニングするかしないか](https://www.youtube.com/watch?v=0Jo-z-MFxJs)を見てください。

## 事前訓練されたモデルをどのようにファインチューニングできますか？

事前訓練されたモデルをファインチューニングするには、以下が必要です：

- ファインチューニングするための事前訓練されたモデル
- ファインチューニングに使用するデータセット
- ファインチューニングジョブを実行するためのトレーニング環境
- ファインチューニングされたモデルをデプロイするためのホスティング環境

## ファインチューニングの実践

以下のリソースは、選択されたモデルとキュレートされたデータセットを使用した実際の例をステップバイステップで説明するチュートリアルを提供します。これらのチュートリアルを進めるには、特定のプロバイダーのアカウントと、関連するモデルおよびデータセットへのアクセスが必要です。

| プロバイダー | チュートリアル                                                                                                                                                                       | 説明                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                | トレーニングデータを準備し、ファインチューニングジョブを実行し、推論にファインチューニングされたモデルを使用することで、特定のドメイン（「レシピアシスタント」）の`gpt-35-turbo`をファインチューニングする方法を学びます。                                                                                                                                                                                                                                              |
| Azure OpenAI | [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | トレーニングデータを作成してアップロードし、ファインチューニングジョブを実行する手順を踏むことで、`gpt-35-turbo-0613`モデルを**Azure上で**ファインチューニングする方法を学びます。デプロイして新しいモデルを使用します。                                                                                                                                                                                                                                                                 |
| Hugging Face | [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | このブログ投稿では、[transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst)ライブラリと[Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst)を使用して、オープン[データセット](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst)で_open LLM_（例：`CodeLlama 7B`）をファインチューニングする方法を紹介します。 |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 🤗 AutoTrain | [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                         | AutoTrain（またはAutoTrain Advanced）は、Hugging Faceによって開発されたPythonライブラリで、LLMファインチューニングを含む多くの異なるタスクのファインチューニングを可能にします。AutoTrainはコード不要のソリューションであり、ファインチューニングは自身のクラウド、Hugging Face Spaces、またはローカルで行うことができます。ウェブベースのGUI、CLI、yaml設定ファイルを使ったトレーニングの両方をサポートしています。                                                                               |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## 課題

上記のチュートリアルの1つを選択し、それを進めてください。_これらのチュートリアルのバージョンをこのリポジトリのJupyterノートブックで参照用に複製する場合があります。最新バージョンを取得するには、直接オリジナルソースを使用してください。_

## 素晴らしい仕事です！学習を続けましょう。

このレッスンを完了した後は、[Generative AI Learningコレクション](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)をチェックして、生成AIの知識をさらに高めましょう！

おめでとうございます！！このコースのv2シリーズの最終レッスンを完了しました！学習と構築を続けてください。**[RESOURCES](RESOURCES.md?WT.mc_id=academic-105485-koreyst)ページをチェックして、このトピックに関する追加の提案のリストを見てください。

私たちのv1シリーズのレッスンも、より多くの課題とコンセプトで更新されました。知識をリフレッシュするために少し時間を取ってください。そして、これらのレッスンをコミュニティのために改善するために、[質問やフィードバックを共有](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst)してください。

**免責事項**:  
この文書は、機械ベースのAI翻訳サービスを使用して翻訳されています。正確さを期しておりますが、自動翻訳には誤りや不正確さが含まれる可能性があることをご了承ください。原文はその言語での正式な情報源と見なされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用から生じる誤解や誤訳について、当方は責任を負いません。