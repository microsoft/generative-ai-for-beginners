[![Open Source Models](../../../translated_images/ja/18-lesson-banner.f30176815b1a5074.webp)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# LLMのファインチューニング

大規模言語モデルを使って生成AIアプリケーションを構築することには新たな課題が伴います。主な問題は、ユーザーのリクエストに対してモデルが生成するコンテンツの応答品質（正確さと関連性）を確保することです。前回のレッスンでは、既存モデルへの_プロンプト入力の修正_を通じて問題を解決しようとするプロンプトエンジニアリングや検索強化生成のような手法について学びました。

本日のレッスンでは、**ファインチューニング**という第三の手法を紹介します。これは、追加データを使って_モデル自体を再学習させる_ことで課題に対応しようとするものです。詳細を見ていきましょう。

## 学習目標

このレッスンでは、事前学習済み言語モデルのファインチューニングの概念を紹介し、このアプローチの利点と課題を探り、生成AIモデルの性能を向上させるためにファインチューニングをいつ、どのように使用すべきかを案内します。

レッスン終了時には、以下の質問に答えられるようになります：

- 言語モデルのファインチューニングとは何か？
- ファインチューニングはいつ、なぜ役立つのか？
- 事前学習済みモデルをどうやってファインチューニングするのか？
- ファインチューニングの制限は何か？

準備はいいですか？始めましょう。

## 図解ガイド

内容に入る前に全体像を把握したいですか？このレッスンの学習の流れを示す図解ガイドをご覧ください。ファインチューニングの基本概念や動機の理解から、プロセスや実践のベストプラクティスまで説明しています。探求すべき魅力的なテーマですので、ぜひ自己学習のサポートとなる追加リンクを掲載した[リソースページ](./RESOURCES.md?WT.mc_id=academic-105485-koreyst)もご確認ください！

![Illustrated Guide to Fine Tuning Language Models](../../../translated_images/ja/18-fine-tuning-sketchnote.11b21f9ec8a70346.webp)

## 言語モデルのファインチューニングとは？

定義通り、大規模言語モデルはインターネットを含む多様なソースから得られた大量のテキストで_事前学習_されています。前回のレッスンでも学んだように、モデルの応答品質を向上させるには_プロンプトエンジニアリング_や_検索強化生成_のような技術が必要です。

よく使われるプロンプトエンジニアリングの手法は、モデルに_指示（明示的なガイダンス）を与えたり_、_いくつかの例を示したり（暗黙的なガイダンス）_して応答の期待内容を案内することです。これを_few-shot learning_と呼びますが、以下の2つの制約があります：

- モデルのトークン制限により、与えられる例の数が制限され効果が限定的になる。
- トークンコストの関係で、すべてのプロンプトに例を追加するのが高額になり柔軟性を欠く。

ファインチューニングは機械学習でよく行われる手法で、事前学習済みモデルに新しいデータで再学習させることで特定のタスクでの性能を改善します。言語モデルの文脈では、特定タスクやドメインのために_厳選された例のセットで事前学習モデルをファインチューニング_することで、そのタスクやドメイン向けにより正確で関連性の高い**カスタムモデル**を作ることが可能です。ファインチューニングの副次的利点として、few-shot learningに必要な例の数を減らせるため、トークン使用量と関連コストも削減されます。

## いつ、なぜファインチューニングすべきか？

ここで述べているファインチューニングは、元の訓練データセットに含まれていなかった新しいデータを**追加して**再訓練する、**教師ありのファインチューニング**を指します。これは元のデータで異なるハイパーパラメータを使い再訓練する非教師ありファインチューニングとは異なります。

重要な点は、ファインチューニングは専門知識を要する高度な技術であり、適切に行わないと期待した改善が得られず、ターゲットドメインに対するモデル性能を劣化させてしまう可能性もあることです。

したがって、「どうやって」ファインチューニングするかを学ぶ前に、「なぜ」この方法を選ぶのか、そして「いつ」ファインチューニングを開始すべきかを理解することが重要です。まずは以下の問いを自問してください：

- **ユースケース**：ファインチューニングの_目的_は何か？現在の事前学習モデルのどの点を改善したいのか？
- **代替手段**：望む結果を得るために_他の技術_を試みたか？それらで基準を作って比較してください。
  - プロンプトエンジニアリング：関連例を用いたfew-shotプロンプティング等を試し、応答の質を評価。
  - 検索強化生成：データ検索結果をプロンプトに加える方法を試し、応答の質を評価。
- **コスト**：ファインチューニングにかかるコストを把握しているか？
  - チューニング可能性 - 事前学習済みモデルはファインチューニング可能か？
  - 労力 - 訓練データの準備、モデルの評価と改善に必要な作業
  - 計算資源 - ファインチューニングジョブの実行およびファインチューニング済みモデルの展開に要するコスト
  - データ - ファインチューニング効果を得るのに十分な質の例があるか
- **利点**：ファインチューニングによる効果を確認したか？
  - 品質 - ファインチューニング済みモデルはベースラインを上回ったか？
  - コスト - プロンプトを簡素化しトークン使用量を削減できるか？
  - 拡張性 - 基盤モデルを新ドメイン向けに再利用できるか？

これらの質問に答えることで、ファインチューニングがあなたのユースケースに適しているか判断できます。理想的には、利点がコストを上回る場合のみ進めるべきです。進めることが決まったら、次に事前学習モデルをどうやってファインチューニングするかを考えましょう。

意思決定プロセスの理解を深めたいなら、[ファインチューニングすべきか否か](https://www.youtube.com/watch?v=0Jo-z-MFxJs)の動画をご覧ください。

## 事前学習済みモデルをどうファインチューニングするか？

事前学習済みモデルをファインチューニングするには、次が必要です：

- ファインチューニング対象となる事前学習済みモデル
- ファインチューニングに使用するデータセット
- ファインチューニングジョブを実行するためのトレーニング環境
- ファインチューニング済みモデルを展開するホスティング環境

## ファインチューニング実践例

以下のリソースは、選択したモデルと厳選データセットを使った実際の例を段階的に解説するチュートリアルです。これらを試すには、該当プロバイダーのアカウントと関連モデル・データセットへのアクセスが必要です。

| プロバイダー | チュートリアル                                                                                                                                                                   | 説明                                                                                                                                                                                                                                                                                                                                                                                                                               |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [チャットモデルのファインチューニング方法](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)      | 特定ドメイン（例：「レシピアシスタント」）向けに`gpt-35-turbo`をファインチューニングする方法を、訓練データの準備、ファインチューニングジョブの実行、そしてファインチューニング済みモデルの推論利用まで学べます。                                                                                                                                                                                                           |
| Azure OpenAI | [GPT 3.5 Turbo のファインチューニングチュートリアル](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | Azure上での`gpt-35-turbo-0613`モデルのファインチューニング方法を学習。訓練データの作成・アップロードから、ファインチューニングジョブの実行、新モデルの展開と利用まで。                                                                                                                                                                                                                                                     |
| Hugging Face | [Hugging FaceでLLMをファインチューニングする](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                   | オープンLLM（例: `CodeLlama 7B`）を[transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst)ライブラリと[Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst)を使い、Hugging Face上のオープン[データセット](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst)でファインチューニングします。 |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 🤗 AutoTrain | [AutoTrainでLLMをファインチューニングする](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                            | AutoTrain（またはAutoTrain Advanced）はHugging Faceが開発したPythonライブラリで、多様なタスクのファインチューニングに対応しLLMのファインチューニングも可能です。ノーコードソリューションで、クラウド、Hugging Face Spaces、ローカル環境で実行できます。ウェブGUI、CLI、yaml設定ファイルからのトレーニングをサポートします。                                                                                               |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 🦥 Unsloth   | [UnslothでLLMをファインチューニングする](https://github.com/unslothai/unsloth)                                                                                            | UnslothはLLMファインチューニングと強化学習(RL)をサポートするオープンソースフレームワークです。ローカルでのトレーニング、評価、展開を準備済みの[ノートブック](https://github.com/unslothai/notebooks)で効率化し、音声合成(TTS)、BERT、多モーダルモデルも対応。入門にはステップバイステップの[ファインチューニングガイド](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide)が役立ちます。                                                                         |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
## 課題

上記チュートリアルのいずれかを選び実際に試してみましょう。_これらのチュートリアルの参考用Jupyterノートブック版をこのリポジトリに作成する可能性がありますが、最新の状態を得るためには必ず元のチュートリアルを参照してください_。

## よくできました！学習を続けましょう。

このレッスンを終えたら、[生成AI学習コレクション](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)をチェックして、生成AIに関する知識をさらに深めてください！

おめでとうございます！このコースのv2シリーズ最終レッスンを完了しました！学習と構築を止めないでください。トピックに特化した追加提案のリストは[リソースページ](RESOURCES.md?WT.mc_id=academic-105485-koreyst)も参照してください。

v1シリーズのレッスンも課題と概念が追加され更新されています。ぜひ知識をリフレッシュし、[ご質問やフィードバック](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst)を共有して、コミュニティのためのレッスン改善にご協力ください。

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**免責事項**：  
本書類はAI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されました。正確さを期しておりますが、自動翻訳には誤りや不正確な部分が含まれる場合があります。原文の母国語による文書が正式な情報源とみなされます。重要な情報に関しては、専門の人間翻訳を推奨いたします。本翻訳の使用による誤解や解釈の相違に関して、当方は一切の責任を負いかねます。
<!-- CO-OP TRANSLATOR DISCLAIMER END -->