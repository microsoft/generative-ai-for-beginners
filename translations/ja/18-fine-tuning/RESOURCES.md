# 自己学習のためのリソース

このレッスンは、OpenAIとAzure OpenAIの主要なリソースを参考にして構築されています。以下は、自己学習の旅のための非包括的なリストです。

## 1. 主なリソース

| タイトル/リンク                                                                                                                                                                                                                   | 説明                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning with OpenAI Models](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | 微調整は、プロンプトに収まるより多くの例で学習することで、コストを削減し、応答の質を向上させ、低遅延のリクエストを可能にします。**OpenAIから微調整の概要を取得してください。**                                                                                    |
| [What is Fine-Tuning with Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | **微調整とは何か（概念）**、なぜそれを検討すべきか（動機付けの問題）、どのデータを使用するか（トレーニング）、質を測定する方法を理解する                                                                                                                                                                           |
| [Customize a model with fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Azure OpenAI Serviceを使用すると、微調整を使用して個人のデータセットにモデルを合わせることができます。Azure AI Studio、Python SDK、またはREST APIを使用して**どのように微調整するか（プロセス）**を学ぶことができます。                                                                                                                                |
| [Recommendations for LLM fine-tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLMは特定のドメイン、タスク、データセットでうまく機能しない場合や、不正確または誤解を招く出力を生成する場合があります。**この問題に対する可能な解決策として微調整を検討すべき時期はいつですか？**                                                                                                                                  |
| [Continuous Fine Tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | 連続微調整は、すでに微調整されたモデルをベースモデルとして選択し、新しいトレーニング例セットで**さらに微調整する**反復プロセスです。                                                                                                                                                     |
| [Fine-tuning and function calling](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | **関数呼び出しの例を使用して**モデルを微調整することで、より正確で一貫した出力を得ることができ、同様のフォーマットの応答とコスト削減を実現できます。                                                                                                                                        |
| [Fine-tuning Models: Azure OpenAI Guidance](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Azure OpenAIで**どのモデルが微調整可能か**、そしてそれらがどの地域で利用可能かを理解するためにこの表を参照してください。必要に応じてトークン制限やトレーニングデータの有効期限も確認してください。                                                                                                                            |
| [To Fine Tune or Not To Fine Tune? That is the Question](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | この30分の**2023年10月**のAIショーエピソードでは、微調整を行うかどうかの決定を助けるための利点、欠点、実践的な洞察を議論します。                                                                                                                                                                                        |
| [Getting Started With LLM Fine-Tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning?WT.mc_id=academic-105485-koreyst)                                             | この**AIプレイブック**リソースは、データ要件、フォーマット、ハイパーパラメータの微調整、および知っておくべき課題/制限について説明します。                                                                                                                                                                         |
| **チュートリアル**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | サンプルの微調整データセットを作成し、微調整の準備をし、微調整ジョブを作成し、Azureで微調整されたモデルをデプロイする方法を学びます。                                                                                                                                                                                    |
| **チュートリアル**: [Fine-tune a Llama 2 model in Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studioでは、_ローコード開発者に適したUIベースのワークフローを使用して_、大規模言語モデルを個人のデータセットに合わせることができます。この例を参照してください。                                                                                                                                                               |
| **チュートリアル**:[Fine-tune Hugging Face models for a single GPU on Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | この記事では、Hugging Face transformersライブラリを使用して、Azure DataBricks + Hugging Face Trainerライブラリで単一GPUでHugging Faceモデルを微調整する方法を説明します。                                                                                                                                                |
| **トレーニング:** [Fine-tune a foundation model with Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Azure Machine Learningのモデルカタログには、特定のタスクに合わせて微調整できる多くのオープンソースモデルが提供されています。このモジュールは[AzureML生成AI学習パス](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)から試してみてください。 |
| **チュートリアル:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | W&Bを使用してMicrosoft AzureでGPT-3.5またはGPT-4モデルを微調整することで、モデルのパフォーマンスの詳細な追跡と分析が可能になります。このガイドは、OpenAI微調整ガイドの概念を拡張し、Azure OpenAI用の具体的なステップと機能を提供します。                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. 二次リソース

このセクションには、探求する価値のある追加のリソースが含まれていますが、このレッスンではカバーできなかったものです。これらは将来のレッスンで取り上げられるか、または後日、二次課題オプションとして取り上げられる可能性があります。現時点では、これらを使用してこのトピックに関する自身の専門知識と知識を構築してください。

| タイトル/リンク                                                                                                                                                                                                            | 説明                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Data preparation and analysis for chat model fine-tuning](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | このノートブックは、チャットモデルの微調整に使用されるチャットデータセットを前処理および分析するためのツールとして機能します。フォーマットエラーをチェックし、基本的な統計を提供し、微調整コストのためのトークン数を推定します。参照: [gpt-3.5-turboの微調整方法](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)。                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning for Retrieval Augmented Generation (RAG) with Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | このノートブックの目的は、OpenAIモデルをRetrieval Augmented Generation (RAG)のために微調整する方法の包括的な例を紹介することです。また、QdrantとFew-Shot Learningを統合して、モデルのパフォーマンスを向上させ、誤情報を減らします。                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT with Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B)はAI開発者向けプラットフォームで、モデルのトレーニング、微調整、基盤モデルの活用のためのツールを提供しています。最初に[OpenAI微調整](https://docs.wandb.ai/guides/integrations/openai?WT.mc_id=academic-105485-koreyst)ガイドを読み、次にCookbookの演習を試してみてください。                                                                                                                                                                                                                  |
| **Community Tutorial** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - fine-tuning for Small Language Models                                                   | Microsoftの新しい小型モデルである[Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst)を紹介します。このチュートリアルでは、Phi-2を微調整する方法を説明し、ユニークなデータセットを構築し、QLoRAを使用してモデルを微調整する方法を示します。                                                                                                                                                                       |
| **Hugging Face Tutorial** [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | このブログ投稿では、Hugging Face TRL、Transformers、データセットを使用して2024年にオープンなLLMを微調整する方法を説明します。ユースケースを定義し、開発環境をセットアップし、データセットを準備し、モデルを微調整し、テスト・評価し、最終的に本番環境にデプロイします。                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | [最先端の機械学習モデル](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst)のトレーニングとデプロイメントをより迅速かつ簡単に実現します。このリポジトリには、YouTubeビデオガイダンス付きのColabフレンドリーなチュートリアルが含まれており、微調整が可能です。**最近の[ローカルファースト](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)アップデートを反映しています**。[AutoTrainのドキュメント](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst)をお読みください。 |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**免責事項**:
この文書は、機械ベースのAI翻訳サービスを使用して翻訳されています。正確性を期しておりますが、自動翻訳には誤りや不正確さが含まれる可能性があることをご了承ください。元の言語での文書を権威ある情報源として考慮すべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用から生じる誤解や誤解釈について、当社は責任を負いません。