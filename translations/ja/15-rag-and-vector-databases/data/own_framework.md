# ニューラルネットワーク入門. 多層パーセプトロン

前のセクションでは、最も単純なニューラルネットワークモデルである一層パーセプトロン、線形二クラス分類モデルについて学びました。

このセクションでは、このモデルをより柔軟なフレームワークに拡張し、以下を可能にします：

* 二クラスに加えて、**多クラス分類**を行う
* 分類に加えて、**回帰問題**を解く
* 線形分離不可能なクラスを分離する

また、異なるニューラルネットワークアーキテクチャを構築できるようにするためのPythonによる独自のモジュラーフレームワークも開発します。

## 機械学習の形式化

まずは機械学習問題を形式化することから始めましょう。トレーニングデータセット**X**とラベル**Y**があり、最も正確な予測を行うモデル*f*を構築する必要があるとします。予測の品質は**損失関数**ℒによって測定されます。以下の損失関数がよく使用されます：

* 数値を予測する必要がある回帰問題の場合、**絶対誤差**∑<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|または**二乗誤差**∑<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>を使用できます。
* 分類の場合、**0-1損失**（モデルの**精度**と本質的に同じ）または**ロジスティック損失**を使用します。

一層パーセプトロンの場合、関数*f*は線形関数*f(x)=wx+b*として定義されました（ここで*w*は重み行列、*x*は入力特徴ベクトル、*b*はバイアスベクトルです）。異なるニューラルネットワークアーキテクチャでは、この関数はより複雑な形をとることができます。

> 分類の場合、ネットワーク出力として対応するクラスの確率を得ることが望ましいことがよくあります。任意の数値を確率に変換するため（例えば、出力を正規化するため）、**ソフトマックス**関数σをよく使用し、関数*f*は*f(x)=σ(wx+b)*となります。

上記の*f*の定義において、*w*と*b*は**パラメータ**θ=⟨*w,b*⟩と呼ばれます。データセット⟨**X**,**Y**⟩が与えられた場合、パラメータθの関数としてデータセット全体の総合的な誤差を計算できます。

> ✅ **ニューラルネットワークのトレーニングの目的は、パラメータθを変化させて誤差を最小化することです**

## 勾配降下法による最適化

関数最適化の有名な方法に**勾配降下法**があります。アイデアは、パラメータに対する損失関数の導関数（多次元の場合は**勾配**と呼ばれる）を計算し、誤差が減少するようにパラメータを変化させることです。これは以下のように形式化できます：

* パラメータをいくつかのランダムな値で初期化する w<sup>(0)</sup>, b<sup>(0)</sup>
* 次のステップを何度も繰り返す：
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-η∂ℒ/∂w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-η∂ℒ/∂b

トレーニング中、最適化ステップはデータセット全体を考慮して計算されることが想定されています（損失はすべてのトレーニングサンプルを通じて合計として計算されることを思い出してください）。しかし、実際には**ミニバッチ**と呼ばれるデータセットの小さな部分を取り、データのサブセットに基づいて勾配を計算します。サブセットは毎回ランダムに取られるため、この方法は**確率的勾配降下法**（SGD）と呼ばれます。

## 多層パーセプトロンと逆伝播

一層ネットワークは、上で見たように、線形分離可能なクラスを分類することができます。より豊かなモデルを構築するために、ネットワークのいくつかの層を組み合わせることができます。数学的には、関数*f*がより複雑な形を持ち、いくつかのステップで計算されることを意味します：
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>α(z<sub>1</sub>)+b<sub>2</sub>
* f = σ(z<sub>2</sub>)

ここで、αは**非線形活性化関数**、σはソフトマックス関数、パラメータθ=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>です。

勾配降下アルゴリズムは同じままですが、勾配を計算するのがより難しくなります。チェーン微分則を考慮すると、導関数を次のように計算できます：

* ∂ℒ/∂w<sub>2</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂w<sub>2</sub>)
* ∂ℒ/∂w<sub>1</sub> = (∂ℒ/∂σ)(∂σ/∂z<sub>2</sub>)(∂z<sub>2</sub>/∂α)(∂α/∂z<sub>1</sub>)(∂z<sub>1</sub>/∂w<sub>1</sub>)

> ✅ チェーン微分則は、パラメータに対する損失関数の導関数を計算するために使用されます。

これらの表現の左端部分はすべて同じであることに注意してください。そのため、損失関数から始めて計算グラフを「逆方向」にたどることで、効果的に導関数を計算できます。このようにして、多層パーセプトロンをトレーニングする方法は**逆伝播**または「バックプロップ」と呼ばれます。

> TODO: 画像引用

> ✅ ノートブックの例で逆伝播についてより詳細に説明します。

## 結論

このレッスンでは、独自のニューラルネットワークライブラリを構築し、シンプルな2次元分類タスクに使用しました。

## 🚀 チャレンジ

付随するノートブックでは、多層パーセプトロンを構築し、トレーニングするための独自のフレームワークを実装します。現代のニューラルネットワークがどのように動作するかを詳細に見ることができます。

OwnFrameworkノートブックに進み、それを進めてください。

## レビューと自己学習

逆伝播はAIとMLで一般的に使用されるアルゴリズムであり、より詳細に学ぶ価値があります。

## 課題

このラボでは、このレッスンで構築したフレームワークを使用して、MNIST手書き数字分類を解決するよう求められます。

* 手順
* ノートブック

**免責事項**:  
この文書は機械翻訳AIサービスを使用して翻訳されています。正確さを期しておりますが、自動翻訳には誤りや不正確さが含まれる場合がありますのでご注意ください。元の言語での原文を正式な情報源とみなすべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤解釈について、当社は責任を負いません。