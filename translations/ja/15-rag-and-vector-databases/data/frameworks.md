# ニューラルネットワークフレームワーク

既に学んだように、ニューラルネットワークを効率的にトレーニングするためには、次の2つのことを行う必要があります：

* テンソルを操作すること、例として乗算、加算、シグモイドやソフトマックスのような関数の計算
* 勾配降下法による最適化を行うために、すべての表現の勾配を計算すること

`numpy` ライブラリは最初の部分を行うことができますが、勾配を計算するための仕組みが必要です。前のセクションで開発したフレームワークでは、`backward` メソッド内で全ての微分関数を手動でプログラムする必要がありました。理想的には、フレームワークは定義できる*任意の表現*の勾配を計算する機会を提供すべきです。

もう一つ重要なことは、GPUやTPUのような専門的な計算ユニットで計算を行うことができることです。ディープニューラルネットワークのトレーニングには*非常に多くの*計算が必要であり、GPUでそれらの計算を並列化することが非常に重要です。

> ✅ 'parallelize' という用語は、計算を複数のデバイスに分散することを意味します。

現在、最も人気のあるニューラルフレームワークは、TensorFlowとPyTorchです。どちらも、CPUとGPUの両方でテンソルを操作するための低レベルAPIを提供しています。低レベルAPIの上には、それぞれKerasとPyTorch Lightningと呼ばれる高レベルAPIもあります。

低レベルAPI | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
高レベルAPI| Keras| Pytorch

**低レベルAPI**は、いわゆる**計算グラフ**を構築することを可能にします。このグラフは、与えられた入力パラメータで出力（通常は損失関数）をどのように計算するかを定義し、GPUでの計算が可能であれば、そこにプッシュすることができます。この計算グラフを微分し、勾配を計算する関数があり、それをモデルパラメータの最適化に利用できます。

**高レベルAPI**は、ニューラルネットワークを**層のシーケンス**として捉え、多くのニューラルネットワークの構築を容易にします。モデルのトレーニングは通常、データを準備し、`fit` 関数を呼び出して行います。

高レベルAPIは、典型的なニューラルネットワークを迅速に構築することを可能にし、多くの詳細を気にせずに済みます。一方で、低レベルAPIはトレーニングプロセスに対するより多くのコントロールを提供し、新しいニューラルネットワークアーキテクチャを扱う研究で多く使用されます。

また、両方のAPIを一緒に使用できることを理解することも重要です。例えば、低レベルAPIを使用して独自のネットワーク層アーキテクチャを開発し、それを高レベルAPIで構築しトレーニングされた大規模なネットワーク内で使用することができます。または、高レベルAPIを使用して層のシーケンスとしてネットワークを定義し、その後、自分自身の低レベルのトレーニングループを使用して最適化を行うこともできます。両方のAPIは同じ基本的な概念を使用し、一緒にうまく機能するように設計されています。

## 学習

このコースでは、PyTorchとTensorFlowの両方についてのコンテンツを提供しています。お好みのフレームワークを選んで、対応するノートブックを進めてください。どちらのフレームワークを選ぶべきか分からない場合は、インターネット上での**PyTorch vs. TensorFlow**に関する議論を読んでください。両方のフレームワークを見て、理解を深めるのも良いでしょう。

可能な限り、高レベルAPIを使用してシンプルに進めます。しかし、ニューラルネットワークがどのように動作するかを基礎から理解することが重要だと考えているため、最初は低レベルAPIとテンソルを使って作業を始めます。ただし、早く進めたい、またはこれらの詳細を学ぶのに多くの時間をかけたくない場合は、それらをスキップして高レベルAPIのノートブックに進むことができます。

## ✍️ 演習: フレームワーク

次のノートブックで学習を続けてください：

低レベルAPI | TensorFlow+Keras ノートブック | PyTorch
--------------|-------------------------------------|--------------------------------
高レベルAPI| Keras | *PyTorch Lightning*

フレームワークをマスターしたら、過学習の概念を復習しましょう。

# 過学習

過学習は機械学習において非常に重要な概念であり、正しく理解することが非常に重要です！

次のような5つの点を近似する問題を考えてみましょう（以下のグラフで`x`で表されます）：

!linear | overfit
-------------------------|--------------------------
**線形モデル、2パラメータ** | **非線形モデル、7パラメータ**
トレーニング誤差 = 5.3 | トレーニング誤差 = 0
検証誤差 = 5.1 | 検証誤差 = 20

* 左側には、良好な直線近似が見られます。パラメータの数が適切であるため、モデルは点の分布の背後にある考えを正しく理解しています。
* 右側では、モデルが強力すぎます。5つの点しかなく、モデルが7つのパラメータを持っているため、すべての点を通過するように調整でき、トレーニング誤差を0にします。しかし、これによりデータの背後にある正しいパターンを理解することができなくなり、検証誤差が非常に高くなります。

モデルの豊かさ（パラメータの数）とトレーニングサンプルの数の間で適切なバランスを取ることが非常に重要です。

## 過学習が発生する理由

  * トレーニングデータが不足している
  * モデルが強力すぎる
  * 入力データにノイズが多すぎる

## 過学習を検出する方法

上記のグラフから分かるように、過学習は非常に低いトレーニング誤差と高い検証誤差で検出できます。通常、トレーニング中はトレーニング誤差と検証誤差の両方が減少し始め、ある時点で検証誤差が減少を止め、上昇し始めることがあります。これは過学習の兆候であり、この時点でトレーニングを停止するべきである（または少なくともモデルのスナップショットを作成する）という指標になります。

## 過学習を防ぐ方法

過学習が発生している場合は、次のいずれかを行うことができます：

 * トレーニングデータの量を増やす
 * モデルの複雑さを減らす
 * 後で考慮するドロップアウトなどの正則化手法を使用する

## 過学習とバイアス・バリアンストレードオフ

過学習は、統計学におけるより一般的な問題であるバイアス・バリアンストレードオフの一例です。モデルの誤差の可能性のある原因を考えると、2種類の誤差があります：

* **バイアス誤差**は、アルゴリズムがトレーニングデータ間の関係を正しくキャプチャできないことによって引き起こされます。これは、モデルが十分に強力でないこと（**アンダーフィッティング**）が原因である可能性があります。
* **バリアンス誤差**は、モデルが入力データのノイズを意味のある関係の代わりに近似することによって引き起こされます（**過学習**）。

トレーニング中、バイアス誤差は減少し（モデルがデータを近似することを学ぶため）、バリアンス誤差は増加します。過学習を防ぐためには、トレーニングを手動で（過学習を検出したとき）または自動で（正則化を導入することによって）停止することが重要です。

## 結論

このレッスンでは、2つの最も人気のあるAIフレームワークであるTensorFlowとPyTorchのさまざまなAPIの違いについて学びました。加えて、非常に重要なトピックである過学習について学びました。

## 🚀 チャレンジ

付属のノートブックには、下部に「タスク」があります。ノートブックを通して作業し、タスクを完了してください。

## レビューと自己学習

次のトピックについて調査を行ってください：

- TensorFlow
- PyTorch
- 過学習

次の質問を自問してください：

- TensorFlowとPyTorchの違いは何ですか？
- 過学習とアンダーフィッティングの違いは何ですか？

## 課題

このラボでは、PyTorchまたはTensorFlowを使用して、単層および多層の完全結合ネットワークを用いて2つの分類問題を解決することを求められています。

**免責事項**:  
この文書は、機械ベースのAI翻訳サービスを使用して翻訳されています。正確さを追求しておりますが、自動翻訳には誤りや不正確さが含まれる場合があることをご了承ください。元の言語で書かれた原文を公式な情報源と見なすべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤解釈について、当社は責任を負いません。