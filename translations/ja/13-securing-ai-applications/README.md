# ジェネレーティブAIアプリケーションのセキュリティ強化

[![ジェネレーティブAIアプリケーションのセキュリティ強化](../../../translated_images/13-lesson-banner.png?WT.028697a53f1c3c0ea07dafd10617ce0380ac2b809bb145d7171be69e83daac89.ja.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## はじめに

このレッスンでは以下の内容をカバーします：

- AIシステムにおけるセキュリティ。
- AIシステムに対する一般的なリスクと脅威。
- AIシステムを保護するための方法と考慮事項。

## 学習目標

このレッスンを完了すると、以下について理解が深まります：

- AIシステムに対する脅威とリスク。
- AIシステムを保護するための一般的な方法と実践。
- セキュリティテストを実施することで、予期しない結果やユーザーの信頼の低下を防ぐ方法。

## ジェネレーティブAIにおけるセキュリティとは何か？

人工知能（AI）や機械学習（ML）技術が私たちの生活をますます形作る中で、顧客データだけでなくAIシステム自体を保護することが重要です。AI/MLは、間違った決定が重大な結果を招く可能性のある業界で、高価値の意思決定プロセスを支援するためにますます利用されています。

考慮すべき主なポイントは以下の通りです：

- **AI/MLの影響**：AI/MLは日常生活に大きな影響を与えており、それを保護することが不可欠になっています。
- **セキュリティの課題**：AI/MLの影響は、トロールや組織化されたグループによる高度な攻撃からAIベースの製品を保護する必要性に適切に対応するために、適切な注意を払う必要があります。
- **戦略的問題**：テクノロジー業界は、長期的な顧客の安全性とデータセキュリティを確保するために、戦略的な課題に積極的に取り組む必要があります。

さらに、機械学習モデルは、悪意のある入力と無害な異常データを区別することがほとんどできません。トレーニングデータの重要なソースは、未編集で管理されていない公開データセットから派生しており、第三者の貢献を受け入れることができます。攻撃者はデータセットを侵害する必要はなく、自由に貢献することができます。データの構造やフォーマットが正しいままであれば、低信頼度の悪意のあるデータが時間とともに高信頼度の信頼できるデータになります。

これが、モデルが意思決定に使用するデータストアの整合性と保護を確保することが重要な理由です。

## AIの脅威とリスクの理解

AIおよび関連システムに関しては、データポイズニングが現在最も重要なセキュリティ脅威として際立っています。データポイズニングとは、誰かが意図的にAIのトレーニングに使用される情報を変更し、誤った結果を引き起こすことです。標準化された検出および緩和方法が欠如しており、トレーニングのために信頼できないまたは未編集の公開データセットに依存しているためです。データの整合性を維持し、欠陥のあるトレーニングプロセスを防ぐためには、データの出所と系統を追跡することが重要です。さもなければ、「ゴミを入れればゴミが出る」という古い格言が当てはまり、モデルの性能が損なわれます。

データポイズニングがモデルにどのように影響するかの例を以下に示します：

1. **ラベルの反転**：バイナリ分類タスクで、敵対者がトレーニングデータの小さな部分のラベルを意図的に反転させます。例えば、無害なサンプルが悪意のあるものとしてラベル付けされ、モデルが誤った関連付けを学習します。\
   **例**：スパムフィルターが正当なメールをスパムとして誤分類する。
2. **特徴量のポイズニング**：攻撃者がトレーニングデータの特徴を微妙に変更し、バイアスを導入したりモデルを誤解させたりします。\
   **例**：製品説明に無関係なキーワードを追加して、推薦システムを操作する。
3. **データインジェクション**：トレーニングセットに悪意のあるデータを注入して、モデルの動作に影響を与える。\
   **例**：偽のユーザーレビューを導入して、感情分析の結果を歪める。
4. **バックドア攻撃**：敵対者がトレーニングデータに隠れたパターン（バックドア）を挿入します。モデルはこのパターンを認識し、トリガーされると悪意のある動作をします。\
   **例**：特定の人物を誤認識する顔認識システム。

MITRE Corporationは、AIシステムに対する現実の攻撃で敵対者が使用する戦術と技術の知識ベースである[ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)を作成しました。

> AI対応システムの脆弱性は増加しており、AIの導入により既存システムの攻撃面が従来のサイバー攻撃を超えて拡大しています。ATLASは、これらのユニークで進化する脆弱性への認識を高めるために開発されました。ATLASは、MITRE ATT&CK®フレームワークをモデルにしており、その戦術、技術、手順（TTP）はATT&CKのものと補完的です。

伝統的なサイバーセキュリティで高度な脅威エミュレーションシナリオの計画に広く使用されているMITRE ATT&CK®フレームワークと同様に、ATLASは新たな攻撃に対する防御を理解し準備するのに役立つ検索可能なTTPセットを提供します。

さらに、Open Web Application Security Project (OWASP)は、LLMを利用するアプリケーションで見つかった最も重大な脆弱性の"[トップ10リスト](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)"を作成しました。このリストは、前述のデータポイズニングやその他の脅威のリスクを強調しています：

- **プロンプトインジェクション**：攻撃者が大規模言語モデル（LLM）を慎重に設計された入力を通じて操作し、意図された動作を超えて動作させる手法。
- **サプライチェーンの脆弱性**：LLMが使用するアプリケーションを構成するコンポーネントやソフトウェア（Pythonモジュールや外部データセットなど）は、それ自体が脆弱である可能性があり、予期しない結果、バイアスの導入、さらには基盤インフラストラクチャの脆弱性を引き起こす可能性があります。
- **過度の依存**：LLMは誤った情報を生成しやすく、不正確または安全でない結果を提供することがあります。いくつかの文書化された状況では、人々が結果をそのまま受け入れ、意図しない現実世界での否定的な結果を招いています。

Microsoft Cloud AdvocateのRod Trentは、これらや他の新たなAIの脅威について深く掘り下げ、これらのシナリオに最善で対処するための広範なガイダンスを提供する無料の電子書籍、[Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst)を執筆しています。

## AIシステムとLLMのセキュリティテスト

人工知能（AI）は、さまざまな分野や産業を変革し、社会に新たな可能性と利益をもたらしています。しかし、AIはデータプライバシー、バイアス、説明性の欠如、潜在的な悪用など、重大な課題やリスクも抱えています。したがって、AIシステムが安全で責任あるものであり、倫理的および法的基準を遵守し、ユーザーや利害関係者から信頼されることが重要です。

セキュリティテストは、AIシステムまたはLLMのセキュリティを評価し、その脆弱性を特定して悪用するプロセスです。これは、開発者、ユーザー、または第三者の監査人によって、テストの目的と範囲に応じて実施されることがあります。AIシステムとLLMの最も一般的なセキュリティテスト方法のいくつかは次のとおりです：

- **データサニタイゼーション**：これは、AIシステムまたはLLMのトレーニングデータまたは入力から機密情報や個人情報を削除または匿名化するプロセスです。データサニタイゼーションは、機密情報や個人データの露出を減らすことで、データの漏洩や悪意のある操作を防ぐのに役立ちます。
- **敵対的テスト**：これは、AIシステムまたはLLMの入力または出力に敵対的な例を生成して適用し、その堅牢性と敵対的攻撃に対する耐性を評価するプロセスです。敵対的テストは、攻撃者によって悪用される可能性のあるAIシステムまたはLLMの脆弱性と弱点を特定し、軽減するのに役立ちます。
- **モデル検証**：これは、AIシステムまたはLLMのモデルパラメータまたはアーキテクチャの正確性と完全性を検証するプロセスです。モデル検証は、モデルが保護され認証されていることを確認することで、モデルの盗用を検出し防止するのに役立ちます。
- **出力検証**：これは、AIシステムまたはLLMの出力の品質と信頼性を検証するプロセスです。出力検証は、出力が一貫して正確であることを確認することで、悪意のある操作を検出し修正するのに役立ちます。

AIシステムのリーダーであるOpenAIは、AIの安全性に貢献することを目指して、レッドチーミングネットワークの一環として、AIシステムの出力をテストするための一連の_安全性評価_を設定しています。

> 評価は、単純なQ&Aテストからより複雑なシミュレーションまでさまざまです。具体的な例として、OpenAIがAIの行動をさまざまな角度から評価するために開発したサンプル評価を以下に示します：

#### 説得

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst)：AIシステムが別のAIシステムに秘密の言葉を言わせることができるか？
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst)：AIシステムが別のAIシステムに寄付を促すことができるか？
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst)：AIシステムが別のAIシステムの政治提案への支持を影響させることができるか？

#### ステガノグラフィー（隠れたメッセージ）

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst)：AIシステムが別のAIシステムに見つからずに秘密のメッセージを渡すことができるか？
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst)：AIシステムがメッセージを圧縮し、秘密のメッセージを隠すために復元できるか？
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst)：AIシステムが直接のコミュニケーションなしに別のAIシステムと協調できるか？

### AIセキュリティ

AIシステムを悪意のある攻撃、悪用、または意図しない結果から保護することを目指すことが重要です。これには、AIシステムの安全性、信頼性、信頼性を確保するための措置を講じることが含まれます。具体的には、以下のようなことが必要です：

- AIモデルのトレーニングと実行に使用されるデータとアルゴリズムを保護する
- AIシステムへの不正アクセス、操作、または破壊を防ぐ
- AIシステムにおけるバイアス、差別、倫理的問題を検出し軽減する
- AIの決定と行動の説明責任、透明性、説明可能性を確保する
- AIシステムの目標と価値を人間と社会のものに一致させる

AIセキュリティは、AIシステムとデータの整合性、可用性、機密性を確保するために重要です。AIセキュリティの課題と機会には以下のようなものがあります：

- 機会：サイバーセキュリティ戦略にAIを組み込むことで、脅威の特定と応答時間の改善に重要な役割を果たすことができます。AIは、フィッシング、マルウェア、ランサムウェアなどのサイバー攻撃の検出と軽減を自動化し、強化するのに役立ちます。
- 課題：AIは、敵対者によって洗練された攻撃を開始するためにも使用される可能性があります。例えば、偽のまたは誤解を招くコンテンツの生成、ユーザーのなりすまし、AIシステムの脆弱性の悪用などです。したがって、AI開発者は、悪用に対して堅牢で耐性のあるシステムを設計する責任を負っています。

### データ保護

LLMは、使用するデータのプライバシーとセキュリティにリスクをもたらす可能性があります。例えば、LLMはトレーニングデータから個人名、住所、パスワード、クレジットカード番号などの機密情報を記憶して漏洩する可能性があります。また、悪意のあるアクターによって脆弱性やバイアスを悪用される可能性もあります。したがって、これらのリスクを認識し、LLMで使用するデータを保護するための適切な措置を講じることが重要です。LLMで使用するデータを保護するために取るべきいくつかのステップは次のとおりです：

- **LLMと共有するデータの量と種類を制限する**：目的に必要で関連性のあるデータのみを共有し、機密性、秘密性、個人情報のデータを共有しないようにします。ユーザーは、LLMと共有するデータを匿名化または暗号化することも推奨されます。例えば、識別情報を削除またはマスクする、または安全な通信チャネルを使用するなどです。
- **LLMが生成するデータを検証する**：LLMが生成する出力の正確性と品質

**免責事項**:  
この文書は機械翻訳サービスを使用して翻訳されています。正確さを期すよう努めていますが、自動翻訳には誤りや不正確さが含まれる可能性があります。原文の言語での文書を権威ある情報源と見なすべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤解釈について、当社は一切の責任を負いません。