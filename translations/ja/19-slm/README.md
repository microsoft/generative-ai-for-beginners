```markdown
# 初心者向け生成AIのための小型言語モデル入門

生成AIは、新しいコンテンツを生成できるシステムを作成することに焦点を当てた人工知能の魅力的な分野です。このコンテンツは、テキストや画像から音楽、さらには仮想環境全体にまで及びます。生成AIの最も興味深い応用の1つは、言語モデルの領域にあります。

## 小型言語モデルとは何ですか？

小型言語モデル（SLM）は、大型言語モデル（LLM）の縮小版を表し、LLMの多くのアーキテクチャの原則と技術を活用しながら、計算の負荷を大幅に削減しています。SLMは、人間のようなテキストを生成するように設計された言語モデルのサブセットです。GPT-4のような大規模なモデルとは異なり、SLMはよりコンパクトで効率的であり、計算リソースが限られているアプリケーションに理想的です。サイズが小さいにもかかわらず、さまざまなタスクを実行できます。通常、SLMはLLMを圧縮または蒸留することによって構築され、元のモデルの機能と言語能力のかなりの部分を保持することを目指しています。このモデルサイズの削減により、全体的な複雑さが減少し、SLMはメモリ使用量と計算要件の両方の観点でより効率的になります。これらの最適化にもかかわらず、SLMは次のような幅広い自然言語処理（NLP）タスクを実行できます：

- テキスト生成：一貫性のある文脈に適した文や段落を作成する。
- テキスト補完：与えられたプロンプトに基づいて文を予測し、補完する。
- 翻訳：テキストをある言語から別の言語に変換する。
- 要約：長いテキストを短く消化しやすい要約に凝縮する。

ただし、パフォーマンスや理解の深さに関しては、より大きなモデルと比較していくつかのトレードオフがあります。

## 小型言語モデルはどのように機能しますか？

SLMは大量のテキストデータで訓練されます。訓練中に、言語のパターンと構造を学習し、文法的に正しく文脈に適したテキストを生成できるようになります。訓練プロセスには以下が含まれます：

- データ収集：さまざまなソースからの大量のテキストデータを収集する。
- 前処理：データをクリーンアップし、訓練に適した形式に整理する。
- 訓練：機械学習アルゴリズムを使用して、モデルにテキストを理解し生成する方法を教える。
- 微調整：特定のタスクでのパフォーマンスを向上させるためにモデルを調整する。

SLMの開発は、モバイルデバイスやエッジコンピューティングプラットフォームなど、リソースが制約された環境に展開できるモデルの必要性の高まりと一致しています。効率に焦点を当てることで、SLMはパフォーマンスとアクセシビリティのバランスを取り、さまざまなドメインでの幅広いアプリケーションを可能にします。

![slm](../../../translated_images/slm.png?WT.85221b66c3ce1b5e21e84c783c7ba31848501cd5c9557bb7fdf13173edafd675.ja.mc_id=academic-105485-koreyst)

## 学習目標

このレッスンでは、SLMの知識を紹介し、Microsoft Phi-3と組み合わせてテキストコンテンツ、ビジョン、MoEの異なるシナリオを学びます。このレッスンの終わりまでに、次の質問に答えられるようになることを目指します：

- SLMとは何か
- SLMとLLMの違いは何か
- Microsoft Phi-3/3.5ファミリーとは何か
- Microsoft Phi-3/3.5ファミリーをどのように推論するか

準備はいいですか？始めましょう。

## 大型言語モデル（LLM）と小型言語モデル（SLM）の違い

LLMとSLMの両方は、確率的機械学習の基本原則に基づいて構築されており、アーキテクチャ設計、訓練手法、データ生成プロセス、およびモデル評価技術において類似したアプローチを採用しています。しかし、これら2つのモデルタイプを区別するいくつかの重要な要因があります。

## 小型言語モデルの応用

SLMは、次のような幅広い応用があります：

- チャットボット：カスタマーサポートを提供し、ユーザーと会話形式で交流する。
- コンテンツ作成：ライターを支援してアイデアを生成したり、記事全体を執筆したりする。
- 教育：学生の執筆課題を支援したり、新しい言語を学ぶのを助けたりする。
- アクセシビリティ：テキスト読み上げシステムなど、障害を持つ個人のためのツールを作成する。

**サイズ**

LLMとSLMの主な違いは、モデルの規模にあります。ChatGPT（GPT-4）のようなLLMは、推定1.76兆のパラメータを持つことができるのに対し、Mistral 7BのようなオープンソースのSLMは、はるかに少ない約70億のパラメータで設計されています。この差異は主にモデルアーキテクチャと訓練プロセスの違いによるものです。たとえば、ChatGPTはエンコーダデコーダフレームワーク内で自己注意メカニズムを使用するのに対し、Mistral 7Bはスライディングウィンドウ注意を使用し、デコーダのみのモデルでより効率的な訓練を可能にしています。このアーキテクチャの違いは、これらのモデルの複雑さとパフォーマンスに深い影響を与えています。

**理解**

SLMは特定のドメイン内でのパフォーマンスを最適化するために通常設計されており、非常に専門的ですが、複数の知識分野にわたる広範な文脈理解を提供する能力においては限られている可能性があります。対照的に、LLMはより包括的なレベルで人間のような知性をシミュレートすることを目指しています。広範かつ多様なデータセットで訓練されたLLMは、さまざまなドメインで優れたパフォーマンスを発揮するように設計されており、より大きな汎用性と適応性を提供します。したがって、LLMは自然言語処理やプログラミングなど、より広範な下流タスクに適しています。

**計算**

LLMの訓練と展開はリソース集約型のプロセスであり、通常、大規模なGPUクラスターを含む大規模な計算インフラストラクチャを必要とします。たとえば、ChatGPTのようなモデルをゼロから訓練するには、数千のGPUが長期間にわたって必要になることがあります。対照的に、パラメータ数が少ないSLMは、計算リソースの観点でよりアクセスしやすいです。Mistral 7Bのようなモデルは、適度なGPU機能を備えたローカルマシンで訓練および実行できますが、訓練には複数のGPUを数時間にわたって使用する必要があります。

**バイアス**

バイアスは、主に訓練データの性質に起因するLLMの既知の問題です。これらのモデルは、インターネットからの生の公開データに依存することが多く、特定のグループを過小評価または誤って表現したり、誤ったラベルを導入したり、方言、地理的変動、文法規則に影響された言語バイアスを反映したりする可能性があります。さらに、LLMのアーキテクチャの複雑さは、注意深く微調整しないと気付かれないままバイアスを悪化させる可能性があります。一方、SLMはより制約されたドメイン固有のデータセットで訓練されるため、こうしたバイアスに対して本質的にあまり影響を受けにくいですが、それでも免れるわけではありません。

**推論**

SLMのサイズが小さいため、推論速度において大きな利点があります。ローカルハードウェアで効率的に出力を生成でき、広範な並列処理を必要としません。対照的に、LLMはそのサイズと複雑さのために、許容できる推論時間を達成するために大量の並列計算リソースを必要とすることがよくあります。複数の同時ユーザーが存在すると、特に大規模に展開された場合、LLMの応答時間がさらに遅くなります。

要約すると、LLMとSLMはどちらも機械学習の基礎を共有していますが、モデルサイズ、リソース要件、文脈理解、バイアスへの感受性、および推論速度に関して大きく異なります。これらの違いは、それぞれのユースケースに対する適合性を反映しており、LLMはより汎用性があるがリソースを多く必要とし、SLMは特定のドメインにおける効率を提供し、計算要求が減少しています。

***注：この章では、Microsoft Phi-3 / 3.5を例にSLMを紹介します。***

## Phi-3 / Phi-3.5ファミリーの紹介

Phi-3 / 3.5ファミリーは主にテキスト、ビジョン、エージェント（MoE）アプリケーションシナリオを対象としています：

### Phi-3 / 3.5 Instruct

主にテキスト生成、チャット補完、コンテンツ情報抽出などを対象としています。

**Phi-3-mini**

3.8Bの言語モデルはMicrosoft Azure AI Studio、Hugging Face、およびOllamaで利用可能です。Phi-3モデルは、同等またはより大きなサイズの言語モデルを主要なベンチマークで大幅に上回ります（以下のベンチマーク数値を参照、数値が高いほど良い）。Phi-3-miniは、サイズが2倍のモデルを上回り、Phi-3-smallおよびPhi-3-mediumはGPT-3.5を含むより大きなモデルを上回ります。

**Phi-3-small & medium**

わずか7Bのパラメータで、Phi-3-smallはさまざまな言語、推論、コーディング、数学のベンチマークでGPT-3.5Tを上回ります。14Bのパラメータを持つPhi-3-mediumはこの傾向を続け、Gemini 1.0 Proを上回ります。

**Phi-3.5-mini**

Phi-3-miniのアップグレードと考えることができます。パラメータは変わらないが、複数の言語（20以上の言語をサポート：アラビア語、中国語、チェコ語、デンマーク語、オランダ語、英語、フィンランド語、フランス語、ドイツ語、ヘブライ語、ハンガリー語、イタリア語、日本語、韓国語、ノルウェー語、ポーランド語、ポルトガル語、ロシア語、スペイン語、スウェーデン語、タイ語、トルコ語、ウクライナ語）をサポートする能力が向上し、長いコンテキストのサポートが強化されます。Phi-3.5-miniは、同じサイズの言語モデルを上回り、サイズが2倍のモデルと同等です。

### Phi-3 / 3.5 Vision

Phi-3/3.5のInstructモデルをPhiの理解力と考えることができ、VisionはPhiに世界を理解するための目を与えます。

**Phi-3-Vision**

Phi-3-visionはわずか4.2Bのパラメータで、Claude-3 HaikuやGemini 1.0 Pro Vなどのより大きなモデルを、一般的な視覚的推論タスク、OCR、テーブルや図の理解タスクで上回ります。

**Phi-3.5-Vision**

Phi-3.5-VisionはPhi-3-Visionのアップグレードでもあり、複数の画像のサポートを追加しています。画像だけでなく動画も見ることができる視覚の向上と考えることができます。Phi-3.5-visionは、OCR、テーブルおよびチャートの理解タスクでClaude-3.5 SonnetやGemini 1.5 Flashなどのより大きなモデルを上回り、一般的な視覚知識推論タスクで同等です。マルチフレーム入力をサポートし、つまり複数の入力画像に対して推論を行います。

### Phi-3.5-MoE

***Mixture of Experts(MoE)***は、モデルをはるかに少ない計算で事前訓練できるようにし、密集モデルと同じ計算予算でモデルまたはデータセットサイズを劇的に拡大できることを意味します。特に、MoEモデルは、事前訓練中に密集モデルと同等の品質をはるかに速く達成するべきです。Phi-3.5-MoEは16×3.8Bのエキスパートモジュールで構成されています。Phi-3.5-MoEは、わずか6.6Bのアクティブパラメータで、はるかに大きなモデルと同等の推論、言語理解、数学のレベルを達成します。

Phi-3/3.5ファミリーモデルをさまざまなシナリオに基づいて使用できます。LLMとは異なり、Phi-3/3.5-miniまたはPhi-3/3.5-Visionをエッジデバイスに展開できます。

## Phi-3/3.5ファミリーモデルの使用方法

Phi-3/3.5をさまざまなシナリオで使用することを希望しています。次に、さまざまなシナリオに基づいてPhi-3/3.5を使用します。

![phi3](../../../translated_images/phi3.png?WT.0d1077c4470f7b6eef536aba4426fa8df26762844164cc3883d455ab5251bad1.ja.mc_id=academic-105485-koreyst)

### 推論の違い

CloudのAPI

**GitHub Models**

GitHub Models
```

最も直接的な方法です。GitHub Modelsを通じてPhi-3/3.5-Instructモデルに迅速にアクセスできます。Azure AI Inference SDK / OpenAI SDKと組み合わせることで、コードを通じてAPIにアクセスし、Phi-3/3.5-Instruct呼び出しを完了することができます。また、Playgroundを通じて異なる効果をテストすることもできます。 - デモ: 中国語シナリオにおけるPhi-3-miniとPhi-3.5-miniの効果の比較 ![phi3](../../../translated_images/gh1.png?WT.6d1c7f5cd66199192bb0ec8bf56b27b8fdbb5bc46ba2a4f32023cc4ceb9f6494.ja.mc_id=academic-105485-koreyst) ![phi35](../../../translated_images/gh2.png?WT.3f28f8b48f74275bc8749ddd82aae8bf38ff461d2d66d437c291806aa930ffb1.ja.mc_id=academic-105485-koreyst) **Azure AI Studio** また、ビジョンとMoEモデルを使用したい場合は、Azure AI Studioを使用して呼び出しを完了することができます。興味がある方は、Phi-3 Cookbookを読んで、Azure AI Studioを通じてPhi-3/3.5 Instruct, Vision, MoEを呼び出す方法を学ぶことができます。[このリンクをクリック](https://github.com/microsoft/Phi-3CookBook/blob/main/md/02.QuickStart/AzureAIStudio_QuickStart.md?WT.mc_id=academic-105485-koreyst) **NVIDIA NIM** AzureやGitHubが提供するクラウドベースのモデルカタログソリューションに加えて、[Nivida NIM](https://developer.nvidia.com/nim?WT.mc_id=academic-105485-koreyst)を使用して関連する呼び出しを完了することもできます。NIVIDA NIMを訪れて、Phi-3/3.5 FamilyのAPI呼び出しを完了できます。NVIDIA NIM (NVIDIA Inference Microservices) は、クラウド、データセンター、ワークステーションなど、さまざまな環境でAIモデルを効率的に展開するために設計された加速推論マイクロサービスのセットです。NVIDIA NIMの主な機能は次のとおりです: - **展開の容易さ:** NIMはAIモデルを単一のコマンドで展開でき、既存のワークフローに簡単に統合できます。 - **最適化されたパフォーマンス:** NVIDIAの事前最適化された推論エンジン（TensorRTやTensorRT-LLMなど）を活用して、低レイテンシーと高スループットを実現します。 - **スケーラビリティ:** NIMはKubernetes上でのオートスケーリングをサポートし、さまざまなワークロードを効果的に処理できます。 - **セキュリティとコントロール:** 組織は、自社の管理インフラストラクチャ上でNIMマイクロサービスをセルフホスティングすることで、データとアプリケーションを制御できます。 - **標準API:** NIMは業界標準のAPIを提供し、チャットボットやAIアシスタントなどのAIアプリケーションの構築と統合を容易にします。NIMは、AIモデルの展開と運用を簡素化し、NVIDIA GPU上で効率的に実行されることを保証するNVIDIA AI Enterpriseの一部です。 - デモ: Nividia NIMを使用してPhi-3.5-Vision-APIを呼び出す [[このリンクをクリック](../../../19-slm/python/Phi-3-Vision-Nividia-NIM.ipynb)] ### ローカル環境でのPhi-3/3.5の推論 Phi-3やGPT-3のような言語モデルに関連する推論は、入力に基づいて応答や予測を生成するプロセスを指します。Phi-3にプロンプトや質問を提供すると、訓練されたニューラルネットワークを使用して、訓練されたデータのパターンと関係を分析することで、最も可能性の高い関連性のある応答を推論します。 **Hugging Face Transformer** Hugging Face Transformersは、自然言語処理（NLP）やその他の機械学習タスクのために設計された強力なライブラリです。以下はその主なポイントです: 1. **事前学習済みモデル:** テキスト分類、固有表現抽出、質問応答、要約、翻訳、テキスト生成など、さまざまなタスクに使用できる数千の事前学習済みモデルを提供します。 2. **フレームワークの相互運用性:** PyTorch、TensorFlow、JAXなど、複数のディープラーニングフレームワークをサポートしており、1つのフレームワークでモデルをトレーニングし、別のフレームワークで使用できます。 3. **マルチモーダル機能:** NLPに加えて、コンピュータビジョン（例：画像分類、物体検出）や音声処理（例：音声認識、音声分類）もサポートしています。 4. **使いやすさ:** モデルのダウンロードとファインチューニングを簡単に行えるAPIとツールを提供し、初心者と専門家の両方にアクセスしやすいです。 5. **コミュニティとリソース:** Hugging Faceには活気あるコミュニティと、ユーザーがライブラリを最大限に活用できるようにするための豊富なドキュメント、チュートリアル、ガイドがあります。[公式ドキュメント](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst)または[GitHubリポジトリ](https://github.com/huggingface/transformers?WT.mc_id=academic-105485-koreyst)。これは最も一般的に使用される方法ですが、GPUアクセラレーションも必要です。結局のところ、VisionやMoEのようなシーンは多くの計算を必要とし、量子化されていない場合、CPUでは非常に制限されます。 - デモ: Transformerを使用してPhi-3.5-Instuctを呼び出す [このリンクをクリック](../../../19-slm/python/phi35-instruct-demo.ipynb) - デモ: Transformerを使用してPhi-3.5-Visionを呼び出す[このリンクをクリック](../../../19-slm/python/phi35-vision-demo.ipynb) - デモ: Transformerを使用してPhi-3.5-MoEを呼び出す[このリンクをクリック](../../../19-slm/python/phi35_moe_demo.ipynb) **Ollama** [Ollama](https://ollama.com/?WT.mc_id=academic-105485-koreyst)は、大規模言語モデル（LLM）をローカルマシンで簡単に実行できるように設計されたプラットフォームです。Llama 3.1、Phi 3、Mistral、Gemma 2など、さまざまなモデルをサポートしています。このプラットフォームは、モデルの重み、設定、データを1つのパッケージにまとめることで、ユーザーが独自のモデルをカスタマイズして作成しやすくしています。OllamaはmacOS、Linux、Windowsで利用可能です。クラウドサービスに依存せずにLLMを実験または展開したい場合に最適なツールです。Ollamaは最も直接的な方法であり、次のステートメントを実行するだけです。 ```bash

ollama run phi3.5

``` **ONNX Runtime for GenAI** [ONNX Runtime](https://github.com/microsoft/onnxruntime-genai?WT.mc_id=academic-105485-koreyst)は、クロスプラットフォームの推論およびトレーニングの機械学習アクセラレータです。ONNX Runtime for Generative AI (GENAI)は、さまざまなプラットフォームで生成AIモデルを効率的に実行するのに役立つ強力なツールです。 ## ONNX Runtimeとは何ですか？ ONNX Runtimeは、機械学習モデルの高性能な推論を可能にするオープンソースプロジェクトです。これは、機械学習モデルを表現する標準であるOpen Neural Network Exchange (ONNX)形式のモデルをサポートします。ONNX Runtime推論は、PyTorchやTensorFlow/Kerasなどのディープラーニングフレームワーク、およびscikit-learn、LightGBM、XGBoostなどの古典的な機械学習ライブラリからのモデルをサポートし、より高速な顧客体験とコスト削減を可能にします。ONNX Runtimeは、さまざまなハードウェア、ドライバ、オペレーティングシステムと互換性があり、グラフの最適化と変換と並行して、該当する場合はハードウェアアクセラレータを活用して最適なパフォーマンスを提供します。 ## 生成AIとは何ですか？ 生成AIは、訓練されたデータに基づいてテキスト、画像、音楽などの新しいコンテンツを生成できるAIシステムを指します。例として、GPT-3のような言語モデルや、Stable Diffusionのような画像生成モデルがあります。ONNX Runtime for GenAIライブラリは、ONNXモデルのための生成AIループを提供し、ONNX Runtimeによる推論、ロジット処理、検索とサンプリング、KVキャッシュ管理を含みます。 ## ONNX Runtime for GENAI ONNX Runtime for GENAIは、生成AIモデルをサポートするためにONNX Runtimeの機能を拡張します。主な機能は次のとおりです: - **幅広いプラットフォームサポート:** Windows、Linux、macOS、Android、iOSなど、さまざまなプラットフォームで動作します。 - **モデルサポート:** LLaMA、GPT-Neo、BLOOMなど、多くの人気のある生成AIモデルをサポートしています。 - **パフォーマンス最適化:** NVIDIA GPU、AMD GPUなど、さまざまなハードウェアアクセラレータ向けの最適化を含みます。 - **使いやすさ:** アプリケーションへの簡単な統合を可能にするAPIを提供し、最小限のコードでテキスト、画像、その他のコンテンツを生成できます。 - ユーザーは高レベルのgenerate()メソッドを呼び出すことができ、またはモデルの各反復をループで実行し、1回に1トークンを生成し、ループ内で生成パラメータをオプションで更新することができます。 - ONNX runtumeはまた、貪欲/ビームサーチとTopP、TopKサンプリングをサポートしてトークンシーケンスを生成し、繰り返しペナルティなどの組み込みロジット処理を提供します。カスタムスコアリングを簡単に追加することもできます。 ## はじめに ONNX Runtime for GENAIを使い始めるには、次の手順に従うことができます: ### ONNX Runtimeをインストールする: ```Python
pip install onnxruntime
``` ### 生成AI拡張をインストールする: ```Python
pip install onnxruntime-genai
``` ### モデルを実行する: Pythonでの簡単な例です: ```Python
import onnxruntime_genai as og

model = og.Model('path_to_your_model.onnx')

tokenizer = og.Tokenizer(model)

input_text = "Hello, how are you?"

input_tokens = tokenizer.encode(input_text)

output_tokens = model.generate(input_tokens)

output_text = tokenizer.decode(output_tokens)

print(output_text) 
``` ### デモ: ONNX Runtime GenAIを使用してPhi-3.5-Visionを呼び出す ```python

import onnxruntime_genai as og

model_path = './Your Phi-3.5-vision-instruct ONNX Path'

img_path = './Your Image Path'

model = og.Model(model_path)

processor = model.create_multimodal_processor()

tokenizer_stream = processor.create_stream()

text = "Your Prompt"

prompt = "<|user|>\n"

prompt += "<|image_1|>\n"

prompt += f"{text}<|end|>\n"

prompt += "<|assistant|>\n"

image = og.Images.open(img_path)

inputs = processor(prompt, images=image)

params = og.GeneratorParams(model)

params.set_inputs(inputs)

params.set_search_options(max_length=3072)

generator = og.Generator(model, params)

while not generator.is_done():

    generator.compute_logits()
    
    generator.generate_next_token()

    new_token = generator.get_next_tokens()[0]
    
    code += tokenizer_stream.decode(new_token)
    
    print(tokenizer_stream.decode(new_token), end='', flush=True)

``` **その他** ONNX RuntimeやOllamaの参照方法に加えて、さまざまなメーカーが提供するモデル参照方法に基づいて、量子化モデルの参照を完了することもできます。Apple Metalを使用したApple MLXフレームワーク、NPUを使用したQualcomm QNN、CPU/GPUを使用したIntel OpenVINOなどです。また、[Phi-3 Cookbook](https://github.com/microsoft/phi-3cookbook?WT.mc_id=academic-105485-koreyst)からさらに多くのコンテンツを入手できます。 ## もっと 私たちはPhi-3/3.5 Familyの基本を学びましたが、SLMについてもっと学ぶためには、より多くの知識が必要です。Phi-3 Cookbookで答えを見つけることができます。さらに学びたい場合は、[Phi-3 Cookbook](https://github.com/microsoft/phi-3cookbook?WT.mc_id=academic-105485-koreyst)を訪問してください。

**免責事項**:  
この文書は、機械翻訳サービスを使用して翻訳されています。正確さを期していますが、自動翻訳には誤りや不正確さが含まれる可能性があります。元の言語での原文が信頼できる情報源と見なされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤訳について、当社は責任を負いません。