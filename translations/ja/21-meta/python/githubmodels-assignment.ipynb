{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metaファミリーモデルでの開発\n",
    "\n",
    "## はじめに\n",
    "\n",
    "このレッスンでは以下の内容を扱います：\n",
    "\n",
    "- Metaファミリーの2つの主要モデル、Llama 3.1とLlama 3.2の紹介\n",
    "- 各モデルの利用シーンや用途の理解\n",
    "- 各モデルの特徴を示すコードサンプル\n",
    "\n",
    "## Metaファミリーのモデル\n",
    "\n",
    "このレッスンでは、Metaファミリー、または「Llama Herd」と呼ばれる2つのモデル、Llama 3.1とLlama 3.2を紹介します。\n",
    "\n",
    "これらのモデルはさまざまなバリエーションがあり、Github Modelマーケットプレイスで利用できます。Github Modelsを使って[AIモデルでプロトタイピングする方法](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst)について、さらに詳しく知ることができます。\n",
    "\n",
    "モデルのバリエーション：\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*注：Llama 3もGithub Modelsで利用可能ですが、このレッスンでは扱いません*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "4050億パラメータを持つLlama 3.1は、オープンソースのLLMカテゴリに属します。\n",
    "\n",
    "このモデルは、以前のLlama 3から以下の点でアップグレードされています：\n",
    "\n",
    "- より大きなコンテキストウィンドウ - 128kトークン（従来は8kトークン）\n",
    "- 最大出力トークン数の増加 - 4096（従来は2048）\n",
    "- 多言語対応の向上 - 学習トークン数の増加による\n",
    "\n",
    "これらの強化により、Llama 3.1はGenAIアプリケーションの構築時に、より複雑なユースケースに対応できるようになりました。主な例は以下の通りです：\n",
    "- ネイティブ関数呼び出し - LLMのワークフロー外で外部ツールや関数を呼び出す機能\n",
    "- RAGパフォーマンスの向上 - より広いコンテキストウィンドウによる\n",
    "- 合成データ生成 - ファインチューニングなどのタスクに有効なデータを作成する能力\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネイティブ関数呼び出し\n",
    "\n",
    "Llama 3.1は、関数やツールの呼び出しをより効果的に行えるようにファインチューニングされています。また、ユーザーからのプロンプトに応じて、モデルが使用すべきだと判断できる2つの組み込みツールも備えています。これらのツールは以下の通りです。\n",
    "\n",
    "- **Brave Search** - ウェブ検索を行うことで天気などの最新情報を取得できます\n",
    "- **Wolfram Alpha** - より複雑な数学的計算に利用できるため、自分で関数を書く必要がありません\n",
    "\n",
    "また、LLMが呼び出せる独自のカスタムツールを作成することも可能です。\n",
    "\n",
    "以下のコード例では:\n",
    "\n",
    "- 利用可能なツール（brave_search, wolfram_alpha）をシステムプロンプトで定義します。\n",
    "- 特定の都市の天気について尋ねるユーザープロンプトを送信します。\n",
    "- LLMはBrave Searchツールへのツールコールで応答します。これは次のような形になります `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*注: この例はツールコールのみを行います。結果を取得したい場合は、Brave APIページで無料アカウントを作成し、関数自体を定義する必要があります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Llama 3.1の制限の一つは、LLMでありながらマルチモーダル対応ができないことです。つまり、画像など異なる種類の入力をプロンプトとして使い、応答を返すことができません。この機能はLlama 3.2の主な特徴の一つです。その他の特徴は以下の通りです。\n",
    "\n",
    "- マルチモーダル対応 - テキストと画像の両方のプロンプトを評価できる\n",
    "- 小規模から中規模のバリエーション（11Bと90B） - 柔軟なデプロイメントが可能\n",
    "- テキスト専用バリエーション（1Bと3B） - エッジやモバイルデバイスでの利用や低遅延を実現\n",
    "\n",
    "マルチモーダル対応は、オープンソースモデルの世界において大きな進歩です。下記のコード例では、画像とテキストの両方のプロンプトを使い、Llama 3.2 90Bから画像の分析を得ています。\n",
    "\n",
    "### Llama 3.2によるマルチモーダル対応\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学びはここで終わりではありません。旅を続けましょう\n",
    "\n",
    "このレッスンを終えたら、[Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) をチェックして、生成AIの知識をさらに深めましょう！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**免責事項**:  \n本書類は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性には努めておりますが、自動翻訳には誤りや不正確な表現が含まれる場合があります。原文（元の言語の文書）が正式な情報源と見なされるべきです。重要な情報については、専門の人間による翻訳を推奨します。本翻訳の利用により生じたいかなる誤解や誤訳についても、当方は一切の責任を負いかねます。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:39:17+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "ja"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}