# 生成AIを責任を持って使用する

[![生成AIを責任を持って使用する](../../../translated_images/03-lesson-banner.png?WT.b0b917735411b39a55748e827c5c3121004890110b27f306bfe685c450c81ff9.ja.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst)

> _このレッスンのビデオを見るには、上の画像をクリックしてください_

AI、特に生成AIに魅了されるのは簡単ですが、それを責任を持って使用する方法を考える必要があります。出力が公正で有害でないことを保証する方法などを考慮する必要があります。この章では、考慮すべきことやAIの使用を改善するための積極的な手段を提供することを目的としています。

## はじめに

このレッスンでは以下をカバーします:

- 生成AIアプリケーションを構築する際に責任あるAIを優先するべき理由
- 責任あるAIの基本原則と生成AIとの関連性
- 戦略とツールを通じてこれらの責任あるAI原則を実践する方法

## 学習目標

このレッスンを完了すると以下が理解できます:

- 生成AIアプリケーションを構築する際の責任あるAIの重要性
- 生成AIアプリケーションを構築する際に責任あるAIの基本原則を考え適用するタイミング
- 責任あるAIの概念を実践するために利用可能なツールと戦略

## 責任あるAIの原則

生成AIへの興奮はかつてないほど高まっています。この興奮は、多くの新しい開発者、注目、資金をこの分野にもたらしました。生成AIを使用して製品や企業を構築しようとする人々にとって非常にポジティブなことですが、同時に責任を持って進めることも重要です。

このコース全体を通じて、スタートアップとAI教育製品の構築に焦点を当てています。責任あるAIの原則：公正性、包括性、信頼性/安全性、セキュリティとプライバシー、透明性、説明責任を使用します。これらの原則を使用して、生成AIを製品に使用する際の関連性を探求します。

## 責任あるAIを優先するべき理由

製品を構築する際に、ユーザーの最善の利益を考慮した人間中心のアプローチを取ることは、最良の結果につながります。

生成AIのユニークさは、ユーザーにとって有用な回答、情報、ガイダンス、コンテンツを作成する力です。多くの手作業を必要とせずに非常に印象的な結果を生み出すことができます。適切な計画と戦略がない場合、残念ながらユーザー、製品、社会全体にとって有害な結果をもたらす可能性もあります。

これらの潜在的に有害な結果の一部（すべてではありません）を見てみましょう:

### 幻覚

幻覚とは、LLMが完全に意味不明な内容や他の情報源に基づいて事実として知られていることを誤った形で生成することを指す用語です。

たとえば、学生がモデルに歴史的な質問をする機能をスタートアップに組み込んだとしましょう。学生が質問`Who was the sole survivor of Titanic?`をします。

モデルは以下のような応答を生成します:

![Prompt saying "Who was the sole survivor of the Titanic"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp)

> _(出典: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

これは非常に自信を持った詳細な回答ですが、残念ながら間違っています。少しの調査でも、タイタニックの災害には生存者が複数いたことがわかります。このトピックについて調査を始めたばかりの学生にとって、この回答は疑問を持たずに事実として扱われるほど説得力があるかもしれません。これにより、AIシステムが信頼できないものとなり、スタートアップの評判に悪影響を与える可能性があります。

任意のLLMの各反復で、幻覚を最小限に抑えるパフォーマンスの向上が見られます。この改善にもかかわらず、アプリケーションの開発者およびユーザーとして、これらの制限を常に意識しておく必要があります。

### 有害なコンテンツ

以前のセクションで、LLMが不正確または意味不明な応答を生成する場合について説明しました。もう一つ注意すべきリスクは、モデルが有害なコンテンツで応答する場合です。

有害なコンテンツは以下のように定義されます:

- 自傷行為や特定のグループへの危害を促す指示を提供すること。
- 憎悪的または軽蔑的なコンテンツ。
- 攻撃や暴力行為の計画を指導すること。
- 違法なコンテンツを見つける方法や違法行為を行う方法を指示すること。
- 性的に露骨なコンテンツを表示すること。

スタートアップとして、この種のコンテンツが学生に見られないようにするための適切なツールと戦略を持っていることを確認したいと考えています。

### 公正性の欠如

公正性は「AIシステムが偏見や差別を受けず、すべての人を公正かつ平等に扱うことを保証すること」と定義されます。生成AIの世界では、モデルの出力によって疎外されたグループの排他的な世界観が強化されないようにすることが重要です。

このような出力は、ユーザーにとってのポジティブな製品体験を構築する上で破壊的であるだけでなく、社会全体にさらなる害を引き起こします。アプリケーション開発者として、生成AIを使用してソリューションを構築する際には、常に幅広く多様なユーザーベースを考慮するべきです。

## 生成AIを責任を持って使用する方法

責任ある生成AIの重要性を確認したところで、AIソリューションを責任を持って構築するために取るべき4つのステップを見てみましょう:

![Mitigate Cycle](../../../translated_images/mitigate-cycle.png?WT.ffc987e1880649a302a311432b78f49faa64e46f65df6350c9c409b5ed79549b.ja.mc_id=academic-105485-koreyst)

### 潜在的な危害を測定する

ソフトウェアテストでは、アプリケーションにおけるユーザーの期待される行動をテストします。同様に、ユーザーが最も使用する可能性のある多様なプロンプトをテストすることは、潜在的な危害を測定する良い方法です。

スタートアップが教育製品を構築しているため、教育に関連するプロンプトのリストを準備することが良いでしょう。これは、特定の科目、歴史的事実、学生生活に関するプロンプトをカバーするためです。

### 潜在的な危害を軽減する

モデルとその応答によって引き起こされる潜在的な危害を防止または制限する方法を見つける時が来ました。これを4つの異なる層で見ることができます:

![Mitigation Layers](../../../translated_images/mitigation-layers.png?WT.cb109f48e143f1ff4dee760b4b0c9477c7d11c2fe57f3efdd89f68c1109f2de6.ja.mc_id=academic-105485-koreyst)

- **モデル**。適切なユースケースに適したモデルを選択します。GPT-4のような大規模で複雑なモデルは、より小さく特定のユースケースに適用すると有害なコンテンツのリスクを引き起こす可能性があります。トレーニングデータを使用して微調整することも、有害なコンテンツのリスクを減少させます。

- **安全システム**。安全システムは、モデルを提供するプラットフォーム上で危害を軽減するためのツールと設定のセットです。例としては、Azure OpenAIサービスのコンテンツフィルタリングシステムがあります。システムはまた、脱獄攻撃やボットからのリクエストなどの望ましくない活動を検出する必要があります。

- **メタプロンプト**。メタプロンプトとグラウンディングは、特定の行動や情報に基づいてモデルを指示または制限する方法です。これは、システム入力を使用してモデルの特定の制限を定義することが含まれます。さらに、システムの範囲やドメインに関連性のある出力を提供することも含まれます。

また、信頼できる情報源の選択からのみ情報を引き出すために、情報検索強化生成（RAG）のような技術を使用することもできます。このコースの後半では、[検索アプリケーションの構築](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)に関するレッスンがあります。

- **ユーザーエクスペリエンス**。最終層は、ユーザーがアプリケーションのインターフェースを通じてモデルと直接対話する場所です。この方法で、UI/UXを設計して、ユーザーがモデルに送信できる入力の種類や、ユーザーに表示されるテキストや画像を制限できます。AIアプリケーションを展開する際には、生成AIアプリケーションができることとできないことについて透明性を持つことも重要です。

[AIアプリケーションのUXデザイン](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)に専念したレッスンがあります。

- **モデルの評価**。LLMと連携することは、モデルがトレーニングされたデータを常に制御できないため、挑戦的です。それにもかかわらず、モデルのパフォーマンスと出力を常に評価する必要があります。モデルの正確性、類似性、根拠、出力の関連性を測定することは依然として重要です。これにより、利害関係者とユーザーに透明性と信頼を提供するのに役立ちます。

### 責任ある生成AIソリューションを運用する

AIアプリケーションに関する運用プラクティスを構築することが最終段階です。これには、法務やセキュリティなどのスタートアップの他の部分と協力して、すべての規制ポリシーに準拠していることを確認することが含まれます。起動前には、配信、インシデントの処理、ロールバックに関する計画を立て、ユーザーに対する危害の拡大を防ぐことも重要です。

## ツール

責任あるAIソリューションの開発は多くの作業のように思えるかもしれませんが、それだけの価値のある作業です。生成AIの分野が成長するにつれて、開発者がワークフローに責任を効率的に統合するのを助けるためのツールも成熟していきます。たとえば、[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)は、APIリクエストを通じて有害なコンテンツや画像を検出するのに役立ちます。

## 知識チェック

責任あるAIの使用を確保するために注意すべきことは何ですか？

1. 回答が正しいこと。
2. 有害な使用、AIが犯罪目的に使用されないこと。
3. AIが偏見や差別を受けないことを保証すること。

A: 2と3が正解です。責任あるAIは、有害な影響やバイアスを軽減する方法を考慮するのに役立ちます。

## 🚀 チャレンジ

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)について調べ、あなたの使用に採用できるものを確認してください。

## 素晴らしい仕事、学習を続けましょう

このレッスンを完了した後は、[生成AI学習コレクション](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)をチェックして、生成AIの知識をさらに向上させましょう！

次のレッスン4では、[プロンプトエンジニアリングの基本](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)を見ていきます！

**免責事項**:  
この文書は、機械ベースのAI翻訳サービスを使用して翻訳されています。正確性を期すよう努めていますが、自動翻訳には誤りや不正確さが含まれる場合がありますのでご注意ください。原文の母国語による文書が信頼できる情報源と見なされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤解釈について、当社は責任を負いません。