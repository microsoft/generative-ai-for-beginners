{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# మెటా ఫ్యామిలీ మోడల్స్‌తో నిర్మాణం\n",
    "\n",
    "## పరిచయం\n",
    "\n",
    "ఈ పాఠం కవర్ చేస్తుంది:\n",
    "\n",
    "- రెండు ప్రధాన మెటా ఫ్యామిలీ మోడల్స్ - ల్లామా 3.1 మరియు ల్లామా 3.2 ని అన్వేషించడం\n",
    "- ప్రతి మోడల్ కోసం ఉపయోగాల మరియు పరిస్థితులను అర్థం చేసుకోవడం\n",
    "- ప్రతి మోడల్ యొక్క ప్రత్యేక లక్షణాలను చూపించే కోడ్ నమూనా\n",
    "\n",
    "## మెటా ఫ్యామిలీ ఆఫ్ మోడల్స్\n",
    "\n",
    "ఈ పాఠంలో, మేము మెటా ఫ్యామిలీ లేదా \"ల్లామా హర్డ్\" నుండి 2 మోడల్స్ - ల్లామా 3.1 మరియు ల్లామా 3.2 ని అన్వేషిస్తాము\n",
    "\n",
    "ఈ మోడల్స్ వివిధ వేరియంట్లలో వస్తాయి మరియు గిట్‌హబ్ మోడల్ మార్కెట్‌ప్లేస్‌లో అందుబాటులో ఉన్నాయి. గిట్‌హబ్ మోడల్స్‌ను ఉపయోగించి [AI మోడల్స్‌తో ప్రోటోటైపింగ్](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) గురించి మరిన్ని వివరాలు ఇక్కడ ఉన్నాయి.\n",
    "\n",
    "మోడల్ వేరియంట్లు:\n",
    "- ల్లామా 3.1 - 70B ఇన్‌స్ట్రక్ట్\n",
    "- ల్లామా 3.1 - 405B ఇన్‌స్ట్రక్ట్\n",
    "- ల్లామా 3.2 - 11B విజన్ ఇన్‌స్ట్రక్ట్\n",
    "- ల్లామా 3.2 - 90B విజన్ ఇన్‌స్ట్రక్ట్\n",
    "\n",
    "*గమనిక: ల్లామా 3 కూడా గిట్‌హబ్ మోడల్స్‌లో అందుబాటులో ఉంది కానీ ఈ పాఠంలో కవర్ చేయబడదు*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1 \n",
    "\n",
    "405 బిలియన్ పరామితులతో, Llama 3.1 ఓపెన్ సోర్స్ LLM వర్గంలోకి వస్తుంది. \n",
    "\n",
    "ఈ మోడల్ పూర్వ విడుదల అయిన Llama 3 కంటే మెరుగైనది: \n",
    "\n",
    "- పెద్ద కాంటెక్స్ట్ విండో - 128k టోకెన్లు vs 8k టోకెన్లు \n",
    "- పెద్ద గరిష్ట అవుట్‌పుట్ టోకెన్లు - 4096 vs 2048 \n",
    "- మెరుగైన బహుభాషా మద్దతు - శిక్షణ టోకెన్ల పెరుగుదల కారణంగా \n",
    "\n",
    "ఇవి Llama 3.1 కి మరింత సంక్లిష్టమైన వినియోగాల నిర్వహణను సులభతరం చేస్తాయి, GenAI అప్లికేషన్లు నిర్మించేటప్పుడు: \n",
    "- స్థానిక ఫంక్షన్ కాలింగ్ - LLM వర్క్‌ఫ్లో వెలుపల ఉన్న బాహ్య టూల్స్ మరియు ఫంక్షన్లను పిలవగల సామర్థ్యం\n",
    "- మెరుగైన RAG పనితీరు - పెద్ద కాంటెక్స్ట్ విండో కారణంగా \n",
    "- సింథటిక్ డేటా ఉత్పత్తి - ఫైన్-ట్యూనింగ్ వంటి పనుల కోసం సమర్థవంతమైన డేటాను సృష్టించే సామర్థ్యం\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### స్థానిక ఫంక్షన్ కాలింగ్\n",
    "\n",
    "Llama 3.1 ఫంక్షన్ లేదా టూల్ కాల్స్ చేయడంలో మరింత సమర్థవంతంగా ఉండేందుకు ఫైన్-ట్యూన్ చేయబడింది. ఇది రెండు బిల్ట్-ఇన్ టూల్స్‌ను కూడా కలిగి ఉంది, ఇవి యూజర్ ప్రాంప్ట్ ఆధారంగా ఉపయోగించాల్సిన అవసరం ఉన్నట్లు మోడల్ గుర్తించగలదు. ఈ టూల్స్ ఇవి:\n",
    "\n",
    "- **Brave Search** - వెబ్ సెర్చ్ ద్వారా వాతావరణం వంటి తాజా సమాచారాన్ని పొందడానికి ఉపయోగించవచ్చు\n",
    "- **Wolfram Alpha** - మరింత క్లిష్టమైన గణిత లెక్కింపులకు ఉపయోగించవచ్చు, కాబట్టి మీ స్వంత ఫంక్షన్లు రాయాల్సిన అవసరం లేదు.\n",
    "\n",
    "మీరు కూడా LLM కాల్ చేయగల మీ స్వంత కస్టమ్ టూల్స్ సృష్టించవచ్చు.\n",
    "\n",
    "క్రింది కోడ్ ఉదాహరణలో:\n",
    "\n",
    "- అందుబాటులో ఉన్న టూల్స్ (brave_search, wolfram_alpha) ను సిస్టమ్ ప్రాంప్ట్‌లో నిర్వచిస్తాము.\n",
    "- ఒక నగరంలో వాతావరణం గురించి అడిగే యూజర్ ప్రాంప్ట్ పంపిస్తాము.\n",
    "- LLM Brave Search టూల్‌కు టూల్ కాల్‌తో స్పందిస్తుంది, ఇది ఇలా కనిపిస్తుంది `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*గమనిక: ఈ ఉదాహరణ కేవలం టూల్ కాల్ చేస్తుంది, మీరు ఫలితాలు పొందాలనుకుంటే, Brave API పేజీలో ఉచిత ఖాతాను సృష్టించి ఫంక్షన్‌ను నిర్వచించాల్సి ఉంటుంది*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2 \n",
    "\n",
    "LLM అయినప్పటికీ, Llama 3.1 కి ఉన్న ఒక పరిమితి మల్టిమోడాలిటీ. అంటే, ఇమేజ్‌లను ప్రాంప్ట్‌లుగా ఉపయోగించి వివిధ రకాల ఇన్‌పుట్‌లను ఉపయోగించగలగడం మరియు ప్రతిస్పందనలు ఇవ్వగలగడం. ఈ సామర్థ్యం Llama 3.2 యొక్క ప్రధాన లక్షణాలలో ఒకటి. ఈ లక్షణాలలో కూడా ఉన్నాయి: \n",
    "\n",
    "- మల్టిమోడాలిటీ - టెక్స్ట్ మరియు ఇమేజ్ ప్రాంప్ట్‌లను రెండింటినీ అంచనా వేయగల సామర్థ్యం \n",
    "- చిన్న నుండి మధ్యస్థ పరిమాణం వేరియేషన్లు (11B మరియు 90B) - ఇది అనువైన డిప్లాయ్‌మెంట్ ఎంపికలను అందిస్తుంది, \n",
    "- టెక్స్ట్-ఓన్లీ వేరియేషన్లు (1B మరియు 3B) - ఇది మోడల్‌ను ఎడ్జ్ / మొబైల్ పరికరాలపై డిప్లాయ్ చేయడానికి అనుమతిస్తుంది మరియు తక్కువ లేటెన్సీని అందిస్తుంది \n",
    "\n",
    "మల్టిమోడల్ మద్దతు ఓపెన్ సోర్స్ మోడల్స్ ప్రపంచంలో ఒక పెద్ద అడుగు సూచిస్తుంది. క్రింది కోడ్ ఉదాహరణ Llama 3.2 90B నుండి ఇమేజ్ విశ్లేషణ పొందడానికి ఇమేజ్ మరియు టెక్స్ట్ ప్రాంప్ట్ రెండింటినీ తీసుకుంటుంది. \n",
    "\n",
    "### Llama 3.2 తో మల్టిమోడల్ మద్దతు\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ఇక్కడే నేర్చుకోవడం ఆగదు, ప్రయాణాన్ని కొనసాగించండి\n",
    "\n",
    "ఈ పాఠం పూర్తి చేసిన తర్వాత, మీ జనరేటివ్ AI జ్ఞానాన్ని మరింత పెంచుకోవడానికి మా [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ను చూడండి!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**అస్పష్టత**:  \nఈ పత్రాన్ని AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో పొరపాట్లు లేదా తప్పిదాలు ఉండవచ్చు. మూల పత్రం దాని స్వదేశీ భాషలో అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి, ప్రొఫెషనల్ మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వాడకం వల్ల కలిగే ఏవైనా అపార్థాలు లేదా తప్పుదారితీసే అర్థాలు కోసం మేము బాధ్యత వహించము.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-12-19T20:57:54+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "te"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}