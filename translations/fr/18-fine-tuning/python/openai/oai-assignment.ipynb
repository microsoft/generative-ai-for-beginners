{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affinage des modèles Open AI\n",
    "\n",
    "Ce carnet est basé sur les directives actuelles fournies dans la documentation [Affinage](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst) d'Open AI.\n",
    "\n",
    "L'affinage améliore les performances des modèles de base pour votre application en les réentraînant avec des données supplémentaires et un contexte pertinent pour ce cas d'utilisation ou scénario spécifique. Notez que les techniques d'ingénierie de prompt comme le _few shot learning_ et la _génération augmentée par récupération_ vous permettent d'améliorer le prompt par défaut avec des données pertinentes pour améliorer la qualité. Cependant, ces approches sont limitées par la taille maximale de la fenêtre de tokens du modèle de base ciblé.\n",
    "\n",
    "Avec l'affinage, nous réentraînons effectivement le modèle lui-même avec les données requises (ce qui nous permet d'utiliser beaucoup plus d'exemples que ce qui peut tenir dans la fenêtre maximale de tokens) - et déployons une version _personnalisée_ du modèle qui n'a plus besoin d'avoir des exemples fournis au moment de l'inférence. Cela améliore non seulement l'efficacité de notre conception de prompt (nous avons plus de flexibilité pour utiliser la fenêtre de tokens pour d'autres choses) mais améliore potentiellement aussi nos coûts (en réduisant le nombre de tokens que nous devons envoyer au modèle au moment de l'inférence).\n",
    "\n",
    "L'affinage comporte 4 étapes :\n",
    "1. Préparer les données d'entraînement et les télécharger.\n",
    "1. Lancer le travail d'entraînement pour obtenir un modèle affiné.\n",
    "1. Évaluer le modèle affiné et itérer pour la qualité.\n",
    "1. Déployer le modèle affiné pour l'inférence une fois satisfait.\n",
    "\n",
    "Notez que tous les modèles de base ne supportent pas l'affinage - [consultez la documentation OpenAI](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned?WT.mc_id=academic-105485-koreyst) pour les informations les plus récentes. Vous pouvez également affiner un modèle déjà affiné. Dans ce tutoriel, nous utiliserons `gpt-35-turbo` comme modèle de base cible pour l'affinage.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 1.1 : Préparez votre jeu de données\n",
    "\n",
    "Construisons un chatbot qui vous aide à comprendre le tableau périodique des éléments en répondant aux questions sur un élément avec un limerick. Dans _ce_ tutoriel simple, nous allons simplement créer un jeu de données pour entraîner le modèle avec quelques exemples de réponses montrant le format attendu des données. Dans un cas d'utilisation réel, vous devrez créer un jeu de données avec beaucoup plus d'exemples. Vous pourrez également utiliser un jeu de données ouvert (pour votre domaine d'application) s'il en existe un, et le reformater pour l'utiliser dans l'affinage.\n",
    "\n",
    "Puisque nous nous concentrons sur `gpt-35-turbo` et recherchons une réponse en un seul tour (complétion de chat), nous pouvons créer des exemples en utilisant [ce format suggéré](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?WT.mc_id=academic-105485-koreyst) reflétant les exigences de complétion de chat d'OpenAI. Si vous attendez un contenu conversationnel multi-tours, vous utiliseriez le [format d'exemple multi-tours](https://platform.openai.com/docs/guides/fine-tuning/multi-turn-chat-examples?WT.mc_id=academic-105485-koreyst) qui inclut un paramètre `weight` pour indiquer quels messages doivent être utilisés (ou non) dans le processus d'affinage.\n",
    "\n",
    "Nous utiliserons ici le format plus simple en un seul tour pour notre tutoriel. Les données sont au format [jsonl](https://jsonlines.org/?WT.mc_id=academic-105485-koreyst) avec 1 enregistrement par ligne, chacun représenté comme un objet JSON formaté. L'extrait ci-dessous montre 2 enregistrements en exemple - voir [training-data.jsonl](../../../../../18-fine-tuning/python/openai/training-data.jsonl) pour l'ensemble complet d'exemples (10 exemples) que nous utiliserons pour notre tutoriel d'affinage. **Remarque :** Chaque enregistrement _doit_ être défini sur une seule ligne (non réparti sur plusieurs lignes comme c'est typique dans un fichier JSON formaté)\n",
    "\n",
    "```json\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Gallium\"}, {\"role\": \"assistant\", \"content\": \"Gallium, oh gallium, so light - Melts in your hand, oh what a sight - At 86 degrees - Its liquid with ease - And in semiconductors, it's out of sight\"}]}\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Hydrogen\"}, {\"role\": \"assistant\", \"content\": \"Hydrogen, the first in the line - The lightest of all, so divine - It's in water, you see - And in stars, it's the key - The universe's most common sign\"}]}\n",
    "```\n",
    "\n",
    "Dans un cas d'utilisation réel, vous aurez besoin d'un ensemble d'exemples beaucoup plus grand pour de bons résultats - le compromis sera entre la qualité des réponses et le temps/coûts pour l'affinage. Nous utilisons un petit ensemble afin de pouvoir compléter l'affinage rapidement pour illustrer le processus. Voir [cet exemple du OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst) pour un tutoriel d'affinage plus complexe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Étape 1.2 Téléchargez votre jeu de données\n",
    "\n",
    "Téléchargez les données en utilisant l'API Files [comme décrit ici](https://platform.openai.com/docs/guides/fine-tuning/upload-a-training-file). Notez que pour exécuter ce code, vous devez d'abord avoir effectué les étapes suivantes :\n",
    " - Installer le package Python `openai` (assurez-vous d'utiliser une version >=0.28.0 pour les dernières fonctionnalités)\n",
    " - Définir la variable d'environnement `OPENAI_API_KEY` avec votre clé API OpenAI\n",
    "Pour en savoir plus, consultez le [guide d'installation](./../../../00-course-setup/02-setup-local.md?WT.mc_id=academic-105485-koreyst) fourni pour le cours.\n",
    "\n",
    "Maintenant, exécutez le code pour créer un fichier à télécharger à partir de votre fichier JSONL local.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-JdAJcagdOTG6ACNlFWzuzmyV', bytes=4021, created_at=1715566183, filename='training-data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
      "Training File ID: file-JdAJcagdOTG6ACNlFWzuzmyV\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_file = client.files.create(\n",
    "  file=open(\"./training-data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(ft_file)\n",
    "print(\"Training File ID: \" + ft_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Étape 2.1 : Créer la tâche de fine-tuning avec le SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', created_at=1715566184, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-EZ6ag0n0S6Zm8eV9BSWKmE6l', result_files=[], seed=830529052, status='validating_files', trained_tokens=None, training_file='file-JdAJcagdOTG6ACNlFWzuzmyV', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)\n",
      "Fine-tuning Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_filejob = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file.id, \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "print(ft_filejob)\n",
    "print(\"Fine-tuning Job ID: \" + ft_filejob.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Étape 2.2 : Vérifier le statut du travail\n",
    "\n",
    "Voici quelques actions que vous pouvez effectuer avec l’API `client.fine_tuning.jobs` :\n",
    "- `client.fine_tuning.jobs.list(limit=<n>)` - Lister les n derniers travaux de fine-tuning\n",
    "- `client.fine_tuning.jobs.retrieve(<job_id>)` - Obtenir les détails d’un travail de fine-tuning spécifique\n",
    "- `client.fine_tuning.jobs.cancel(<job_id>)` - Annuler un travail de fine-tuning\n",
    "- `client.fine_tuning.jobs.list_events(fine_tuning_job_id=<job_id>, limit=<b>)` - Lister jusqu’à n événements du travail\n",
    "- `client.fine_tuning.jobs.create(model=\"gpt-35-turbo\", training_file=\"your-training-file.jsonl\", ...)`\n",
    "\n",
    "La première étape du processus est _la validation du fichier d’entraînement_ pour s’assurer que les données sont au bon format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-GkWiDgZmOsuv4q5cSTEGscY6', created_at=1715566184, level='info', message='Validating training file: file-JdAJcagdOTG6ACNlFWzuzmyV', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-3899xdVTO3LN7Q7LkKLMJUnb', created_at=1715566184, level='info', message='Created fine-tuning job: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_filejob.id, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n",
      "Status: running\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "# Once the training data is validated\n",
    "# Track the job status to see if it is running and when it is complete\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(\"Trained Tokens:\", response.trained_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Étape 2.3 : Suivre les événements pour surveiller les progrès\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 85/100: training loss=0.14\n",
      "Step 86/100: training loss=0.00\n",
      "Step 87/100: training loss=0.00\n",
      "Step 88/100: training loss=0.07\n",
      "Step 89/100: training loss=0.00\n",
      "Step 90/100: training loss=0.00\n",
      "Step 91/100: training loss=0.00\n",
      "Step 92/100: training loss=0.00\n",
      "Step 93/100: training loss=0.00\n",
      "Step 94/100: training loss=0.00\n",
      "Step 95/100: training loss=0.08\n",
      "Step 96/100: training loss=0.05\n",
      "Step 97/100: training loss=0.00\n",
      "Step 98/100: training loss=0.00\n",
      "Step 99/100: training loss=0.00\n",
      "Step 100/100: training loss=0.00\n",
      "Checkpoint created at step 80 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyyF2:ckpt-step-80\n",
      "Checkpoint created at step 90 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyzhK:ckpt-step-90\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "# You can also track progress in a more granular way by checking for events\n",
    "# Refresh this code till you get the `The job has successfully completed` message\n",
    "response = client.fine_tuning.jobs.list_events(ft_filejob.id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2.4 : Voir le statut dans le tableau de bord OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez également consulter le statut en visitant le site Web d'OpenAI et en explorant la section _Fine-tuning_ de la plateforme. Cela vous montrera le statut du travail en cours, et vous permettra également de suivre l'historique des exécutions des travaux précédents. Dans cette capture d'écran, vous pouvez voir que l'exécution précédente a échoué, et que la deuxième exécution a réussi. Pour contexte, cela s'est produit lorsque la première exécution utilisait un fichier JSON avec des enregistrements mal formatés - une fois corrigé, la deuxième exécution s'est terminée avec succès et a rendu le modèle disponible à l'utilisation.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fr/fine-tuned-model-status.563271727bf7bfba.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez également consulter les messages d'état et les métriques en faisant défiler plus bas dans le tableau de bord visuel comme indiqué :\n",
    "\n",
    "| Messages | Métriques |\n",
    "|:---|:---|\n",
    "| ![Messages](../../../../../translated_images/fr/fine-tuned-messages-panel.4ed0c2da5ea1313b.webp) |  ![Metrics](../../../../../translated_images/fr/fine-tuned-metrics-panel.700d7e4995a65229.webp)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Étape 3.1 : Récupérer l'ID et tester le modèle affiné dans le code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the identity of the fine-tuned model once ready\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "fine_tuned_model_id = response.fine_tuned_model\n",
    "print(\"Fine-tuned Model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Strontium, a metal so bright - It's in fireworks, a dazzling sight - It's in bones, you see - And in tea, it's the key - It's the fortieth, so pure, that's the right\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# You can then use that model to generate completions from the SDK as shown\n",
    "# Or you can load that model into the OpenAI Playground (in the UI) to validate it from there.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Elle, a factual chatbot that answers questions about elements in the periodic table with a limerick\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Strontium\"},\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Étape 3.2 : Charger et tester le modèle affiné dans le Playground\n",
    "\n",
    "Vous pouvez maintenant tester le modèle affiné de deux manières. Tout d'abord, vous pouvez visiter le Playground et utiliser le menu déroulant Models pour sélectionner votre modèle affiné parmi les options listées. L'autre option est d'utiliser l'option \"Playground\" affichée dans le panneau Fine-tuning (voir capture d'écran ci-dessus) qui lance la vue _comparative_ suivante montrant les versions du modèle de base et du modèle affiné côte à côte pour une évaluation rapide.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fr/fine-tuned-playground-compare.56e06f0ad8922016.webp)\n",
    "\n",
    "Il suffit de remplir le contexte système utilisé dans vos données d'entraînement et de fournir votre question de test. Vous remarquerez que les deux côtés sont mis à jour avec le même contexte et la même question. Lancez la comparaison et vous verrez la différence dans les réponses entre les deux. _Notez comment le modèle affiné rend la réponse dans le format que vous avez fourni dans vos exemples tandis que le modèle de base suit simplement l'invite système_.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fr/fine-tuned-playground-launch.5a26495c983c6350.webp)\n",
    "\n",
    "Vous remarquerez que la comparaison fournit également le nombre de tokens pour chaque modèle, ainsi que le temps pris pour l'inférence. **Cet exemple spécifique est simpliste et destiné à montrer le processus sans refléter un jeu de données ou un scénario réel**. Vous pouvez constater que les deux échantillons montrent le même nombre de tokens (le contexte système et l'invite utilisateur sont identiques) avec le modèle affiné prenant plus de temps pour l'inférence (modèle personnalisé).\n",
    "\n",
    "Dans des scénarios réels, vous n'utiliserez pas un exemple simplifié comme celui-ci, mais un affinage sur des données réelles (par exemple, un catalogue produit pour le service client) où la qualité de la réponse sera bien plus évidente. Dans _ce_ contexte, obtenir une qualité de réponse équivalente avec le modèle de base nécessitera plus d'ingénierie de prompt personnalisée, ce qui augmentera l'utilisation des tokens et potentiellement le temps de traitement lié à l'inférence. _Pour essayer cela, consultez les exemples de fine-tuning dans l'OpenAI Cookbook pour commencer._\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Avertissement** :  \nCe document a été traduit à l’aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d’assurer l’exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d’origine doit être considéré comme la source faisant foi. Pour les informations critiques, une traduction professionnelle réalisée par un humain est recommandée. Nous déclinons toute responsabilité en cas de malentendus ou de mauvaises interprétations résultant de l’utilisation de cette traduction.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "1725725c956564056baf895e6ca92aa5",
   "translation_date": "2025-12-19T08:38:06+00:00",
   "source_file": "18-fine-tuning/python/openai/oai-assignment.ipynb",
   "language_code": "fr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}