# Ressources pour l'apprentissage autonome

La leçon a été construite en utilisant un certain nombre de ressources principales d'OpenAI et d'Azure OpenAI comme références pour la terminologie et les tutoriels. Voici une liste non exhaustive, pour vos propres parcours d'apprentissage autonome.

## 1. Ressources principales

| Titre/Lien                                                                                                                                                                                                                   | Description                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning avec les modèles OpenAI](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                  | Le fine-tuning améliore l'apprentissage par quelques exemples en s'entraînant sur beaucoup plus d'exemples que ceux qui peuvent tenir dans l'invite, vous faisant économiser des coûts, améliorant la qualité des réponses et permettant des requêtes à faible latence. **Obtenez un aperçu du fine-tuning d'OpenAI.**                                            |
| [Qu'est-ce que le Fine-Tuning avec Azure OpenAI ?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)         | Comprenez **ce qu'est le fine-tuning (concept)**, pourquoi vous devriez vous y intéresser (problème motivant), quelles données utiliser (entraînement) et comment mesurer la qualité                                                                                                                                          |
| [Personnaliser un modèle avec le fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Le service Azure OpenAI vous permet d'adapter nos modèles à vos ensembles de données personnels en utilisant le fine-tuning. Apprenez **comment affiner (processus)** les modèles sélectionnés en utilisant Azure AI Studio, le SDK Python ou l'API REST.                                                                 |
| [Recommandations pour le fine-tuning des LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                           | Les LLM peuvent ne pas bien fonctionner sur des domaines, tâches ou ensembles de données spécifiques, ou peuvent produire des résultats inexacts ou trompeurs. **Quand devriez-vous envisager le fine-tuning** comme solution possible à cela ?                                                                                     |
| [Fine-Tuning Continu](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)               | Le fine-tuning continu est le processus itératif de sélection d'un modèle déjà affiné comme modèle de base et **de l'affiner davantage** sur de nouveaux ensembles d'exemples d'entraînement.                                                                                                                                  |
| [Fine-tuning et appel de fonction](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                      | Affiner votre modèle **avec des exemples d'appel de fonction** peut améliorer la sortie du modèle en obtenant des résultats plus précis et cohérents - avec des réponses formatées de manière similaire et des économies de coûts.                                                                                         |
| [Fine-tuning des modèles : Guide Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                  | Consultez ce tableau pour comprendre **quels modèles peuvent être affinés** dans Azure OpenAI, et dans quelles régions ils sont disponibles. Consultez leurs limites de jetons et les dates d'expiration des données d'entraînement si nécessaire.                                                                 |
| [Fine-Tune or Not To Fine-Tune? That is the Question](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                     | Cet épisode de 30 minutes **d'octobre 2023** de l'AI Show discute des avantages, des inconvénients et des perspectives pratiques qui vous aident à prendre cette décision.                                                                                                                                                  |
| [Démarrer avec le Fine-Tuning des LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning?WT.mc_id=academic-105485-koreyst)                                          | Cette ressource **AI Playbook** vous guide à travers les exigences de données, le formatage, le fine-tuning des hyperparamètres et les défis/limitations que vous devez connaître.                                                                                                                                          |
| **Tutoriel** : [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                             | Apprenez à créer un ensemble de données d'exemple pour le fine-tuning, à vous préparer pour le fine-tuning, à créer un travail de fine-tuning et à déployer le modèle affiné sur Azure.                                                                                                                                     |
| **Tutoriel** : [Affiner un modèle Llama 2 dans Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                               | Azure AI Studio vous permet d'adapter de grands modèles de langage à vos ensembles de données personnels _en utilisant un flux de travail basé sur une interface utilisateur adapté aux développeurs à faible code_. Voir cet exemple.                                                                                      |
| **Tutoriel** : [Affiner les modèles Hugging Face pour un seul GPU sur Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)      | Cet article décrit comment affiner un modèle Hugging Face avec la bibliothèque Hugging Face transformers sur un seul GPU avec Azure DataBricks + les bibliothèques Hugging Face Trainer.                                                                                                                                   |
| **Formation :** [Affiner un modèle de base avec Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)             | Le catalogue de modèles dans Azure Machine Learning propose de nombreux modèles open source que vous pouvez affiner pour votre tâche spécifique. Essayez ce module [du chemin d'apprentissage AzureML Generative AI](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Tutoriel :** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                           | Affiner les modèles GPT-3.5 ou GPT-4 sur Microsoft Azure en utilisant W&B permet un suivi et une analyse détaillés des performances du modèle. Ce guide étend les concepts du guide OpenAI Fine-Tuning avec des étapes et des fonctionnalités spécifiques pour Azure OpenAI.                                                 |
|                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                               |

## 2. Ressources secondaires

Cette section rassemble des ressources supplémentaires qui valent la peine d'être explorées, mais que nous n'avons pas eu le temps de couvrir dans cette leçon. Elles peuvent être abordées dans une leçon future, ou comme option de devoir secondaire, à une date ultérieure. Pour l'instant, utilisez-les pour développer votre propre expertise et vos connaissances sur ce sujet.

| Titre/Lien                                                                                                                                                                                                           | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook** : [Préparation et analyse des données pour le fine-tuning du modèle de chat](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                     | Ce carnet sert d'outil pour prétraiter et analyser l'ensemble de données de chat utilisé pour le fine-tuning d'un modèle de chat. Il vérifie les erreurs de format, fournit des statistiques de base et estime les comptes de jetons pour les coûts de fine-tuning. Voir : [Méthode de fine-tuning pour gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook** : [Fine-Tuning pour la génération augmentée par récupération (RAG) avec Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | L'objectif de ce carnet est de parcourir un exemple complet de la manière d'affiner les modèles OpenAI pour la génération augmentée par récupération (RAG). Nous intégrerons également Qdrant et l'apprentissage par quelques exemples pour améliorer les performances du modèle et réduire les fabrications.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook** : [Fine-tuning GPT avec Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                          | Weights & Biases (W&B) est la plateforme de développement AI, avec des outils pour entraîner des modèles, affiner des modèles et exploiter des modèles de base. Lisez d'abord leur guide [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai?WT.mc_id=academic-105485-koreyst), puis essayez l'exercice du Cookbook.                                                                                                                                                                                                                  |
| **Tutoriel Communautaire** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - fine-tuning pour les petits modèles de langage                                    | Découvrez [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), le nouveau petit modèle de Microsoft, remarquablement puissant mais compact. Ce tutoriel vous guidera à travers le fine-tuning de Phi-2, en vous montrant comment construire un ensemble de données unique et affiner le modèle en utilisant QLoRA.                                                                                                                                                                       |
| **Tutoriel Hugging Face** [Comment affiner les LLMs en 2024 avec Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                           | Cet article de blog vous guide à travers le processus d'affinage des LLMs ouverts en utilisant Hugging Face TRL, Transformers & datasets en 2024. Vous définissez un cas d'utilisation, configurez un environnement de développement, préparez un ensemble de données, affinez le modèle, le testez et l'évaluez, puis le déployez en production.                                                                                                                                                                                                                                                                |
| **Hugging Face : [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                           | Apporte un entraînement et des déploiements plus rapides et plus faciles des [modèles d'apprentissage automatique à la pointe](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Le dépôt propose des tutoriels adaptés à Colab avec des vidéos YouTube, pour le fine-tuning. **Reflète la récente mise à jour [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)**. Lisez la [documentation AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

**Avertissement** :  
Ce document a été traduit à l'aide de services de traduction automatisés par intelligence artificielle. Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de faire appel à une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interprétations erronées résultant de l'utilisation de cette traduction.