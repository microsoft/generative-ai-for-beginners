<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-07-09T17:34:11+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "fr"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.fr.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Affiner votre LLM

L‚Äôutilisation de grands mod√®les de langage pour cr√©er des applications d‚ÄôIA g√©n√©rative apporte de nouveaux d√©fis. Un enjeu cl√© est d‚Äôassurer la qualit√© des r√©ponses (pr√©cision et pertinence) g√©n√©r√©es par le mod√®le pour une requ√™te utilisateur donn√©e. Dans les le√ßons pr√©c√©dentes, nous avons abord√© des techniques comme l‚Äôing√©nierie de prompt et la g√©n√©ration augment√©e par r√©cup√©ration, qui tentent de r√©soudre ce probl√®me en _modifiant l‚Äôentr√©e du prompt_ du mod√®le existant.

Dans la le√ßon d‚Äôaujourd‚Äôhui, nous discutons d‚Äôune troisi√®me technique, le **fine-tuning**, qui cherche √† relever ce d√©fi en _r√©entra√Ænant le mod√®le lui-m√™me_ avec des donn√©es suppl√©mentaires. Entrons dans les d√©tails.

## Objectifs d‚Äôapprentissage

Cette le√ßon introduit le concept de fine-tuning pour les mod√®les de langage pr√©-entra√Æn√©s, explore les avantages et les d√©fis de cette approche, et fournit des conseils sur quand et comment utiliser le fine-tuning pour am√©liorer les performances de vos mod√®les d‚ÄôIA g√©n√©rative.

√Ä la fin de cette le√ßon, vous devriez √™tre capable de r√©pondre aux questions suivantes :

- Qu‚Äôest-ce que le fine-tuning pour les mod√®les de langage ?
- Quand et pourquoi le fine-tuning est-il utile ?
- Comment puis-je affiner un mod√®le pr√©-entra√Æn√© ?
- Quelles sont les limites du fine-tuning ?

Pr√™t ? Commen√ßons.

## Guide illustr√©

Vous souhaitez avoir une vue d‚Äôensemble de ce que nous allons couvrir avant de plonger dans le sujet ? D√©couvrez ce guide illustr√© qui d√©crit le parcours d‚Äôapprentissage de cette le√ßon ‚Äì depuis la compr√©hension des concepts cl√©s et de la motivation du fine-tuning, jusqu‚Äô√† la ma√Ætrise du processus et des bonnes pratiques pour r√©aliser cette t√¢che. C‚Äôest un sujet passionnant √† explorer, alors n‚Äôoubliez pas de consulter la page [Ressources](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) pour des liens suppl√©mentaires qui soutiendront votre apprentissage autonome !

![Guide illustr√© du fine-tuning des mod√®les de langage](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.fr.png)

## Qu‚Äôest-ce que le fine-tuning pour les mod√®les de langage ?

Par d√©finition, les grands mod√®les de langage sont _pr√©-entra√Æn√©s_ sur de grandes quantit√©s de textes provenant de sources diverses, y compris Internet. Comme nous l‚Äôavons vu dans les le√ßons pr√©c√©dentes, nous avons besoin de techniques comme l‚Äô_ing√©nierie de prompt_ et la _g√©n√©ration augment√©e par r√©cup√©ration_ pour am√©liorer la qualit√© des r√©ponses du mod√®le aux questions de l‚Äôutilisateur (¬´ prompts ¬ª).

Une technique populaire d‚Äôing√©nierie de prompt consiste √† donner au mod√®le plus d‚Äôindications sur ce qui est attendu dans la r√©ponse, soit en fournissant des _instructions_ (guidage explicite), soit en _donnant quelques exemples_ (guidage implicite). Cela s‚Äôappelle l‚Äô_apprentissage par quelques exemples_ (few-shot learning), mais cette m√©thode pr√©sente deux limites :

- Les limites de tokens du mod√®le peuvent restreindre le nombre d‚Äôexemples que vous pouvez fournir, ce qui limite son efficacit√©.
- Le co√ªt en tokens peut rendre on√©reux l‚Äôajout d‚Äôexemples √† chaque prompt, ce qui r√©duit la flexibilit√©.

Le fine-tuning est une pratique courante dans les syst√®mes d‚Äôapprentissage automatique o√π l‚Äôon prend un mod√®le pr√©-entra√Æn√© et on le r√©entra√Æne avec de nouvelles donn√©es pour am√©liorer ses performances sur une t√¢che sp√©cifique. Dans le contexte des mod√®les de langage, on peut affiner le mod√®le pr√©-entra√Æn√© _avec un ensemble d‚Äôexemples s√©lectionn√©s pour une t√¢che ou un domaine d‚Äôapplication donn√©_ afin de cr√©er un **mod√®le personnalis√©** qui sera plus pr√©cis et pertinent pour cette t√¢che ou ce domaine sp√©cifique. Un avantage secondaire du fine-tuning est qu‚Äôil peut aussi r√©duire le nombre d‚Äôexemples n√©cessaires pour l‚Äôapprentissage par quelques exemples, diminuant ainsi l‚Äôutilisation des tokens et les co√ªts associ√©s.

## Quand et pourquoi faut-il affiner les mod√®les ?

Dans _ce_ contexte, lorsque nous parlons de fine-tuning, nous faisons r√©f√©rence au fine-tuning **supervis√©**, o√π le r√©entra√Ænement se fait en **ajoutant de nouvelles donn√©es** qui ne faisaient pas partie du jeu de donn√©es d‚Äôentra√Ænement initial. Cela diff√®re d‚Äôune approche de fine-tuning non supervis√© o√π le mod√®le est r√©entra√Æn√© sur les donn√©es originales, mais avec des hyperparam√®tres diff√©rents.

L‚Äôessentiel √† retenir est que le fine-tuning est une technique avanc√©e qui n√©cessite un certain niveau d‚Äôexpertise pour obtenir les r√©sultats souhait√©s. S‚Äôil est mal r√©alis√©, il peut ne pas apporter les am√©liorations attendues, voire d√©grader les performances du mod√®le pour le domaine cibl√©.

Avant d‚Äôapprendre ¬´ comment ¬ª affiner les mod√®les de langage, vous devez savoir ¬´ pourquoi ¬ª vous devriez emprunter cette voie, et ¬´ quand ¬ª commencer le processus de fine-tuning. Commencez par vous poser ces questions :

- **Cas d‚Äôusage** : Quel est votre _cas d‚Äôusage_ pour le fine-tuning ? Quel aspect du mod√®le pr√©-entra√Æn√© actuel souhaitez-vous am√©liorer ?
- **Alternatives** : Avez-vous essay√© _d‚Äôautres techniques_ pour atteindre les r√©sultats souhait√©s ? Utilisez-les pour cr√©er une base de comparaison.
  - Ing√©nierie de prompt : Essayez des techniques comme le few-shot prompting avec des exemples de r√©ponses pertinentes. √âvaluez la qualit√© des r√©ponses.
  - G√©n√©ration augment√©e par r√©cup√©ration : Essayez d‚Äôaugmenter les prompts avec des r√©sultats de requ√™tes extraits de vos donn√©es. √âvaluez la qualit√© des r√©ponses.
- **Co√ªts** : Avez-vous identifi√© les co√ªts li√©s au fine-tuning ?
  - Possibilit√© d‚Äôajustement ‚Äì le mod√®le pr√©-entra√Æn√© est-il disponible pour le fine-tuning ?
  - Effort ‚Äì pour pr√©parer les donn√©es d‚Äôentra√Ænement, √©valuer et affiner le mod√®le.
  - Calcul ‚Äì pour ex√©cuter les t√¢ches de fine-tuning et d√©ployer le mod√®le affin√©.
  - Donn√©es ‚Äì acc√®s √† un nombre suffisant d‚Äôexemples de qualit√© pour un impact significatif.
- **B√©n√©fices** : Avez-vous confirm√© les avantages du fine-tuning ?
  - Qualit√© ‚Äì le mod√®le affin√© a-t-il surpass√© la base de r√©f√©rence ?
  - Co√ªt ‚Äì r√©duit-il l‚Äôutilisation des tokens en simplifiant les prompts ?
  - Extensibilit√© ‚Äì pouvez-vous r√©utiliser le mod√®le de base pour de nouveaux domaines ?

En r√©pondant √† ces questions, vous devriez pouvoir d√©cider si le fine-tuning est la bonne approche pour votre cas d‚Äôusage. Id√©alement, cette approche est valable uniquement si les b√©n√©fices l‚Äôemportent sur les co√ªts. Une fois la d√©cision prise, il est temps de r√©fl√©chir √† _comment_ affiner le mod√®le pr√©-entra√Æn√©.

Vous souhaitez approfondir le processus de prise de d√©cision ? Regardez [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Comment affiner un mod√®le pr√©-entra√Æn√© ?

Pour affiner un mod√®le pr√©-entra√Æn√©, vous devez disposer de :

- un mod√®le pr√©-entra√Æn√© √† affiner
- un jeu de donn√©es √† utiliser pour le fine-tuning
- un environnement d‚Äôentra√Ænement pour ex√©cuter la t√¢che de fine-tuning
- un environnement d‚Äôh√©bergement pour d√©ployer le mod√®le affin√©

## Fine-tuning en pratique

Les ressources suivantes proposent des tutoriels pas √† pas pour vous guider √† travers un exemple concret utilisant un mod√®le s√©lectionn√© avec un jeu de donn√©es choisi. Pour suivre ces tutoriels, vous devez avoir un compte chez le fournisseur concern√©, ainsi que l‚Äôacc√®s au mod√®le et aux jeux de donn√©es correspondants.

| Fournisseur  | Tutoriel                                                                                                                                                                      | Description                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)               | Apprenez √† affiner un `gpt-35-turbo` pour un domaine sp√©cifique (¬´ assistant recettes ¬ª) en pr√©parant les donn√©es d‚Äôentra√Ænement, en lan√ßant la t√¢che de fine-tuning, et en utilisant le mod√®le affin√© pour l‚Äôinf√©rence.                                                                                                                                                                                                           |
| Azure OpenAI | [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | Apprenez √† affiner un mod√®le `gpt-35-turbo-0613` **sur Azure** en suivant les √©tapes de cr√©ation et d‚Äôupload des donn√©es d‚Äôentra√Ænement, d‚Äôex√©cution de la t√¢che de fine-tuning, puis de d√©ploiement et d‚Äôutilisation du nouveau mod√®le.                                                                                                                                                                                        |
| Hugging Face | [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                              | Ce billet de blog vous guide pour affiner un _LLM open source_ (ex : `CodeLlama 7B`) en utilisant la biblioth√®que [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) et [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) avec des [datasets](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) ouverts sur Hugging Face. |
|              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| ü§ó AutoTrain | [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                        | AutoTrain (ou AutoTrain Advanced) est une biblioth√®que Python d√©velopp√©e par Hugging Face qui permet le fine-tuning pour de nombreuses t√¢ches, y compris le fine-tuning de LLM. AutoTrain est une solution sans code et le fine-tuning peut √™tre r√©alis√© dans votre propre cloud, sur Hugging Face Spaces ou localement. Il supporte une interface web, une CLI et l‚Äôentra√Ænement via des fichiers de configuration yaml.                              |
|              |                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                 |

## Exercice

Choisissez un des tutoriels ci-dessus et suivez-le. _Nous pourrions reproduire une version de ces tutoriels dans des Jupyter Notebooks dans ce d√©p√¥t √† titre de r√©f√©rence uniquement. Veuillez utiliser directement les sources originales pour obtenir les versions les plus r√©centes_.

## Excellent travail ! Continuez √† apprendre.

Apr√®s avoir termin√© cette le√ßon, consultez notre [collection d‚Äôapprentissage sur l‚ÄôIA g√©n√©rative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer √† approfondir vos connaissances en IA g√©n√©rative !

F√©licitations !! Vous avez termin√© la derni√®re le√ßon de la s√©rie v2 de ce cours ! Ne vous arr√™tez pas d‚Äôapprendre et de cr√©er. \*\*Consultez la page [RESSOURCES](RESOURCES.md?WT.mc_id=academic-105485-koreyst) pour une liste de suggestions suppl√©mentaires sur ce sujet.

Notre s√©rie v1 de le√ßons a √©galement √©t√© mise √† jour avec plus d‚Äôexercices et de concepts. Prenez donc un moment pour rafra√Æchir vos connaissances ‚Äì et n‚Äôh√©sitez pas √† [partager vos questions et retours](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) pour nous aider √† am√©liorer ces le√ßons pour la communaut√©.

**Avertissement** :  
Ce document a √©t√© traduit √† l‚Äôaide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d‚Äôassurer l‚Äôexactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d‚Äôorigine doit √™tre consid√©r√© comme la source faisant foi. Pour les informations critiques, une traduction professionnelle r√©alis√©e par un humain est recommand√©e. Nous d√©clinons toute responsabilit√© en cas de malentendus ou de mauvaises interpr√©tations r√©sultant de l‚Äôutilisation de cette traduction.