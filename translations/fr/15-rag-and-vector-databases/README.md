<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-07-09T16:01:19+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "fr"
}
-->
# Retrieval Augmented Generation (RAG) et bases de donn√©es vectorielles

[![Retrieval Augmented Generation (RAG) et bases de donn√©es vectorielles](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.fr.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

Dans la le√ßon sur les applications de recherche, nous avons bri√®vement vu comment int√©grer vos propres donn√©es dans les grands mod√®les de langage (LLM). Dans cette le√ßon, nous approfondirons les concepts d‚Äôancrage de vos donn√©es dans votre application LLM, le fonctionnement du processus et les m√©thodes de stockage des donn√©es, incluant √† la fois les embeddings et le texte.

> **Vid√©o √† venir bient√¥t**

## Introduction

Dans cette le√ßon, nous aborderons les points suivants :

- Une introduction √† RAG, ce que c‚Äôest et pourquoi il est utilis√© en IA (intelligence artificielle).

- Comprendre ce que sont les bases de donn√©es vectorielles et en cr√©er une pour notre application.

- Un exemple pratique sur la fa√ßon d‚Äôint√©grer RAG dans une application.

## Objectifs d‚Äôapprentissage

Apr√®s avoir termin√© cette le√ßon, vous serez capable de :

- Expliquer l‚Äôimportance de RAG dans la r√©cup√©ration et le traitement des donn√©es.

- Configurer une application RAG et ancrer vos donn√©es √† un LLM.

- Int√©grer efficacement RAG et les bases de donn√©es vectorielles dans des applications LLM.

## Notre sc√©nario : enrichir nos LLM avec nos propres donn√©es

Pour cette le√ßon, nous souhaitons ajouter nos propres notes dans la startup √©ducative, ce qui permettra au chatbot d‚Äôobtenir plus d‚Äôinformations sur les diff√©rents sujets. En utilisant les notes que nous avons, les apprenants pourront mieux √©tudier et comprendre les diff√©rents th√®mes, facilitant ainsi la r√©vision pour leurs examens. Pour cr√©er notre sc√©nario, nous utiliserons :

- `Azure OpenAI` : le LLM que nous utiliserons pour cr√©er notre chatbot

- `Le√ßon AI for beginners sur les r√©seaux neuronaux` : ce sera la base de donn√©es sur laquelle nous ancrerons notre LLM

- `Azure AI Search` et `Azure Cosmos DB` : base de donn√©es vectorielle pour stocker nos donn√©es et cr√©er un index de recherche

Les utilisateurs pourront cr√©er des quiz d‚Äôentra√Ænement √† partir de leurs notes, des flashcards de r√©vision et les r√©sumer en synth√®ses concises. Pour commencer, voyons ce qu‚Äôest RAG et comment il fonctionne :

## Retrieval Augmented Generation (RAG)

Un chatbot aliment√© par un LLM traite les requ√™tes des utilisateurs pour g√©n√©rer des r√©ponses. Il est con√ßu pour √™tre interactif et dialoguer avec les utilisateurs sur une large vari√©t√© de sujets. Cependant, ses r√©ponses sont limit√©es au contexte fourni et √† ses donn√©es d‚Äôentra√Ænement de base. Par exemple, la connaissance de GPT-4 s‚Äôarr√™te en septembre 2021, ce qui signifie qu‚Äôil ne conna√Æt pas les √©v√©nements survenus apr√®s cette date. De plus, les donn√©es utilis√©es pour entra√Æner les LLM excluent les informations confidentielles telles que les notes personnelles ou le manuel produit d‚Äôune entreprise.

### Comment fonctionnent les RAG (Retrieval Augmented Generation)

![sch√©ma montrant le fonctionnement des RAG](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.fr.png)

Supposons que vous souhaitiez d√©ployer un chatbot qui cr√©e des quiz √† partir de vos notes, vous aurez besoin d‚Äôune connexion √† la base de connaissances. C‚Äôest l√† que RAG intervient. Les RAG fonctionnent de la mani√®re suivante :

- **Base de connaissances :** Avant la r√©cup√©ration, ces documents doivent √™tre ing√©r√©s et pr√©trait√©s, g√©n√©ralement en d√©coupant les documents volumineux en morceaux plus petits, en les transformant en embeddings textuels et en les stockant dans une base de donn√©es.

- **Requ√™te utilisateur :** l‚Äôutilisateur pose une question

- **R√©cup√©ration :** Lorsque l‚Äôutilisateur pose une question, le mod√®le d‚Äôembedding r√©cup√®re les informations pertinentes dans notre base de connaissances pour fournir un contexte suppl√©mentaire qui sera int√©gr√© dans la requ√™te.

- **G√©n√©ration augment√©e :** le LLM am√©liore sa r√©ponse en se basant sur les donn√©es r√©cup√©r√©es. Cela permet √† la r√©ponse g√©n√©r√©e de ne pas seulement s‚Äôappuyer sur les donn√©es pr√©-entra√Æn√©es, mais aussi sur les informations pertinentes issues du contexte ajout√©. Les donn√©es r√©cup√©r√©es servent √† enrichir les r√©ponses du LLM. Le LLM renvoie ensuite une r√©ponse √† la question de l‚Äôutilisateur.

![sch√©ma montrant l‚Äôarchitecture des RAG](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.fr.png)

L‚Äôarchitecture des RAG est mise en ≈ìuvre √† l‚Äôaide de transformers compos√©s de deux parties : un encodeur et un d√©codeur. Par exemple, lorsqu‚Äôun utilisateur pose une question, le texte d‚Äôentr√©e est ¬´ encod√© ¬ª en vecteurs capturant le sens des mots, puis ces vecteurs sont ¬´ d√©cod√©s ¬ª dans notre index de documents et g√©n√®rent un nouveau texte bas√© sur la requ√™te utilisateur. Le LLM utilise un mod√®le encodeur-d√©codeur pour g√©n√©rer la sortie.

Deux approches pour impl√©menter RAG selon l‚Äôarticle propos√© : [Retrieval-Augmented Generation for Knowledge intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sont :

- **_RAG-Sequence_** : utilise les documents r√©cup√©r√©s pour pr√©dire la meilleure r√©ponse possible √† une requ√™te utilisateur

- **RAG-Token** : utilise les documents pour g√©n√©rer le token suivant, puis les r√©cup√®re pour r√©pondre √† la requ√™te de l‚Äôutilisateur

### Pourquoi utiliser les RAG ?¬†

- **Richesse d‚Äôinformation :** garantit que les r√©ponses textuelles sont √† jour et actuelles. Cela am√©liore donc les performances sur des t√¢ches sp√©cifiques √† un domaine en acc√©dant √† la base de connaissances interne.

- R√©duit les fabrications en utilisant des **donn√©es v√©rifiables** dans la base de connaissances pour fournir un contexte aux requ√™tes des utilisateurs.

- C‚Äôest **√©conomique**, car ils sont plus abordables que le fine-tuning d‚Äôun LLM.

## Cr√©ation d‚Äôune base de connaissances

Notre application est bas√©e sur nos donn√©es personnelles, c‚Äôest-√†-dire la le√ßon sur les r√©seaux neuronaux du programme AI For Beginners.

### Bases de donn√©es vectorielles

Une base de donn√©es vectorielle, contrairement aux bases de donn√©es traditionnelles, est une base sp√©cialis√©e con√ßue pour stocker, g√©rer et rechercher des vecteurs encod√©s. Elle stocke des repr√©sentations num√©riques des documents. D√©composer les donn√©es en embeddings num√©riques facilite la compr√©hension et le traitement des donn√©es par notre syst√®me d‚ÄôIA.

Nous stockons nos embeddings dans des bases de donn√©es vectorielles car les LLM ont une limite sur le nombre de tokens qu‚Äôils acceptent en entr√©e. Comme il est impossible de passer l‚Äôint√©gralit√© des embeddings √† un LLM, nous devons les d√©couper en morceaux et, lorsqu‚Äôun utilisateur pose une question, les embeddings les plus proches de la question seront renvoy√©s avec la requ√™te. Le d√©coupage r√©duit √©galement les co√ªts li√©s au nombre de tokens transmis √† un LLM.

Parmi les bases de donn√©es vectorielles populaires, on trouve Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant et DeepLake. Vous pouvez cr√©er un mod√®le Azure Cosmos DB avec Azure CLI en utilisant la commande suivante :

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Du texte aux embeddings

Avant de stocker nos donn√©es, nous devons les convertir en embeddings vectoriels avant de les enregistrer dans la base. Si vous travaillez avec de longs documents ou textes, vous pouvez les d√©couper en fonction des requ√™tes que vous attendez. Le d√©coupage peut se faire au niveau des phrases ou des paragraphes. Comme le d√©coupage tire son sens des mots qui l‚Äôentourent, vous pouvez ajouter un autre contexte √† un morceau, par exemple en ajoutant le titre du document ou en incluant un texte avant ou apr√®s le morceau. Vous pouvez d√©couper les donn√©es comme suit :

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Une fois d√©coup√©s, nous pouvons ensuite encoder notre texte en utilisant diff√©rents mod√®les d‚Äôembeddings. Parmi les mod√®les que vous pouvez utiliser : word2vec, ada-002 d‚ÄôOpenAI, Azure Computer Vision et bien d‚Äôautres. Le choix du mod√®le d√©pendra des langues utilis√©es, du type de contenu encod√© (texte/images/audio), de la taille d‚Äôentr√©e qu‚Äôil peut encoder et de la longueur de l‚Äôembedding produit.

Un exemple d‚Äôembedding de texte utilisant le mod√®le `text-embedding-ada-002` d‚ÄôOpenAI est :
![un embedding du mot chat](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.fr.png)

## Recherche et recherche vectorielle

Lorsqu‚Äôun utilisateur pose une question, le r√©cup√©rateur la transforme en vecteur √† l‚Äôaide de l‚Äôencodeur de requ√™te, puis il recherche dans notre index de documents les vecteurs pertinents li√©s √† la requ√™te. Une fois cela fait, il convertit √† la fois le vecteur d‚Äôentr√©e et les vecteurs des documents en texte et les transmet au LLM.

### R√©cup√©ration

La r√©cup√©ration intervient lorsque le syst√®me tente de trouver rapidement les documents dans l‚Äôindex qui r√©pondent aux crit√®res de recherche. L‚Äôobjectif du r√©cup√©rateur est d‚Äôobtenir des documents qui seront utilis√©s pour fournir un contexte et ancrer le LLM sur vos donn√©es.

Plusieurs m√©thodes existent pour effectuer une recherche dans notre base de donn√©es, telles que :

- **Recherche par mots-cl√©s** - utilis√©e pour les recherches textuelles

- **Recherche s√©mantique** - utilise le sens s√©mantique des mots

- **Recherche vectorielle** - convertit les documents du texte en repr√©sentations vectorielles √† l‚Äôaide de mod√®les d‚Äôembeddings. La r√©cup√©ration se fait en interrogeant les documents dont les vecteurs sont les plus proches de la question de l‚Äôutilisateur.

- **Hybride** - une combinaison de recherche par mots-cl√©s et vectorielle.

Un d√©fi de la r√©cup√©ration survient lorsqu‚Äôil n‚Äôy a pas de r√©ponse similaire √† la requ√™te dans la base, le syst√®me renverra alors la meilleure information disponible. Cependant, vous pouvez utiliser des tactiques comme d√©finir une distance maximale pour la pertinence ou utiliser une recherche hybride combinant mots-cl√©s et recherche vectorielle. Dans cette le√ßon, nous utiliserons la recherche hybride, une combinaison des deux. Nous stockerons nos donn√©es dans un dataframe avec des colonnes contenant les morceaux ainsi que les embeddings.

### Similarit√© vectorielle

Le r√©cup√©rateur cherchera dans la base de connaissances les embeddings proches les uns des autres, le plus proche voisin, car ce sont des textes similaires. Dans le sc√©nario o√π un utilisateur pose une question, celle-ci est d‚Äôabord encod√©e puis mise en correspondance avec des embeddings similaires. La mesure la plus courante pour √©valuer la similarit√© entre vecteurs est la similarit√© cosinus, bas√©e sur l‚Äôangle entre deux vecteurs.

Nous pouvons aussi mesurer la similarit√© avec d‚Äôautres alternatives comme la distance euclidienne, qui est la ligne droite entre les extr√©mit√©s des vecteurs, ou le produit scalaire qui mesure la somme des produits des √©l√©ments correspondants de deux vecteurs.

### Index de recherche

Lors de la r√©cup√©ration, il est n√©cessaire de construire un index de recherche pour notre base de connaissances avant d‚Äôeffectuer la recherche. Un index stockera nos embeddings et pourra rapidement retrouver les morceaux les plus similaires m√™me dans une base volumineuse. Nous pouvons cr√©er notre index localement avec :

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Reclassement

Une fois que vous avez interrog√© la base de donn√©es, vous pourriez avoir besoin de trier les r√©sultats du plus pertinent au moins pertinent. Un LLM de reclassement utilise le Machine Learning pour am√©liorer la pertinence des r√©sultats de recherche en les ordonnant du plus pertinent au moins pertinent. Avec Azure AI Search, le reclassement est effectu√© automatiquement gr√¢ce √† un reranker s√©mantique. Voici un exemple de reclassement utilisant les plus proches voisins :

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Mise en ≈ìuvre compl√®te

La derni√®re √©tape consiste √† int√©grer notre LLM pour obtenir des r√©ponses ancr√©es dans nos donn√©es. Nous pouvons l‚Äôimpl√©menter comme suit :

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## √âvaluation de notre application

### M√©triques d‚Äô√©valuation

- Qualit√© des r√©ponses fournies, en s‚Äôassurant qu‚Äôelles sonnent naturelles, fluides et humaines

- Ancrage des donn√©es : √©valuer si la r√©ponse provient bien des documents fournis

- Pertinence : v√©rifier que la r√©ponse correspond et est li√©e √† la question pos√©e

- Fluidit√© : v√©rifier que la r√©ponse est grammaticalement correcte

## Cas d‚Äôusage pour RAG (Retrieval Augmented Generation) et bases de donn√©es vectorielles

Il existe de nombreux cas d‚Äôusage o√π les appels de fonction peuvent am√©liorer votre application, tels que :

- Questions-R√©ponses : ancrer les donn√©es de votre entreprise dans un chat utilis√© par les employ√©s pour poser des questions.

- Syst√®mes de recommandation : cr√©er un syst√®me qui associe les valeurs les plus similaires, par exemple films, restaurants, etc.

- Services de chatbot : stocker l‚Äôhistorique des conversations et personnaliser les √©changes en fonction des donn√©es utilisateur.

- Recherche d‚Äôimages bas√©e sur les embeddings vectoriels, utile pour la reconnaissance d‚Äôimages et la d√©tection d‚Äôanomalies.

## R√©sum√©

Nous avons couvert les bases fondamentales de RAG, depuis l‚Äôajout de nos donn√©es √† l‚Äôapplication, la requ√™te utilisateur jusqu‚Äô√† la sortie. Pour simplifier la cr√©ation de RAG, vous pouvez utiliser des frameworks tels que Semantic Kernel, Langchain ou Autogen.

## Exercice

Pour poursuivre votre apprentissage de Retrieval Augmented Generation (RAG), vous pouvez construire :

- Une interface front-end pour l‚Äôapplication en utilisant le framework de votre choix

- Utiliser un framework, soit LangChain soit Semantic Kernel, et recr√©er votre application.

F√©licitations pour avoir termin√© la le√ßon üëè.

## L‚Äôapprentissage ne s‚Äôarr√™te pas ici, continuez votre parcours

Apr√®s avoir termin√© cette le√ßon, consultez notre [collection d‚Äôapprentissage sur l‚ÄôIA g√©n√©rative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer √† approfondir vos connaissances en IA g√©n√©rative !

**Avertissement** :  
Ce document a √©t√© traduit √† l‚Äôaide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d‚Äôassurer l‚Äôexactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d‚Äôorigine doit √™tre consid√©r√© comme la source faisant foi. Pour les informations critiques, une traduction professionnelle r√©alis√©e par un humain est recommand√©e. Nous d√©clinons toute responsabilit√© en cas de malentendus ou de mauvaises interpr√©tations r√©sultant de l‚Äôutilisation de cette traduction.