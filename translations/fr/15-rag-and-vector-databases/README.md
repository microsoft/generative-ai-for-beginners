# G√©n√©ration Augment√©e par R√©cup√©ration (RAG) et Bases de Donn√©es Vectorielles

[![G√©n√©ration Augment√©e par R√©cup√©ration (RAG) et Bases de Donn√©es Vectorielles](../../../translated_images/15-lesson-banner.png?WT.ae1ec4b596c9c2b74121dd24c30143380d4789a9ef381276dbbc9fd5d7abc3d5.fr.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

Dans la le√ßon sur les applications de recherche, nous avons bri√®vement appris √† int√©grer vos propres donn√©es dans les mod√®les de langage de grande taille (LLM). Dans cette le√ßon, nous allons approfondir les concepts de base de vos donn√©es dans votre application LLM, la m√©canique du processus et les m√©thodes de stockage des donn√©es, y compris les embeddings et le texte.

> **Vid√©o √† venir bient√¥t**

## Introduction

Dans cette le√ßon, nous aborderons les points suivants :

- Une introduction √† RAG, ce que c'est et pourquoi il est utilis√© en intelligence artificielle (IA).

- Comprendre ce que sont les bases de donn√©es vectorielles et en cr√©er une pour notre application.

- Un exemple pratique sur la fa√ßon d'int√©grer RAG dans une application.

## Objectifs d'apprentissage

Apr√®s avoir termin√© cette le√ßon, vous serez capable de :

- Expliquer l'importance de RAG dans la r√©cup√©ration et le traitement des donn√©es.

- Configurer une application RAG et ancrer vos donn√©es √† un LLM.

- Int√©gration efficace de RAG et des bases de donn√©es vectorielles dans les applications LLM.

## Notre sc√©nario : am√©liorer nos LLMs avec nos propres donn√©es

Pour cette le√ßon, nous voulons ajouter nos propres notes dans la startup √©ducative, ce qui permet au chatbot d'obtenir plus d'informations sur les diff√©rents sujets. En utilisant les notes que nous avons, les apprenants pourront mieux √©tudier et comprendre les diff√©rents sujets, ce qui facilitera la r√©vision pour leurs examens. Pour cr√©er notre sc√©nario, nous utiliserons :

- `Azure OpenAI:` le LLM que nous utiliserons pour cr√©er notre chatbot

- `AI for beginners' lesson on Neural Networks` : ce sera les donn√©es sur lesquelles nous baserons notre LLM

- `Azure AI Search` et `Azure Cosmos DB:` base de donn√©es vectorielle pour stocker nos donn√©es et cr√©er un index de recherche

Les utilisateurs pourront cr√©er des quiz de pratique √† partir de leurs notes, des fiches de r√©vision et les r√©sumer en aper√ßus concis. Pour commencer, regardons ce qu'est RAG et comment cela fonctionne :

## G√©n√©ration Augment√©e par R√©cup√©ration (RAG)

Un chatbot aliment√© par un LLM traite les invites des utilisateurs pour g√©n√©rer des r√©ponses. Il est con√ßu pour √™tre interactif et engage les utilisateurs sur une large gamme de sujets. Cependant, ses r√©ponses sont limit√©es au contexte fourni et √† ses donn√©es de formation de base. Par exemple, la coupure de connaissance de GPT-4 est septembre 2021, ce qui signifie qu'il n'a pas connaissance des √©v√©nements survenus apr√®s cette p√©riode. De plus, les donn√©es utilis√©es pour entra√Æner les LLMs excluent les informations confidentielles telles que des notes personnelles ou le manuel produit d'une entreprise.

### Comment fonctionnent les RAGs (G√©n√©ration Augment√©e par R√©cup√©ration)

![sch√©ma montrant comment fonctionnent les RAGs](../../../translated_images/how-rag-works.png?WT.fde75879826c169b53e16dc0d0d6691172c75b314400f380d40a9f31244eba0e.fr.mc_id=academic-105485-koreyst)

Supposons que vous souhaitiez d√©ployer un chatbot qui cr√©e des quiz √† partir de vos notes, vous aurez besoin d'une connexion √† la base de connaissances. C'est l√† que RAG vient √† la rescousse. Les RAGs fonctionnent comme suit :

- **Base de connaissances :** Avant la r√©cup√©ration, ces documents doivent √™tre ing√©r√©s et pr√©trait√©s, g√©n√©ralement en d√©composant de grands documents en morceaux plus petits, en les transformant en embeddings de texte et en les stockant dans une base de donn√©es.

- **Requ√™te utilisateur :** l'utilisateur pose une question

- **R√©cup√©ration :** Lorsqu'un utilisateur pose une question, le mod√®le d'embedding r√©cup√®re des informations pertinentes de notre base de connaissances pour fournir plus de contexte qui sera incorpor√© dans l'invite.

- **G√©n√©ration augment√©e :** le LLM am√©liore sa r√©ponse en fonction des donn√©es r√©cup√©r√©es. Cela permet √† la r√©ponse g√©n√©r√©e de ne pas seulement se baser sur les donn√©es pr√©-entra√Æn√©es mais aussi sur des informations pertinentes du contexte ajout√©. Les donn√©es r√©cup√©r√©es sont utilis√©es pour augmenter les r√©ponses du LLM. Le LLM retourne ensuite une r√©ponse √† la question de l'utilisateur.

![sch√©ma montrant l'architecture des RAGs](../../../translated_images/encoder-decode.png?WT.80c3c9669a10e85d1f7e9dc7f7f0d416a71e16d2f8a6da93267e55cbfbddbf9f.fr.mc_id=academic-105485-koreyst)

L'architecture des RAGs est mise en ≈ìuvre en utilisant des transformers compos√©s de deux parties : un encodeur et un d√©codeur. Par exemple, lorsqu'un utilisateur pose une question, le texte d'entr√©e est 'encod√©' en vecteurs capturant le sens des mots et les vecteurs sont 'd√©cod√©s' dans notre index de documents et g√©n√®rent un nouveau texte bas√© sur la requ√™te utilisateur. Le LLM utilise √† la fois un mod√®le encodeur-d√©codeur pour g√©n√©rer la sortie.

Deux approches lors de l'impl√©mentation de RAG selon le document propos√© : [G√©n√©ration Augment√©e par R√©cup√©ration pour les t√¢ches NLP intensives en connaissances](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sont :

- **_RAG-Sequence_** en utilisant des documents r√©cup√©r√©s pour pr√©dire la meilleure r√©ponse possible √† une requ√™te utilisateur

- **RAG-Token** en utilisant des documents pour g√©n√©rer le prochain token, puis les r√©cup√©rer pour r√©pondre √† la requ√™te de l'utilisateur

### Pourquoi utiliseriez-vous les RAGs ?¬†

- **Richesse de l'information :** garantit que les r√©ponses textuelles sont √† jour et actuelles. Il am√©liore donc la performance sur les t√¢ches sp√©cifiques au domaine en acc√©dant √† la base de connaissances interne.

- R√©duit la fabrication en utilisant des **donn√©es v√©rifiables** dans la base de connaissances pour fournir un contexte aux requ√™tes des utilisateurs.

- Il est **rentable** car ils sont plus √©conomiques par rapport √† l'ajustement fin d'un LLM.

## Cr√©ation d'une base de connaissances

Notre application est bas√©e sur nos donn√©es personnelles, c'est-√†-dire, la le√ßon sur les r√©seaux neuronaux du curriculum AI For Beginners.

### Bases de donn√©es vectorielles

Une base de donn√©es vectorielle, contrairement aux bases de donn√©es traditionnelles, est une base de donn√©es sp√©cialis√©e con√ßue pour stocker, g√©rer et rechercher des vecteurs int√©gr√©s. Elle stocke des repr√©sentations num√©riques de documents. D√©composer les donn√©es en embeddings num√©riques facilite la compr√©hension et le traitement des donn√©es par notre syst√®me d'IA.

Nous stockons nos embeddings dans des bases de donn√©es vectorielles car les LLMs ont une limite du nombre de tokens qu'ils acceptent en entr√©e. Comme vous ne pouvez pas passer l'int√©gralit√© des embeddings √† un LLM, nous devrons les d√©composer en morceaux et lorsqu'un utilisateur pose une question, les embeddings les plus proches de la question seront retourn√©s avec l'invite. Le d√©coupage r√©duit √©galement les co√ªts sur le nombre de tokens pass√©s √† travers un LLM.

Parmi les bases de donn√©es vectorielles populaires, on trouve Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant et DeepLake. Vous pouvez cr√©er un mod√®le Azure Cosmos DB en utilisant Azure CLI avec la commande suivante :

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Du texte aux embeddings

Avant de stocker nos donn√©es, nous devrons les convertir en embeddings vectoriels avant qu'elles ne soient stock√©es dans la base de donn√©es. Si vous travaillez avec de grands documents ou de longs textes, vous pouvez les d√©couper en fonction des requ√™tes que vous attendez. Le d√©coupage peut se faire au niveau de la phrase ou au niveau du paragraphe. Comme le d√©coupage d√©rive le sens des mots autour d'eux, vous pouvez ajouter un autre contexte √† un morceau, par exemple, en ajoutant le titre du document ou en incluant du texte avant ou apr√®s le morceau. Vous pouvez d√©couper les donn√©es comme suit :

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Une fois d√©coup√©, nous pouvons alors int√©grer notre texte en utilisant diff√©rents mod√®les d'embedding. Certains mod√®les que vous pouvez utiliser incluent : word2vec, ada-002 par OpenAI, Azure Computer Vision et bien d'autres. Le choix du mod√®le √† utiliser d√©pendra des langues que vous utilisez, du type de contenu encod√© (texte/images/audio), de la taille de l'entr√©e qu'il peut encoder et de la longueur de la sortie d'embedding.

Un exemple de texte int√©gr√© en utilisant le mod√®le `text-embedding-ada-002` d'OpenAI est :
![un embedding du mot chat](../../../translated_images/cat.png?WT.6f67a41409b2174c6f543273f4a9f8c38b227112a12831da3070e52f13e03818.fr.mc_id=academic-105485-koreyst)

## R√©cup√©ration et recherche vectorielle

Lorsqu'un utilisateur pose une question, le r√©cup√©rateur la transforme en un vecteur en utilisant l'encodeur de requ√™te, il recherche ensuite dans notre index de recherche de documents les vecteurs pertinents dans le document qui sont li√©s √† l'entr√©e. Une fois termin√©, il convertit √† la fois le vecteur d'entr√©e et les vecteurs de documents en texte et les passe √† travers le LLM.

### R√©cup√©ration

La r√©cup√©ration se produit lorsque le syst√®me essaie de trouver rapidement les documents de l'index qui satisfont les crit√®res de recherche. L'objectif du r√©cup√©rateur est d'obtenir des documents qui seront utilis√©s pour fournir un contexte et ancrer le LLM sur vos donn√©es.

Il existe plusieurs fa√ßons de r√©aliser une recherche dans notre base de donn√©es, telles que :

- **Recherche par mot-cl√©** - utilis√©e pour les recherches textuelles

- **Recherche s√©mantique** - utilise le sens s√©mantique des mots

- **Recherche vectorielle** - convertit les documents de texte en repr√©sentations vectorielles en utilisant des mod√®les d'embedding. La r√©cup√©ration sera effectu√©e en interrogeant les documents dont les repr√©sentations vectorielles sont les plus proches de la question de l'utilisateur.

- **Hybride** - une combinaison de recherche par mot-cl√© et de recherche vectorielle.

Un d√©fi avec la r√©cup√©ration survient lorsqu'il n'y a pas de r√©ponse similaire √† la requ√™te dans la base de donn√©es, le syst√®me renverra alors les meilleures informations qu'il peut obtenir, cependant, vous pouvez utiliser des tactiques telles que d√©finir la distance maximale pour la pertinence ou utiliser une recherche hybride qui combine √† la fois les mots-cl√©s et la recherche vectorielle. Dans cette le√ßon, nous utiliserons la recherche hybride, une combinaison de recherche vectorielle et par mot-cl√©. Nous stockerons nos donn√©es dans un dataframe avec des colonnes contenant les morceaux ainsi que les embeddings.

### Similarit√© vectorielle

Le r√©cup√©rateur recherchera dans la base de connaissances les embeddings qui sont proches les uns des autres, le voisin le plus proche, car ce sont des textes qui sont similaires. Dans le sc√©nario o√π un utilisateur pose une requ√™te, elle est d'abord int√©gr√©e puis associ√©e √† des embeddings similaires. La mesure courante utilis√©e pour trouver √† quel point diff√©rents vecteurs sont similaires est la similarit√© cosinus, qui est bas√©e sur l'angle entre deux vecteurs.

Nous pouvons mesurer la similarit√© en utilisant d'autres alternatives que nous pouvons utiliser, telles que la distance euclidienne qui est la ligne droite entre les points d'extr√©mit√© des vecteurs et le produit scalaire qui mesure la somme des produits des √©l√©ments correspondants de deux vecteurs.

### Index de recherche

Lors de la r√©cup√©ration, nous devrons construire un index de recherche pour notre base de connaissances avant d'effectuer une recherche. Un index stockera nos embeddings et pourra rapidement r√©cup√©rer les morceaux les plus similaires, m√™me dans une grande base de donn√©es. Nous pouvons cr√©er notre index localement en utilisant :

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Reclassement

Une fois que vous avez interrog√© la base de donn√©es, vous pourriez avoir besoin de trier les r√©sultats des plus pertinents. Un LLM de reclassement utilise l'apprentissage automatique pour am√©liorer la pertinence des r√©sultats de recherche en les ordonnant des plus pertinents. En utilisant Azure AI Search, le reclassement est effectu√© automatiquement pour vous en utilisant un reclassement s√©mantique. Un exemple de fonctionnement du reclassement en utilisant les voisins les plus proches :

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Tout rassembler

La derni√®re √©tape consiste √† ajouter notre LLM dans le m√©lange pour pouvoir obtenir des r√©ponses qui sont ancr√©es sur nos donn√©es. Nous pouvons l'impl√©menter comme suit :

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## √âvaluation de notre application

### M√©triques d'√©valuation

- Qualit√© des r√©ponses fournies en veillant √† ce qu'elles sonnent naturelles, fluides et humaines

- Enracinement des donn√©es : √©valuation si la r√©ponse provient des documents fournis

- Pertinence : √©valuation si la r√©ponse correspond et est li√©e √† la question pos√©e

- Fluidit√© - si la r√©ponse a un sens grammatical

## Cas d'utilisation pour l'utilisation de RAG (G√©n√©ration Augment√©e par R√©cup√©ration) et des bases de donn√©es vectorielles

Il existe de nombreux cas d'utilisation diff√©rents o√π les appels de fonction peuvent am√©liorer votre application, tels que :

- Questions et r√©ponses : ancrer les donn√©es de votre entreprise √† un chat qui peut √™tre utilis√© par les employ√©s pour poser des questions.

- Syst√®mes de recommandation : o√π vous pouvez cr√©er un syst√®me qui correspond aux valeurs les plus similaires, par exemple, films, restaurants et bien plus encore.

- Services de chatbot : vous pouvez stocker l'historique des conversations et personnaliser la conversation en fonction des donn√©es de l'utilisateur.

- Recherche d'images bas√©e sur les embeddings vectoriels, utile lors de la reconnaissance d'images et de la d√©tection d'anomalies.

## R√©sum√©

Nous avons couvert les domaines fondamentaux de RAG, de l'ajout de nos donn√©es √† l'application, la requ√™te utilisateur et la sortie. Pour simplifier la cr√©ation de RAG, vous pouvez utiliser des frameworks tels que Semanti Kernel, Langchain ou Autogen.

## Devoir

Pour poursuivre votre apprentissage de la G√©n√©ration Augment√©e par R√©cup√©ration (RAG), vous pouvez construire :

- Construire un front-end pour l'application en utilisant le framework de votre choix

- Utiliser un framework, soit LangChain ou Semantic Kernel, et recr√©er votre application.

F√©licitations pour avoir termin√© la le√ßon üëè.

## L'apprentissage ne s'arr√™te pas ici, continuez le voyage

Apr√®s avoir termin√© cette le√ßon, consultez notre [collection d'apprentissage sur l'IA g√©n√©rative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer √† am√©liorer vos connaissances en IA g√©n√©rative !

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'intelligence artificielle. Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations cruciales, il est recommand√© de faire appel √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des mauvaises interpr√©tations r√©sultant de l'utilisation de cette traduction.