<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b4b0266fbadbba7ded891b6485adc66d",
  "translation_date": "2025-10-17T22:39:35+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "fr"
}
-->
# G√©n√©ration augment√©e par r√©cup√©ration (RAG) et bases de donn√©es vectorielles

[![G√©n√©ration augment√©e par r√©cup√©ration (RAG) et bases de donn√©es vectorielles](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.fr.png)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

Dans la le√ßon sur les applications de recherche, nous avons bri√®vement appris √† int√©grer vos propres donn√©es dans les mod√®les de langage √©tendu (LLMs). Dans cette le√ßon, nous approfondirons les concepts de l'ancrage de vos donn√©es dans votre application LLM, les m√©canismes du processus et les m√©thodes de stockage des donn√©es, y compris les embeddings et le texte.

> **Vid√©o √† venir bient√¥t**

## Introduction

Dans cette le√ßon, nous aborderons les points suivants :

- Une introduction √† RAG, ce que c'est et pourquoi il est utilis√© en intelligence artificielle (IA).

- Comprendre ce que sont les bases de donn√©es vectorielles et en cr√©er une pour notre application.

- Un exemple pratique sur la mani√®re d'int√©grer RAG dans une application.

## Objectifs d'apprentissage

Apr√®s avoir termin√© cette le√ßon, vous serez capable de :

- Expliquer l'importance de RAG dans la r√©cup√©ration et le traitement des donn√©es.

- Configurer une application RAG et ancrer vos donn√©es √† un LLM.

- Int√©grer efficacement RAG et les bases de donn√©es vectorielles dans les applications LLM.

## Notre sc√©nario : am√©liorer nos LLMs avec nos propres donn√©es

Pour cette le√ßon, nous souhaitons ajouter nos propres notes dans la startup √©ducative, ce qui permettra au chatbot d'obtenir plus d'informations sur diff√©rents sujets. En utilisant les notes que nous avons, les apprenants pourront mieux √©tudier et comprendre les diff√©rents sujets, ce qui facilitera leurs r√©visions pour les examens. Pour cr√©er notre sc√©nario, nous utiliserons :

- `Azure OpenAI` : le LLM que nous utiliserons pour cr√©er notre chatbot.

- `Le√ßon pour d√©butants en IA sur les r√©seaux neuronaux` : ce sera les donn√©es sur lesquelles nous ancrerons notre LLM.

- `Azure AI Search` et `Azure Cosmos DB` : base de donn√©es vectorielle pour stocker nos donn√©es et cr√©er un index de recherche.

Les utilisateurs pourront cr√©er des quiz pratiques √† partir de leurs notes, des fiches de r√©vision et les r√©sumer en aper√ßus concis. Pour commencer, examinons ce qu'est RAG et comment cela fonctionne :

## G√©n√©ration augment√©e par r√©cup√©ration (RAG)

Un chatbot aliment√© par un LLM traite les requ√™tes des utilisateurs pour g√©n√©rer des r√©ponses. Il est con√ßu pour √™tre interactif et engage les utilisateurs sur une large gamme de sujets. Cependant, ses r√©ponses sont limit√©es au contexte fourni et √† ses donn√©es d'entra√Ænement de base. Par exemple, la date limite de connaissance de GPT-4 est septembre 2021, ce qui signifie qu'il ne conna√Æt pas les √©v√©nements survenus apr√®s cette p√©riode. De plus, les donn√©es utilis√©es pour entra√Æner les LLMs excluent les informations confidentielles telles que les notes personnelles ou le manuel de produit d'une entreprise.

### Comment fonctionnent les RAGs (G√©n√©ration augment√©e par r√©cup√©ration)

![sch√©ma montrant comment fonctionnent les RAGs](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.fr.png)

Supposons que vous souhaitiez d√©ployer un chatbot qui cr√©e des quiz √† partir de vos notes, vous aurez besoin d'une connexion √† la base de connaissances. C'est l√† que RAG intervient. Les RAGs fonctionnent comme suit :

- **Base de connaissances :** Avant la r√©cup√©ration, ces documents doivent √™tre ing√©r√©s et pr√©trait√©s, g√©n√©ralement en divisant de grands documents en petits morceaux, en les transformant en embeddings textuels et en les stockant dans une base de donn√©es.

- **Requ√™te utilisateur :** l'utilisateur pose une question.

- **R√©cup√©ration :** Lorsqu'un utilisateur pose une question, le mod√®le d'embedding r√©cup√®re des informations pertinentes de notre base de connaissances pour fournir plus de contexte qui sera incorpor√© dans la requ√™te.

- **G√©n√©ration augment√©e :** le LLM am√©liore sa r√©ponse en fonction des donn√©es r√©cup√©r√©es. Cela permet √† la r√©ponse g√©n√©r√©e de ne pas seulement se baser sur les donn√©es pr√©-entra√Æn√©es, mais aussi sur des informations pertinentes issues du contexte ajout√©. Les donn√©es r√©cup√©r√©es sont utilis√©es pour enrichir les r√©ponses du LLM. Le LLM renvoie ensuite une r√©ponse √† la question de l'utilisateur.

![sch√©ma montrant l'architecture des RAGs](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.fr.png)

L'architecture des RAGs est mise en ≈ìuvre √† l'aide de transformateurs comprenant deux parties : un encodeur et un d√©codeur. Par exemple, lorsqu'un utilisateur pose une question, le texte d'entr√©e est "encod√©" en vecteurs capturant le sens des mots, et les vecteurs sont "d√©cod√©s" dans notre index de documents pour g√©n√©rer un nouveau texte bas√© sur la requ√™te de l'utilisateur. Le LLM utilise √† la fois un mod√®le encodeur-d√©codeur pour g√©n√©rer la sortie.

Deux approches lors de la mise en ≈ìuvre de RAG, selon l'article propos√© : [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sont :

- **_RAG-Sequence_** utilisant les documents r√©cup√©r√©s pour pr√©dire la meilleure r√©ponse possible √† une requ√™te utilisateur.

- **RAG-Token** utilisant les documents pour g√©n√©rer le prochain token, puis les r√©cup√©rer pour r√©pondre √† la requ√™te de l'utilisateur.

### Pourquoi utiliser les RAGs ?

- **Richesse d'information :** garantit que les r√©ponses textuelles sont √† jour et actuelles. Cela am√©liore donc les performances sur les t√¢ches sp√©cifiques au domaine en acc√©dant √† la base de connaissances interne.

- R√©duit les fabrications en utilisant des **donn√©es v√©rifiables** dans la base de connaissances pour fournir un contexte aux requ√™tes des utilisateurs.

- C'est **√©conomique** car ils sont plus rentables par rapport √† l'affinement d'un LLM.

## Cr√©ation d'une base de connaissances

Notre application est bas√©e sur nos donn√©es personnelles, c'est-√†-dire la le√ßon sur les r√©seaux neuronaux du programme AI For Beginners.

### Bases de donn√©es vectorielles

Une base de donn√©es vectorielle, contrairement aux bases de donn√©es traditionnelles, est une base de donn√©es sp√©cialis√©e con√ßue pour stocker, g√©rer et rechercher des vecteurs int√©gr√©s. Elle stocke des repr√©sentations num√©riques de documents. La d√©composition des donn√©es en embeddings num√©riques facilite la compr√©hension et le traitement des donn√©es par notre syst√®me d'IA.

Nous stockons nos embeddings dans des bases de donn√©es vectorielles car les LLMs ont une limite sur le nombre de tokens qu'ils acceptent en entr√©e. Comme vous ne pouvez pas transmettre tous les embeddings √† un LLM, nous devrons les diviser en morceaux et, lorsqu'un utilisateur pose une question, les embeddings les plus proches de la question seront retourn√©s avec la requ√™te. Le d√©coupage r√©duit √©galement les co√ªts li√©s au nombre de tokens transmis √† un LLM.

Parmi les bases de donn√©es vectorielles populaires, on trouve Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant et DeepLake. Vous pouvez cr√©er un mod√®le Azure Cosmos DB en utilisant Azure CLI avec la commande suivante :

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Du texte aux embeddings

Avant de stocker nos donn√©es, nous devrons les convertir en embeddings vectoriels avant qu'elles ne soient stock√©es dans la base de donn√©es. Si vous travaillez avec de grands documents ou de longs textes, vous pouvez les d√©couper en fonction des requ√™tes que vous attendez. Le d√©coupage peut √™tre effectu√© au niveau des phrases ou des paragraphes. Comme le d√©coupage d√©rive des significations des mots environnants, vous pouvez ajouter un autre contexte √† un morceau, par exemple, en ajoutant le titre du document ou en incluant du texte avant ou apr√®s le morceau. Vous pouvez d√©couper les donn√©es comme suit :

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Une fois d√©coup√©, nous pouvons ensuite int√©grer notre texte en utilisant diff√©rents mod√®les d'embedding. Parmi les mod√®les que vous pouvez utiliser, on trouve : word2vec, ada-002 d'OpenAI, Azure Computer Vision et bien d'autres. Le choix du mod√®le d√©pendra des langues que vous utilisez, du type de contenu encod√© (texte/images/audio), de la taille de l'entr√©e qu'il peut encoder et de la longueur de la sortie d'embedding.

Un exemple de texte int√©gr√© en utilisant le mod√®le `text-embedding-ada-002` d'OpenAI est :
![un embedding du mot chat](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.fr.png)

## R√©cup√©ration et recherche vectorielle

Lorsqu'un utilisateur pose une question, le r√©cup√©rateur la transforme en vecteur √† l'aide de l'encodeur de requ√™tes, puis recherche dans notre index de recherche de documents les vecteurs pertinents dans le document qui sont li√©s √† l'entr√©e. Une fois termin√©, il convertit √† la fois le vecteur d'entr√©e et les vecteurs de documents en texte et les transmet au LLM.

### R√©cup√©ration

La r√©cup√©ration se produit lorsque le syst√®me essaie de trouver rapidement les documents de l'index qui r√©pondent aux crit√®res de recherche. L'objectif du r√©cup√©rateur est d'obtenir des documents qui seront utilis√©s pour fournir un contexte et ancrer le LLM sur vos donn√©es.

Il existe plusieurs fa√ßons d'effectuer une recherche dans notre base de donn√©es, telles que :

- **Recherche par mots-cl√©s** - utilis√©e pour les recherches textuelles.

- **Recherche s√©mantique** - utilise le sens s√©mantique des mots.

- **Recherche vectorielle** - convertit les documents de texte en repr√©sentations vectorielles √† l'aide de mod√®les d'embedding. La r√©cup√©ration sera effectu√©e en interrogeant les documents dont les repr√©sentations vectorielles sont les plus proches de la question de l'utilisateur.

- **Hybride** - une combinaison de recherche par mots-cl√©s et vectorielle.

Un d√©fi avec la r√©cup√©ration survient lorsqu'il n'y a pas de r√©ponse similaire √† la requ√™te dans la base de donn√©es. Le syst√®me renverra alors les meilleures informations qu'il peut obtenir. Cependant, vous pouvez utiliser des tactiques comme d√©finir la distance maximale pour la pertinence ou utiliser une recherche hybride qui combine √† la fois les mots-cl√©s et la recherche vectorielle. Dans cette le√ßon, nous utiliserons la recherche hybride, une combinaison de recherche vectorielle et par mots-cl√©s. Nous stockerons nos donn√©es dans un dataframe avec des colonnes contenant les morceaux ainsi que les embeddings.

### Similarit√© vectorielle

Le r√©cup√©rateur recherchera dans la base de connaissances des embeddings qui sont proches les uns des autres, les voisins les plus proches, car ce sont des textes similaires. Dans le cas o√π un utilisateur pose une requ√™te, elle est d'abord int√©gr√©e, puis mise en correspondance avec des embeddings similaires. La mesure courante utilis√©e pour trouver √† quel point diff√©rents vecteurs sont similaires est la similarit√© cosinus, qui est bas√©e sur l'angle entre deux vecteurs.

Nous pouvons mesurer la similarit√© en utilisant d'autres alternatives comme la distance euclidienne, qui est la ligne droite entre les points d'extr√©mit√© des vecteurs, et le produit scalaire, qui mesure la somme des produits des √©l√©ments correspondants de deux vecteurs.

### Index de recherche

Lors de la r√©cup√©ration, nous devrons construire un index de recherche pour notre base de connaissances avant d'effectuer une recherche. Un index stockera nos embeddings et pourra rapidement r√©cup√©rer les morceaux les plus similaires, m√™me dans une grande base de donn√©es. Nous pouvons cr√©er notre index localement en utilisant :

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Reclassement

Une fois que vous avez interrog√© la base de donn√©es, vous pourriez avoir besoin de trier les r√©sultats par pertinence. Un LLM de reclassement utilise l'apprentissage automatique pour am√©liorer la pertinence des r√©sultats de recherche en les classant par ordre de pertinence. Avec Azure AI Search, le reclassement est effectu√© automatiquement pour vous √† l'aide d'un reclassement s√©mantique. Un exemple de fonctionnement du reclassement utilisant les voisins les plus proches :

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Tout rassembler

La derni√®re √©tape consiste √† ajouter notre LLM dans le processus afin de pouvoir obtenir des r√©ponses bas√©es sur nos donn√©es. Nous pouvons l'impl√©menter comme suit :

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## √âvaluation de notre application

### M√©triques d'√©valuation

- Qualit√© des r√©ponses fournies, en s'assurant qu'elles semblent naturelles, fluides et humaines.

- Ancrage des donn√©es : √©valuation de la pertinence des r√©ponses provenant des documents fournis.

- Pertinence : √©valuation de la correspondance et de la relation des r√©ponses avec la question pos√©e.

- Fluidit√© : v√©rification que la r√©ponse a du sens grammaticalement.

## Cas d'utilisation de la g√©n√©ration augment√©e par r√©cup√©ration (RAG) et des bases de donn√©es vectorielles

Il existe de nombreux cas d'utilisation o√π les appels de fonction peuvent am√©liorer votre application, tels que :

- Questions et r√©ponses : ancrer les donn√©es de votre entreprise √† un chat qui peut √™tre utilis√© par les employ√©s pour poser des questions.

- Syst√®mes de recommandation : o√π vous pouvez cr√©er un syst√®me qui correspond aux valeurs les plus similaires, par exemple, films, restaurants et bien plus.

- Services de chatbot : vous pouvez stocker l'historique des conversations et personnaliser la conversation en fonction des donn√©es utilisateur.

- Recherche d'images bas√©e sur des embeddings vectoriels, utile pour la reconnaissance d'images et la d√©tection d'anomalies.

## R√©sum√©

Nous avons couvert les aspects fondamentaux de RAG, de l'ajout de nos donn√©es √† l'application, √† la requ√™te utilisateur et √† la sortie. Pour simplifier la cr√©ation de RAG, vous pouvez utiliser des frameworks tels que Semantic Kernel, Langchain ou Autogen.

## Devoir

Pour continuer votre apprentissage sur la g√©n√©ration augment√©e par r√©cup√©ration (RAG), vous pouvez :

- Cr√©er une interface utilisateur pour l'application en utilisant le framework de votre choix.

- Utiliser un framework, soit LangChain soit Semantic Kernel, et recr√©er votre application.

F√©licitations pour avoir termin√© la le√ßon üëè.

## L'apprentissage ne s'arr√™te pas ici, continuez votre parcours

Apr√®s avoir termin√© cette le√ßon, consultez notre [collection d'apprentissage sur l'IA g√©n√©rative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer √† approfondir vos connaissances sur l'IA g√©n√©rative !

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.