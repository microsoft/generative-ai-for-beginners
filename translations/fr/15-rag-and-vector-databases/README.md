<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-05-20T00:58:59+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "fr"
}
-->
# R√©cup√©ration Augment√©e par G√©n√©ration (RAG) et Bases de Donn√©es Vectorielles

[![R√©cup√©ration Augment√©e par G√©n√©ration (RAG) et Bases de Donn√©es Vectorielles](../../../translated_images/15-lesson-banner.799d0cd2229970edb365f6667a4c7b3a0f526eb8698baa7d2e05c3bd49a5d83f.fr.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

Dans la le√ßon sur les applications de recherche, nous avons bri√®vement appris comment int√©grer vos propres donn√©es dans les Mod√®les de Langage de Grande Taille (LLM). Dans cette le√ßon, nous allons approfondir les concepts de la fondation de vos donn√©es dans votre application LLM, les m√©canismes du processus et les m√©thodes de stockage des donn√©es, y compris les embeddings et le texte.

> **Vid√©o √† venir**

## Introduction

Dans cette le√ßon, nous aborderons les points suivants :

- Une introduction √† RAG, ce que c'est et pourquoi il est utilis√© en intelligence artificielle (IA).

- Comprendre ce que sont les bases de donn√©es vectorielles et en cr√©er une pour notre application.

- Un exemple pratique sur comment int√©grer RAG dans une application.

## Objectifs d'apprentissage

Apr√®s avoir termin√© cette le√ßon, vous serez capable de :

- Expliquer l'importance de RAG dans la r√©cup√©ration et le traitement des donn√©es.

- Configurer une application RAG et ancrer vos donn√©es √† un LLM.

- Int√©gration efficace de RAG et des Bases de Donn√©es Vectorielles dans les applications LLM.

## Notre Sc√©nario : am√©liorer nos LLMs avec nos propres donn√©es

Pour cette le√ßon, nous voulons ajouter nos propres notes dans la startup √©ducative, ce qui permet au chatbot d'obtenir plus d'informations sur les diff√©rents sujets. En utilisant les notes que nous avons, les apprenants pourront mieux √©tudier et comprendre les diff√©rents sujets, facilitant ainsi la r√©vision pour leurs examens. Pour cr√©er notre sc√©nario, nous utiliserons :

- `Azure OpenAI:` le LLM que nous utiliserons pour cr√©er notre chatbot

- `AI for beginners' lesson on Neural Networks` : ce sera les donn√©es sur lesquelles nous baserons notre LLM

- `Azure AI Search` et `Azure Cosmos DB:` base de donn√©es vectorielle pour stocker nos donn√©es et cr√©er un index de recherche

Les utilisateurs pourront cr√©er des quiz pratiques √† partir de leurs notes, des cartes de r√©vision et les r√©sumer en aper√ßus concis. Pour commencer, voyons ce qu'est RAG et comment cela fonctionne :

## R√©cup√©ration Augment√©e par G√©n√©ration (RAG)

Un chatbot aliment√© par un LLM traite les invites des utilisateurs pour g√©n√©rer des r√©ponses. Il est con√ßu pour √™tre interactif et engage les utilisateurs sur un large √©ventail de sujets. Cependant, ses r√©ponses sont limit√©es au contexte fourni et √† ses donn√©es de formation de base. Par exemple, la limite de connaissance de GPT-4 est septembre 2021, ce qui signifie qu'il manque de connaissances sur les √©v√©nements survenus apr√®s cette p√©riode. De plus, les donn√©es utilis√©es pour former les LLMs excluent les informations confidentielles telles que des notes personnelles ou le manuel produit d'une entreprise.

### Comment fonctionnent les RAGs (R√©cup√©ration Augment√©e par G√©n√©ration)

![sch√©ma montrant comment fonctionnent les RAGs](../../../translated_images/how-rag-works.d87a7ed9c30f43126bb9e8e259be5d66e16cd1fef65374e6914746ba9bfb0b2f.fr.png)

Supposons que vous souhaitiez d√©ployer un chatbot qui cr√©e des quiz √† partir de vos notes, vous aurez besoin d'une connexion √† la base de connaissances. C'est l√† que RAG vient √† la rescousse. Les RAGs fonctionnent comme suit :

- **Base de connaissances :** Avant la r√©cup√©ration, ces documents doivent √™tre ing√©r√©s et pr√©trait√©s, g√©n√©ralement en d√©coupant de grands documents en morceaux plus petits, en les transformant en embeddings textuels et en les stockant dans une base de donn√©es.

- **Requ√™te utilisateur :** l'utilisateur pose une question

- **R√©cup√©ration :** Lorsqu'un utilisateur pose une question, le mod√®le d'embedding r√©cup√®re des informations pertinentes de notre base de connaissances pour fournir plus de contexte qui sera int√©gr√© dans l'invite.

- **G√©n√©ration augment√©e :** le LLM am√©liore sa r√©ponse en fonction des donn√©es r√©cup√©r√©es. Cela permet √† la r√©ponse g√©n√©r√©e de ne pas seulement √™tre bas√©e sur des donn√©es pr√©-entra√Æn√©es mais aussi sur des informations pertinentes du contexte ajout√©. Les donn√©es r√©cup√©r√©es sont utilis√©es pour augmenter les r√©ponses du LLM. Le LLM retourne ensuite une r√©ponse √† la question de l'utilisateur.

![sch√©ma montrant l'architecture des RAGs](../../../translated_images/encoder-decode.75eebc7093ccefec17568eebc80d3d0b831ecf2ea204566377a04c77a5a57ebb.fr.png)

L'architecture des RAGs est mise en ≈ìuvre √† l'aide de transformateurs compos√©s de deux parties : un encodeur et un d√©codeur. Par exemple, lorsqu'un utilisateur pose une question, le texte d'entr√©e est "encod√©" en vecteurs capturant le sens des mots et les vecteurs sont "d√©cod√©s" dans notre index de documents et g√©n√®rent un nouveau texte bas√© sur la requ√™te de l'utilisateur. Le LLM utilise √† la fois un mod√®le encodeur-d√©codeur pour g√©n√©rer la sortie.

Deux approches lors de l'impl√©mentation de RAG selon le document propos√© : [R√©cup√©ration-Augment√©e par G√©n√©ration pour les t√¢ches NLP intensives en connaissances](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sont :

- **_RAG-Sequence_** utilisant des documents r√©cup√©r√©s pour pr√©dire la meilleure r√©ponse possible √† une requ√™te utilisateur

- **RAG-Token** utilisant des documents pour g√©n√©rer le prochain jeton, puis les r√©cup√©rer pour r√©pondre √† la requ√™te de l'utilisateur

### Pourquoi utiliseriez-vous les RAGs ?

- **Richesse de l'information :** assure que les r√©ponses textuelles sont √† jour et actuelles. Il am√©liore donc les performances sur les t√¢ches sp√©cifiques au domaine en acc√©dant √† la base de connaissances interne.

- R√©duit la fabrication en utilisant des **donn√©es v√©rifiables** dans la base de connaissances pour fournir un contexte aux requ√™tes des utilisateurs.

- Il est **rentable** car ils sont plus √©conomiques par rapport au r√©glage fin d'un LLM

## Cr√©ation d'une base de connaissances

Notre application est bas√©e sur nos donn√©es personnelles, c'est-√†-dire la le√ßon sur les r√©seaux neuronaux du programme AI For Beginners.

### Bases de Donn√©es Vectorielles

Une base de donn√©es vectorielle, contrairement aux bases de donn√©es traditionnelles, est une base de donn√©es sp√©cialis√©e con√ßue pour stocker, g√©rer et rechercher des vecteurs int√©gr√©s. Elle stocke des repr√©sentations num√©riques de documents. La d√©composition des donn√©es en embeddings num√©riques facilite la compr√©hension et le traitement des donn√©es par notre syst√®me d'IA.

Nous stockons nos embeddings dans des bases de donn√©es vectorielles car les LLMs ont une limite du nombre de tokens qu'ils acceptent en entr√©e. Comme vous ne pouvez pas transmettre l'ensemble des embeddings √† un LLM, nous devrons les diviser en morceaux et lorsqu'un utilisateur pose une question, les embeddings les plus proches de la question seront retourn√©s avec l'invite. La fragmentation r√©duit √©galement les co√ªts li√©s au nombre de tokens transmis √† travers un LLM.

Quelques bases de donn√©es vectorielles populaires incluent Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant et DeepLake. Vous pouvez cr√©er un mod√®le Azure Cosmos DB en utilisant Azure CLI avec la commande suivante :

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Du texte aux embeddings

Avant de stocker nos donn√©es, nous devrons les convertir en embeddings vectoriels avant de les stocker dans la base de donn√©es. Si vous travaillez avec de grands documents ou de longs textes, vous pouvez les d√©couper en fonction des requ√™tes que vous attendez. La fragmentation peut √™tre effectu√©e au niveau de la phrase ou au niveau du paragraphe. Comme la fragmentation d√©rive des significations des mots qui les entourent, vous pouvez ajouter un autre contexte √† un morceau, par exemple, en ajoutant le titre du document ou en incluant un texte avant ou apr√®s le morceau. Vous pouvez d√©couper les donn√©es comme suit :

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Une fois d√©coup√©, nous pouvons ensuite int√©grer notre texte en utilisant diff√©rents mod√®les d'embedding. Certains mod√®les que vous pouvez utiliser incluent : word2vec, ada-002 par OpenAI, Azure Computer Vision et bien d'autres. Le choix d'un mod√®le √† utiliser d√©pendra des langues que vous utilisez, du type de contenu encod√© (texte/images/audio), de la taille de l'entr√©e qu'il peut encoder et de la longueur de la sortie d'embedding.

Un exemple de texte int√©gr√© utilisant le mod√®le `text-embedding-ada-002` d'OpenAI est :
![un embedding du mot chat](../../../translated_images/cat.3db013cbca4fd5d90438ea7b312ad0364f7686cf79931ab15cd5922151aea53e.fr.png)

## R√©cup√©ration et Recherche Vectorielle

Lorsqu'un utilisateur pose une question, le r√©cup√©rateur la transforme en un vecteur √† l'aide de l'encodeur de requ√™te, il recherche ensuite dans notre index de recherche de documents les vecteurs pertinents dans le document qui sont li√©s √† l'entr√©e. Une fois termin√©, il convertit √† la fois le vecteur d'entr√©e et les vecteurs de documents en texte et les passe √† travers le LLM.

### R√©cup√©ration

La r√©cup√©ration se produit lorsque le syst√®me tente de trouver rapidement les documents de l'index qui satisfont aux crit√®res de recherche. L'objectif du r√©cup√©rateur est d'obtenir des documents qui seront utilis√©s pour fournir un contexte et ancrer le LLM sur vos donn√©es.

Il existe plusieurs fa√ßons d'effectuer une recherche dans notre base de donn√©es, telles que :

- **Recherche par mots-cl√©s** - utilis√©e pour les recherches textuelles

- **Recherche s√©mantique** - utilise le sens s√©mantique des mots

- **Recherche vectorielle** - convertit les documents de texte en repr√©sentations vectorielles √† l'aide de mod√®les d'embedding. La r√©cup√©ration se fera en interrogeant les documents dont les repr√©sentations vectorielles sont les plus proches de la question de l'utilisateur.

- **Hybride** - une combinaison √† la fois de la recherche par mots-cl√©s et de la recherche vectorielle.

Un d√©fi avec la r√©cup√©ration survient lorsqu'il n'y a pas de r√©ponse similaire √† la requ√™te dans la base de donn√©es, le syst√®me renverra alors la meilleure information qu'il peut obtenir, cependant, vous pouvez utiliser des tactiques telles que d√©finir la distance maximale pour la pertinence ou utiliser une recherche hybride qui combine √† la fois les mots-cl√©s et la recherche vectorielle. Dans cette le√ßon, nous utiliserons la recherche hybride, une combinaison √† la fois de la recherche vectorielle et par mots-cl√©s. Nous stockerons nos donn√©es dans un dataframe avec des colonnes contenant les morceaux ainsi que les embeddings.

### Similarit√© Vectorielle

Le r√©cup√©rateur recherchera dans la base de connaissances des embeddings qui sont proches les uns des autres, le voisin le plus proche, car ce sont des textes similaires. Dans le sc√©nario o√π un utilisateur pose une requ√™te, elle est d'abord int√©gr√©e puis appari√©e avec des embeddings similaires. La mesure commune qui est utilis√©e pour trouver √† quel point diff√©rents vecteurs sont similaires est la similarit√© cosinus qui est bas√©e sur l'angle entre deux vecteurs.

Nous pouvons mesurer la similarit√© en utilisant d'autres alternatives telles que la distance Euclidienne qui est la ligne droite entre les points d'extr√©mit√© des vecteurs et le produit scalaire qui mesure la somme des produits des √©l√©ments correspondants de deux vecteurs.

### Index de Recherche

Lors de la r√©cup√©ration, nous devrons construire un index de recherche pour notre base de connaissances avant d'effectuer la recherche. Un index stockera nos embeddings et pourra rapidement r√©cup√©rer les morceaux les plus similaires m√™me dans une grande base de donn√©es. Nous pouvons cr√©er notre index localement en utilisant :

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Reclassement

Une fois que vous avez interrog√© la base de donn√©es, vous pourriez avoir besoin de trier les r√©sultats du plus pertinent. Un LLM de reclassement utilise l'apprentissage automatique pour am√©liorer la pertinence des r√©sultats de recherche en les classant du plus pertinent. En utilisant Azure AI Search, le reclassement est effectu√© automatiquement pour vous √† l'aide d'un reclasseur s√©mantique. Un exemple de fonctionnement du reclassement utilisant les plus proches voisins :

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Tout assembler

La derni√®re √©tape consiste √† ajouter notre LLM dans le m√©lange pour pouvoir obtenir des r√©ponses qui sont fond√©es sur nos donn√©es. Nous pouvons l'impl√©menter comme suit :

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## √âvaluer notre application

### M√©triques d'√©valuation

- Qualit√© des r√©ponses fournies en s'assurant qu'elles semblent naturelles, fluides et humaines

- Ancrage des donn√©es : √©valuer si la r√©ponse provient des documents fournis

- Pertinence : √©valuer si la r√©ponse correspond et est li√©e √† la question pos√©e

- Fluidit√© - si la r√©ponse a du sens grammaticalement

## Cas d'utilisation pour l'utilisation de RAG (R√©cup√©ration Augment√©e par G√©n√©ration) et des bases de donn√©es vectorielles

Il existe de nombreux cas d'utilisation diff√©rents o√π les appels de fonction peuvent am√©liorer votre application, tels que :

- Questions et R√©ponses : ancrer vos donn√©es d'entreprise √† un chat qui peut √™tre utilis√© par les employ√©s pour poser des questions.

- Syst√®mes de Recommandation : o√π vous pouvez cr√©er un syst√®me qui correspond aux valeurs les plus similaires, par exemple, films, restaurants et bien plus.

- Services de Chatbot : vous pouvez stocker l'historique des discussions et personnaliser la conversation en fonction des donn√©es utilisateur.

- Recherche d'images bas√©e sur des embeddings vectoriels, utile lors de la reconnaissance d'images et de la d√©tection d'anomalies.

## R√©sum√©

Nous avons couvert les domaines fondamentaux de RAG de l'ajout de nos donn√©es √† l'application, de la requ√™te utilisateur et de la sortie. Pour simplifier la cr√©ation de RAG, vous pouvez utiliser des frameworks tels que Semanti Kernel, Langchain ou Autogen.

## Devoir

Pour poursuivre votre apprentissage de la R√©cup√©ration Augment√©e par G√©n√©ration (RAG) vous pouvez construire :

- Construire une interface pour l'application en utilisant le framework de votre choix

- Utiliser un framework, soit LangChain ou Semantic Kernel, et recr√©er votre application.

F√©licitations pour avoir termin√© la le√ßon üëè.

## L'apprentissage ne s'arr√™te pas ici, continuez le voyage

Apr√®s avoir termin√© cette le√ßon, consultez notre [collection d'apprentissage sur l'IA G√©n√©rative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer √† approfondir vos connaissances en IA G√©n√©rative !

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction IA [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.