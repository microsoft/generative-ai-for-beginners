<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2210a0466c812d9defc4df2d9a709ff9",
  "translation_date": "2026-01-18T16:40:54+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "fr"
}
-->
# G√©n√©ration Augment√©e par R√©cup√©ration (RAG) et Bases de Donn√©es Vectorielles

[![G√©n√©ration Augment√©e par R√©cup√©ration (RAG) et Bases de Donn√©es Vectorielles](../../../../../translated_images/fr/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

Dans la le√ßon sur les applications de recherche, nous avons bri√®vement appris comment int√©grer vos propres donn√©es dans les grands mod√®les de langage (LLM). Dans cette le√ßon, nous approfondirons les concepts de l'ancrage de vos donn√©es dans votre application LLM, les m√©canismes du processus et les m√©thodes de stockage des donn√©es, y compris les embeddings et le texte.

> **Vid√©o bient√¥t disponible**

## Introduction

Dans cette le√ßon, nous couvrirons les points suivants :

- Une introduction √† RAG, ce que c'est et pourquoi il est utilis√© en IA (intelligence artificielle).

- Comprendre ce que sont les bases de donn√©es vectorielles et en cr√©er une pour notre application.

- Un exemple pratique sur comment int√©grer RAG dans une application.

## Objectifs d'apprentissage

Apr√®s avoir termin√© cette le√ßon, vous serez capable de :

- Expliquer l'importance de RAG dans la r√©cup√©ration et le traitement de donn√©es.

- Configurer une application RAG et ancrer vos donn√©es √† un LLM.

- Int√©grer efficacement RAG et les bases de donn√©es vectorielles dans les applications LLM.

## Notre sc√©nario : enrichir nos LLM avec nos propres donn√©es

Pour cette le√ßon, nous souhaitons ajouter nos propres notes dans la startup √©ducative, ce qui permettra au chatbot d‚Äôobtenir plus d‚Äôinformations sur les diff√©rents sujets. En utilisant les notes que nous avons, les apprenants pourront mieux √©tudier et comprendre les diff√©rents th√®mes, facilitant ainsi la r√©vision pour leurs examens. Pour cr√©er notre sc√©nario, nous utiliserons :

- `Azure OpenAI :` le LLM que nous utiliserons pour cr√©er notre chatbot

- `Le√ßon AI for beginners sur les r√©seaux neuronaux` : ce seront les donn√©es sur lesquelles nous ancrerons notre LLM

- `Azure AI Search` et `Azure Cosmos DB :` base de donn√©es vectorielle pour stocker nos donn√©es et cr√©er un index de recherche

Les utilisateurs pourront cr√©er des quiz d'entra√Ænement √† partir de leurs notes, des fiches de r√©vision et r√©sumer ces informations en synth√®ses concises. Pour commencer, regardons ce qu‚Äôest RAG et comment il fonctionne :

## G√©n√©ration Augment√©e par R√©cup√©ration (RAG)

Un chatbot aliment√© par un LLM traite les requ√™tes des utilisateurs pour g√©n√©rer des r√©ponses. Il est con√ßu pour √™tre interactif et engage des conversations sur un large √©ventail de sujets. Cependant, ses r√©ponses sont limit√©es au contexte fourni et √† ses donn√©es d'entra√Ænement fondamentales. Par exemple, la connaissance de GPT-4 s'arr√™te en septembre 2021, ce qui signifie qu‚Äôil ne conna√Æt pas les √©v√©nements survenus apr√®s cette date. De plus, les donn√©es utilis√©es pour entra√Æner les LLM n'incluent pas les informations confidentielles telles que les notes personnelles ou le manuel produit d'une entreprise.

### Comment fonctionnent les RAGs (G√©n√©ration Augment√©e par R√©cup√©ration)

![dessin montrant comment fonctionnent les RAGs](../../../../../translated_images/fr/how-rag-works.f5d0ff63942bd3a6.webp)

Supposons que vous souhaitez d√©ployer un chatbot qui cr√©e des quiz √† partir de vos notes, vous aurez besoin d‚Äôune connexion √† la base de connaissances. C‚Äôest l√† que RAG intervient. Les RAGs fonctionnent de la mani√®re suivante :

- **Base de connaissances :** Avant la r√©cup√©ration, ces documents doivent √™tre ing√©r√©s et pr√©trait√©s, g√©n√©ralement en d√©coupant les grands documents en petits fragments, en les transformant en embeddings textuels et en les stockant dans une base de donn√©es.

- **Requ√™te utilisateur :** l'utilisateur pose une question

- **R√©cup√©ration :** lorsque l‚Äôutilisateur pose une question, le mod√®le d‚Äôembedding r√©cup√®re les informations pertinentes de notre base de connaissances pour fournir plus de contexte qui sera incorpor√© dans le prompt.

- **G√©n√©ration augment√©e :** le LLM am√©liore sa r√©ponse sur la base des donn√©es r√©cup√©r√©es. Cela permet √† la r√©ponse g√©n√©r√©e de ne pas se baser uniquement sur les donn√©es pr√©-entra√Æn√©es mais aussi sur les informations pertinentes du contexte ajout√©. Les donn√©es r√©cup√©r√©es servent √† augmenter les r√©ponses du LLM. Le LLM retourne ensuite une r√©ponse √† la question de l‚Äôutilisateur.

![dessin montrant l‚Äôarchitecture des RAGs](../../../../../translated_images/fr/encoder-decode.f2658c25d0eadee2.webp)

L‚Äôarchitecture des RAGs est impl√©ment√©e √† l‚Äôaide de transformers comprenant deux parties : un encodeur et un d√©codeur. Par exemple, lorsqu‚Äôun utilisateur pose une question, le texte d‚Äôentr√©e est ¬´ encod√© ¬ª en vecteurs qui capturent le sens des mots et les vecteurs sont ¬´ d√©cod√©s ¬ª dans notre index documentaire pour g√©n√©rer un nouveau texte bas√© sur la requ√™te. Le LLM utilise un mod√®le encodeur-d√©codeur pour g√©n√©rer la sortie.

Deux approches pour impl√©menter RAG selon l‚Äôarticle propos√© : [Retrieval-Augmented Generation for Knowledge Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sont :

- **_RAG-Sequence_** utilisant les documents r√©cup√©r√©s pour pr√©dire la meilleure r√©ponse possible √† une requ√™te utilisateur

- **RAG-Token** utilisant des documents pour g√©n√©rer le token suivant, puis les r√©cup√©rer pour r√©pondre √† la requ√™te utilisateur

### Pourquoi utiliser les RAGs ?¬†

- **Richesse d'information :** garantit que les r√©ponses textuelles sont √† jour. Cela am√©liore donc les performances sur les t√¢ches sp√©cifiques au domaine en acc√©dant √† la base de connaissances interne.

- R√©duit les fabrications en utilisant des **donn√©es v√©rifiables** dans la base de connaissances pour fournir du contexte aux questions des utilisateurs.

- C‚Äôest **√©conomique**, car ils sont moins co√ªteux compar√©s √† un ajustement fin d'un LLM.

## Cr√©ation d‚Äôune base de connaissances

Notre application est bas√©e sur nos donn√©es personnelles, c‚Äôest-√†-dire la le√ßon sur les r√©seaux neuronaux dans le programme AI For Beginners.

### Bases de donn√©es vectorielles

Une base de donn√©es vectorielle, contrairement aux bases de donn√©es traditionnelles, est une base sp√©cialis√©e con√ßue pour stocker, g√©rer et rechercher des vecteurs int√©gr√©s. Elle stocke des repr√©sentations num√©riques des documents. Fractionner les donn√©es en embeddings num√©riques facilite la compr√©hension et le traitement des donn√©es par notre syst√®me IA.

Nous stockons nos embeddings dans des bases de donn√©es vectorielles car les LLM ont une limite sur le nombre de tokens qu‚Äôils acceptent en entr√©e. Comme on ne peut pas passer l‚Äôint√©gralit√© des embeddings √† un LLM, il faudra les d√©couper en fragments et, lorsqu‚Äôun utilisateur pose une question, les embeddings les plus proches de la question seront retourn√©s avec le prompt. Le fractionnement r√©duit aussi les co√ªts li√©s au nombre de tokens transmis √† un LLM.

Quelques bases de donn√©es vectorielles populaires incluent Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant et DeepLake. Vous pouvez cr√©er un mod√®le Azure Cosmos DB en utilisant Azure CLI avec la commande suivante :

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Du texte aux embeddings

Avant de stocker nos donn√©es, nous devons les convertir en embeddings vectoriels avant de les enregistrer dans la base de donn√©es. Si vous travaillez avec de longs documents ou textes, vous pouvez les d√©couper selon vos requ√™tes attendues. Le d√©coupage peut se faire au niveau des phrases ou au niveau des paragraphes. Comme le d√©coupage tire le sens des mots alentour, vous pouvez ajouter un autre contexte √† un fragment, par exemple en ajoutant le titre du document ou incluant un texte avant ou apr√®s le fragment. Vous pouvez d√©couper les donn√©es comme suit :

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Si le dernier morceau n'a pas atteint la longueur minimale, ajoutez-le quand m√™me
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Une fois d√©coup√©s, nous pouvons ensuite repr√©senter notre texte sous forme d'embeddings avec diff√©rents mod√®les d‚Äôembeddings. Certains mod√®les que vous pouvez utiliser incluent : word2vec, ada-002 de OpenAI, Azure Computer Vision et bien d‚Äôautres. Le choix d‚Äôun mod√®le d√©pendra des langues utilis√©es, du type de contenu encod√© (texte/image/audio), de la taille d‚Äôentr√©e qu‚Äôil peut encoder et de la longueur de sortie de l‚Äôembedding.

Un exemple de texte encod√© avec le mod√®le `text-embedding-ada-002` d‚ÄôOpenAI est :
![un embedding du mot chat](../../../../../translated_images/fr/cat.74cbd7946bc9ca38.webp)

## R√©cup√©ration et recherche vectorielle

Quand un utilisateur pose une question, le r√©cup√©rateur la transforme en vecteur via l‚Äôencodeur de requ√™te, puis il recherche dans notre index de documents les vecteurs pertinents li√©s √† l‚Äôentr√©e. Une fois fait, il convertit √† la fois le vecteur d'entr√©e et les vecteurs documents en texte et les passe au LLM.

### R√©cup√©ration

La r√©cup√©ration intervient quand le syst√®me essaie rapidement de trouver les documents dans l‚Äôindex qui satisfont les crit√®res de recherche. L‚Äôobjectif du r√©cup√©rateur est d‚Äôobtenir des documents qui fourniront du contexte et ancreront le LLM sur vos donn√©es.

Il y a plusieurs fa√ßons de faire la recherche dans notre base de donn√©es, telles que :

- **Recherche par mots-cl√©s** - utilis√©e pour les recherches textuelles

- **Recherche vectorielle** - convertit les documents de texte en repr√©sentations vectorielles via des mod√®les d‚Äôembeddings, permettant une **recherche s√©mantique** bas√©e sur le sens des mots. La r√©cup√©ration se fait en interrogeant les documents dont les repr√©sentations vectorielles sont les plus proches de la question utilisateur.

- **Hybride** - une combinaison de la recherche par mots-cl√©s et vectorielle.

Un d√©fi appara√Æt quand il n‚Äôy a pas de r√©ponse similaire dans la base √† la requ√™te ; le syst√®me retourne alors la meilleure information disponible. Cependant, vous pouvez utiliser des tactiques comme d√©finir la distance maximale pour la pertinence ou utiliser une recherche hybride combinant mots-cl√©s et recherche vectorielle. Dans cette le√ßon, nous utiliserons la recherche hybride, combinaison des deux m√©thodes. Nous stockerons nos donn√©es dans un dataframe avec des colonnes contenant les fragments ainsi que les embeddings.

### Similarit√© vectorielle

Le r√©cup√©rateur va chercher dans la base de connaissances les embeddings proches les uns des autres, le voisin le plus proche, car ce sont des textes similaires. Dans notre sc√©nario, lorsqu‚Äôun utilisateur fait une requ√™te, elle est d'abord encod√©e puis associ√©e aux embeddings similaires. La mesure couramment utilis√©e pour savoir √† quel point diff√©rents vecteurs sont similaires est la similarit√© cosinus, qui se base sur l‚Äôangle entre deux vecteurs.

Nous pouvons mesurer la similarit√© avec d‚Äôautres alternatives comme la distance Euclidienne, qui est la ligne droite entre les extr√©mit√©s de vecteurs, ou le produit scalaire qui mesure la somme des produits des √©l√©ments correspondants de deux vecteurs.

### Index de recherche

Lors de la r√©cup√©ration, nous devons cr√©er un index de recherche pour notre base de connaissances avant d‚Äôex√©cuter la recherche. Un index stockera nos embeddings et pourra rapidement r√©cup√©rer les fragments les plus similaires m√™me dans une grande base de donn√©es. Nous pouvons cr√©er notre index localement en utilisant :

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Cr√©er l'index de recherche
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Pour interroger l'index, vous pouvez utiliser la m√©thode kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```

### R√©organisation des r√©sultats

Une fois que vous avez interrog√© la base de donn√©es, vous pourriez avoir besoin de trier les r√©sultats du plus pertinent. Un LLM de r√©organisation utilise le Machine Learning pour am√©liorer la pertinence des r√©sultats en les ordonnant du plus pertinent. Avec Azure AI Search, la r√©organisation est faite automatiquement pour vous gr√¢ce √† un r√©organisateur s√©mantique. Un exemple de fonctionnement de la r√©organisation avec les plus proches voisins :

```python
# Trouver les documents les plus similaires
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Imprimer les documents les plus similaires
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Mettons tout ensemble

La derni√®re √©tape consiste √† int√©grer notre LLM dans le processus pour obtenir des r√©ponses ancr√©es dans nos donn√©es. Nous pouvons l‚Äôimpl√©menter comme suit :

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convertir la question en un vecteur de requ√™te
    query_vector = create_embeddings(user_input)

    # Trouver les documents les plus similaires
    distances, indices = nbrs.kneighbors([query_vector])

    # ajouter les documents √† la requ√™te pour fournir le contexte
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combiner l'historique et l'entr√©e utilisateur
    history.append(user_input)

    # cr√©er un objet message
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # utiliser la compl√©tion de chat pour g√©n√©rer une r√©ponse
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## √âvaluation de notre application

### M√©triques d‚Äô√©valuation

- Qualit√© des r√©ponses fournies, assurant qu‚Äôelles paraissent naturelles, fluides et humaines

- Ancrage des donn√©es : √©valuer si la r√©ponse vient bien des documents fournis

- Pertinence : √©valuer si la r√©ponse correspond et est li√©e √† la question pos√©e

- Fluidit√© : v√©rifier si la r√©ponse a un sens grammatical

## Cas d‚Äôusage de RAG (G√©n√©ration Augment√©e par R√©cup√©ration) et bases de donn√©es vectorielles

Il existe de nombreux cas o√π les appels de fonctions peuvent am√©liorer votre application, tels que :

- Questions et r√©ponses : ancrer vos donn√©es d‚Äôentreprise dans un chat que les employ√©s peuvent interroger.

- Syst√®mes de recommandation : o√π vous pouvez cr√©er un syst√®me qui associe les valeurs les plus similaires, par exemple des films, restaurants et bien plus.

- Services de chatbot : vous pouvez stocker l‚Äôhistorique des conversations et personnaliser la conversation en fonction des donn√©es utilisateur.

- Recherche d‚Äôimages bas√©e sur des embeddings vectoriels, utile pour la reconnaissance d‚Äôimages et la d√©tection d‚Äôanomalies.

## R√©sum√©

Nous avons couvert les domaines fondamentaux de RAG depuis l‚Äôajout de nos donn√©es √† l‚Äôapplication, la requ√™te utilisateur jusqu‚Äô√† la sortie. Pour simplifier la cr√©ation de RAG, vous pouvez utiliser des frameworks comme Semantic Kernel, Langchain ou Autogen.

## Exercice

Pour poursuivre votre apprentissage de la G√©n√©ration Augment√©e par R√©cup√©ration (RAG), vous pouvez construire :

- Construire une interface frontale pour l‚Äôapplication en utilisant le framework de votre choix

- Utiliser un framework, soit LangChain soit Semantic Kernel, et recr√©er votre application.

F√©licitations pour avoir termin√© la le√ßon üëè.

## L‚Äôapprentissage ne s‚Äôarr√™te pas ici, continuez le voyage

Apr√®s avoir termin√© cette le√ßon, consultez notre [collection d‚Äôapprentissage en IA g√©n√©rative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer √† am√©liorer vos connaissances en IA g√©n√©rative !

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Avertissement** :  
Ce document a √©t√© traduit √† l‚Äôaide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d‚Äôassurer l‚Äôexactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d‚Äôorigine doit √™tre consid√©r√© comme la source faisant foi. Pour les informations critiques, une traduction professionnelle r√©alis√©e par un humain est recommand√©e. Nous d√©clinons toute responsabilit√© pour tout malentendu ou mauvaise interpr√©tation r√©sultant de l‚Äôutilisation de cette traduction.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->