# Introduction aux RÃ©seaux de Neurones. Perceptron Multi-Couche

Dans la section prÃ©cÃ©dente, vous avez dÃ©couvert le modÃ¨le de rÃ©seau de neurones le plus simple : le perceptron Ã  une couche, un modÃ¨le de classification linÃ©aire Ã  deux classes.

Dans cette section, nous allons Ã©tendre ce modÃ¨le pour crÃ©er un cadre plus flexible, nous permettant de :

* effectuer de la **classification multi-classes** en plus de la classification Ã  deux classes
* rÃ©soudre des **problÃ¨mes de rÃ©gression** en plus de la classification
* sÃ©parer des classes qui ne sont pas linÃ©airement sÃ©parables

Nous allons Ã©galement dÃ©velopper notre propre cadre modulaire en Python qui nous permettra de construire diffÃ©rentes architectures de rÃ©seaux de neurones.

## Formalisation de l'Apprentissage Automatique

CommenÃ§ons par formaliser le problÃ¨me de l'apprentissage automatique. Supposons que nous avons un jeu de donnÃ©es d'entraÃ®nement **X** avec des Ã©tiquettes **Y**, et que nous devons construire un modÃ¨le *f* qui fera des prÃ©dictions les plus prÃ©cises possible. La qualitÃ© des prÃ©dictions est mesurÃ©e par une **fonction de perte** â„’. Les fonctions de perte suivantes sont souvent utilisÃ©es :

* Pour un problÃ¨me de rÃ©gression, lorsque nous devons prÃ©dire un nombre, nous pouvons utiliser l'**erreur absolue** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, ou l'**erreur quadratique** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pour la classification, nous utilisons la **perte 0-1** (qui est essentiellement la mÃªme que l'**exactitude** du modÃ¨le), ou la **perte logistique**.

Pour le perceptron Ã  une couche, la fonction *f* Ã©tait dÃ©finie comme une fonction linÃ©aire *f(x)=wx+b* (ici *w* est la matrice de poids, *x* est le vecteur des caractÃ©ristiques d'entrÃ©e, et *b* est le vecteur de biais). Pour diffÃ©rentes architectures de rÃ©seaux de neurones, cette fonction peut prendre une forme plus complexe.

> Dans le cas de la classification, il est souvent souhaitable d'obtenir des probabilitÃ©s des classes correspondantes en sortie du rÃ©seau. Pour convertir des nombres arbitraires en probabilitÃ©s (par exemple, pour normaliser la sortie), nous utilisons souvent la fonction **softmax** Ïƒ, et la fonction *f* devient *f(x)=Ïƒ(wx+b)*

Dans la dÃ©finition de *f* ci-dessus, *w* et *b* sont appelÃ©s **paramÃ¨tres** Î¸=âŸ¨*w,b*âŸ©. Ã‰tant donnÃ© le jeu de donnÃ©es âŸ¨**X**,**Y**âŸ©, nous pouvons calculer une erreur globale sur l'ensemble du jeu de donnÃ©es en fonction des paramÃ¨tres Î¸.

> âœ… **L'objectif de l'entraÃ®nement d'un rÃ©seau de neurones est de minimiser l'erreur en faisant varier les paramÃ¨tres Î¸**

## Optimisation par Descente de Gradient

Il existe une mÃ©thode bien connue d'optimisation de fonction appelÃ©e **descente de gradient**. L'idÃ©e est que nous pouvons calculer une dÃ©rivÃ©e (dans le cas multidimensionnel appelÃ©e **gradient**) de la fonction de perte par rapport aux paramÃ¨tres, et faire varier les paramÃ¨tres de maniÃ¨re Ã  ce que l'erreur diminue. Cela peut Ãªtre formalisÃ© comme suit :

* Initialiser les paramÃ¨tres avec des valeurs alÃ©atoires w<sup>(0)</sup>, b<sup>(0)</sup>
* RÃ©pÃ©ter l'Ã©tape suivante plusieurs fois :
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Pendant l'entraÃ®nement, les Ã©tapes d'optimisation sont censÃ©es Ãªtre calculÃ©es en tenant compte de l'ensemble du jeu de donnÃ©es (rappelez-vous que la perte est calculÃ©e comme une somme sur tous les Ã©chantillons d'entraÃ®nement). Cependant, dans la vie rÃ©elle, nous prenons de petites portions du jeu de donnÃ©es appelÃ©es **mini-lots**, et calculons les gradients sur un sous-ensemble de donnÃ©es. Comme le sous-ensemble est pris alÃ©atoirement Ã  chaque fois, cette mÃ©thode est appelÃ©e **descente de gradient stochastique** (SGD).

## Perceptrons Multi-Couches et RÃ©tropropagation

Un rÃ©seau Ã  une couche, comme nous l'avons vu ci-dessus, est capable de classer des classes linÃ©airement sÃ©parables. Pour construire un modÃ¨le plus riche, nous pouvons combiner plusieurs couches du rÃ©seau. MathÃ©matiquement, cela signifierait que la fonction *f* aurait une forme plus complexe et serait calculÃ©e en plusieurs Ã©tapes :
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Ici, Î± est une **fonction d'activation non-linÃ©aire**, Ïƒ est une fonction softmax, et les paramÃ¨tres Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

L'algorithme de descente de gradient resterait le mÃªme, mais il serait plus difficile de calculer les gradients. Ã‰tant donnÃ© la rÃ¨gle de diffÃ©renciation en chaÃ®ne, nous pouvons calculer les dÃ©rivÃ©es comme suit :

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… La rÃ¨gle de diffÃ©renciation en chaÃ®ne est utilisÃ©e pour calculer les dÃ©rivÃ©es de la fonction de perte par rapport aux paramÃ¨tres.

Notez que la partie la plus Ã  gauche de toutes ces expressions est la mÃªme, et ainsi nous pouvons calculer efficacement les dÃ©rivÃ©es en commenÃ§ant par la fonction de perte et en allant "Ã  l'envers" Ã  travers le graphe computationnel. Ainsi, la mÃ©thode d'entraÃ®nement d'un perceptron multi-couches est appelÃ©e **rÃ©tropropagation**, ou 'backprop'.

> TODO: citation d'image

> âœ… Nous couvrirons la rÃ©tropropagation en beaucoup plus de dÃ©tails dans notre exemple de notebook.

## Conclusion

Dans cette leÃ§on, nous avons construit notre propre bibliothÃ¨que de rÃ©seaux de neurones, et nous l'avons utilisÃ©e pour une tÃ¢che de classification simple en deux dimensions.

## ğŸš€ DÃ©fi

Dans le notebook associÃ©, vous allez implÃ©menter votre propre cadre pour construire et entraÃ®ner des perceptrons multi-couches. Vous pourrez voir en dÃ©tail comment fonctionnent les rÃ©seaux de neurones modernes.

Passez au notebook OwnFramework et travaillez-le.

## RÃ©vision & Auto-Ã©tude

La rÃ©tropropagation est un algorithme courant utilisÃ© en IA et ML, qui mÃ©rite d'Ãªtre Ã©tudiÃ© en dÃ©tail.

## Devoir

Dans ce laboratoire, il vous est demandÃ© d'utiliser le cadre que vous avez construit dans cette leÃ§on pour rÃ©soudre la classification des chiffres manuscrits MNIST.

* Instructions
* Notebook

**Avertissement** :  
Ce document a Ã©tÃ© traduit Ã  l'aide de services de traduction basÃ©s sur l'IA. Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisÃ©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit Ãªtre considÃ©rÃ© comme la source faisant autoritÃ©. Pour des informations critiques, une traduction humaine professionnelle est recommandÃ©e. Nous ne sommes pas responsables des malentendus ou des mauvaises interprÃ©tations rÃ©sultant de l'utilisation de cette traduction.