<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:39:56+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "fr"
}
-->
# Introduction aux rÃ©seaux de neurones. Perceptron multicouche

Dans la section prÃ©cÃ©dente, vous avez dÃ©couvert le modÃ¨le de rÃ©seau de neurones le plus simple : le perceptron Ã  une couche, un modÃ¨le linÃ©aire de classification binaire.

Dans cette section, nous allons Ã©tendre ce modÃ¨le vers un cadre plus flexible, nous permettant de :

* rÃ©aliser une **classification multi-classes** en plus de la classification binaire
* rÃ©soudre des **problÃ¨mes de rÃ©gression** en plus de la classification
* sÃ©parer des classes qui ne sont pas linÃ©airement sÃ©parables

Nous allons Ã©galement dÃ©velopper notre propre framework modulaire en Python, qui nous permettra de construire diffÃ©rentes architectures de rÃ©seaux de neurones.

## Formalisation de lâ€™apprentissage automatique

CommenÃ§ons par formaliser le problÃ¨me dâ€™apprentissage automatique. Supposons que nous disposons dâ€™un jeu de donnÃ©es dâ€™entraÃ®nement **X** avec des Ã©tiquettes **Y**, et que nous devons construire un modÃ¨le *f* qui fera les prÃ©dictions les plus prÃ©cises possibles. La qualitÃ© des prÃ©dictions est mesurÃ©e par une **fonction de perte** â„’. Les fonctions de perte suivantes sont souvent utilisÃ©es :

* Pour un problÃ¨me de rÃ©gression, lorsque lâ€™on doit prÃ©dire un nombre, on peut utiliser lâ€™**erreur absolue** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, ou lâ€™**erreur quadratique** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pour la classification, on utilise la **perte 0-1** (qui correspond essentiellement Ã  la **prÃ©cision** du modÃ¨le), ou la **perte logistique**.

Pour le perceptron Ã  une couche, la fonction *f* Ã©tait dÃ©finie comme une fonction linÃ©aire *f(x)=wx+b* (ici *w* est la matrice de poids, *x* le vecteur des caractÃ©ristiques dâ€™entrÃ©e, et *b* le vecteur de biais). Pour diffÃ©rentes architectures de rÃ©seaux de neurones, cette fonction peut prendre une forme plus complexe.

> Dans le cas de la classification, il est souvent souhaitable dâ€™obtenir des probabilitÃ©s des classes correspondantes en sortie du rÃ©seau. Pour convertir des nombres arbitraires en probabilitÃ©s (par exemple pour normaliser la sortie), on utilise souvent la fonction **softmax** Ïƒ, et la fonction *f* devient *f(x)=Ïƒ(wx+b)*

Dans la dÃ©finition de *f* ci-dessus, *w* et *b* sont appelÃ©s **paramÃ¨tres** Î¸=âŸ¨*w,b*âŸ©. Ã‰tant donnÃ© le jeu de donnÃ©es âŸ¨**X**,**Y**âŸ©, on peut calculer une erreur globale sur lâ€™ensemble du jeu de donnÃ©es en fonction des paramÃ¨tres Î¸.

> âœ… **Lâ€™objectif de lâ€™entraÃ®nement du rÃ©seau de neurones est de minimiser lâ€™erreur en faisant varier les paramÃ¨tres Î¸**

## Optimisation par descente de gradient

Il existe une mÃ©thode bien connue dâ€™optimisation de fonction appelÃ©e **descente de gradient**. Lâ€™idÃ©e est que lâ€™on peut calculer une dÃ©rivÃ©e (dans le cas multidimensionnel appelÃ©e **gradient**) de la fonction de perte par rapport aux paramÃ¨tres, et faire varier ces paramÃ¨tres de maniÃ¨re Ã  rÃ©duire lâ€™erreur. Cela peut se formaliser ainsi :

* Initialiser les paramÃ¨tres avec des valeurs alÃ©atoires w<sup>(0)</sup>, b<sup>(0)</sup>
* RÃ©pÃ©ter lâ€™Ã©tape suivante plusieurs fois :
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Pendant lâ€™entraÃ®nement, les Ã©tapes dâ€™optimisation sont censÃ©es Ãªtre calculÃ©es en considÃ©rant lâ€™ensemble du jeu de donnÃ©es (rappelez-vous que la perte est calculÃ©e comme une somme sur tous les Ã©chantillons dâ€™entraÃ®nement). Cependant, dans la pratique, on prend de petites portions du jeu de donnÃ©es appelÃ©es **minibatchs**, et on calcule les gradients sur un sous-ensemble des donnÃ©es. Comme ce sous-ensemble est choisi alÃ©atoirement Ã  chaque fois, cette mÃ©thode sâ€™appelle **descente de gradient stochastique** (SGD).

## Perceptrons multicouches et rÃ©tropropagation

Un rÃ©seau Ã  une couche, comme nous lâ€™avons vu, est capable de classer des classes linÃ©airement sÃ©parables. Pour construire un modÃ¨le plus riche, on peut combiner plusieurs couches du rÃ©seau. MathÃ©matiquement, cela signifie que la fonction *f* aura une forme plus complexe, et sera calculÃ©e en plusieurs Ã©tapes :
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Ici, Î± est une **fonction dâ€™activation non linÃ©aire**, Ïƒ est une fonction softmax, et les paramÃ¨tres Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Lâ€™algorithme de descente de gradient reste le mÃªme, mais il devient plus difficile de calculer les gradients. En appliquant la rÃ¨gle de dÃ©rivation en chaÃ®ne, on peut calculer les dÃ©rivÃ©es comme suit :

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… La rÃ¨gle de dÃ©rivation en chaÃ®ne est utilisÃ©e pour calculer les dÃ©rivÃ©es de la fonction de perte par rapport aux paramÃ¨tres.

Notez que la partie la plus Ã  gauche de toutes ces expressions est la mÃªme, ce qui nous permet de calculer efficacement les dÃ©rivÃ©es en partant de la fonction de perte et en remontant Â« Ã  rebours Â» dans le graphe de calcul. Câ€™est pourquoi la mÃ©thode dâ€™entraÃ®nement dâ€™un perceptron multicouche sâ€™appelle **rÃ©tropropagation**, ou 'backprop'.

> TODO: image citation

> âœ… Nous aborderons la rÃ©tropropagation en beaucoup plus de dÃ©tails dans notre exemple de notebook.

## Conclusion

Dans cette leÃ§on, nous avons construit notre propre bibliothÃ¨que de rÃ©seaux de neurones, et nous lâ€™avons utilisÃ©e pour une tÃ¢che simple de classification bidimensionnelle.

## ğŸš€ DÃ©fi

Dans le notebook associÃ©, vous allez implÃ©menter votre propre framework pour construire et entraÃ®ner des perceptrons multicouches. Vous pourrez ainsi voir en dÃ©tail comment fonctionnent les rÃ©seaux de neurones modernes.

Passez au notebook OwnFramework et suivez-le pas Ã  pas.

## RÃ©vision & Auto-apprentissage

La rÃ©tropropagation est un algorithme courant en IA et en apprentissage automatique, qui mÃ©rite dâ€™Ãªtre Ã©tudiÃ© plus en profondeur.

## Exercice

Dans ce laboratoire, vous Ãªtes invitÃ© Ã  utiliser le framework que vous avez construit dans cette leÃ§on pour rÃ©soudre la classification des chiffres manuscrits MNIST.

* Instructions
* Notebook

**Avertissement** :  
Ce document a Ã©tÃ© traduit Ã  lâ€™aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions dâ€™assurer lâ€™exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue dâ€™origine doit Ãªtre considÃ©rÃ© comme la source faisant foi. Pour les informations critiques, une traduction professionnelle rÃ©alisÃ©e par un humain est recommandÃ©e. Nous dÃ©clinons toute responsabilitÃ© en cas de malentendus ou de mauvaises interprÃ©tations rÃ©sultant de lâ€™utilisation de cette traduction.