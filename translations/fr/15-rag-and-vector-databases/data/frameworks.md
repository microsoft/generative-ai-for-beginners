<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:24:30+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "fr"
}
-->
# Frameworks de rÃ©seaux de neurones

Comme nous l'avons dÃ©jÃ  vu, pour pouvoir entraÃ®ner efficacement des rÃ©seaux de neurones, il faut faire deux choses :

* OpÃ©rer sur des tenseurs, par exemple multiplier, additionner, et calculer certaines fonctions comme sigmoid ou softmax
* Calculer les gradients de toutes les expressions, afin de rÃ©aliser une optimisation par descente de gradient

Alors que la bibliothÃ¨que `numpy` peut gÃ©rer la premiÃ¨re partie, nous avons besoin dâ€™un mÃ©canisme pour calculer les gradients. Dans notre framework dÃ©veloppÃ© dans la section prÃ©cÃ©dente, nous devions programmer manuellement toutes les fonctions dÃ©rivÃ©es dans la mÃ©thode `backward`, qui rÃ©alise la rÃ©tropropagation. IdÃ©alement, un framework devrait nous permettre de calculer les gradients de *nâ€™importe quelle expression* que nous pouvons dÃ©finir.

Un autre point important est de pouvoir effectuer les calculs sur GPU, ou toute autre unitÃ© de calcul spÃ©cialisÃ©e, comme TPU. Lâ€™entraÃ®nement des rÃ©seaux de neurones profonds nÃ©cessite *beaucoup* de calculs, et pouvoir parallÃ©liser ces calculs sur des GPU est trÃ¨s important.

> âœ… Le terme Â« parallÃ©liser Â» signifie rÃ©partir les calculs sur plusieurs dispositifs.

Actuellement, les deux frameworks de rÃ©seaux de neurones les plus populaires sont : TensorFlow et PyTorch. Les deux fournissent une API bas niveau pour manipuler les tenseurs Ã  la fois sur CPU et GPU. Au-dessus de cette API bas niveau, il existe aussi une API plus haut niveau, appelÃ©e respectivement Keras et PyTorch Lightning.

API bas niveau | TensorFlow | PyTorch  
--------------|------------|---------  
API haut niveau | Keras | PyTorch Lightning

Les **API bas niveau** dans les deux frameworks permettent de construire ce quâ€™on appelle des **graphes de calcul**. Ce graphe dÃ©finit comment calculer la sortie (gÃ©nÃ©ralement la fonction de perte) Ã  partir des paramÃ¨tres dâ€™entrÃ©e, et peut Ãªtre envoyÃ© pour calcul sur GPU, si disponible. Il existe des fonctions pour diffÃ©rencier ce graphe de calcul et calculer les gradients, qui peuvent ensuite Ãªtre utilisÃ©s pour optimiser les paramÃ¨tres du modÃ¨le.

Les **API haut niveau** considÃ¨rent essentiellement les rÃ©seaux de neurones comme une **sÃ©quence de couches**, et facilitent grandement la construction de la plupart des rÃ©seaux. Lâ€™entraÃ®nement du modÃ¨le nÃ©cessite gÃ©nÃ©ralement de prÃ©parer les donnÃ©es puis dâ€™appeler une fonction `fit` pour lancer le processus.

Lâ€™API haut niveau permet de construire rapidement des rÃ©seaux typiques sans se soucier de nombreux dÃ©tails. En mÃªme temps, lâ€™API bas niveau offre beaucoup plus de contrÃ´le sur le processus dâ€™entraÃ®nement, et est donc trÃ¨s utilisÃ©e en recherche, lorsque lâ€™on travaille sur de nouvelles architectures de rÃ©seaux.

Il est aussi important de comprendre que vous pouvez utiliser les deux API ensemble, par exemple en dÃ©veloppant votre propre architecture de couche rÃ©seau avec lâ€™API bas niveau, puis en lâ€™intÃ©grant dans un rÃ©seau plus large construit et entraÃ®nÃ© avec lâ€™API haut niveau. Ou bien vous pouvez dÃ©finir un rÃ©seau avec lâ€™API haut niveau comme une sÃ©quence de couches, puis utiliser votre propre boucle dâ€™entraÃ®nement bas niveau pour optimiser. Les deux API partagent les mÃªmes concepts fondamentaux et sont conÃ§ues pour bien fonctionner ensemble.

## Apprentissage

Dans ce cours, nous proposons la plupart du contenu Ã  la fois pour PyTorch et TensorFlow. Vous pouvez choisir votre framework prÃ©fÃ©rÃ© et ne suivre que les notebooks correspondants. Si vous ne savez pas quel framework choisir, lisez quelques discussions sur internet Ã  propos de **PyTorch vs. TensorFlow**. Vous pouvez aussi explorer les deux frameworks pour mieux comprendre.

Dans la mesure du possible, nous utiliserons les API haut niveau pour plus de simplicitÃ©. Cependant, nous pensons quâ€™il est important de comprendre le fonctionnement des rÃ©seaux de neurones depuis les bases, donc au dÃ©but nous commenÃ§ons par travailler avec lâ€™API bas niveau et les tenseurs. Toutefois, si vous souhaitez avancer rapidement sans passer trop de temps sur ces dÃ©tails, vous pouvez les sauter et aller directement aux notebooks avec lâ€™API haut niveau.

## âœï¸ Exercices : Frameworks

Poursuivez votre apprentissage dans les notebooks suivants :

API bas niveau | Notebook TensorFlow+Keras | PyTorch  
--------------|----------------------------|---------  
API haut niveau | Keras | *PyTorch Lightning*

AprÃ¨s avoir maÃ®trisÃ© les frameworks, rÃ©capitulons la notion de surapprentissage.

# Surapprentissage

Le surapprentissage est un concept extrÃªmement important en apprentissage automatique, et il est crucial de bien le comprendre !

ConsidÃ©rons le problÃ¨me suivant dâ€™approximation de 5 points (reprÃ©sentÃ©s par des `x` sur les graphiques ci-dessous) :

!linear | overfit  
-------------------------|--------------------------  
**ModÃ¨le linÃ©aire, 2 paramÃ¨tres** | **ModÃ¨le non-linÃ©aire, 7 paramÃ¨tres**  
Erreur dâ€™entraÃ®nement = 5.3 | Erreur dâ€™entraÃ®nement = 0  
Erreur de validation = 5.1 | Erreur de validation = 20

* Ã€ gauche, on voit une bonne approximation par une droite. Comme le nombre de paramÃ¨tres est adÃ©quat, le modÃ¨le saisit correctement la distribution des points.
* Ã€ droite, le modÃ¨le est trop puissant. Comme nous nâ€™avons que 5 points et que le modÃ¨le a 7 paramÃ¨tres, il peut sâ€™ajuster pour passer par tous les points, ce qui fait que lâ€™erreur dâ€™entraÃ®nement est nulle. Cependant, cela empÃªche le modÃ¨le de comprendre le vrai motif derriÃ¨re les donnÃ©es, dâ€™oÃ¹ une erreur de validation trÃ¨s Ã©levÃ©e.

Il est trÃ¨s important de trouver un bon Ã©quilibre entre la richesse du modÃ¨le (nombre de paramÃ¨tres) et le nombre dâ€™Ã©chantillons dâ€™entraÃ®nement.

## Pourquoi le surapprentissage se produit

  * Pas assez de donnÃ©es dâ€™entraÃ®nement  
  * ModÃ¨le trop puissant  
  * Trop de bruit dans les donnÃ©es dâ€™entrÃ©e

## Comment dÃ©tecter le surapprentissage

Comme on peut le voir sur le graphique ci-dessus, le surapprentissage se dÃ©tecte par une erreur dâ€™entraÃ®nement trÃ¨s faible, et une erreur de validation Ã©levÃ©e. Normalement, pendant lâ€™entraÃ®nement, on observe que les erreurs dâ€™entraÃ®nement et de validation diminuent toutes les deux, puis Ã  un certain moment lâ€™erreur de validation peut cesser de diminuer et commencer Ã  augmenter. Câ€™est un signe de surapprentissage, et un indicateur quâ€™il faudrait probablement arrÃªter lâ€™entraÃ®nement Ã  ce moment-lÃ  (ou au moins sauvegarder un instantanÃ© du modÃ¨le).

surapprentissage

## Comment prÃ©venir le surapprentissage

Si vous constatez quâ€™un surapprentissage se produit, vous pouvez faire lâ€™une des choses suivantes :

 * Augmenter la quantitÃ© de donnÃ©es dâ€™entraÃ®nement  
 * RÃ©duire la complexitÃ© du modÃ¨le  
 * Utiliser une technique de rÃ©gularisation, comme le Dropout, que nous verrons plus tard.

## Surapprentissage et compromis biais-variance

Le surapprentissage est en fait un cas particulier dâ€™un problÃ¨me plus gÃ©nÃ©ral en statistiques appelÃ© compromis biais-variance. Si lâ€™on considÃ¨re les sources possibles dâ€™erreur dans notre modÃ¨le, on peut distinguer deux types dâ€™erreurs :

* Les **erreurs de biais** sont causÃ©es par notre algorithme qui nâ€™arrive pas Ã  capturer correctement la relation dans les donnÃ©es dâ€™entraÃ®nement. Cela peut venir du fait que notre modÃ¨le nâ€™est pas assez puissant (**sous-apprentissage**).
* Les **erreurs de variance**, qui sont causÃ©es par le modÃ¨le qui approxime le bruit dans les donnÃ©es dâ€™entrÃ©e au lieu de la relation significative (**surapprentissage**).

Pendant lâ€™entraÃ®nement, lâ€™erreur de biais diminue (car notre modÃ¨le apprend Ã  approximer les donnÃ©es), tandis que lâ€™erreur de variance augmente. Il est important dâ€™arrÃªter lâ€™entraÃ®nement â€“ soit manuellement (lorsquâ€™on dÃ©tecte un surapprentissage), soit automatiquement (en introduisant une rÃ©gularisation) â€“ pour Ã©viter le surapprentissage.

## Conclusion

Dans cette leÃ§on, vous avez appris les diffÃ©rences entre les diffÃ©rentes API des deux frameworks dâ€™IA les plus populaires, TensorFlow et PyTorch. Vous avez Ã©galement dÃ©couvert un sujet trÃ¨s important : le surapprentissage.

## ğŸš€ DÃ©fi

Dans les notebooks associÃ©s, vous trouverez des Â« tÃ¢ches Â» en bas de page ; travaillez-les et complÃ©tez les exercices.

## RÃ©vision & Auto-apprentissage

Faites des recherches sur les sujets suivants :

- TensorFlow  
- PyTorch  
- Surapprentissage

Posez-vous les questions suivantes :

- Quelle est la diffÃ©rence entre TensorFlow et PyTorch ?  
- Quelle est la diffÃ©rence entre surapprentissage et sous-apprentissage ?

## Travail Ã  faire

Dans ce laboratoire, vous Ãªtes invitÃ© Ã  rÃ©soudre deux problÃ¨mes de classification en utilisant des rÃ©seaux entiÃ¨rement connectÃ©s Ã  une ou plusieurs couches, avec PyTorch ou TensorFlow.

**Avertissement** :  
Ce document a Ã©tÃ© traduit Ã  lâ€™aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions dâ€™assurer lâ€™exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue dâ€™origine doit Ãªtre considÃ©rÃ© comme la source faisant foi. Pour les informations critiques, une traduction professionnelle rÃ©alisÃ©e par un humain est recommandÃ©e. Nous dÃ©clinons toute responsabilitÃ© en cas de malentendus ou de mauvaises interprÃ©tations rÃ©sultant de lâ€™utilisation de cette traduction.