{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Cette leçon couvrira : \n",
    "- Qu'est-ce que l'appel de fonction et ses cas d'utilisation \n",
    "- Comment créer un appel de fonction en utilisant OpenAI \n",
    "- Comment intégrer un appel de fonction dans une application \n",
    "\n",
    "## Objectifs d'apprentissage \n",
    "\n",
    "Après avoir terminé cette leçon, vous saurez comment faire et comprendre : \n",
    "\n",
    "- Le but de l'utilisation de l'appel de fonction \n",
    "- Configurer un appel de fonction en utilisant le service OpenAI \n",
    "- Concevoir des appels de fonction efficaces pour le cas d'utilisation de votre application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendre les appels de fonction\n",
    "\n",
    "Pour cette leçon, nous voulons créer une fonctionnalité pour notre startup éducative qui permet aux utilisateurs d'utiliser un chatbot pour trouver des cours techniques. Nous recommanderons des cours adaptés à leur niveau de compétence, leur rôle actuel et la technologie qui les intéresse.\n",
    "\n",
    "Pour cela, nous utiliserons une combinaison de :\n",
    " - `OpenAI` pour créer une expérience de chat pour l'utilisateur\n",
    " - `Microsoft Learn Catalog API` pour aider les utilisateurs à trouver des cours en fonction de leur demande\n",
    " - `Function Calling` pour prendre la requête de l'utilisateur et l'envoyer à une fonction afin de faire la requête API.\n",
    "\n",
    "Pour commencer, voyons pourquoi nous voudrions utiliser les appels de fonction en premier lieu :\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # obtenir une nouvelle réponse de GPT où il peut voir la réponse de la fonction\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi l'appel de fonction\n",
    "\n",
    "Si vous avez suivi une autre leçon de ce cours, vous comprenez probablement la puissance de l'utilisation des grands modèles de langage (LLM). Espérons que vous pouvez également voir certaines de leurs limites.\n",
    "\n",
    "L'appel de fonction est une fonctionnalité du service OpenAI conçue pour répondre aux défis suivants :\n",
    "\n",
    "Formatage incohérent des réponses :\n",
    "- Avant l'appel de fonction, les réponses d'un grand modèle de langage étaient non structurées et incohérentes. Les développeurs devaient écrire un code de validation complexe pour gérer chaque variation dans la sortie.\n",
    "\n",
    "Intégration limitée avec des données externes :\n",
    "- Avant cette fonctionnalité, il était difficile d'incorporer des données provenant d'autres parties d'une application dans un contexte de chat.\n",
    "\n",
    "En standardisant les formats de réponse et en permettant une intégration transparente avec des données externes, l'appel de fonction simplifie le développement et réduit le besoin de logique de validation supplémentaire.\n",
    "\n",
    "Les utilisateurs ne pouvaient pas obtenir de réponses comme « Quel temps fait-il actuellement à Stockholm ? ». Cela est dû au fait que les modèles étaient limités à la période sur laquelle les données avaient été entraînées.\n",
    "\n",
    "Regardons l'exemple ci-dessous qui illustre ce problème :\n",
    "\n",
    "Disons que nous voulons créer une base de données de données d'étudiants afin de pouvoir leur suggérer le bon cours. Ci-dessous, nous avons deux descriptions d'étudiants qui sont très similaires dans les données qu'elles contiennent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voulons envoyer ceci à un LLM pour analyser les données. Cela pourra ensuite être utilisé dans notre application pour envoyer cela à une API ou le stocker dans une base de données.\n",
    "\n",
    "Créons deux invites identiques dans lesquelles nous indiquons au LLM quelles informations nous intéressent :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voulons envoyer ceci à un LLM pour analyser les parties importantes pour notre produit. Ainsi, nous pouvons créer deux invites identiques pour instruire le LLM :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir créé ces deux invites, nous les enverrons au LLM en utilisant `openai.ChatCompletion`. Nous stockons l'invite dans la variable `messages` et attribuons le rôle à `user`. Cela permet d'imiter un message d'un utilisateur écrit à un chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant envoyer les deux requêtes au LLM et examiner la réponse que nous recevons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même si les invites sont les mêmes et que les descriptions sont similaires, nous pouvons obtenir différents formats pour la propriété `Grades`.\n",
    "\n",
    "Si vous exécutez la cellule ci-dessus plusieurs fois, le format peut être `3.7` ou `3.7 GPA`.\n",
    "\n",
    "Cela s'explique par le fait que le LLM prend des données non structurées sous forme d'invite écrite et renvoie également des données non structurées. Nous devons disposer d'un format structuré afin de savoir à quoi nous attendre lors du stockage ou de l'utilisation de ces données.\n",
    "\n",
    "En utilisant l'appel fonctionnel, nous pouvons nous assurer de recevoir des données structurées en retour. Lors de l'utilisation de l'appel fonctionnel, le LLM n'appelle ni n'exécute réellement aucune fonction. Au lieu de cela, nous créons une structure que le LLM doit suivre pour ses réponses. Nous utilisons ensuite ces réponses structurées pour savoir quelle fonction exécuter dans nos applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagramme de flux d'appel de fonction](../../../../translated_images/Function-Flow.083875364af4f4bb.fr.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons ensuite prendre ce qui est retourné par la fonction et le renvoyer au LLM. Le LLM répondra alors en utilisant un langage naturel pour répondre à la requête de l'utilisateur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas d'utilisation pour l'utilisation des appels de fonction\n",
    "\n",
    "**Appeler des outils externes**  \n",
    "Les chatbots sont excellents pour fournir des réponses aux questions des utilisateurs. En utilisant les appels de fonction, les chatbots peuvent utiliser les messages des utilisateurs pour accomplir certaines tâches. Par exemple, un étudiant peut demander au chatbot « Envoyer un e-mail à mon instructeur en disant que j'ai besoin de plus d'aide sur ce sujet ». Cela peut déclencher un appel de fonction à `send_email(to: string, body: string)`\n",
    "\n",
    "\n",
    "**Créer des requêtes API ou base de données**  \n",
    "Les utilisateurs peuvent trouver des informations en utilisant un langage naturel qui est converti en une requête formatée ou une requête API. Un exemple pourrait être un enseignant qui demande « Qui sont les étudiants qui ont terminé le dernier devoir » ce qui pourrait appeler une fonction nommée `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "\n",
    "**Créer des données structurées**  \n",
    "Les utilisateurs peuvent prendre un bloc de texte ou un CSV et utiliser le LLM pour en extraire des informations importantes. Par exemple, un étudiant peut convertir un article Wikipédia sur les accords de paix pour créer des fiches de révision IA. Cela peut être fait en utilisant une fonction appelée `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Création de votre premier appel de fonction\n",
    "\n",
    "Le processus de création d'un appel de fonction comprend 3 étapes principales :  \n",
    "1. Appeler l'API Chat Completions avec une liste de vos fonctions et un message utilisateur  \n",
    "2. Lire la réponse du modèle pour effectuer une action, c'est-à-dire exécuter une fonction ou un appel d'API  \n",
    "3. Faire un autre appel à l'API Chat Completions avec la réponse de votre fonction pour utiliser cette information afin de créer une réponse pour l'utilisateur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flux d'un appel de fonction](../../../../translated_images/LLM-Flow.3285ed8caf4796d7.fr.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Éléments d'un appel de fonction \n",
    "\n",
    "#### Entrée des utilisateurs \n",
    "\n",
    "La première étape consiste à créer un message utilisateur. Cela peut être attribué dynamiquement en prenant la valeur d'une entrée texte ou vous pouvez attribuer une valeur ici. Si c'est votre première fois à travailler avec l'API Chat Completions, nous devons définir le `role` et le `content` du message. \n",
    "\n",
    "Le `role` peut être soit `system` (création de règles), `assistant` (le modèle) ou `user` (l'utilisateur final). Pour l'appel de fonction, nous allons l'assigner en tant que `user` avec une question d'exemple. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de fonctions.\n",
    "\n",
    "Ensuite, nous allons définir une fonction et les paramètres de cette fonction. Nous utiliserons ici une seule fonction appelée `search_courses`, mais vous pouvez en créer plusieurs.\n",
    "\n",
    "**Important** : Les fonctions sont incluses dans le message système destiné au LLM et seront comptabilisées dans le nombre de tokens disponibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Définitions** \n",
    "\n",
    "La structure de définition de fonction comporte plusieurs niveaux, chacun avec ses propres propriétés. Voici une répartition de la structure imbriquée :\n",
    "\n",
    "**Propriétés de fonction de niveau supérieur :**\n",
    "\n",
    "`name` - Le nom de la fonction que nous souhaitons appeler. \n",
    "\n",
    "`description` - C'est la description du fonctionnement de la fonction. Il est important ici d'être précis et clair. \n",
    "\n",
    "`parameters` - Une liste de valeurs et de formats que vous souhaitez que le modèle produise dans sa réponse. \n",
    "\n",
    "**Propriétés de l'objet Parameters :**\n",
    "\n",
    "`type` - Le type de données de l'objet parameters (généralement \"object\")\n",
    "\n",
    "`properties` - Liste des valeurs spécifiques que le modèle utilisera pour sa réponse. \n",
    "\n",
    "**Propriétés des paramètres individuels :**\n",
    "\n",
    "`name` - Défini implicitement par la clé de la propriété (par exemple, \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Le type de données de ce paramètre spécifique (par exemple, \"string\", \"number\", \"boolean\") \n",
    "\n",
    "`description` - Description du paramètre spécifique. \n",
    "\n",
    "**Propriétés optionnelles :**\n",
    "\n",
    "`required` - Un tableau listant les paramètres requis pour que l'appel de fonction soit complété. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appeler la fonction  \n",
    "Après avoir défini une fonction, nous devons maintenant l’inclure dans l’appel à l’API Chat Completion. Nous le faisons en ajoutant `functions` à la requête. Dans ce cas, `functions=functions`.  \n",
    "\n",
    "Il y a aussi une option pour définir `function_call` sur `auto`. Cela signifie que nous laisserons le LLM décider quelle fonction doit être appelée en fonction du message de l’utilisateur plutôt que de l’assigner nous-mêmes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons maintenant la réponse et voyons comment elle est formatée :\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Vous pouvez voir que le nom de la fonction est appelé et qu'à partir du message de l'utilisateur, le LLM a pu trouver les données correspondant aux arguments de la fonction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Intégration des appels de fonction dans une application. \n",
    "\n",
    "\n",
    "Après avoir testé la réponse formatée du LLM, nous pouvons maintenant l'intégrer dans une application. \n",
    "\n",
    "### Gestion du flux \n",
    "\n",
    "Pour intégrer cela dans notre application, suivons les étapes suivantes : \n",
    "\n",
    "Tout d'abord, effectuons l'appel aux services OpenAI et stockons le message dans une variable appelée `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant définir la fonction qui appellera l'API Microsoft Learn pour obtenir une liste de cours :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme bonne pratique, nous verrons ensuite si le modèle souhaite appeler une fonction. Après cela, nous créerons l'une des fonctions disponibles et la ferons correspondre à la fonction qui est appelée.  \n",
    "Nous prendrons ensuite les arguments de la fonction et les mapperons aux arguments provenant du LLM.\n",
    "\n",
    "Enfin, nous ajouterons le message d'appel de fonction et les valeurs qui ont été retournées par le message `search_courses`. Cela donne au LLM toutes les informations dont il a besoin pour  \n",
    "répondre à l'utilisateur en utilisant un langage naturel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant envoyer le message mis à jour au LLM afin de recevoir une réponse en langage naturel au lieu d'une réponse formatée en JSON d'API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Défi de code \n",
    "\n",
    "Excellent travail ! Pour continuer votre apprentissage de l'appel de fonction OpenAI, vous pouvez créer : https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    " - Plus de paramètres de la fonction qui pourraient aider les apprenants à trouver plus de cours. Vous pouvez trouver les paramètres API disponibles ici :  \n",
    " - Créez un autre appel de fonction qui prend plus d'informations de l'apprenant, comme sa langue maternelle  \n",
    " - Créez une gestion des erreurs lorsque l'appel de fonction et/ou l'appel API ne renvoie aucun cours approprié\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Avertissement** :  \nCe document a été traduit à l’aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d’assurer l’exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d’origine doit être considéré comme la source faisant foi. Pour les informations critiques, une traduction professionnelle réalisée par un humain est recommandée. Nous déclinons toute responsabilité en cas de malentendus ou de mauvaises interprétations résultant de l’utilisation de cette traduction.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T08:37:21+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "fr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}