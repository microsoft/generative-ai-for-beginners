{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Cette leçon va couvrir : \n",
    "- Ce qu'est l'appel de fonction et dans quels cas l'utiliser \n",
    "- Comment créer un appel de fonction avec OpenAI \n",
    "- Comment intégrer un appel de fonction dans une application \n",
    "\n",
    "## Objectifs d'apprentissage \n",
    "\n",
    "Après avoir terminé cette leçon, vous saurez comment et comprendrez : \n",
    "\n",
    "- L'intérêt d'utiliser l'appel de fonction \n",
    "- Configurer un appel de fonction avec le service OpenAI \n",
    "- Concevoir des appels de fonction efficaces pour le cas d'usage de votre application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendre les appels de fonctions\n",
    "\n",
    "Pour cette leçon, nous voulons créer une fonctionnalité pour notre startup éducative qui permet aux utilisateurs d’utiliser un chatbot pour trouver des cours techniques. Nous recommanderons des cours adaptés à leur niveau de compétence, leur poste actuel et la technologie qui les intéresse.\n",
    "\n",
    "Pour réaliser cela, nous allons utiliser une combinaison de :\n",
    " - `OpenAI` pour créer une expérience de chat pour l’utilisateur\n",
    " - `Microsoft Learn Catalog API` pour aider les utilisateurs à trouver des cours selon leur demande\n",
    " - `Function Calling` pour prendre la requête de l’utilisateur et l’envoyer à une fonction afin de faire la requête API.\n",
    "\n",
    "Pour commencer, voyons pourquoi nous voudrions utiliser l’appel de fonction à la base :\n",
    "\n",
    "print(\"Messages dans la prochaine requête :\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # obtenir une nouvelle réponse de GPT où il peut voir la réponse de la fonction\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi l’appel de fonctions\n",
    "\n",
    "Si vous avez suivi une autre leçon de ce cours, vous comprenez probablement la puissance des grands modèles de langage (LLMs). Vous avez sans doute aussi remarqué certaines de leurs limites.\n",
    "\n",
    "L’appel de fonctions est une fonctionnalité du service OpenAI conçue pour répondre aux défis suivants :\n",
    "\n",
    "Formatage de réponse incohérent :\n",
    "- Avant l’appel de fonctions, les réponses d’un grand modèle de langage étaient non structurées et variables. Les développeurs devaient écrire du code de validation complexe pour gérer chaque variation dans la sortie.\n",
    "\n",
    "Intégration limitée avec des données externes :\n",
    "- Avant cette fonctionnalité, il était difficile d’intégrer des données provenant d’autres parties d’une application dans un contexte de chat.\n",
    "\n",
    "En standardisant les formats de réponse et en permettant une intégration fluide avec des données externes, l’appel de fonctions simplifie le développement et réduit le besoin de logique de validation supplémentaire.\n",
    "\n",
    "Les utilisateurs ne pouvaient pas obtenir de réponses comme « Quel temps fait-il actuellement à Stockholm ? ». En effet, les modèles étaient limités à la période sur laquelle les données avaient été entraînées.\n",
    "\n",
    "Regardons l’exemple ci-dessous qui illustre ce problème :\n",
    "\n",
    "Imaginons que nous souhaitions créer une base de données d’étudiants afin de leur proposer le cours le plus adapté. Ci-dessous, nous avons deux descriptions d’étudiants qui contiennent des informations très similaires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous souhaitons envoyer ceci à un LLM pour analyser les données. Cela pourra ensuite être utilisé dans notre application pour l'envoyer à une API ou le stocker dans une base de données.\n",
    "\n",
    "Créons deux invites identiques dans lesquelles nous indiquons au LLM quelles informations nous intéressent :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voulons envoyer ceci à un LLM pour qu'il analyse les parties importantes pour notre produit. Ainsi, nous pouvons créer deux invites identiques pour instruire le LLM :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir créé ces deux invites, nous les enverrons au LLM en utilisant `openai.ChatCompletion`. Nous stockons l'invite dans la variable `messages` et attribuons le rôle à `user`. Cela permet d'imiter un message d'un utilisateur écrit à un chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même si les invites sont identiques et que les descriptions se ressemblent, on peut obtenir différents formats pour la propriété `Grades`.\n",
    "\n",
    "Si vous exécutez la cellule ci-dessus plusieurs fois, le format peut être `3.7` ou `3.7 GPA`.\n",
    "\n",
    "Cela s’explique par le fait que le LLM prend des données non structurées sous forme d’invite écrite et renvoie également des données non structurées. Il nous faut donc un format structuré afin de savoir à quoi nous attendre lors du stockage ou de l’utilisation de ces données.\n",
    "\n",
    "En utilisant l’appel de fonctions, on peut s’assurer de recevoir des données structurées en retour. Lorsqu’on utilise l’appel de fonctions, le LLM n’exécute ni n’appelle réellement aucune fonction. À la place, on crée une structure que le LLM doit suivre pour ses réponses. On utilise ensuite ces réponses structurées pour savoir quelle fonction exécuter dans nos applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagramme de flux d'appel de fonction](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.fr.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas d’utilisation des appels de fonctions\n",
    "\n",
    "**Appeler des outils externes**  \n",
    "Les chatbots sont très efficaces pour répondre aux questions des utilisateurs. Grâce à l’appel de fonctions, les chatbots peuvent utiliser les messages des utilisateurs pour accomplir certaines tâches. Par exemple, un étudiant peut demander au chatbot : « Envoie un email à mon professeur pour lui dire que j’ai besoin de plus d’aide sur ce sujet ». Cela peut déclencher un appel à la fonction `send_email(to: string, body: string)`\n",
    "\n",
    "**Créer des requêtes API ou base de données**  \n",
    "Les utilisateurs peuvent rechercher des informations en langage naturel, qui sont ensuite transformées en requête formatée ou en appel API. Par exemple, un enseignant peut demander : « Qui sont les étudiants qui ont terminé le dernier devoir ? », ce qui peut appeler une fonction nommée `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Créer des données structurées**  \n",
    "Les utilisateurs peuvent prendre un bloc de texte ou un fichier CSV et utiliser le LLM pour en extraire les informations importantes. Par exemple, un étudiant peut transformer un article Wikipédia sur des accords de paix pour créer des fiches de révision avec l’IA. Cela peut se faire en utilisant une fonction appelée `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Créer votre premier appel de fonction\n",
    "\n",
    "Le processus de création d’un appel de fonction comprend 3 étapes principales :\n",
    "1. Appeler l’API Chat Completions avec une liste de vos fonctions et un message utilisateur\n",
    "2. Lire la réponse du modèle pour effectuer une action, c’est-à-dire exécuter une fonction ou un appel d’API\n",
    "3. Faire un autre appel à l’API Chat Completions avec la réponse de votre fonction afin d’utiliser cette information pour créer une réponse à l’utilisateur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flux d'un appel de fonction](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.fr.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Éléments d’un appel de fonction\n",
    "\n",
    "#### Saisie de l’utilisateur\n",
    "\n",
    "La première étape consiste à créer un message utilisateur. Cela peut être attribué dynamiquement en récupérant la valeur d’un champ de saisie texte, ou vous pouvez définir une valeur ici. Si c’est la première fois que vous travaillez avec l’API Chat Completions, il faut définir le `role` et le `content` du message.\n",
    "\n",
    "Le `role` peut être soit `system` (pour définir des règles), `assistant` (le modèle) ou `user` (l’utilisateur final). Pour l’appel de fonction, nous allons choisir `user` et donner un exemple de question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de fonctions.\n",
    "\n",
    "Nous allons maintenant définir une fonction ainsi que ses paramètres. Nous allons utiliser ici une seule fonction appelée `search_courses`, mais vous pouvez en créer plusieurs.\n",
    "\n",
    "**Important** : Les fonctions sont incluses dans le message système envoyé au LLM et seront comptabilisées dans le nombre de tokens disponibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Définitions**\n",
    "\n",
    "La structure de définition de fonction comporte plusieurs niveaux, chacun avec ses propres propriétés. Voici un aperçu de la structure imbriquée :\n",
    "\n",
    "**Propriétés de la fonction au niveau supérieur :**\n",
    "\n",
    "`name` - Le nom de la fonction que l’on souhaite appeler.\n",
    "\n",
    "`description` - Il s’agit de la description du fonctionnement de la fonction. Il est important d’être précis et clair ici.\n",
    "\n",
    "`parameters` - Une liste des valeurs et du format que vous souhaitez que le modèle produise dans sa réponse.\n",
    "\n",
    "**Propriétés de l’objet Parameters :**\n",
    "\n",
    "`type` - Le type de données de l’objet parameters (généralement « object »)\n",
    "\n",
    "`properties` - Liste des valeurs spécifiques que le modèle utilisera pour sa réponse\n",
    "\n",
    "**Propriétés de chaque paramètre :**\n",
    "\n",
    "`name` - Défini implicitement par la clé de propriété (par exemple, « role », « product », « level »)\n",
    "\n",
    "`type` - Le type de données de ce paramètre spécifique (par exemple, « string », « number », « boolean »)\n",
    "\n",
    "`description` - Description du paramètre spécifique\n",
    "\n",
    "**Propriétés optionnelles :**\n",
    "\n",
    "`required` - Un tableau qui liste les paramètres nécessaires pour que l’appel de la fonction soit complété\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appeler la fonction\n",
    "Après avoir défini une fonction, il faut maintenant l’inclure dans l’appel à l’API Chat Completion. Pour cela, on ajoute `functions` à la requête. Dans ce cas, on utilise `functions=functions`.\n",
    "\n",
    "Il est aussi possible de définir `function_call` sur `auto`. Cela signifie que l’on laisse le LLM choisir quelle fonction appeler en fonction du message de l’utilisateur, au lieu de le faire nous-mêmes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons maintenant la réponse et examinons son format :\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Vous pouvez voir que le nom de la fonction est appelé et, à partir du message de l'utilisateur, le LLM a pu trouver les données pour remplir les arguments de la fonction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Intégrer les appels de fonctions dans une application.\n",
    "\n",
    "Après avoir testé la réponse formatée du LLM, nous pouvons maintenant l’intégrer dans une application.\n",
    "\n",
    "### Gérer le flux\n",
    "\n",
    "Pour intégrer cela dans notre application, suivons les étapes suivantes :\n",
    "\n",
    "Tout d’abord, faisons l’appel aux services OpenAI et stockons le message dans une variable appelée `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant définir la fonction qui appellera l’API Microsoft Learn pour obtenir une liste de cours :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En tant que bonne pratique, nous allons ensuite vérifier si le modèle souhaite appeler une fonction. Ensuite, nous créerons l’une des fonctions disponibles et l’associerons à la fonction appelée.  \n",
    "Nous prendrons ensuite les arguments de la fonction et les mapperons aux arguments provenant du LLM.\n",
    "\n",
    "Enfin, nous ajouterons le message d’appel de fonction ainsi que les valeurs retournées par le message `search_courses`. Cela donne au LLM toutes les informations nécessaires pour\n",
    "répondre à l’utilisateur en langage naturel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Défi de code\n",
    "\n",
    "Bravo ! Pour approfondir votre apprentissage sur l’appel de fonctions OpenAI, vous pouvez créer : https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Plus de paramètres pour la fonction afin d’aider les apprenants à trouver davantage de cours. Vous pouvez consulter les paramètres disponibles de l’API ici :\n",
    " - Créez un autre appel de fonction qui prend en compte plus d’informations sur l’apprenant, comme sa langue maternelle\n",
    " - Mettez en place une gestion des erreurs lorsque l’appel de fonction et/ou l’appel à l’API ne retourne aucun cours pertinent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Avertissement** :  \nCe document a été traduit à l’aide du service de traduction par IA [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d’assurer l’exactitude de la traduction, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d’origine doit être considéré comme la source faisant autorité. Pour toute information critique, il est recommandé de recourir à une traduction humaine professionnelle. Nous déclinons toute responsabilité en cas de malentendus ou d’interprétations erronées résultant de l’utilisation de cette traduction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T20:40:11+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "fr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}