<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:08:59+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "fr"
}
-->
# S√©curiser vos applications d‚ÄôIA g√©n√©rative

[![S√©curiser vos applications d‚ÄôIA g√©n√©rative](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.fr.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Introduction

Cette le√ßon abordera :

- La s√©curit√© dans le contexte des syst√®mes d‚ÄôIA.
- Les risques et menaces courants pour les syst√®mes d‚ÄôIA.
- Les m√©thodes et consid√©rations pour s√©curiser les syst√®mes d‚ÄôIA.

## Objectifs d‚Äôapprentissage

√Ä l‚Äôissue de cette le√ßon, vous comprendrez :

- Les menaces et risques pesant sur les syst√®mes d‚ÄôIA.
- Les m√©thodes et bonnes pratiques courantes pour s√©curiser les syst√®mes d‚ÄôIA.
- Comment la mise en place de tests de s√©curit√© peut pr√©venir des r√©sultats inattendus et pr√©server la confiance des utilisateurs.

## Que signifie la s√©curit√© dans le contexte de l‚ÄôIA g√©n√©rative ?

√Ä mesure que les technologies d‚ÄôIntelligence Artificielle (IA) et d‚ÄôApprentissage Automatique (ML) fa√ßonnent de plus en plus notre quotidien, il est essentiel de prot√©ger non seulement les donn√©es clients, mais aussi les syst√®mes d‚ÄôIA eux-m√™mes. L‚ÄôIA/ML est de plus en plus utilis√©e pour soutenir des processus d√©cisionnels √† forte valeur ajout√©e dans des secteurs o√π une mauvaise d√©cision peut avoir de graves cons√©quences.

Voici les points cl√©s √† retenir :

- **Impact de l‚ÄôIA/ML** : L‚ÄôIA/ML a un impact important sur la vie quotidienne, ce qui rend leur protection indispensable.
- **D√©fis de s√©curit√©** : Cet impact n√©cessite une attention particuli√®re pour prot√©ger les produits bas√©s sur l‚ÄôIA contre des attaques sophistiqu√©es, qu‚Äôelles proviennent de trolls ou de groupes organis√©s.
- **Probl√®mes strat√©giques** : L‚Äôindustrie technologique doit anticiper ces d√©fis strat√©giques pour garantir la s√©curit√© √† long terme des clients et la protection des donn√©es.

De plus, les mod√®les d‚Äôapprentissage automatique ont souvent du mal √† distinguer les entr√©es malveillantes des donn√©es anormales mais inoffensives. Une grande partie des donn√©es d‚Äôentra√Ænement provient de jeux de donn√©es publics non filtr√©s et non mod√©r√©s, ouverts aux contributions tierces. Les attaquants n‚Äôont pas besoin de compromettre ces jeux de donn√©es, ils peuvent simplement y contribuer. Avec le temps, des donn√©es malveillantes peu fiables deviennent des donn√©es de confiance si leur structure ou format reste correct.

C‚Äôest pourquoi il est crucial de garantir l‚Äôint√©grit√© et la protection des bases de donn√©es utilis√©es par vos mod√®les pour prendre des d√©cisions.

## Comprendre les menaces et risques li√©s √† l‚ÄôIA

En mati√®re d‚ÄôIA et de syst√®mes associ√©s, l‚Äôempoisonnement des donn√©es est aujourd‚Äôhui la menace de s√©curit√© la plus importante. L‚Äôempoisonnement des donn√©es consiste √† modifier intentionnellement les informations utilis√©es pour entra√Æner une IA, ce qui la pousse √† faire des erreurs. Cela s‚Äôexplique par l‚Äôabsence de m√©thodes standardis√©es de d√©tection et d‚Äôatt√©nuation, ainsi que par notre d√©pendance √† des jeux de donn√©es publics non fiables ou non filtr√©s pour l‚Äôentra√Ænement. Pour maintenir l‚Äôint√©grit√© des donn√©es et √©viter un processus d‚Äôentra√Ænement d√©faillant, il est crucial de suivre l‚Äôorigine et la tra√ßabilit√© de vos donn√©es. Sinon, le vieil adage ¬´ garbage in, garbage out ¬ª s‚Äôapplique, entra√Ænant une d√©gradation des performances du mod√®le.

Voici des exemples d‚Äôimpact de l‚Äôempoisonnement des donn√©es sur vos mod√®les :

1. **Inversion d‚Äô√©tiquettes** : Dans une t√¢che de classification binaire, un adversaire inverse intentionnellement les √©tiquettes d‚Äôun petit sous-ensemble de donn√©es d‚Äôentra√Ænement. Par exemple, des √©chantillons b√©nins sont √©tiquet√©s comme malveillants, ce qui conduit le mod√®le √† apprendre de mauvaises associations.\
   **Exemple** : Un filtre anti-spam classant √† tort des emails l√©gitimes comme spam √† cause d‚Äô√©tiquettes manipul√©es.
2. **Empoisonnement des caract√©ristiques** : Un attaquant modifie subtilement des caract√©ristiques dans les donn√©es d‚Äôentra√Ænement pour introduire un biais ou tromper le mod√®le.\
   **Exemple** : Ajouter des mots-cl√©s hors sujet dans des descriptions de produits pour manipuler les syst√®mes de recommandation.
3. **Injection de donn√©es** : Injection de donn√©es malveillantes dans le jeu d‚Äôentra√Ænement pour influencer le comportement du mod√®le.\
   **Exemple** : Introduire de faux avis utilisateurs pour fausser les r√©sultats d‚Äôanalyse de sentiment.
4. **Attaques par porte d√©rob√©e** : Un adversaire ins√®re un motif cach√© (porte d√©rob√©e) dans les donn√©es d‚Äôentra√Ænement. Le mod√®le apprend √† reconna√Ætre ce motif et agit de mani√®re malveillante lorsqu‚Äôil est d√©clench√©.\
   **Exemple** : Un syst√®me de reconnaissance faciale entra√Æn√© avec des images contenant une porte d√©rob√©e qui identifie mal une personne sp√©cifique.

La soci√©t√© MITRE a cr√©√© [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), une base de connaissances des tactiques et techniques utilis√©es par les adversaires dans des attaques r√©elles contre les syst√®mes d‚ÄôIA.

> Le nombre de vuln√©rabilit√©s dans les syst√®mes int√©grant l‚ÄôIA est en augmentation, car l‚Äôint√©gration de l‚ÄôIA √©largit la surface d‚Äôattaque des syst√®mes existants au-del√† des cyberattaques traditionnelles. Nous avons d√©velopp√© ATLAS pour sensibiliser √† ces vuln√©rabilit√©s uniques et √©volutives, alors que la communaut√© mondiale int√®gre de plus en plus l‚ÄôIA dans divers syst√®mes. ATLAS est calqu√© sur le framework MITRE ATT&CK¬Æ et ses tactiques, techniques et proc√©dures (TTP) compl√®tent celles d‚ÄôATT&CK.

√Ä l‚Äôinstar du framework MITRE ATT&CK¬Æ, largement utilis√© en cybers√©curit√© traditionnelle pour planifier des sc√©narios d‚Äô√©mulation de menaces avanc√©es, ATLAS fournit un ensemble de TTP facilement consultables pour mieux comprendre et se pr√©parer √† d√©fendre contre les attaques √©mergentes.

Par ailleurs, l‚ÄôOpen Web Application Security Project (OWASP) a √©tabli une "[liste Top 10](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" des vuln√©rabilit√©s les plus critiques dans les applications utilisant des LLM. Cette liste met en lumi√®re les risques li√©s √† des menaces telles que l‚Äôempoisonnement des donn√©es mentionn√© pr√©c√©demment, ainsi que d‚Äôautres comme :

- **Injection de prompt** : une technique o√π les attaquants manipulent un Large Language Model (LLM) via des entr√©es soigneusement con√ßues, le poussant √† agir en dehors de son comportement pr√©vu.
- **Vuln√©rabilit√©s de la cha√Æne d‚Äôapprovisionnement** : Les composants et logiciels constituant les applications utilis√©es par un LLM, comme des modules Python ou des jeux de donn√©es externes, peuvent √™tre compromis, entra√Ænant des r√©sultats inattendus, des biais introduits, voire des vuln√©rabilit√©s dans l‚Äôinfrastructure sous-jacente.
- **D√©pendance excessive** : Les LLM sont faillibles et ont tendance √† halluciner, fournissant des r√©sultats inexacts ou dangereux. Dans plusieurs cas document√©s, des personnes ont pris ces r√©sultats pour argent comptant, ce qui a conduit √† des cons√©quences n√©gatives r√©elles non intentionnelles.

Rod Trent, Microsoft Cloud Advocate, a √©crit un ebook gratuit, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), qui explore en profondeur ces menaces √©mergentes et offre des conseils d√©taill√©s pour y faire face au mieux.

## Tests de s√©curit√© pour les syst√®mes d‚ÄôIA et les LLM

L‚Äôintelligence artificielle (IA) transforme de nombreux domaines et industries, offrant de nouvelles possibilit√©s et avantages pour la soci√©t√©. Cependant, l‚ÄôIA pose aussi des d√©fis et risques importants, tels que la confidentialit√© des donn√©es, les biais, le manque d‚Äôexplicabilit√© et les usages abusifs potentiels. Il est donc crucial de garantir que les syst√®mes d‚ÄôIA soient s√©curis√©s et responsables, c‚Äôest-√†-dire qu‚Äôils respectent des normes √©thiques et l√©gales et inspirent confiance aux utilisateurs et parties prenantes.

Le test de s√©curit√© consiste √† √©valuer la s√©curit√© d‚Äôun syst√®me d‚ÄôIA ou d‚Äôun LLM en identifiant et exploitant ses vuln√©rabilit√©s. Il peut √™tre r√©alis√© par des d√©veloppeurs, des utilisateurs ou des auditeurs tiers, selon l‚Äôobjectif et le p√©rim√®tre du test. Parmi les m√©thodes de test de s√©curit√© les plus courantes pour les syst√®mes d‚ÄôIA et les LLM, on trouve :

- **Assainissement des donn√©es** : processus de suppression ou d‚Äôanonymisation des informations sensibles ou priv√©es dans les donn√©es d‚Äôentra√Ænement ou les entr√©es d‚Äôun syst√®me d‚ÄôIA ou LLM. L‚Äôassainissement des donn√©es aide √† pr√©venir les fuites et manipulations malveillantes en r√©duisant l‚Äôexposition des donn√©es confidentielles ou personnelles.
- **Tests adversariaux** : g√©n√©ration et application d‚Äôexemples adversariaux aux entr√©es ou sorties d‚Äôun syst√®me d‚ÄôIA ou LLM pour √©valuer sa robustesse et sa r√©silience face aux attaques adversariales. Ces tests permettent d‚Äôidentifier et d‚Äôatt√©nuer les vuln√©rabilit√©s exploitables par des attaquants.
- **V√©rification du mod√®le** : processus de v√©rification de la justesse et de la compl√©tude des param√®tres ou de l‚Äôarchitecture d‚Äôun mod√®le d‚ÄôIA ou LLM. La v√©rification du mod√®le aide √† d√©tecter et pr√©venir le vol de mod√®le en garantissant sa protection et son authentification.
- **Validation des sorties** : processus de validation de la qualit√© et de la fiabilit√© des r√©sultats produits par un syst√®me d‚ÄôIA ou LLM. La validation des sorties permet de d√©tecter et corriger les manipulations malveillantes en assurant la coh√©rence et l‚Äôexactitude des r√©sultats.

OpenAI, leader dans les syst√®mes d‚ÄôIA, a mis en place une s√©rie d‚Äô_√©valuations de s√©curit√©_ dans le cadre de leur initiative de red teaming, visant √† tester les sorties des syst√®mes d‚ÄôIA pour contribuer √† la s√©curit√© de l‚ÄôIA.

> Les √©valuations peuvent aller de simples questions-r√©ponses √† des simulations plus complexes. Voici quelques exemples concrets d‚Äô√©valuations d√©velopp√©es par OpenAI pour analyser les comportements d‚ÄôIA sous diff√©rents angles :

#### Persuasion

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst) : Quelle est la capacit√© d‚Äôun syst√®me d‚ÄôIA √† tromper un autre syst√®me d‚ÄôIA pour qu‚Äôil prononce un mot secret ?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst) : Quelle est la capacit√© d‚Äôun syst√®me d‚ÄôIA √† convaincre un autre syst√®me d‚ÄôIA de faire un don d‚Äôargent ?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst) : Quelle est la capacit√© d‚Äôun syst√®me d‚ÄôIA √† influencer le soutien d‚Äôun autre syst√®me d‚ÄôIA √† une proposition politique ?

#### St√©ganographie (messagerie cach√©e)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst) : Quelle est la capacit√© d‚Äôun syst√®me d‚ÄôIA √† transmettre des messages secrets sans √™tre d√©tect√© par un autre syst√®me d‚ÄôIA ?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst) : Quelle est la capacit√© d‚Äôun syst√®me d‚ÄôIA √† compresser et d√©compresser des messages pour permettre la dissimulation de messages secrets ?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst) : Quelle est la capacit√© d‚Äôun syst√®me d‚ÄôIA √† se coordonner avec un autre syst√®me d‚ÄôIA sans communication directe ?

### S√©curit√© de l‚ÄôIA

Il est imp√©ratif de prot√©ger les syst√®mes d‚ÄôIA contre les attaques malveillantes, les usages abusifs ou les cons√©quences non intentionnelles. Cela inclut de prendre des mesures pour garantir la s√©curit√©, la fiabilit√© et la confiance dans les syst√®mes d‚ÄôIA, telles que :

- S√©curiser les donn√©es et algorithmes utilis√©s pour entra√Æner et faire fonctionner les mod√®les d‚ÄôIA
- Pr√©venir les acc√®s non autoris√©s, manipulations ou sabotages des syst√®mes d‚ÄôIA
- D√©tecter et att√©nuer les biais, discriminations ou probl√®mes √©thiques dans les syst√®mes d‚ÄôIA
- Assurer la responsabilit√©, la transparence et l‚Äôexplicabilit√© des d√©cisions et actions de l‚ÄôIA
- Aligner les objectifs et valeurs des syst√®mes d‚ÄôIA avec ceux des humains et de la soci√©t√©

La s√©curit√© de l‚ÄôIA est essentielle pour garantir l‚Äôint√©grit√©, la disponibilit√© et la confidentialit√© des syst√®mes et donn√©es d‚ÄôIA. Parmi les d√©fis et opportunit√©s de la s√©curit√© de l‚ÄôIA :

- Opportunit√© : Int√©grer l‚ÄôIA dans les strat√©gies de cybers√©curit√©, car elle peut jouer un r√¥le cl√© dans l‚Äôidentification des menaces et l‚Äôam√©lioration des temps de r√©ponse. L‚ÄôIA peut aider √† automatiser et renforcer la d√©tection et l‚Äôatt√©nuation des cyberattaques, telles que le phishing, les malwares ou les ransomwares.
- D√©fi : L‚ÄôIA peut aussi √™tre utilis√©e par des adversaires pour lancer des attaques sophistiqu√©es, comme g√©n√©rer du contenu faux ou trompeur, usurper des utilisateurs ou exploiter des vuln√©rabilit√©s dans les syst√®mes d‚ÄôIA. Les d√©veloppeurs d‚ÄôIA ont donc une responsabilit√© particuli√®re de concevoir des syst√®mes robustes et r√©silients face aux usages abusifs.

### Protection des donn√©es

Les LLM peuvent pr√©senter des risques pour la confidentialit√© et la s√©curit√© des donn√©es qu‚Äôils utilisent. Par exemple, ils peuvent m√©moriser et divulguer des informations sensibles issues de leurs donn√©es d‚Äôentra√Ænement, comme des noms personnels, adresses, mots de passe ou num√©ros de carte bancaire. Ils peuvent aussi √™tre manipul√©s ou attaqu√©s par des acteurs malveillants cherchant √† exploiter leurs vuln√©rabilit√©s ou biais. Il est donc important de conna√Ætre ces risques et de prendre des mesures appropri√©es pour prot√©ger les donn√©es utilis√©es avec les LLM. Voici quelques √©tapes √† suivre pour prot√©ger ces donn√©es :

- **Limiter la quantit√© et le type de donn√©es partag√©es avec les LLM** : Ne partager que les donn√©es n√©cessaires et pertinentes pour les usages pr√©vus, et √©viter de transmettre des donn√©es sensibles, confidentielles ou personnelles. Les utilisateurs doivent aussi anonymiser ou chiffrer les donn√©es partag√©es, par exemple en supprimant ou masquant toute information identifiable, ou en utilisant des canaux de communication s√©curis√©s.
- **V√©rifier les donn√©es g√©n√©r√©es par les LLM** : Toujours contr√¥ler la pr√©cision et la qualit√© des r√©sultats produits par les LLM pour s‚Äôassurer qu‚Äôils ne contiennent pas d‚Äôinformations ind√©sirables ou inappropri√©es.
- **Signaler et alerter en cas de violation ou incident de donn√©es** : √ätre vigilant face √† toute activit√© ou comportement suspect ou anormal des LLM, comme la g√©n√©ration de textes hors sujet, inexacts, offensants ou nuisibles. Cela peut indiquer une violation ou un incident de s√©curit√©.

La s√©curit√©, la gouvernance et la conformit√© des donn√©es sont essentielles pour toute organisation souhaitant exploiter la puissance des donn√©es et de l‚ÄôIA dans un environnement multi-cloud. S√©curiser et gouverner toutes vos donn√©es est une t√¢che complexe et multifacette. Il faut prot√©ger et g√©rer diff√©rents types de donn√©es (structur√©es, non structur√©es et g√©n√©r√©es par l‚ÄôIA) situ√©es √† divers endroits sur plusieurs clouds, tout en tenant compte des r√©glementations actuelles et futures en mati√®re de s√©curit√©, gouvernance et IA. Pour prot√©ger vos donn√©es, il est recommand√© d‚Äôadopter certaines bonnes pratiques et pr√©cautions, telles que :

- Utiliser des services ou plateformes cloud offrant des fonctionnalit√©s de protection et de confidentialit√© des donn√©es.
- Employer des outils de qualit√© et de validation des donn√©es pour d√©tecter erreurs, incoh√©rences ou anomalies.
- Appliquer des cadres de gouvernance et d‚Äô√©thique des donn√©es pour garantir un usage responsable et transparent.

### √âmuler les menaces r√©elles - red teaming IA

L‚Äô√©mulation des menaces r√©elles est d√©sormais une pratique standard pour construire des syst√®mes d‚ÄôIA r√©silients, en utilisant des outils, tactiques et proc√©dures similaires pour identifier les risques pesant sur les syst√®mes et tester la r√©action des d√©fenseurs.
> La pratique du red teaming en IA a √©volu√© pour prendre une signification plus large : elle ne se limite pas √† la recherche de vuln√©rabilit√©s de s√©curit√©, mais inclut √©galement la d√©tection d'autres d√©faillances du syst√®me, comme la g√©n√©ration de contenus potentiellement nuisibles. Les syst√®mes d'IA comportent de nouveaux risques, et le red teaming est essentiel pour comprendre ces risques in√©dits, tels que l'injection de prompt et la production de contenus non fond√©s. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.fr.png)]()

Voici les principaux enseignements qui ont fa√ßonn√© le programme AI Red Team de Microsoft.

1. **Port√©e √©tendue du red teaming en IA :**  
   Le red teaming en IA englobe d√©sormais √† la fois les aspects de s√©curit√© et les r√©sultats li√©s √† l‚ÄôIA responsable (RAI). Traditionnellement, le red teaming se concentrait sur la s√©curit√©, en consid√©rant le mod√®le comme un vecteur (par exemple, le vol du mod√®le sous-jacent). Cependant, les syst√®mes d‚ÄôIA introduisent de nouvelles vuln√©rabilit√©s de s√©curit√© (par exemple, l‚Äôinjection de prompt, l‚Äôempoisonnement), n√©cessitant une attention particuli√®re. Au-del√† de la s√©curit√©, le red teaming en IA explore aussi les questions d‚Äô√©quit√© (par exemple, les st√©r√©otypes) et les contenus nuisibles (par exemple, la glorification de la violence). Identifier ces probl√®mes t√¥t permet de prioriser les investissements en d√©fense.

2. **√âchecs malveillants et b√©nins :**  
   Le red teaming en IA prend en compte les √©checs tant du point de vue malveillant que b√©nin. Par exemple, lors du red teaming du nouveau Bing, nous examinons non seulement comment des adversaires malveillants peuvent subvertir le syst√®me, mais aussi comment des utilisateurs ordinaires peuvent rencontrer des contenus probl√©matiques ou nuisibles. Contrairement au red teaming traditionnel ax√© principalement sur les acteurs malveillants, le red teaming en IA consid√®re une gamme plus large de profils et de d√©faillances potentielles.

3. **Nature dynamique des syst√®mes d‚ÄôIA :**  
   Les applications d‚ÄôIA √©voluent constamment. Dans les applications bas√©es sur de grands mod√®les de langage, les d√©veloppeurs s‚Äôadaptent aux exigences changeantes. Le red teaming continu garantit une vigilance constante et une adaptation aux risques en √©volution.

Le red teaming en IA n‚Äôest pas exhaustif et doit √™tre consid√©r√© comme un compl√©ment √† d‚Äôautres contr√¥les tels que le [contr√¥le d‚Äôacc√®s bas√© sur les r√¥les (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) et des solutions compl√®tes de gestion des donn√©es. Il vise √† renforcer une strat√©gie de s√©curit√© qui privil√©gie des solutions d‚ÄôIA s√ªres et responsables, prenant en compte la confidentialit√© et la s√©curit√© tout en cherchant √† minimiser les biais, les contenus nuisibles et la d√©sinformation susceptibles d‚Äô√©roder la confiance des utilisateurs.

Voici une liste de lectures compl√©mentaires pour mieux comprendre comment le red teaming peut aider √† identifier et att√©nuer les risques dans vos syst√®mes d‚ÄôIA :

- [Planifier le red teaming pour les grands mod√®les de langage (LLM) et leurs applications](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [Qu‚Äôest-ce que le OpenAI Red Teaming Network ?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming - Une pratique cl√© pour construire des solutions d‚ÄôIA plus s√ªres et responsables](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), une base de connaissances des tactiques et techniques utilis√©es par les adversaires dans des attaques r√©elles contre des syst√®mes d‚ÄôIA.

## V√©rification des connaissances

Quelle pourrait √™tre une bonne approche pour maintenir l‚Äôint√©grit√© des donn√©es et pr√©venir les usages abusifs ?

1. Mettre en place des contr√¥les d‚Äôacc√®s bas√©s sur les r√¥les solides pour la gestion et l‚Äôacc√®s aux donn√©es  
1. Impl√©menter et auditer l‚Äô√©tiquetage des donn√©es pour √©viter la mauvaise repr√©sentation ou l‚Äôusage abusif des donn√©es  
1. S‚Äôassurer que votre infrastructure IA prend en charge le filtrage de contenu

R : 1, Bien que les trois recommandations soient excellentes, veiller √† attribuer les bons privil√®ges d‚Äôacc√®s aux donn√©es aux utilisateurs est un levier majeur pour pr√©venir la manipulation et la mauvaise repr√©sentation des donn√©es utilis√©es par les LLM.

## üöÄ D√©fi

Approfondissez vos connaissances sur la mani√®re de [gouverner et prot√©ger les informations sensibles](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) √† l‚Äô√®re de l‚ÄôIA.

## Excellent travail, continuez votre apprentissage

Apr√®s avoir termin√© cette le√ßon, consultez notre [collection d‚Äôapprentissage sur l‚ÄôIA g√©n√©rative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer √† approfondir vos connaissances en IA g√©n√©rative !

Rendez-vous √† la le√ßon 14 o√π nous aborderons [le cycle de vie des applications d‚ÄôIA g√©n√©rative](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst) !

**Avertissement** :  
Ce document a √©t√© traduit √† l‚Äôaide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d‚Äôassurer l‚Äôexactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d‚Äôorigine doit √™tre consid√©r√© comme la source faisant foi. Pour les informations critiques, une traduction professionnelle r√©alis√©e par un humain est recommand√©e. Nous d√©clinons toute responsabilit√© en cas de malentendus ou de mauvaises interpr√©tations r√©sultant de l‚Äôutilisation de cette traduction.