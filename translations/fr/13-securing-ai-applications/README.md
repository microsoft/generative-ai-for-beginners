<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-05-19T22:10:31+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "fr"
}
-->
# S√©curiser vos applications d'IA g√©n√©rative

## Introduction

Cette le√ßon couvrira :

- La s√©curit√© dans le contexte des syst√®mes d'IA.
- Les risques et menaces courants pour les syst√®mes d'IA.
- M√©thodes et consid√©rations pour s√©curiser les syst√®mes d'IA.

## Objectifs d'apprentissage

Apr√®s avoir termin√© cette le√ßon, vous comprendrez :

- Les menaces et risques pour les syst√®mes d'IA.
- Les m√©thodes et pratiques courantes pour s√©curiser les syst√®mes d'IA.
- Comment la mise en ≈ìuvre de tests de s√©curit√© peut pr√©venir des r√©sultats inattendus et l'√©rosion de la confiance des utilisateurs.

## Que signifie la s√©curit√© dans le contexte de l'IA g√©n√©rative ?

Alors que les technologies d'Intelligence Artificielle (IA) et d'Apprentissage Machine (ML) fa√ßonnent de plus en plus nos vies, il est crucial de prot√©ger non seulement les donn√©es des clients mais aussi les syst√®mes d'IA eux-m√™mes. L'IA/ML est de plus en plus utilis√©e pour soutenir des processus d√©cisionnels de grande valeur dans des industries o√π une mauvaise d√©cision peut avoir de graves cons√©quences.

Voici des points cl√©s √† consid√©rer :

- **Impact de l'IA/ML** : L'IA/ML a des impacts significatifs sur la vie quotidienne et il est donc essentiel de les prot√©ger.
- **D√©fis de s√©curit√©** : Cet impact n√©cessite une attention appropri√©e pour prot√©ger les produits bas√©s sur l'IA contre des attaques sophistiqu√©es, qu'elles proviennent de trolls ou de groupes organis√©s.
- **Probl√®mes strat√©giques** : L'industrie technologique doit aborder de mani√®re proactive les d√©fis strat√©giques pour assurer la s√©curit√© √† long terme des clients et la s√©curit√© des donn√©es.

De plus, les mod√®les d'apprentissage machine sont en grande partie incapables de discerner entre les entr√©es malveillantes et les donn√©es anormales b√©nignes. Une source importante de donn√©es d'entra√Ænement provient de jeux de donn√©es publics non mod√©r√©s, ouverts aux contributions de tiers. Les attaquants n'ont pas besoin de compromettre les jeux de donn√©es lorsqu'ils peuvent y contribuer librement. Avec le temps, les donn√©es malveillantes √† faible confiance deviennent des donn√©es de haute confiance, si la structure/le format des donn√©es reste correct.

C'est pourquoi il est crucial de garantir l'int√©grit√© et la protection des magasins de donn√©es que vos mod√®les utilisent pour prendre des d√©cisions.

## Comprendre les menaces et les risques de l'IA

En termes de syst√®mes d'IA et connexes, l'empoisonnement des donn√©es se distingue comme la menace de s√©curit√© la plus importante aujourd'hui. L'empoisonnement des donn√©es se produit lorsque quelqu'un modifie intentionnellement les informations utilis√©es pour entra√Æner une IA, provoquant ainsi des erreurs. Cela est d√ª √† l'absence de m√©thodes standardis√©es de d√©tection et d'att√©nuation, combin√©e √† notre d√©pendance √† l'√©gard de jeux de donn√©es publics non fiables ou non mod√©r√©s pour l'entra√Ænement. Pour maintenir l'int√©grit√© des donn√©es et √©viter un processus d'entra√Ænement d√©fectueux, il est crucial de suivre l'origine et la lign√©e de vos donn√©es. Sinon, le vieil adage "garbage in, garbage out" s'applique, entra√Ænant une performance compromise du mod√®le.

Voici des exemples de la mani√®re dont l'empoisonnement des donn√©es peut affecter vos mod√®les :

1. **Renversement d'√©tiquettes** : Dans une t√¢che de classification binaire, un adversaire renverse intentionnellement les √©tiquettes d'un petit sous-ensemble de donn√©es d'entra√Ænement. Par exemple, des √©chantillons b√©nins sont √©tiquet√©s comme malveillants, amenant le mod√®le √† apprendre des associations incorrectes.\
   **Exemple** : Un filtre anti-spam classant √† tort des e-mails l√©gitimes comme spam en raison d'√©tiquettes manipul√©es.
2. **Empoisonnement de caract√©ristiques** : Un attaquant modifie subtilement les caract√©ristiques des donn√©es d'entra√Ænement pour introduire un biais ou tromper le mod√®le.\
   **Exemple** : Ajouter des mots-cl√©s non pertinents aux descriptions de produits pour manipuler les syst√®mes de recommandation.
3. **Injection de donn√©es** : Injecter des donn√©es malveillantes dans le jeu d'entra√Ænement pour influencer le comportement du mod√®le.\
   **Exemple** : Introduire de faux avis d'utilisateurs pour fausser les r√©sultats d'analyse de sentiment.
4. **Attaques par porte d√©rob√©e** : Un adversaire ins√®re un motif cach√© (porte d√©rob√©e) dans les donn√©es d'entra√Ænement. Le mod√®le apprend √† reconna√Ætre ce motif et adopte un comportement malveillant lorsqu'il est d√©clench√©.\
   **Exemple** : Un syst√®me de reconnaissance faciale entra√Æn√© avec des images contenant des portes d√©rob√©es qui identifie √† tort une personne sp√©cifique.

La MITRE Corporation a cr√©√© [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), une base de connaissances des tactiques et techniques employ√©es par les adversaires dans des attaques r√©elles sur des syst√®mes d'IA.

> Il existe un nombre croissant de vuln√©rabilit√©s dans les syst√®mes dot√©s d'IA, car l'incorporation de l'IA augmente la surface d'attaque des syst√®mes existants au-del√† de celles des cyberattaques traditionnelles. Nous avons d√©velopp√© ATLAS pour sensibiliser √† ces vuln√©rabilit√©s uniques et en √©volution, alors que la communaut√© mondiale int√®gre de plus en plus l'IA dans divers syst√®mes. ATLAS est model√© d'apr√®s le cadre MITRE ATT&CK¬Æ et ses tactiques, techniques et proc√©dures (TTPs) sont compl√©mentaires de celles de ATT&CK.

Tout comme le cadre MITRE ATT&CK¬Æ, largement utilis√© dans la cybers√©curit√© traditionnelle pour planifier des sc√©narios d'√©mulation de menaces avanc√©es, ATLAS fournit un ensemble de TTPs facilement consultable qui peut aider √† mieux comprendre et se pr√©parer √† d√©fendre contre les attaques √©mergentes.

De plus, le projet Open Web Application Security Project (OWASP) a cr√©√© une "[liste des 10 principales](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" vuln√©rabilit√©s les plus critiques trouv√©es dans les applications utilisant des LLMs. La liste met en √©vidence les risques de menaces telles que l'empoisonnement des donn√©es mentionn√© pr√©c√©demment ainsi que d'autres comme :

- **Injection de prompts** : une technique o√π les attaquants manipulent un mod√®le de langage √† grande √©chelle (LLM) par des entr√©es soigneusement con√ßues, le faisant se comporter en dehors de son comportement pr√©vu.
- **Vuln√©rabilit√©s de la cha√Æne d'approvisionnement** : Les composants et logiciels qui composent les applications utilis√©es par un LLM, tels que les modules Python ou les jeux de donn√©es externes, peuvent eux-m√™mes √™tre compromis, entra√Ænant des r√©sultats inattendus, des biais introduits et m√™me des vuln√©rabilit√©s dans l'infrastructure sous-jacente.
- **D√©pendance excessive** : Les LLMs sont faillibles et ont tendance √† halluciner, fournissant des r√©sultats inexacts ou dangereux. Dans plusieurs circonstances document√©es, les gens ont pris les r√©sultats pour argent comptant, entra√Ænant des cons√©quences n√©gatives impr√©vues dans le monde r√©el.

L'Advocate Cloud de Microsoft, Rod Trent, a √©crit un ebook gratuit, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), qui plonge profond√©ment dans ces menaces √©mergentes de l'IA et fournit des conseils approfondis sur la meilleure fa√ßon de traiter ces sc√©narios.

## Tests de s√©curit√© pour les syst√®mes d'IA et les LLMs

L'intelligence artificielle (IA) transforme divers domaines et industries, offrant de nouvelles possibilit√©s et avantages pour la soci√©t√©. Cependant, l'IA pose √©galement des d√©fis et des risques importants, tels que la confidentialit√© des donn√©es, le biais, le manque d'explicabilit√© et une utilisation potentielle abusive. Par cons√©quent, il est crucial de garantir que les syst√®mes d'IA sont s√©curis√©s et responsables, c'est-√†-dire qu'ils respectent les normes √©thiques et l√©gales et peuvent √™tre dignes de confiance pour les utilisateurs et les parties prenantes.

Les tests de s√©curit√© sont le processus d'√©valuation de la s√©curit√© d'un syst√®me d'IA ou d'un LLM, en identifiant et en exploitant leurs vuln√©rabilit√©s. Cela peut √™tre effectu√© par des d√©veloppeurs, des utilisateurs ou des auditeurs tiers, en fonction de l'objectif et de la port√©e des tests. Certaines des m√©thodes de test de s√©curit√© les plus courantes pour les syst√®mes d'IA et les LLMs sont :

- **Assainissement des donn√©es** : C'est le processus de suppression ou d'anonymisation des informations sensibles ou priv√©es des donn√©es d'entra√Ænement ou de l'entr√©e d'un syst√®me d'IA ou d'un LLM. L'assainissement des donn√©es peut aider √† pr√©venir la fuite de donn√©es et la manipulation malveillante en r√©duisant l'exposition de donn√©es confidentielles ou personnelles.
- **Tests adversaires** : C'est le processus de g√©n√©ration et d'application d'exemples adversaires √† l'entr√©e ou √† la sortie d'un syst√®me d'IA ou d'un LLM pour √©valuer sa robustesse et sa r√©silience contre les attaques adversaires. Les tests adversaires peuvent aider √† identifier et att√©nuer les vuln√©rabilit√©s et faiblesses d'un syst√®me d'IA ou d'un LLM qui pourraient √™tre exploit√©es par des attaquants.
- **V√©rification du mod√®le** : C'est le processus de v√©rification de l'exactitude et de l'exhaustivit√© des param√®tres ou de l'architecture du mod√®le d'un syst√®me d'IA ou d'un LLM. La v√©rification du mod√®le peut aider √† d√©tecter et pr√©venir le vol de mod√®les en garantissant que le mod√®le est prot√©g√© et authentifi√©.
- **Validation de la sortie** : C'est le processus de validation de la qualit√© et de la fiabilit√© de la sortie d'un syst√®me d'IA ou d'un LLM. La validation de la sortie peut aider √† d√©tecter et corriger la manipulation malveillante en garantissant que la sortie est coh√©rente et pr√©cise.

OpenAI, un leader dans les syst√®mes d'IA, a mis en place une s√©rie d'_√©valuations de s√©curit√©_ dans le cadre de leur initiative de r√©seau de red teaming, visant √† tester la sortie des syst√®mes d'IA dans l'espoir de contribuer √† la s√©curit√© de l'IA.

### S√©curit√© de l'IA

Il est imp√©ratif que nous visons √† prot√©ger les syst√®mes d'IA contre les attaques malveillantes, les abus ou les cons√©quences impr√©vues. Cela inclut de prendre des mesures pour assurer la s√©curit√©, la fiabilit√© et la confiance des syst√®mes d'IA, telles que :

- S√©curiser les donn√©es et les algorithmes utilis√©s pour entra√Æner et ex√©cuter les mod√®les d'IA
- Emp√™cher l'acc√®s non autoris√©, la manipulation ou le sabotage des syst√®mes d'IA
- D√©tecter et att√©nuer les biais, la discrimination ou les probl√®mes √©thiques dans les syst√®mes d'IA
- Assurer la responsabilit√©, la transparence et l'explicabilit√© des d√©cisions et actions de l'IA
- Aligner les objectifs et les valeurs des syst√®mes d'IA avec ceux des humains et de la soci√©t√©

La s√©curit√© de l'IA est importante pour assurer l'int√©grit√©, la disponibilit√© et la confidentialit√© des syst√®mes d'IA et des donn√©es. Certains des d√©fis et opportunit√©s de la s√©curit√© de l'IA sont :

- Opportunit√© : Incorporer l'IA dans les strat√©gies de cybers√©curit√© car elle peut jouer un r√¥le crucial dans l'identification des menaces et l'am√©lioration des temps de r√©ponse. L'IA peut aider √† automatiser et augmenter la d√©tection et l'att√©nuation des cyberattaques, telles que le phishing, les logiciels malveillants ou les ransomwares.
- D√©fi : L'IA peut √©galement √™tre utilis√©e par des adversaires pour lancer des attaques sophistiqu√©es, telles que g√©n√©rer du contenu faux ou trompeur, usurper l'identit√© des utilisateurs ou exploiter les vuln√©rabilit√©s des syst√®mes d'IA. Par cons√©quent, les d√©veloppeurs d'IA ont une responsabilit√© unique de concevoir des syst√®mes robustes et r√©silients contre les abus.

### Protection des donn√©es

Les LLMs peuvent poser des risques pour la confidentialit√© et la s√©curit√© des donn√©es qu'ils utilisent. Par exemple, les LLMs peuvent potentiellement m√©moriser et divulguer des informations sensibles de leurs donn√©es d'entra√Ænement, telles que des noms personnels, des adresses, des mots de passe ou des num√©ros de carte de cr√©dit. Ils peuvent √©galement √™tre manipul√©s ou attaqu√©s par des acteurs malveillants qui veulent exploiter leurs vuln√©rabilit√©s ou biais. Par cons√©quent, il est important d'√™tre conscient de ces risques et de prendre des mesures appropri√©es pour prot√©ger les donn√©es utilis√©es avec les LLMs. Il existe plusieurs √©tapes que vous pouvez suivre pour prot√©ger les donn√©es utilis√©es avec les LLMs. Ces √©tapes incluent :

- **Limiter la quantit√© et le type de donn√©es qu'ils partagent avec les LLMs** : Ne partagez que les donn√©es n√©cessaires et pertinentes pour les objectifs pr√©vus, et √©vitez de partager des donn√©es sensibles, confidentielles ou personnelles. Les utilisateurs doivent √©galement anonymiser ou crypter les donn√©es qu'ils partagent avec les LLMs, par exemple en supprimant ou en masquant toute information d'identification, ou en utilisant des canaux de communication s√©curis√©s.
- **V√©rifier les donn√©es g√©n√©r√©es par les LLMs** : V√©rifiez toujours l'exactitude et la qualit√© de la sortie g√©n√©r√©e par les LLMs pour vous assurer qu'elles ne contiennent pas d'informations ind√©sirables ou inappropri√©es.
- **Signaler et alerter toute violation de donn√©es ou incident** : Soyez vigilant face √† toute activit√© ou comportement suspect ou anormal des LLMs, comme g√©n√©rer des textes qui sont hors sujet, inexacts, offensants ou nuisibles. Cela pourrait indiquer une violation de donn√©es ou un incident de s√©curit√©.

La s√©curit√©, la gouvernance et la conformit√© des donn√©es sont essentielles pour toute organisation qui souhaite tirer parti de la puissance des donn√©es et de l'IA dans un environnement multi-cloud. S√©curiser et gouverner toutes vos donn√©es est une entreprise complexe et √† multiples facettes. Vous devez s√©curiser et gouverner diff√©rents types de donn√©es (structur√©es, non structur√©es et g√©n√©r√©es par l'IA) dans diff√©rents emplacements √† travers plusieurs clouds, et vous devez tenir compte des r√©glementations existantes et futures en mati√®re de s√©curit√©, de gouvernance et d'IA. Pour prot√©ger vos donn√©es, vous devez adopter certaines meilleures pratiques et pr√©cautions, telles que :

- Utilisez des services ou des plateformes cloud qui offrent des fonctionnalit√©s de protection et de confidentialit√© des donn√©es.
- Utilisez des outils de qualit√© et de validation des donn√©es pour v√©rifier vos donn√©es afin de d√©tecter des erreurs, des incoh√©rences ou des anomalies.
- Utilisez des cadres de gouvernance et d'√©thique des donn√©es pour garantir que vos donn√©es sont utilis√©es de mani√®re responsable et transparente.

### √âmuler les menaces du monde r√©el - Red teaming de l'IA

L'√©mulation des menaces du monde r√©el est d√©sormais consid√©r√©e comme une pratique standard dans la construction de syst√®mes d'IA r√©silients en employant des outils, des tactiques et des proc√©dures similaires pour identifier les risques pour les syst√®mes et tester la r√©ponse des d√©fenseurs.

> La pratique du red teaming de l'IA a √©volu√© pour prendre un sens plus large : elle couvre non seulement la recherche de vuln√©rabilit√©s de s√©curit√©, mais inclut √©galement la recherche d'autres d√©faillances du syst√®me, telles que la g√©n√©ration de contenu potentiellement nuisible. Les syst√®mes d'IA pr√©sentent de nouveaux risques, et le red teaming est essentiel pour comprendre ces risques nouveaux, tels que l'injection de prompts et la production de contenu non fond√©. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

Voici une liste de lectures suppl√©mentaires qui peuvent vous aider √† mieux comprendre comment le red teaming peut aider √† identifier et att√©nuer les risques dans vos syst√®mes d'IA :

- [Planification du red teaming pour les mod√®les de langage √† grande √©chelle (LLMs) et leurs applications](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Qu'est-ce que le r√©seau de red teaming d'OpenAI ?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - Une pratique cl√© pour construire des solutions d'IA plus s√ªres et plus responsables](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), une base de connaissances des tactiques et techniques employ√©es par les adversaires dans des attaques r√©elles sur des syst√®mes d'IA.

## V√©rification des connaissances

Quelle pourrait √™tre une bonne approche pour maintenir l'int√©grit√© des donn√©es et pr√©venir les abus ?

1. Avoir des contr√¥les stricts bas√©s sur les r√¥les pour l'acc√®s et la gestion des donn√©es
1. Mettre en ≈ìuvre et auditer l'√©tiquetage des donn√©es pour √©viter la mauvaise repr√©sentation ou l'abus des donn√©es
1. S'assurer que votre infrastructure d'IA prend en charge le filtrage de contenu

R:1, Bien que les trois soient d'excellentes recommandations, s'assurer que vous attribuez les privil√®ges d'acc√®s aux donn√©es appropri√©s aux utilisateurs contribuera grandement √† pr√©venir la manipulation et la mauvaise repr√©sentation des donn√©es utilis√©es par les LLMs.

## üöÄ D√©fi

Renseignez-vous davantage sur la mani√®re dont vous pouvez [gouverner et prot√©ger les informations sensibles](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) √† l'√®re de l'IA.

## Bon travail, continuez votre apprentissage

Apr√®s avoir termin√© cette le√ßon, consultez notre [collection d'apprentissage sur l'IA g√©n√©rative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer √† am√©liorer vos connaissances en IA g√©n√©rative !

Passez √† la le√ßon 14 o√π nous examinerons [le cycle de vie des applications d'IA g√©n√©rative](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst) !

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction IA [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.