<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T22:38:18+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "fr"
}
-->
# S√©curiser vos applications d'IA g√©n√©rative

[![S√©curiser vos applications d'IA g√©n√©rative](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.fr.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Introduction

Cette le√ßon couvrira :

- La s√©curit√© dans le contexte des syst√®mes d'IA.
- Les risques et menaces courants pour les syst√®mes d'IA.
- Les m√©thodes et consid√©rations pour s√©curiser les syst√®mes d'IA.

## Objectifs d'apprentissage

Apr√®s avoir termin√© cette le√ßon, vous comprendrez :

- Les menaces et risques pour les syst√®mes d'IA.
- Les m√©thodes et pratiques courantes pour s√©curiser les syst√®mes d'IA.
- Comment la mise en ≈ìuvre de tests de s√©curit√© peut pr√©venir des r√©sultats inattendus et la perte de confiance des utilisateurs.

## Que signifie la s√©curit√© dans le contexte de l'IA g√©n√©rative ?

Alors que les technologies d'Intelligence Artificielle (IA) et d'Apprentissage Automatique (ML) fa√ßonnent de plus en plus nos vies, il est essentiel de prot√©ger non seulement les donn√©es des clients, mais aussi les syst√®mes d'IA eux-m√™mes. L'IA/ML est de plus en plus utilis√©e pour soutenir des processus d√©cisionnels de grande valeur dans des industries o√π une mauvaise d√©cision peut entra√Æner des cons√©quences graves.

Voici les points cl√©s √† consid√©rer :

- **Impact de l'IA/ML** : L'IA/ML ont des impacts significatifs sur la vie quotidienne et, en tant que tels, leur protection est devenue essentielle.
- **D√©fis de s√©curit√©** : L'impact de l'IA/ML n√©cessite une attention particuli√®re pour r√©pondre au besoin de prot√©ger les produits bas√©s sur l'IA contre des attaques sophistiqu√©es, qu'elles proviennent de trolls ou de groupes organis√©s.
- **Probl√®mes strat√©giques** : L'industrie technologique doit aborder de mani√®re proactive les d√©fis strat√©giques pour garantir la s√©curit√© des clients et la protection des donn√©es √† long terme.

De plus, les mod√®les d'apprentissage automatique sont largement incapables de distinguer les entr√©es malveillantes des donn√©es anormales b√©nignes. Une source importante de donn√©es d'entra√Ænement provient de jeux de donn√©es publics non mod√©r√©s et non v√©rifi√©s, ouverts aux contributions de tiers. Les attaquants n'ont pas besoin de compromettre les jeux de donn√©es lorsqu'ils peuvent y contribuer librement. Avec le temps, des donn√©es malveillantes √† faible confiance deviennent des donn√©es de haute confiance si leur structure/format reste correct.

C'est pourquoi il est crucial de garantir l'int√©grit√© et la protection des bases de donn√©es que vos mod√®les utilisent pour prendre des d√©cisions.

## Comprendre les menaces et risques li√©s √† l'IA

En ce qui concerne l'IA et les syst√®mes associ√©s, l'empoisonnement des donn√©es est aujourd'hui la menace de s√©curit√© la plus importante. L'empoisonnement des donn√©es se produit lorsque quelqu'un modifie intentionnellement les informations utilis√©es pour entra√Æner une IA, ce qui la conduit √† commettre des erreurs. Cela est d√ª √† l'absence de m√©thodes standardis√©es de d√©tection et de mitigation, combin√©e √† notre d√©pendance √† des jeux de donn√©es publics non v√©rifi√©s ou non mod√©r√©s pour l'entra√Ænement. Pour maintenir l'int√©grit√© des donn√©es et √©viter un processus d'entra√Ænement d√©fectueux, il est crucial de suivre l'origine et la provenance de vos donn√©es. Sinon, l'adage "garbage in, garbage out" reste vrai, entra√Ænant une performance compromise du mod√®le.

Voici des exemples de la mani√®re dont l'empoisonnement des donn√©es peut affecter vos mod√®les :

1. **Renversement des √©tiquettes** : Dans une t√¢che de classification binaire, un adversaire modifie intentionnellement les √©tiquettes d'un petit sous-ensemble de donn√©es d'entra√Ænement. Par exemple, des √©chantillons b√©nins sont √©tiquet√©s comme malveillants, ce qui conduit le mod√®le √† apprendre des associations incorrectes.\
   **Exemple** : Un filtre anti-spam classant √† tort des courriels l√©gitimes comme spam en raison d'√©tiquettes manipul√©es.
2. **Empoisonnement des caract√©ristiques** : Un attaquant modifie subtilement les caract√©ristiques des donn√©es d'entra√Ænement pour introduire des biais ou induire le mod√®le en erreur.\
   **Exemple** : Ajouter des mots-cl√©s non pertinents aux descriptions de produits pour manipuler les syst√®mes de recommandation.
3. **Injection de donn√©es** : Injecter des donn√©es malveillantes dans l'ensemble d'entra√Ænement pour influencer le comportement du mod√®le.\
   **Exemple** : Introduire de fausses critiques d'utilisateurs pour fausser les r√©sultats d'analyse de sentiment.
4. **Attaques par porte d√©rob√©e** : Un adversaire ins√®re un motif cach√© (porte d√©rob√©e) dans les donn√©es d'entra√Ænement. Le mod√®le apprend √† reconna√Ætre ce motif et agit de mani√®re malveillante lorsqu'il est d√©clench√©.\
   **Exemple** : Un syst√®me de reconnaissance faciale entra√Æn√© avec des images contenant des portes d√©rob√©es qui identifie √† tort une personne sp√©cifique.

La MITRE Corporation a cr√©√© [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), une base de connaissances sur les tactiques et techniques utilis√©es par les adversaires dans des attaques r√©elles contre les syst√®mes d'IA.

> Il existe un nombre croissant de vuln√©rabilit√©s dans les syst√®mes activ√©s par l'IA, car l'int√©gration de l'IA augmente la surface d'attaque des syst√®mes existants au-del√† de celles des cyberattaques traditionnelles. Nous avons d√©velopp√© ATLAS pour sensibiliser √† ces vuln√©rabilit√©s uniques et en √©volution, alors que la communaut√© mondiale int√®gre de plus en plus l'IA dans divers syst√®mes. ATLAS est bas√© sur le cadre MITRE ATT&CK¬Æ et ses tactiques, techniques et proc√©dures (TTP) sont compl√©mentaires √† celles d'ATT&CK.

Tout comme le cadre MITRE ATT&CK¬Æ, largement utilis√© en cybers√©curit√© traditionnelle pour planifier des sc√©narios avanc√©s d'√©mulation de menaces, ATLAS fournit un ensemble de TTP facilement consultables qui peuvent aider √† mieux comprendre et se pr√©parer √† d√©fendre contre les attaques √©mergentes.

De plus, l'Open Web Application Security Project (OWASP) a cr√©√© une "[liste des 10 principales vuln√©rabilit√©s](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" les plus critiques trouv√©es dans les applications utilisant des LLM. La liste met en √©vidence les risques de menaces telles que l'empoisonnement des donn√©es mentionn√© pr√©c√©demment, ainsi que d'autres comme :

- **Injection de prompts** : une technique o√π les attaquants manipulent un mod√®le de langage large (LLM) via des entr√©es soigneusement con√ßues, le faisant agir en dehors de son comportement pr√©vu.
- **Vuln√©rabilit√©s de la cha√Æne d'approvisionnement** : Les composants et logiciels qui composent les applications utilis√©es par un LLM, tels que les modules Python ou les jeux de donn√©es externes, peuvent eux-m√™mes √™tre compromis, entra√Ænant des r√©sultats inattendus, des biais introduits et m√™me des vuln√©rabilit√©s dans l'infrastructure sous-jacente.
- **D√©pendance excessive** : Les LLM sont faillibles et ont tendance √† halluciner, fournissant des r√©sultats inexacts ou dangereux. Dans plusieurs circonstances document√©es, les gens ont pris les r√©sultats pour argent comptant, entra√Ænant des cons√©quences n√©gatives impr√©vues dans le monde r√©el.

Rod Trent, Cloud Advocate chez Microsoft, a √©crit un ebook gratuit, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), qui explore en profondeur ces menaces √©mergentes li√©es √† l'IA et fournit des conseils d√©taill√©s sur la meilleure fa√ßon de g√©rer ces sc√©narios.

## Tests de s√©curit√© pour les syst√®mes d'IA et les LLM

L'intelligence artificielle (IA) transforme divers domaines et industries, offrant de nouvelles possibilit√©s et avantages pour la soci√©t√©. Cependant, l'IA pose √©galement des d√©fis et des risques importants, tels que la confidentialit√© des donn√©es, les biais, le manque d'explicabilit√© et les utilisations potentielles abusives. Il est donc crucial de s'assurer que les syst√®mes d'IA sont s√©curis√©s et responsables, c'est-√†-dire qu'ils respectent les normes √©thiques et l√©gales et qu'ils peuvent √™tre dignes de confiance pour les utilisateurs et les parties prenantes.

Les tests de s√©curit√© sont le processus d'√©valuation de la s√©curit√© d'un syst√®me d'IA ou d'un LLM, en identifiant et en exploitant leurs vuln√©rabilit√©s. Cela peut √™tre effectu√© par des d√©veloppeurs, des utilisateurs ou des auditeurs tiers, en fonction de l'objectif et de la port√©e des tests. Certaines des m√©thodes de test de s√©curit√© les plus courantes pour les syst√®mes d'IA et les LLM sont :

- **Assainissement des donn√©es** : Il s'agit du processus de suppression ou d'anonymisation des informations sensibles ou priv√©es des donn√©es d'entra√Ænement ou des entr√©es d'un syst√®me d'IA ou d'un LLM. L'assainissement des donn√©es peut aider √† pr√©venir les fuites de donn√©es et les manipulations malveillantes en r√©duisant l'exposition des donn√©es confidentielles ou personnelles.
- **Tests adverses** : Il s'agit du processus de g√©n√©ration et d'application d'exemples adverses aux entr√©es ou sorties d'un syst√®me d'IA ou d'un LLM pour √©valuer sa robustesse et sa r√©silience face aux attaques adverses. Les tests adverses peuvent aider √† identifier et √† att√©nuer les vuln√©rabilit√©s et faiblesses d'un syst√®me d'IA ou d'un LLM qui pourraient √™tre exploit√©es par des attaquants.
- **V√©rification du mod√®le** : Il s'agit du processus de v√©rification de l'exactitude et de l'exhaustivit√© des param√®tres ou de l'architecture du mod√®le d'un syst√®me d'IA ou d'un LLM. La v√©rification du mod√®le peut aider √† d√©tecter et √† pr√©venir le vol de mod√®le en s'assurant que le mod√®le est prot√©g√© et authentifi√©.
- **Validation des sorties** : Il s'agit du processus de validation de la qualit√© et de la fiabilit√© des sorties d'un syst√®me d'IA ou d'un LLM. La validation des sorties peut aider √† d√©tecter et √† corriger les manipulations malveillantes en s'assurant que les sorties sont coh√©rentes et pr√©cises.

OpenAI, un leader dans les syst√®mes d'IA, a mis en place une s√©rie d'_√©valuations de s√©curit√©_ dans le cadre de leur initiative de r√©seau de red teaming, visant √† tester les sorties des syst√®mes d'IA dans l'espoir de contribuer √† la s√©curit√© de l'IA.

> Les √©valuations peuvent aller de simples tests de questions-r√©ponses √† des simulations plus complexes. Voici des exemples concrets d'√©valuations d√©velopp√©es par OpenAI pour √©valuer les comportements de l'IA sous diff√©rents angles :

#### Persuasion

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst) : Dans quelle mesure un syst√®me d'IA peut-il tromper un autre syst√®me d'IA pour qu'il dise un mot secret ?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst) : Dans quelle mesure un syst√®me d'IA peut-il convaincre un autre syst√®me d'IA de faire un don d'argent ?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst) : Dans quelle mesure un syst√®me d'IA peut-il influencer le soutien d'un autre syst√®me d'IA √† une proposition politique ?

#### St√©ganographie (messages cach√©s)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst) : Dans quelle mesure un syst√®me d'IA peut-il transmettre des messages secrets sans √™tre d√©tect√© par un autre syst√®me d'IA ?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst) : Dans quelle mesure un syst√®me d'IA peut-il compresser et d√©compresser des messages pour permettre de cacher des messages secrets ?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst) : Dans quelle mesure un syst√®me d'IA peut-il se coordonner avec un autre syst√®me d'IA, sans communication directe ?

### S√©curit√© de l'IA

Il est imp√©ratif de prot√©ger les syst√®mes d'IA contre les attaques malveillantes, les abus ou les cons√©quences involontaires. Cela inclut de prendre des mesures pour garantir la s√©curit√©, la fiabilit√© et la confiance des syst√®mes d'IA, telles que :

- S√©curiser les donn√©es et algorithmes utilis√©s pour entra√Æner et ex√©cuter les mod√®les d'IA.
- Pr√©venir l'acc√®s non autoris√©, la manipulation ou le sabotage des syst√®mes d'IA.
- D√©tecter et att√©nuer les biais, discriminations ou probl√®mes √©thiques dans les syst√®mes d'IA.
- Garantir la responsabilit√©, la transparence et l'explicabilit√© des d√©cisions et actions de l'IA.
- Aligner les objectifs et les valeurs des syst√®mes d'IA avec ceux des humains et de la soci√©t√©.

La s√©curit√© de l'IA est essentielle pour garantir l'int√©grit√©, la disponibilit√© et la confidentialit√© des syst√®mes d'IA et des donn√©es. Certains des d√©fis et opportunit√©s li√©s √† la s√©curit√© de l'IA sont :

- Opportunit√© : Int√©grer l'IA dans les strat√©gies de cybers√©curit√©, car elle peut jouer un r√¥le crucial dans l'identification des menaces et l'am√©lioration des temps de r√©ponse. L'IA peut aider √† automatiser et √† augmenter la d√©tection et la mitigation des cyberattaques, telles que le phishing, les logiciels malveillants ou les ransomwares.
- D√©fi : L'IA peut √©galement √™tre utilis√©e par des adversaires pour lancer des attaques sophistiqu√©es, telles que la g√©n√©ration de contenu faux ou trompeur, l'usurpation d'identit√© ou l'exploitation de vuln√©rabilit√©s dans les syst√®mes d'IA. Par cons√©quent, les d√©veloppeurs d'IA ont une responsabilit√© unique de concevoir des syst√®mes robustes et r√©silients contre les abus.

### Protection des donn√©es

Les LLM peuvent pr√©senter des risques pour la confidentialit√© et la s√©curit√© des donn√©es qu'ils utilisent. Par exemple, les LLM peuvent potentiellement m√©moriser et divulguer des informations sensibles provenant de leurs donn√©es d'entra√Ænement, telles que des noms personnels, des adresses, des mots de passe ou des num√©ros de carte de cr√©dit. Ils peuvent √©galement √™tre manipul√©s ou attaqu√©s par des acteurs malveillants cherchant √† exploiter leurs vuln√©rabilit√©s ou biais. Il est donc important d'√™tre conscient de ces risques et de prendre des mesures appropri√©es pour prot√©ger les donn√©es utilis√©es avec les LLM. Voici plusieurs √©tapes que vous pouvez suivre pour prot√©ger les donn√©es utilis√©es avec les LLM :

- **Limiter la quantit√© et le type de donn√©es partag√©es avec les LLM** : Partagez uniquement les donn√©es n√©cessaires et pertinentes pour les objectifs vis√©s, et √©vitez de partager des donn√©es sensibles, confidentielles ou personnelles. Les utilisateurs devraient √©galement anonymiser ou chiffrer les donn√©es qu'ils partagent avec les LLM, par exemple en supprimant ou masquant toute information identifiante, ou en utilisant des canaux de communication s√©curis√©s.
- **V√©rifier les donn√©es g√©n√©r√©es par les LLM** : V√©rifiez toujours l'exactitude et la qualit√© des sorties g√©n√©r√©es par les LLM pour vous assurer qu'elles ne contiennent pas d'informations ind√©sirables ou inappropri√©es.
- **Signaler et alerter en cas de violation ou d'incident de donn√©es** : Soyez vigilant face √† toute activit√© ou comportement suspect ou anormal des LLM, comme la g√©n√©ration de textes qui sont hors sujet, inexacts, offensants ou nuisibles. Cela pourrait indiquer une violation de donn√©es ou un incident de s√©curit√©.

La s√©curit√©, la gouvernance et la conformit√© des donn√©es sont essentielles pour toute organisation souhaitant exploiter la puissance des donn√©es et de l'IA dans un environnement multi-cloud. S√©curiser et gouverner toutes vos donn√©es est une entreprise complexe et multifacette. Vous devez s√©curiser et gouverner diff√©rents types de donn√©es (structur√©es, non structur√©es et donn√©es g√©n√©r√©es par l'IA) dans diff√©rents emplacements √† travers plusieurs clouds, et vous devez tenir compte des r√©glementations existantes et futures en mati√®re de s√©curit√© des donn√©es, de gouvernance et d'IA. Pour prot√©ger vos donn√©es, vous devez adopter certaines bonnes pratiques et pr√©cautions, telles que :

- Utiliser des services ou plateformes cloud offrant des fonctionnalit√©s de protection et de confidentialit√© des donn√©es.
- Utiliser des outils de qualit√© et de validation des donn√©es pour v√©rifier vos donn√©es afin de d√©tecter les erreurs, incoh√©rences ou anomalies.
- Utiliser des cadres de gouvernance et d'√©thique des donn√©es pour garantir que vos donn√©es sont utilis√©es de mani√®re responsable et transparente.

### √âmulation des menaces r√©elles - Red teaming pour l'IA
Simuler des menaces r√©elles est d√©sormais consid√©r√© comme une pratique standard pour construire des syst√®mes d'IA r√©silients en utilisant des outils, tactiques et proc√©dures similaires afin d'identifier les risques pour les syst√®mes et tester la r√©ponse des d√©fenseurs.

> La pratique du red teaming en IA a √©volu√© pour prendre une signification plus large : elle ne se limite pas √† rechercher des vuln√©rabilit√©s de s√©curit√©, mais inclut √©galement l'exploration d'autres d√©faillances du syst√®me, telles que la g√©n√©ration de contenu potentiellement nuisible. Les syst√®mes d'IA pr√©sentent de nouveaux risques, et le red teaming est essentiel pour comprendre ces risques in√©dits, comme l'injection de prompts et la production de contenu non fond√©. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Conseils et ressources pour le red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.fr.png)]()

Voici les principaux enseignements qui ont fa√ßonn√© le programme de Red Team IA de Microsoft.

1. **Port√©e √©tendue du red teaming en IA :**  
   Le red teaming en IA englobe d√©sormais √† la fois les r√©sultats en mati√®re de s√©curit√© et ceux li√©s √† l'IA responsable (RAI). Traditionnellement, le red teaming se concentrait sur les aspects de s√©curit√©, en traitant le mod√®le comme un vecteur (par exemple, le vol du mod√®le sous-jacent). Cependant, les syst√®mes d'IA introduisent de nouvelles vuln√©rabilit√©s de s√©curit√© (par exemple, l'injection de prompts, l'empoisonnement), n√©cessitant une attention particuli√®re. Au-del√† de la s√©curit√©, le red teaming en IA examine √©galement les probl√®mes d'√©quit√© (par exemple, les st√©r√©otypes) et le contenu nuisible (par exemple, la glorification de la violence). L'identification pr√©coce de ces probl√®mes permet de prioriser les investissements dans la d√©fense.  

2. **D√©faillances malveillantes et b√©nignes :**  
   Le red teaming en IA prend en compte les d√©faillances √† la fois malveillantes et b√©nignes. Par exemple, lors du red teaming du nouveau Bing, nous explorons non seulement comment des adversaires malveillants peuvent subvertir le syst√®me, mais aussi comment des utilisateurs r√©guliers peuvent rencontrer du contenu probl√©matique ou nuisible. Contrairement au red teaming de s√©curit√© traditionnel, qui se concentre principalement sur les acteurs malveillants, le red teaming en IA prend en compte une gamme plus large de profils et de d√©faillances potentielles.  

3. **Nature dynamique des syst√®mes d'IA :**  
   Les applications d'IA √©voluent constamment. Dans les applications de mod√®les de langage de grande taille, les d√©veloppeurs s'adaptent aux exigences changeantes. Un red teaming continu garantit une vigilance constante et une adaptation aux risques en √©volution.  

Le red teaming en IA n'est pas exhaustif et doit √™tre consid√©r√© comme un compl√©ment √† d'autres contr√¥les tels que [le contr√¥le d'acc√®s bas√© sur les r√¥les (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) et des solutions compl√®tes de gestion des donn√©es. Il est con√ßu pour compl√©ter une strat√©gie de s√©curit√© qui vise √† utiliser des solutions d'IA s√ªres et responsables, prenant en compte la confidentialit√© et la s√©curit√© tout en cherchant √† minimiser les biais, le contenu nuisible et la d√©sinformation qui peuvent √©roder la confiance des utilisateurs.  

Voici une liste de lectures suppl√©mentaires qui peuvent vous aider √† mieux comprendre comment le red teaming peut aider √† identifier et att√©nuer les risques dans vos syst√®mes d'IA :  

- [Planification du red teaming pour les mod√®les de langage de grande taille (LLMs) et leurs applications](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [Qu'est-ce que le r√©seau de red teaming d'OpenAI ?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [Red Teaming en IA - Une pratique cl√© pour construire des solutions d'IA plus s√ªres et responsables](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), une base de connaissances sur les tactiques et techniques utilis√©es par les adversaires dans des attaques r√©elles contre les syst√®mes d'IA.  

## V√©rification des connaissances  

Quelle pourrait √™tre une bonne approche pour maintenir l'int√©grit√© des donn√©es et pr√©venir les abus ?  

1. Mettre en place des contr√¥les d'acc√®s bas√©s sur les r√¥les solides pour la gestion et l'acc√®s aux donn√©es  
1. Impl√©menter et auditer l'√©tiquetage des donn√©es pour √©viter la mauvaise repr√©sentation ou l'utilisation abusive des donn√©es  
1. S'assurer que votre infrastructure d'IA prend en charge le filtrage de contenu  

A:1, Bien que les trois recommandations soient excellentes, s'assurer que vous attribuez les privil√®ges d'acc√®s aux donn√©es appropri√©s aux utilisateurs contribuera grandement √† pr√©venir la manipulation et la mauvaise repr√©sentation des donn√©es utilis√©es par les LLMs.  

## üöÄ D√©fi  

Renseignez-vous davantage sur la mani√®re de [g√©rer et prot√©ger les informations sensibles](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) √† l'√®re de l'IA.  

## Excellent travail, continuez votre apprentissage  

Apr√®s avoir termin√© cette le√ßon, consultez notre [collection d'apprentissage sur l'IA g√©n√©rative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer √† approfondir vos connaissances sur l'IA g√©n√©rative !  

Rendez-vous √† la le√ßon 14 o√π nous examinerons [le cycle de vie des applications d'IA g√©n√©rative](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst) !  

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.