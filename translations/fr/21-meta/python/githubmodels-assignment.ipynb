{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travailler avec les modèles de la famille Meta\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Cette leçon abordera :\n",
    "\n",
    "- L’exploration des deux principaux modèles de la famille Meta - Llama 3.1 et Llama 3.2\n",
    "- La compréhension des cas d’utilisation et des scénarios pour chaque modèle\n",
    "- Un exemple de code pour illustrer les fonctionnalités uniques de chaque modèle\n",
    "\n",
    "## La famille de modèles Meta\n",
    "\n",
    "Dans cette leçon, nous allons explorer 2 modèles de la famille Meta, aussi appelée « Llama Herd » - Llama 3.1 et Llama 3.2\n",
    "\n",
    "Ces modèles existent en plusieurs variantes et sont disponibles sur le marketplace Github Model. Voici plus d’informations sur l’utilisation des modèles Github pour [prototyper avec des modèles d’IA](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Variantes de modèles :\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Remarque : Llama 3 est également disponible sur Github Models mais ne sera pas abordé dans cette leçon*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Avec 405 milliards de paramètres, Llama 3.1 appartient à la catégorie des LLM open source.\n",
    "\n",
    "Ce modèle est une amélioration par rapport à la version précédente Llama 3 en offrant :\n",
    "\n",
    "- Une fenêtre de contexte plus grande - 128k tokens contre 8k tokens\n",
    "- Un nombre maximal de tokens en sortie plus élevé - 4096 contre 2048\n",
    "- Un meilleur support multilingue - grâce à l’augmentation du nombre de tokens d’entraînement\n",
    "\n",
    "Ces améliorations permettent à Llama 3.1 de gérer des cas d’usage plus complexes lors de la création d’applications GenAI, notamment :\n",
    "- Appels de fonctions natifs - la capacité d’appeler des outils et fonctions externes en dehors du flux de travail du LLM\n",
    "- Meilleures performances RAG - grâce à la fenêtre de contexte plus large\n",
    "- Génération de données synthétiques - la capacité de créer des données efficaces pour des tâches comme l’ajustement fin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appel natif de fonctions\n",
    "\n",
    "Llama 3.1 a été entraîné pour être plus efficace dans l’utilisation de fonctions ou d’outils. Il dispose également de deux outils intégrés que le modèle peut identifier comme nécessaires selon la demande de l’utilisateur. Ces outils sont :\n",
    "\n",
    "- **Brave Search** – Permet d’obtenir des informations à jour, comme la météo, en effectuant une recherche sur le web\n",
    "- **Wolfram Alpha** – Permet de réaliser des calculs mathématiques complexes, ce qui évite d’avoir à écrire vos propres fonctions.\n",
    "\n",
    "Vous pouvez aussi créer vos propres outils personnalisés que le LLM pourra utiliser.\n",
    "\n",
    "Dans l’exemple de code ci-dessous :\n",
    "\n",
    "- Nous définissons les outils disponibles (brave_search, wolfram_alpha) dans le prompt système.\n",
    "- Nous envoyons un prompt utilisateur qui demande la météo dans une certaine ville.\n",
    "- Le LLM répondra par un appel à l’outil Brave Search, qui ressemblera à ceci : `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Remarque : Cet exemple ne fait qu’appeler l’outil. Si vous souhaitez obtenir les résultats, il faudra créer un compte gratuit sur la page de l’API Brave et définir la fonction vous-même.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Bien que Llama 3.1 soit un LLM, l’une de ses limites est la multimodalité. Autrement dit, la capacité d’utiliser différents types d’entrées, comme des images, en guise d’invites et de fournir des réponses. Cette fonctionnalité fait partie des principaux atouts de Llama 3.2. Parmi les nouveautés, on retrouve également :\n",
    "\n",
    "- Multimodalité – capable d’analyser à la fois des invites textuelles et des images\n",
    "- Variantes de taille petite à moyenne (11B et 90B) – cela offre des options de déploiement flexibles,\n",
    "- Variantes texte uniquement (1B et 3B) – cela permet au modèle d’être utilisé sur des appareils mobiles ou en périphérie, avec une faible latence\n",
    "\n",
    "La prise en charge de la multimodalité marque une avancée majeure dans le domaine des modèles open source. L’exemple de code ci-dessous utilise à la fois une image et une invite textuelle pour obtenir une analyse de l’image par Llama 3.2 90B.\n",
    "\n",
    "### Prise en charge de la multimodalité avec Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'apprentissage ne s'arrête pas ici, poursuivez votre parcours\n",
    "\n",
    "Après avoir terminé cette leçon, consultez notre [collection d'apprentissage sur l'IA générative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer à approfondir vos connaissances en IA générative !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Avertissement** :  \nCe document a été traduit à l’aide du service de traduction par IA [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d’assurer l’exactitude de la traduction, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d’origine doit être considéré comme la source faisant autorité. Pour les informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous déclinons toute responsabilité en cas de malentendus ou d’interprétations erronées résultant de l’utilisation de cette traduction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:35:45+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "fr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}