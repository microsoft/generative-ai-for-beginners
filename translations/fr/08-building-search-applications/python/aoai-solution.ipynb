{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour exécuter les notebooks suivants, si vous ne l'avez pas encore fait, vous devez déployer un modèle qui utilise `text-embedding-ada-002` comme modèle de base et définir le nom du déploiement dans le fichier .env sous `AZURE_OPENAI_EMBEDDINGS_ENDPOINT`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-05-15\"\n",
    "  )\n",
    "\n",
    "model = os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT']\n",
    "\n",
    "SIMILARITIES_RESULTS_THRESHOLD = 0.75\n",
    "DATASET_NAME = \"../embedding_index_3m.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons charger l’Index d’Embedding dans un DataFrame Pandas. L’Index d’Embedding est stocké dans un fichier JSON appelé `embedding_index_3m.json`. L’Index d’Embedding contient les embeddings pour chacune des transcriptions YouTube jusqu’à la fin octobre 2023.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(source: str) -> pd.core.frame.DataFrame:\n",
    "    # Load the video session index\n",
    "    pd_vectors = pd.read_json(source)\n",
    "    return pd_vectors.drop(columns=[\"text\"], errors=\"ignore\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous allons créer une fonction appelée `get_videos` qui va rechercher la requête dans l’Embedding Index. Cette fonction retournera les 5 vidéos les plus similaires à la requête. Voici comment la fonction fonctionne :\n",
    "\n",
    "1. D’abord, une copie de l’Embedding Index est créée.\n",
    "2. Ensuite, l’Embedding de la requête est calculé en utilisant l’API OpenAI Embedding.\n",
    "3. Une nouvelle colonne appelée `similarity` est alors ajoutée à l’Embedding Index. La colonne `similarity` contient la similarité cosinus entre l’Embedding de la requête et l’Embedding de chaque segment vidéo.\n",
    "4. L’Embedding Index est ensuite filtré selon la colonne `similarity`. Seules les vidéos ayant une similarité cosinus supérieure ou égale à 0,75 sont conservées.\n",
    "5. Enfin, l’Embedding Index est trié selon la colonne `similarity` et les 5 vidéos les plus pertinentes sont retournées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    if len(a) > len(b):\n",
    "        b = np.pad(b, (0, len(a) - len(b)), 'constant')\n",
    "    elif len(b) > len(a):\n",
    "        a = np.pad(a, (0, len(b) - len(a)), 'constant')\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_videos(\n",
    "    query: str, dataset: pd.core.frame.DataFrame, rows: int\n",
    ") -> pd.core.frame.DataFrame:\n",
    "    # create a copy of the dataset\n",
    "    video_vectors = dataset.copy()\n",
    "\n",
    "    # get the embeddings for the query    \n",
    "    query_embeddings = client.embeddings.create(input=query, model=model).data[0].embedding\n",
    "\n",
    "    # create a new column with the calculated similarity for each row\n",
    "    video_vectors[\"similarity\"] = video_vectors[\"ada_v2\"].apply(\n",
    "        lambda x: cosine_similarity(np.array(query_embeddings), np.array(x))\n",
    "    )\n",
    "\n",
    "    # filter the videos by similarity\n",
    "    mask = video_vectors[\"similarity\"] >= SIMILARITIES_RESULTS_THRESHOLD\n",
    "    video_vectors = video_vectors[mask].copy()\n",
    "\n",
    "    # sort the videos by similarity\n",
    "    video_vectors = video_vectors.sort_values(by=\"similarity\", ascending=False).head(\n",
    "        rows\n",
    "    )\n",
    "\n",
    "    # return the top rows\n",
    "    return video_vectors.head(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction est très simple, elle affiche simplement les résultats de la requête de recherche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(videos: pd.core.frame.DataFrame, query: str):\n",
    "    def _gen_yt_url(video_id: str, seconds: int) -> str:\n",
    "        \"\"\"convert time in format 00:00:00 to seconds\"\"\"\n",
    "        return f\"https://youtu.be/{video_id}?t={seconds}\"\n",
    "\n",
    "    print(f\"\\nVideos similar to '{query}':\")\n",
    "    for _, row in videos.iterrows():\n",
    "        youtube_url = _gen_yt_url(row[\"videoId\"], row[\"seconds\"])\n",
    "        print(f\" - {row['title']}\")\n",
    "        print(f\"   Summary: {' '.join(row['summary'].split()[:15])}...\")\n",
    "        print(f\"   YouTube: {youtube_url}\")\n",
    "        print(f\"   Similarity: {row['similarity']}\")\n",
    "        print(f\"   Speakers: {row['speaker']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tout d'abord, l'index d'embedding est chargé dans un DataFrame Pandas.\n",
    "2. Ensuite, l'utilisateur est invité à saisir une requête.\n",
    "3. Puis, la fonction `get_videos` est appelée pour rechercher la requête dans l'index d'embedding.\n",
    "4. Enfin, la fonction `display_results` est appelée pour afficher les résultats à l'utilisateur.\n",
    "5. L'utilisateur est ensuite invité à saisir une nouvelle requête. Ce processus se répète jusqu'à ce que l'utilisateur saisisse `exit`.\n",
    "\n",
    "![](../../../../translated_images/notebook-search.1e320b9c7fcbb0bc1436d98ea6ee73b4b54ca47990a1c952b340a2cadf8ac1ca.fr.png)\n",
    "\n",
    "Vous serez invité à saisir une requête. Entrez une requête puis appuyez sur Entrée. L'application retournera une liste de vidéos pertinentes par rapport à la requête. L'application fournira également un lien vers l'endroit précis de la vidéo où se trouve la réponse à la question.\n",
    "\n",
    "Voici quelques exemples de requêtes à essayer :\n",
    "\n",
    "- Qu'est-ce qu'Azure Machine Learning ?\n",
    "- Comment fonctionnent les réseaux de neurones convolutifs ?\n",
    "- Qu'est-ce qu'un réseau de neurones ?\n",
    "- Puis-je utiliser les notebooks Jupyter avec Azure Machine Learning ?\n",
    "- Qu'est-ce qu'ONNX ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_vectors = load_dataset(DATASET_NAME)\n",
    "\n",
    "# get user query from imput\n",
    "while True:\n",
    "    query = input(\"Enter a query: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    videos = get_videos(query, pd_vectors, 5)\n",
    "    display_results(videos, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Avertissement** :  \nCe document a été traduit à l’aide du service de traduction par IA [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d’assurer l’exactitude de la traduction, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d’origine doit être considéré comme la source faisant autorité. Pour les informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous déclinons toute responsabilité en cas de malentendus ou d’interprétations erronées résultant de l’utilisation de cette traduction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "32c6b8e9e87156b9c63ee62a6fd7f526",
   "translation_date": "2025-08-25T18:37:39+00:00",
   "source_file": "08-building-search-applications/python/aoai-solution.ipynb",
   "language_code": "fr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}