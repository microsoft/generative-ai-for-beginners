<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2b4c36be7d66b32e4fac47761718b4a9",
  "translation_date": "2025-07-09T11:22:46+00:00",
  "source_file": "05-advanced-prompts/README.md",
  "language_code": "fr"
}
-->

> "G√©n√©rer du code pour une API Web Python"
```python
#import necessary libraries
import flask
from flask import request, jsonify

#create an instance of the Flask class
app = flask.Flask(__name__)

#create an endpoint for the API
@app.route('/api/v1/endpoint', methods=['GET'])
def api_endpoint():
    #get the request data
    data = request.get_json()

    #process the data
    result = process_data(data)

    #return the result
    return jsonify(result)

#function to process the data
def process_data(data):
    #process the data
    result = {'result': 'success'}

    #return the result
    return result

#run the Flask app
if __name__ == '__main__':
    app.run()

```

Relancer le prompt nous donne ce r√©sultat :

```python
#import necessary libraries
import flask
from flask import request, jsonify

#create a Flask app
app = flask.Flask(__name__)

#create a route for the API
@app.route('/api', methods=['GET'])
def api_call():
    #get the data from the request
    data = request.get_json()

    #process the data
    result = process_data(data)

    #return the result
    return jsonify(result)

#define the process_data function
def process_data(data):
    #do something with the data
    result = data + 1

    #return the result
    return result

#run the app
if __name__ == '__main__':
    app.run()

```

Il n‚Äôy a qu‚Äôune toute petite diff√©rence entre ces deux sorties. Faisons l‚Äôinverse cette fois, r√©glons la temp√©rature √† 0,9 :

```python
# Import necessary libraries
import flask
from flask import request, jsonify

# Create a Flask app
app = flask.Flask(__name__)

# Create a route for the API
@app.route('/api', methods=['GET'])
def api_call():
    # Get the data from the request
    data = request.args

    # Process the data
    result = process_data(data)

    # Return the result
    return jsonify(result)

# Function to process the data
def process_data(data):
    # Do something with the data
    result = data + 1

    # Return the result
    return result

# Run the app
if __name__ == '__main__':
    app.run()

```

et la deuxi√®me tentative avec une temp√©rature √† 0,9 :

```python
import flask
from flask import request, jsonify

# create the Flask app
app = flask.Flask(__name__)
app.config['DEBUG'] = True

# create some test data
books = [
    {'id': 0, 'title': 'A Fire Upon The Deep', 'author': 'Vernor Vinge', 'first_sentence': 'The coldsleep itself was dreamless.', 'year_published': '1992'},
    {'id': 1, 'title': 'The Ones Who Walk Away From Omelas', 'author': 'Ursula K. Le Guin', 'first_sentence': 'With a clamor of bells that set the swallows soaring, the Festival of Summer came to the city Omelas, bright-towered by the sea.', 'published': '1973'},
    {'id': 2, 'title': 'Dhalgren', 'author': 'Samuel R. Delany', 'first_sentence': 'to wound the autumnal city.', 'published': '1975'}
]

# create an endpoint
@app.route('/', methods=['GET'])
def home():
    return '''<h1>Welcome to our book API!</h1>'''

@app.route('/api/v1/resources/books

```

Comme vous pouvez le voir, les r√©sultats ne pourraient pas √™tre plus vari√©s.

> Notez qu‚Äôil existe d‚Äôautres param√®tres que vous pouvez modifier pour varier la sortie, comme top-k, top-p, repetition penalty, length penalty et diversity penalty, mais ceux-ci d√©passent le cadre de ce cours.

## Bonnes pratiques

Il existe de nombreuses pratiques que vous pouvez appliquer pour essayer d‚Äôobtenir ce que vous souhaitez. Vous trouverez votre propre style √† mesure que vous utiliserez le prompting de plus en plus.

En plus des techniques que nous avons abord√©es, voici quelques bonnes pratiques √† consid√©rer lors du prompting d‚Äôun LLM.

Voici quelques bonnes pratiques √† garder en t√™te :

- **Sp√©cifiez le contexte**. Le contexte est important, plus vous pouvez pr√©ciser comme le domaine, le sujet, etc., mieux c‚Äôest.
- Limitez la sortie. Si vous voulez un nombre pr√©cis d‚Äô√©l√©ments ou une longueur sp√©cifique, indiquez-le.
- **Pr√©cisez √† la fois quoi et comment**. N‚Äôoubliez pas de mentionner √† la fois ce que vous voulez et comment vous le voulez, par exemple ¬´ Cr√©ez une API Web Python avec les routes products et customers, divisez-la en 3 fichiers ¬ª.
- **Utilisez des mod√®les**. Souvent, vous voudrez enrichir vos prompts avec des donn√©es de votre entreprise. Utilisez des mod√®les pour cela. Les mod√®les peuvent contenir des variables que vous remplacez par des donn√©es r√©elles.
- **Orthographiez correctement**. Les LLM peuvent vous fournir une r√©ponse correcte, mais si vous orthographiez bien, vous obtiendrez une meilleure r√©ponse.

## Exercice

Voici un code en Python montrant comment construire une API simple avec Flask :

```python
from flask import Flask, request

app = Flask(__name__)

@app.route('/')
def hello():
    name = request.args.get('name', 'World')
    return f'Hello, {name}!'

if __name__ == '__main__':
    app.run()
```

Utilisez un assistant IA comme GitHub Copilot ou ChatGPT et appliquez la technique de ¬´ self-refine ¬ª pour am√©liorer le code.

## Solution

Essayez de r√©soudre l‚Äôexercice en ajoutant des prompts appropri√©s au code.

> [!TIP]
> Formulez un prompt demandant une am√©lioration, il est judicieux de limiter le nombre d‚Äôam√©liorations. Vous pouvez aussi demander une am√©lioration dans un domaine pr√©cis, par exemple architecture, performance, s√©curit√©, etc.

[Solution](../../../05-advanced-prompts/python/aoai-solution.py)

## V√©rification des connaissances

Pourquoi utiliserais-je le chain-of-thought prompting ? Donnez-moi 1 r√©ponse correcte et 2 r√©ponses incorrectes.

1. Pour apprendre au LLM comment r√©soudre un probl√®me.  
1. B, Pour apprendre au LLM √† trouver des erreurs dans le code.  
1. C, Pour demander au LLM de proposer diff√©rentes solutions.

A : 1, car le chain-of-thought consiste √† montrer au LLM comment r√©soudre un probl√®me en lui fournissant une s√©rie d‚Äô√©tapes, ainsi que des probl√®mes similaires et leur r√©solution.

## üöÄ D√©fi

Vous venez d‚Äôutiliser la technique de self-refine dans l‚Äôexercice. Prenez n‚Äôimporte quel programme que vous avez cr√©√© et r√©fl√©chissez aux am√©liorations que vous souhaiteriez y apporter. Utilisez maintenant la technique de self-refine pour appliquer les changements propos√©s. Quel est votre avis sur le r√©sultat, meilleur ou pire ?

## Excellent travail ! Continuez √† apprendre

Apr√®s avoir termin√© cette le√ßon, consultez notre [collection d‚Äôapprentissage sur l‚ÄôIA g√©n√©rative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour continuer √† approfondir vos connaissances en IA g√©n√©rative !

Rendez-vous √† la Le√ßon 6 o√π nous appliquerons nos connaissances en Prompt Engineering en [cr√©ant des applications de g√©n√©ration de texte](../06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst)

**Avertissement** :  
Ce document a √©t√© traduit √† l‚Äôaide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d‚Äôassurer l‚Äôexactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d‚Äôorigine doit √™tre consid√©r√© comme la source faisant foi. Pour les informations critiques, une traduction professionnelle r√©alis√©e par un humain est recommand√©e. Nous d√©clinons toute responsabilit√© en cas de malentendus ou de mauvaises interpr√©tations r√©sultant de l‚Äôutilisation de cette traduction.