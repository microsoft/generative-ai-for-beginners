{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Chapitre 7 : Créer des applications de chat\n",
    "## Démarrage rapide avec l’API Github Models\n",
    "\n",
    "Ce notebook est adapté du [dépôt d’exemples Azure OpenAI](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst) qui comprend des notebooks permettant d’accéder aux services [Azure OpenAI](notebook-azure-openai.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Vue d'ensemble  \n",
    "« Les grands modèles de langage sont des fonctions qui associent un texte à un autre texte. Lorsqu’on leur fournit une chaîne de texte en entrée, un grand modèle de langage tente de prédire le texte qui suivra »(1). Ce carnet « démarrage rapide » présente aux utilisateurs les concepts clés des LLM, les principaux prérequis pour commencer avec AML, une introduction simplifiée à la conception de prompts, ainsi que plusieurs exemples courts illustrant différents cas d’usage.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Table des matières  \n",
    "\n",
    "[Aperçu](../../../../07-building-chat-applications/python)  \n",
    "[Comment utiliser le service OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[1. Créer votre service OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[2. Installation](../../../../07-building-chat-applications/python)    \n",
    "[3. Identifiants](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Cas d’utilisation](../../../../07-building-chat-applications/python)    \n",
    "[1. Résumer un texte](../../../../07-building-chat-applications/python)  \n",
    "[2. Classer un texte](../../../../07-building-chat-applications/python)  \n",
    "[3. Générer de nouveaux noms de produits](../../../../07-building-chat-applications/python)  \n",
    "[4. Affiner un classificateur](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Références](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Créez votre première invite  \n",
    "Ce court exercice vous donnera une introduction de base à la soumission d'invites à un modèle dans Github Models pour une tâche simple de \"résumé\".\n",
    "\n",
    "\n",
    "**Étapes** :  \n",
    "1. Installez la bibliothèque `azure-ai-inference` dans votre environnement python, si ce n'est pas déjà fait.  \n",
    "2. Chargez les bibliothèques d'aide standard et configurez les identifiants pour Github Models.  \n",
    "3. Choisissez un modèle pour votre tâche  \n",
    "4. Rédigez une invite simple pour le modèle  \n",
    "5. Envoyez votre requête à l'API du modèle !\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Installez `azure-ai-inference`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Trouver le bon modèle  \n",
    "Les modèles GPT-3.5-turbo ou GPT-4 peuvent comprendre et générer du langage naturel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Conception de prompts  \n",
    "\n",
    "« La magie des grands modèles de langage, c’est qu’en étant entraînés à minimiser cette erreur de prédiction sur d’immenses quantités de texte, les modèles finissent par apprendre des concepts utiles pour ces prédictions. Par exemple, ils apprennent des notions comme »(1) :\n",
    "\n",
    "* comment épeler\n",
    "* comment fonctionne la grammaire\n",
    "* comment reformuler\n",
    "* comment répondre à des questions\n",
    "* comment tenir une conversation\n",
    "* comment écrire dans de nombreuses langues\n",
    "* comment coder\n",
    "* etc.\n",
    "\n",
    "#### Comment contrôler un grand modèle de langage  \n",
    "« Parmi toutes les entrées d’un grand modèle de langage, c’est de loin le prompt textuel qui a le plus d’influence »(1).\n",
    "\n",
    "On peut inciter les grands modèles de langage à produire des résultats de plusieurs façons :\n",
    "\n",
    "Instruction : Dites au modèle ce que vous voulez  \n",
    "Complétion : Amenez le modèle à compléter le début de ce que vous souhaitez  \n",
    "Démonstration : Montrez au modèle ce que vous attendez, soit avec :  \n",
    "Quelques exemples dans le prompt  \n",
    "Des centaines ou des milliers d’exemples dans un jeu de données d’entraînement pour l’ajustement fin »\n",
    "\n",
    "\n",
    "\n",
    "#### Il existe trois règles de base pour créer des prompts :\n",
    "\n",
    "**Montrer et expliquer**. Soyez clair sur ce que vous attendez, que ce soit par des instructions, des exemples, ou une combinaison des deux. Si vous voulez que le modèle classe une liste d’éléments par ordre alphabétique ou qu’il catégorise un paragraphe selon le sentiment, montrez-lui ce que vous attendez.\n",
    "\n",
    "**Fournir des données de qualité**. Si vous essayez de créer un classificateur ou d’amener le modèle à suivre un schéma, assurez-vous d’avoir suffisamment d’exemples. Relisez bien vos exemples — le modèle est généralement assez intelligent pour repérer les fautes d’orthographe et vous donner une réponse, mais il pourrait aussi supposer que c’est intentionnel, ce qui peut influencer la réponse.\n",
    "\n",
    "**Vérifiez vos paramètres.** Les paramètres temperature et top_p contrôlent à quel point le modèle est déterministe dans la génération de la réponse. Si vous attendez une réponse unique et correcte, il vaut mieux les régler plus bas. Si vous souhaitez des réponses plus variées, vous pouvez les augmenter. L’erreur la plus fréquente avec ces paramètres est de croire qu’ils contrôlent « l’intelligence » ou la « créativité » du modèle.\n",
    "\n",
    "\n",
    "Source : https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Résumer un texte  \n",
    "#### Défi  \n",
    "Résumez un texte en ajoutant un 'tl;dr:' à la fin d’un passage. Remarquez comment le modèle comprend comment accomplir plusieurs tâches sans instructions supplémentaires. Vous pouvez essayer des consignes plus détaillées que tl;dr pour modifier le comportement du modèle et personnaliser le résumé que vous obtenez(3).  \n",
    "\n",
    "Des travaux récents ont montré des progrès importants sur de nombreuses tâches et benchmarks en NLP grâce à une pré-formation sur un vaste corpus de textes, suivie d’un ajustement sur une tâche spécifique. Bien que cette méthode soit généralement indépendante de la tâche dans son architecture, elle nécessite tout de même des jeux de données d’ajustement spécifiques à la tâche, comportant des milliers ou des dizaines de milliers d’exemples. À l’inverse, les humains peuvent généralement accomplir une nouvelle tâche linguistique avec seulement quelques exemples ou de simples instructions – ce que les systèmes NLP actuels ont encore du mal à faire. Ici, nous montrons qu’augmenter la taille des modèles de langage améliore nettement les performances indépendantes de la tâche, même avec peu d’exemples, et peut parfois rivaliser avec les meilleures approches d’ajustement précédentes. \n",
    "\n",
    "\n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Exercices pour plusieurs cas d’utilisation  \n",
    "1. Résumer un texte  \n",
    "2. Classer un texte  \n",
    "3. Générer de nouveaux noms de produits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Classer un texte  \n",
    "#### Défi  \n",
    "Classer des éléments dans des catégories fournies au moment de l’inférence. Dans l’exemple suivant, nous donnons à la fois les catégories et le texte à classer dans l’invite (*playground_reference).\n",
    "\n",
    "Demande du client : Bonjour, une des touches de mon clavier d’ordinateur portable s’est cassée récemment et j’aurais besoin d’un remplacement :\n",
    "\n",
    "Catégorie classée :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Générer de nouveaux noms de produits\n",
    "#### Défi\n",
    "Créez des noms de produits à partir de mots exemples. Ici, nous incluons dans l’invite des informations sur le produit pour lequel nous allons générer des noms. Nous fournissons également un exemple similaire pour montrer le modèle que nous souhaitons obtenir. Nous avons aussi réglé la valeur de la température à un niveau élevé pour augmenter l’aléatoire et obtenir des réponses plus innovantes.\n",
    "\n",
    "Description du produit : Un appareil à milkshake pour la maison\n",
    "Mots de base : rapide, sain, compact.\n",
    "Noms de produits : HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Description du produit : Une paire de chaussures qui s’adapte à toutes les tailles de pied.\n",
    "Mots de base : adaptable, ajustement, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Références  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Exemples OpenAI Studio](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Bonnes pratiques pour l’ajustement fin de GPT-3 pour la classification de texte](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Pour plus d’aide  \n",
    "[Équipe de commercialisation OpenAI](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Contributeurs\n",
    "* [Chew-Yean Yam](https://www.linkedin.com/in/cyyam/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Avertissement** :  \nCe document a été traduit à l’aide du service de traduction par IA [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d’assurer l’exactitude de la traduction, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d’origine doit être considéré comme la source faisant autorité. Pour les informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous déclinons toute responsabilité en cas de malentendus ou d’interprétations erronées résultant de l’utilisation de cette traduction.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "3eeb8e5cad61b52a8366f6259a53ed49",
   "translation_date": "2025-08-25T17:19:19+00:00",
   "source_file": "07-building-chat-applications/python/githubmodels-assignment.ipynb",
   "language_code": "fr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}