<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a45c318dc6ebc2604f35b8b829f93af2",
  "translation_date": "2025-05-19T09:34:10+00:00",
  "source_file": "04-prompt-engineering-fundamentals/README.md",
  "language_code": "fr"
}
-->
# Fondamentaux de l'Ing√©nierie des Prompts

## Introduction
Ce module couvre les concepts essentiels et les techniques pour cr√©er des prompts efficaces dans les mod√®les d'IA g√©n√©rative. La fa√ßon dont vous r√©digez votre prompt pour un LLM est √©galement importante. Un prompt soigneusement con√ßu peut obtenir une meilleure qualit√© de r√©ponse. Mais que signifient exactement des termes comme _prompt_ et _ing√©nierie des prompts_ ? Et comment am√©liorer l'_entr√©e_ du prompt que j'envoie au LLM ? Ce sont les questions auxquelles nous allons essayer de r√©pondre dans ce chapitre et le suivant.

L'_IA g√©n√©rative_ est capable de cr√©er du nouveau contenu (par exemple, texte, images, audio, code, etc.) en r√©ponse aux demandes des utilisateurs. Elle y parvient en utilisant des _Mod√®les de Langage de Grande Taille_ comme la s√©rie GPT ("Generative Pre-trained Transformer") d'OpenAI, qui sont entra√Æn√©s √† utiliser le langage naturel et le code.

Les utilisateurs peuvent d√©sormais interagir avec ces mod√®les en utilisant des paradigmes familiers comme le chat, sans avoir besoin d'expertise technique ou de formation. Les mod√®les sont _bas√©s sur des prompts_ - les utilisateurs envoient une entr√©e textuelle (prompt) et obtiennent la r√©ponse de l'IA (compl√©tion). Ils peuvent ensuite "discuter avec l'IA" de mani√®re it√©rative, dans des conversations √† plusieurs tours, en affinant leur prompt jusqu'√† ce que la r√©ponse corresponde √† leurs attentes.

Les "prompts" deviennent maintenant l'interface de _programmation_ principale pour les applications d'IA g√©n√©rative, indiquant aux mod√®les quoi faire et influen√ßant la qualit√© des r√©ponses retourn√©es. L'"ing√©nierie des prompts" est un domaine d'√©tude en pleine croissance qui se concentre sur la _conception et l'optimisation_ des prompts pour fournir des r√©ponses coh√©rentes et de qualit√© √† grande √©chelle.

## Objectifs d'apprentissage

Dans cette le√ßon, nous apprenons ce qu'est l'Ing√©nierie des Prompts, pourquoi elle est importante, et comment nous pouvons cr√©er des prompts plus efficaces pour un mod√®le donn√© et un objectif d'application. Nous comprendrons les concepts de base et les meilleures pratiques pour l'ing√©nierie des prompts - et nous d√©couvrirons un environnement "bac √† sable" interactif Jupyter Notebooks o√π nous pourrons voir ces concepts appliqu√©s √† des exemples r√©els.

√Ä la fin de cette le√ßon, nous serons capables de :

1. Expliquer ce qu'est l'ing√©nierie des prompts et pourquoi elle est importante.
2. D√©crire les composants d'un prompt et comment ils sont utilis√©s.
3. Apprendre les meilleures pratiques et techniques pour l'ing√©nierie des prompts.
4. Appliquer les techniques apprises √† des exemples r√©els, en utilisant un point de terminaison OpenAI.

## Termes cl√©s

Ing√©nierie des Prompts : La pratique de concevoir et de raffiner les entr√©es pour guider les mod√®les d'IA vers la production de sorties souhait√©es.  
Tokenisation : Le processus de conversion du texte en unit√©s plus petites, appel√©es tokens, qu'un mod√®le peut comprendre et traiter.  
LLMs ajust√©s par instruction : Mod√®les de Langage de Grande Taille (LLMs) qui ont √©t√© ajust√©s avec des instructions sp√©cifiques pour am√©liorer la pr√©cision et la pertinence de leurs r√©ponses.

## Bac √† sable d'apprentissage

L'ing√©nierie des prompts est actuellement plus un art qu'une science. La meilleure fa√ßon d'am√©liorer notre intuition est de _pratiquer davantage_ et d'adopter une approche d'essais et d'erreurs qui combine expertise dans le domaine d'application avec des techniques recommand√©es et des optimisations sp√©cifiques au mod√®le.

Le Notebook Jupyter accompagnant cette le√ßon fournit un environnement de _bac √† sable_ o√π vous pouvez essayer ce que vous apprenez - au fur et √† mesure ou dans le cadre du d√©fi de code √† la fin. Pour ex√©cuter les exercices, vous aurez besoin de :

1. **Une cl√© API Azure OpenAI** - le point de terminaison du service pour un LLM d√©ploy√©.
2. **Un environnement Python** - dans lequel le Notebook peut √™tre ex√©cut√©.
3. **Variables d'environnement locales** - _compl√©tez les √©tapes du [SETUP](./../00-course-setup/SETUP.md?WT.mc_id=academic-105485-koreyst) maintenant pour vous pr√©parer_.

Le notebook est fourni avec des exercices de _d√©marrage_ - mais vous √™tes encourag√© √† ajouter vos propres sections _Markdown_ (description) et _Code_ (demandes de prompts) pour essayer plus d'exemples ou d'id√©es - et d√©velopper votre intuition pour la conception de prompts.

## Guide illustr√©

Vous souhaitez avoir une vue d'ensemble de ce que couvre cette le√ßon avant de plonger dedans ? Consultez ce guide illustr√©, qui vous donne une id√©e des principaux sujets abord√©s et des points cl√©s √† retenir pour chaque section. La feuille de route de la le√ßon vous guide de la compr√©hension des concepts et des d√©fis de base √† leur r√©solution avec des techniques d'ing√©nierie des prompts et des meilleures pratiques. Notez que la section "Techniques avanc√©es" de ce guide fait r√©f√©rence √† du contenu couvert dans le _chapitre suivant_ de ce programme.

## Notre Startup

Maintenant, parlons de la fa√ßon dont _ce sujet_ est li√© √† notre mission de startup pour [apporter l'innovation de l'IA √† l'√©ducation](https://educationblog.microsoft.com/2023/06/collaborating-to-bring-ai-innovation-to-education?WT.mc_id=academic-105485-koreyst). Nous voulons cr√©er des applications d'apprentissage _personnalis√©_ aliment√©es par l'IA - alors r√©fl√©chissons √† la mani√®re dont diff√©rents utilisateurs de notre application pourraient "concevoir" des prompts :

- **Les administrateurs** pourraient demander √† l'IA d'_analyser les donn√©es du programme pour identifier les lacunes dans la couverture_. L'IA peut r√©sumer les r√©sultats ou les visualiser avec du code.
- **Les √©ducateurs** pourraient demander √† l'IA de _g√©n√©rer un plan de cours pour un public cible et un sujet_. L'IA peut construire le plan personnalis√© dans un format sp√©cifi√©.
- **Les √©tudiants** pourraient demander √† l'IA de _les tutorer dans une mati√®re difficile_. L'IA peut maintenant guider les √©tudiants avec des le√ßons, des indices et des exemples adapt√©s √† leur niveau.

Ce n'est que la partie √©merg√©e de l'iceberg. Consultez [Prompts For Education](https://github.com/microsoft/prompts-for-edu/tree/main?WT.mc_id=academic-105485-koreyst) - une biblioth√®que de prompts open-source organis√©e par des experts en √©ducation - pour avoir une id√©e plus large des possibilit√©s ! _Essayez de lancer certains de ces prompts dans le bac √† sable ou en utilisant le OpenAI Playground pour voir ce qui se passe !_

## Qu'est-ce que l'Ing√©nierie des Prompts ?

Nous avons commenc√© cette le√ßon en d√©finissant l'**Ing√©nierie des Prompts** comme le processus de _conception et d'optimisation_ des entr√©es textuelles (prompts) pour fournir des r√©ponses coh√©rentes et de qualit√© (compl√©tions) pour un objectif d'application donn√© et un mod√®le. Nous pouvons penser √† cela comme un processus en deux √©tapes :

- _concevoir_ le prompt initial pour un mod√®le et un objectif donn√©s
- _affiner_ le prompt de mani√®re it√©rative pour am√©liorer la qualit√© de la r√©ponse

C'est n√©cessairement un processus d'essais et d'erreurs qui n√©cessite de l'intuition et des efforts de l'utilisateur pour obtenir des r√©sultats optimaux. Alors pourquoi est-ce important ? Pour r√©pondre √† cette question, nous devons d'abord comprendre trois concepts :

- _Tokenisation_ = comment le mod√®le "voit" le prompt
- _LLMs de base_ = comment le mod√®le de base "traite" un prompt
- _LLMs ajust√©s par instruction_ = comment le mod√®le peut maintenant voir les "t√¢ches"

### Tokenisation

Un LLM voit les prompts comme une _s√©quence de tokens_ o√π diff√©rents mod√®les (ou versions d'un mod√®le) peuvent tokeniser le m√™me prompt de diff√©rentes mani√®res. √âtant donn√© que les LLMs sont entra√Æn√©s sur des tokens (et non sur du texte brut), la fa√ßon dont les prompts sont tokenis√©s a un impact direct sur la qualit√© de la r√©ponse g√©n√©r√©e.

Pour avoir une intuition sur le fonctionnement de la tokenisation, essayez des outils comme le [Tokenizer d'OpenAI](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst) montr√© ci-dessous. Copiez votre prompt - et voyez comment il est converti en tokens, en pr√™tant attention √† la fa√ßon dont les espaces blancs et les signes de ponctuation sont trait√©s. Notez que cet exemple montre un ancien LLM (GPT-3) - donc l'essayer avec un mod√®le plus r√©cent peut produire un r√©sultat diff√©rent.

### Concept : Mod√®les de Base

Une fois qu'un prompt est tokenis√©, la fonction principale du ["LLM de base"](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) (ou mod√®le de base) est de pr√©dire le token dans cette s√©quence. √âtant donn√© que les LLMs sont entra√Æn√©s sur d'√©normes ensembles de donn√©es textuelles, ils ont une bonne compr√©hension des relations statistiques entre les tokens et peuvent faire cette pr√©diction avec une certaine confiance. Notez qu'ils ne comprennent pas le _sens_ des mots dans le prompt ou le token ; ils voient juste un mod√®le qu'ils peuvent "compl√©ter" avec leur prochaine pr√©diction. Ils peuvent continuer √† pr√©dire la s√©quence jusqu'√† ce qu'elle soit termin√©e par l'intervention de l'utilisateur ou une condition pr√©√©tablie.

Vous voulez voir comment fonctionne la compl√©tion bas√©e sur les prompts ? Entrez le prompt ci-dessus dans le [_Chat Playground_](https://oai.azure.com/playground?WT.mc_id=academic-105485-koreyst) d'Azure OpenAI avec les param√®tres par d√©faut. Le syst√®me est configur√© pour traiter les prompts comme des demandes d'informations - vous devriez donc voir une compl√©tion qui satisfait ce contexte.

Mais que se passe-t-il si l'utilisateur souhaite voir quelque chose de sp√©cifique qui r√©pond √† certains crit√®res ou objectifs de t√¢che ? C'est l√† que les LLMs _ajust√©s par instruction_ entrent en jeu.

### Concept : LLMs Ajust√©s par Instruction

Un [LLM Ajust√© par Instruction](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) commence par le mod√®le de base et l'affine avec des exemples ou des paires d'entr√©e/sortie (par exemple, des "messages" √† plusieurs tours) qui peuvent contenir des instructions claires - et la r√©ponse de l'IA tente de suivre cette instruction.

Cela utilise des techniques comme l'apprentissage par renforcement avec retour humain (RLHF) qui peuvent entra√Æner le mod√®le √† _suivre des instructions_ et _apprendre des retours_ afin qu'il produise des r√©ponses mieux adapt√©es aux applications pratiques et plus pertinentes pour les objectifs des utilisateurs.

Essayons-le - revisitez le prompt ci-dessus, mais maintenant changez le _message syst√®me_ pour fournir l'instruction suivante comme contexte :

> _R√©sum√© du contenu fourni pour un √©l√®ve de deuxi√®me ann√©e. Limitez le r√©sultat √† un paragraphe avec 3 √† 5 puces._

Voyez comment le r√©sultat est maintenant ajust√© pour refl√©ter l'objectif et le format souhait√©s ? Un √©ducateur peut maintenant utiliser directement cette r√©ponse dans ses diapositives pour cette classe.

## Pourquoi avons-nous besoin de l'Ing√©nierie des Prompts ?

Maintenant que nous savons comment les prompts sont trait√©s par les LLMs, parlons de _pourquoi_ nous avons besoin de l'ing√©nierie des prompts. La r√©ponse r√©side dans le fait que les LLMs actuels posent un certain nombre de d√©fis qui rendent les _compl√©tions fiables et coh√©rentes_ plus difficiles √† obtenir sans mettre d'effort dans la construction et l'optimisation des prompts. Par exemple :

1. **Les r√©ponses des mod√®les sont stochastiques.** Le _m√™me prompt_ produira probablement des r√©ponses diff√©rentes avec diff√©rents mod√®les ou versions de mod√®les. Et il peut m√™me produire des r√©sultats diff√©rents avec le _m√™me mod√®le_ √† diff√©rents moments. _Les techniques d'ing√©nierie des prompts peuvent nous aider √† minimiser ces variations en fournissant de meilleures balises_.

2. **Les mod√®les peuvent fabriquer des r√©ponses.** Les mod√®les sont pr√©-entra√Æn√©s avec des ensembles de donn√©es _grands mais finis_, ce qui signifie qu'ils n'ont pas connaissance des concepts en dehors de cette port√©e de formation. En cons√©quence, ils peuvent produire des compl√©tions qui sont inexactes, imaginaires ou directement contradictoires avec des faits connus. _Les techniques d'ing√©nierie des prompts aident les utilisateurs √† identifier et √† att√©nuer ces fabrications, par exemple en demandant √† l'IA des citations ou un raisonnement_.

3. **Les capacit√©s des mod√®les varieront.** Les mod√®les plus r√©cents ou les g√©n√©rations de mod√®les auront des capacit√©s plus riches mais apporteront √©galement des particularit√©s uniques et des compromis en termes de co√ªt et de complexit√©. _L'ing√©nierie des prompts peut nous aider √† d√©velopper des meilleures pratiques et des workflows qui abstraient les diff√©rences et s'adaptent aux exigences sp√©cifiques des mod√®les de mani√®re √©volutive et transparente_.

Voyons cela en action dans le OpenAI ou Azure OpenAI Playground :

- Utilisez le m√™me prompt avec diff√©rents d√©ploiements de LLM (par exemple, OpenAI, Azure OpenAI, Hugging Face) - avez-vous vu les variations ?
- Utilisez le m√™me prompt √† plusieurs reprises avec le _m√™me_ d√©ploiement de LLM (par exemple, le terrain de jeu Azure OpenAI) - comment ces variations diff√®rent-elles ?

### Exemple de Fabrications

Dans ce cours, nous utilisons le terme **"fabrication"** pour faire r√©f√©rence au ph√©nom√®ne o√π les LLMs g√©n√®rent parfois des informations factuellement incorrectes en raison de limitations dans leur formation ou d'autres contraintes. Vous avez peut-√™tre aussi entendu cela appel√© _"hallucinations"_ dans des articles populaires ou des articles de recherche. Cependant, nous recommandons fortement d'utiliser le terme _"fabrication"_ pour ne pas anthropomorphiser accidentellement le comportement en attribuant un trait humain √† un r√©sultat pilot√© par une machine. Cela renforce √©galement les [directives d'IA Responsable](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-105485-koreyst) d'un point de vue terminologique, en supprimant les termes qui peuvent √©galement √™tre consid√©r√©s comme offensants ou non inclusifs dans certains contextes.

Vous voulez avoir une id√©e de la fa√ßon dont les fabrications fonctionnent ? Pensez √† un prompt qui demande √† l'IA de g√©n√©rer du contenu pour un sujet inexistant (pour s'assurer qu'il ne se trouve pas dans l'ensemble de donn√©es d'entra√Ænement). Par exemple - j'ai essay√© ce prompt :

> **Prompt :** g√©n√©rez un plan de cours sur la Guerre Martienne de 2076.

Une recherche sur le web m'a montr√© qu'il existait des r√©cits fictifs (par exemple, des s√©ries t√©l√©vis√©es ou des livres) sur les guerres martiennes - mais aucune en 2076. Le bon sens nous dit aussi que 2076 est _dans le futur_ et ne peut donc pas √™tre associ√© √† un √©v√©nement r√©el.

Alors que se passe-t-il lorsque nous ex√©cutons ce prompt avec diff√©rents fournisseurs de LLM ?

Comme pr√©vu, chaque mod√®le (ou version de mod√®le) produit des r√©ponses l√©g√®rement diff√©rentes gr√¢ce au comportement stochastique et aux variations des capacit√©s des mod√®les. Par exemple, un mod√®le cible un public de 8√®me ann√©e tandis que l'autre suppose un lyc√©en. Mais les trois mod√®les ont g√©n√©r√© des r√©ponses qui pourraient convaincre un utilisateur non inform√© que l'√©v√©nement √©tait r√©el.

Les techniques d'ing√©nierie des prompts comme le _m√©taprompting_ et la _configuration de la temp√©rature_ peuvent r√©duire les fabrications des mod√®les dans une certaine mesure. Les nouvelles _architectures_ d'ing√©nierie des prompts int√®grent √©galement de nouveaux outils et techniques de mani√®re transparente dans le flux de prompts, pour att√©nuer ou r√©duire certains de ces effets.

## √âtude de Cas : GitHub Copilot

Concluons cette section en ayant une id√©e de la fa√ßon dont l'ing√©nierie des prompts est utilis√©e dans des solutions r√©elles en examinant une √©tude de cas : [GitHub Copilot](https://github.com/features/copilot?WT.mc_id=academic-105485-koreyst).

GitHub Copilot est votre "Programmeur Partenaire IA" - il convertit les prompts textuels en compl√©tions de code et est int√©gr√© dans votre environnement de d√©veloppement (par exemple, Visual Studio Code) pour une exp√©rience utilisateur transparente. Comme document√© dans la s√©rie de blogs ci-dessous, la premi√®re version √©tait bas√©e sur le mod√®le OpenAI Codex - avec des ing√©nieurs r√©alisant rapidement la n√©cessit√© d'affiner le mod√®le et de d√©velopper de meilleures techniques d'ing√©nierie des prompts, pour am√©liorer la qualit√© du code. En juillet, ils ont [d√©voil√© un mod√®le d'IA am√©lior√© qui va au-del√† de Codex](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/?WT.mc_id=academic-105485-koreyst) pour des suggestions encore plus rapides.

Lisez les articles dans l'ordre, pour suivre leur parcours d'apprentissage.

- **Mai 2023** | [GitHub Copilot s'am√©liore pour comprendre votre code](https://github.blog/2023-05-17-how-github-copilot-is-getting-better-at-understanding-your-code/?WT.mc_id=academic-105485-koreyst)
- **Mai 2023** | [√Ä l'int√©rieur de GitHub : Travailler avec les LLMs derri√®re GitHub Copilot](https://github.blog/2023-05-17-inside-github-working-with-the-llms-behind-github-copilot/?WT.mc_id=academic-105485-koreyst).
- **Juin 2023** | [Comment √©crire de meilleurs prompts pour GitHub Copilot](https://github.blog/2023-06-20-how-to-write-better-prompts-for-github-copilot/?WT.mc_id=academic-105485-koreyst).
- **Juillet 2023** | [.. GitHub Copilot va au-del√† de Codex avec un mod√®le d'IA am√©lior√©](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/?WT.mc_id=academic-105485-koreyst)
- **Juillet 2023** | [Guide du d√©veloppeur pour l'ing√©nierie des prompts et les LLMs](
Enfin, la v√©ritable valeur des mod√®les r√©side dans la capacit√© √† cr√©er et publier des _biblioth√®ques de prompts_ pour des domaines d'application verticaux - o√π le mod√®le de prompt est maintenant _optimis√©_ pour refl√©ter le contexte ou les exemples sp√©cifiques √† l'application, rendant les r√©ponses plus pertinentes et pr√©cises pour le public cible. Le d√©p√¥t [Prompts For Edu](https://github.com/microsoft/prompts-for-edu?WT.mc_id=academic-105485-koreyst) est un excellent exemple de cette approche, en cr√©ant une biblioth√®que de prompts pour le domaine √©ducatif, avec un accent sur des objectifs cl√©s comme la planification de le√ßons, la conception de programmes, le tutorat des √©tudiants, etc.

## Contenu de soutien

Si nous consid√©rons la construction de prompts comme ayant une instruction (t√¢che) et un objectif (contenu principal), alors le _contenu secondaire_ est comme un contexte suppl√©mentaire que nous fournissons pour **influencer le r√©sultat d'une certaine mani√®re**. Cela pourrait √™tre des param√®tres de r√©glage, des instructions de formatage, des taxonomies de sujets, etc., qui peuvent aider le mod√®le √† _adapter_ sa r√©ponse pour correspondre aux objectifs ou attentes de l'utilisateur souhait√©.

Par exemple : √âtant donn√© un catalogue de cours avec des m√©tadonn√©es √©tendues (nom, description, niveau, balises de m√©tadonn√©es, instructeur, etc.) sur tous les cours disponibles dans le programme :

- nous pouvons d√©finir une instruction pour "r√©sumer le catalogue de cours pour l'automne 2023"
- nous pouvons utiliser le contenu principal pour fournir quelques exemples du r√©sultat souhait√©
- nous pouvons utiliser le contenu secondaire pour identifier les 5 principales "balises" d'int√©r√™t.

Maintenant, le mod√®le peut fournir un r√©sum√© dans le format montr√© par les quelques exemples - mais si un r√©sultat a plusieurs balises, il peut prioriser les 5 balises identifi√©es dans le contenu secondaire.

---

<!--
MOD√àLE DE LE√áON :
Cette unit√© devrait couvrir le concept central #1.
Renforcez le concept avec des exemples et des r√©f√©rences.

CONCEPT #3 :
Techniques d'ing√©nierie de prompts.
Quelles sont quelques techniques de base pour l'ing√©nierie de prompts ?
Illustrer avec quelques exercices.
-->

## Meilleures pratiques de prompt

Maintenant que nous savons comment les prompts peuvent √™tre _construits_, nous pouvons commencer √† penser √† comment les _concevoir_ pour refl√©ter les meilleures pratiques. Nous pouvons penser √† cela en deux parties - avoir le bon _√©tat d'esprit_ et appliquer les bonnes _techniques_.

### √âtat d'esprit de l'ing√©nierie de prompts

L'ing√©nierie de prompts est un processus d'essais et d'erreurs, donc gardez √† l'esprit trois facteurs directeurs larges :

1. **La compr√©hension du domaine est importante.** La pr√©cision et la pertinence des r√©ponses sont fonction du _domaine_ dans lequel cette application ou cet utilisateur op√®re. Appliquez votre intuition et votre expertise du domaine pour **personnaliser davantage les techniques**. Par exemple, d√©finissez des _personnalit√©s sp√©cifiques au domaine_ dans vos prompts syst√®me, ou utilisez des _mod√®les sp√©cifiques au domaine_ dans vos prompts utilisateur. Fournissez du contenu secondaire qui refl√®te des contextes sp√©cifiques au domaine, ou utilisez des _indices et des exemples sp√©cifiques au domaine_ pour guider le mod√®le vers des mod√®les d'utilisation familiers.

2. **La compr√©hension du mod√®le est importante.** Nous savons que les mod√®les sont stochastiques par nature. Mais les impl√©mentations de mod√®les peuvent √©galement varier en termes de dataset d'entra√Ænement qu'ils utilisent (connaissances pr√©-entra√Æn√©es), des capacit√©s qu'ils fournissent (par exemple, via API ou SDK) et du type de contenu pour lequel ils sont optimis√©s (par exemple, code vs. images vs. texte). Comprenez les forces et les limitations du mod√®le que vous utilisez, et utilisez ces connaissances pour _prioriser les t√¢ches_ ou construire des _mod√®les personnalis√©s_ qui sont optimis√©s pour les capacit√©s du mod√®le.

3. **L'it√©ration et la validation sont importantes.** Les mod√®les √©voluent rapidement, et les techniques pour l'ing√©nierie de prompts aussi. En tant qu'expert du domaine, vous pouvez avoir d'autres contextes ou crit√®res pour _votre_ application sp√©cifique, qui peuvent ne pas s'appliquer √† la communaut√© plus large. Utilisez des outils et techniques d'ing√©nierie de prompts pour "lancer" la construction de prompts, puis it√©rez et validez les r√©sultats en utilisant votre propre intuition et expertise du domaine. Enregistrez vos id√©es et cr√©ez une **base de connaissances** (par exemple, biblioth√®ques de prompts) qui peut √™tre utilis√©e comme nouvelle base par d'autres, pour des it√©rations plus rapides √† l'avenir.

## Meilleures pratiques

Examinons maintenant les meilleures pratiques courantes recommand√©es par [OpenAI](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api?WT.mc_id=academic-105485-koreyst) et les praticiens de [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering#best-practices?WT.mc_id=academic-105485-koreyst).

| Quoi                              | Pourquoi                                                                                                                                                                                                                                               |
| :-------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| √âvaluer les derniers mod√®les.     | Les nouvelles g√©n√©rations de mod√®les sont susceptibles d'avoir des fonctionnalit√©s et une qualit√© am√©lior√©es - mais peuvent √©galement entra√Æner des co√ªts plus √©lev√©s. √âvaluez-les pour leur impact, puis prenez des d√©cisions de migration.                                                                                |
| S√©parer les instructions et le contexte | V√©rifiez si votre mod√®le/fournisseur d√©finit des _d√©limiteurs_ pour distinguer plus clairement les instructions, le contenu principal et secondaire. Cela peut aider les mod√®les √† assigner plus pr√©cis√©ment des poids aux tokens.                                                         |
| Soyez sp√©cifique et clair             | Donnez plus de d√©tails sur le contexte souhait√©, le r√©sultat, la longueur, le format, le style, etc. Cela am√©liorera √† la fois la qualit√© et la coh√©rence des r√©ponses. Capturez les recettes dans des mod√®les r√©utilisables.                                                          |
| Soyez descriptif, utilisez des exemples      | Les mod√®les peuvent mieux r√©pondre √† une approche "montrer et raconter". Commencez par une `zero-shot` approach where you give it an instruction (but no examples) then try `few-shot` as a refinement, providing a few examples of the desired output. Use analogies. |
| Use cues to jumpstart completions | Nudge it towards a desired outcome by giving it some leading words or phrases that it can use as a starting point for the response.                                                                                                               |
| Double Down                       | Sometimes you may need to repeat yourself to the model. Give instructions before and after your primary content, use an instruction and a cue, etc. Iterate & validate to see what works.                                                         |
| Order Matters                     | The order in which you present information to the model may impact the output, even in the learning examples, thanks to recency bias. Try different options to see what works best.                                                               |
| Give the model an ‚Äúout‚Äù           | Give the model a _fallback_ completion response it can provide if it cannot complete the task for any reason. This can reduce chances of models generating false or fabricated responses.                                                         |
|                                   |                                                                                                                                                                                                                                                   |

As with any best practice, remember that _your mileage may vary_ based on the model, the task and the domain. Use these as a starting point, and iterate to find what works best for you. Constantly re-evaluate your prompt engineering process as new models and tools become available, with a focus on process scalability and response quality.

<!--
LESSON TEMPLATE:
This unit should provide a code challenge if applicable

CHALLENGE:
Link to a Jupyter Notebook with only the code comments in the instructions (code sections are empty).

SOLUTION:
Link to a copy of that Notebook with the prompts filled in and run, showing what one example could be.
-->

## Assignment

Congratulations! You made it to the end of the lesson! It's time to put some of those concepts and techniques to the test with real examples!

For our assignment, we'll be using a Jupyter Notebook with exercises you can complete interactively. You can also extend the Notebook with your own Markdown and Code cells to explore ideas and techniques on your own.

### To get started, fork the repo, then

- (Recommended) Launch GitHub Codespaces
- (Alternatively) Clone the repo to your local device and use it with Docker Desktop
- (Alternatively) Open the Notebook with your preferred Notebook runtime environment.

### Next, configure your environment variables

- Copy the `.env.copy` file in repo root to `.env` and fill in the `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_DEPLOYMENT` valeurs. Revenez √† la [section Learning Sandbox](../../../04-prompt-engineering-fundamentals/04-prompt-engineering-fundamentals) pour apprendre comment.

### Ensuite, ouvrez le Jupyter Notebook

- S√©lectionnez le noyau de runtime. Si vous utilisez les options 1 ou 2, s√©lectionnez simplement le noyau Python 3.10.x par d√©faut fourni par le conteneur de d√©veloppement.

Vous √™tes pr√™t √† ex√©cuter les exercices. Notez qu'il n'y a pas de _bonnes ou mauvaises_ r√©ponses ici - juste explorer les options par essais et erreurs et d√©velopper une intuition pour ce qui fonctionne pour un mod√®le et un domaine d'application donn√©s.

_Pour cette raison, il n'y a pas de segments de solution de code dans cette le√ßon. Au lieu de cela, le Notebook aura des cellules Markdown intitul√©es "Ma Solution :" qui montrent un exemple de sortie √† titre de r√©f√©rence._

 <!--
MOD√àLE DE LE√áON :
Conclure la section avec un r√©sum√© et des ressources pour l'apprentissage autonome.
-->

## V√©rification des connaissances

Lequel des √©l√©ments suivants est un bon prompt suivant quelques bonnes pratiques raisonnables ?

1. Montrez-moi une image de voiture rouge
2. Montrez-moi une image de voiture rouge de marque Volvo et mod√®le XC90 gar√©e pr√®s d'une falaise avec le soleil couchant
3. Montrez-moi une image de voiture rouge de marque Volvo et mod√®le XC90

A : 2, c'est le meilleur prompt car il fournit des d√©tails sur "quoi" et entre dans les sp√©cificit√©s (pas n'importe quelle voiture mais une marque et un mod√®le sp√©cifiques) et il d√©crit √©galement l'ensemble du cadre. 3 est le suivant meilleur car il contient √©galement beaucoup de descriptions.

## üöÄ D√©fi

Voyez si vous pouvez tirer parti de la technique de "l'indice" avec le prompt : Compl√©tez la phrase "Montrez-moi une image de voiture rouge de marque Volvo et ". Que r√©pond-il, et comment l'am√©lioreriez-vous ?

## Excellent travail ! Continuez votre apprentissage

Vous voulez en savoir plus sur les diff√©rents concepts d'ing√©nierie de prompts ? Allez sur la [page d'apprentissage continu](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour trouver d'autres excellentes ressources sur ce sujet.

Rendez-vous √† la le√ßon 5 o√π nous examinerons [des techniques de prompts avanc√©es](../05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst) !

**Clause de non-responsabilit√©** :  
Ce document a √©t√© traduit en utilisant le service de traduction AI [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue maternelle doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de faire appel √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.