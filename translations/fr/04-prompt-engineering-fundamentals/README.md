# Fondamentaux de l'Ing√©nierie des Prompts

[![Fondamentaux de l'Ing√©nierie des Prompts](../../../translated_images/04-lesson-banner.png?WT.d904d510033d5f0283f2caff5f735050f929dd196a1fc25fefa18433347fe463.fr.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson4-gh?WT.mc_id=academic-105485-koreyst)

## Introduction
Ce module couvre les concepts et techniques essentiels pour cr√©er des prompts efficaces dans les mod√®les d'IA g√©n√©rative. La fa√ßon dont vous r√©digez votre prompt pour un LLM compte √©galement. Un prompt soigneusement √©labor√© peut am√©liorer la qualit√© de la r√©ponse. Mais que signifient exactement des termes comme _prompt_ et _ing√©nierie des prompts_? Et comment am√©liorer l'_entr√©e_ du prompt que j'envoie au LLM? Ce sont les questions auxquelles nous essaierons de r√©pondre dans ce chapitre et le suivant.

L'_IA g√©n√©rative_ est capable de cr√©er de nouveaux contenus (par exemple, texte, images, audio, code, etc.) en r√©ponse aux demandes des utilisateurs. Elle y parvient en utilisant des _mod√®les de langage de grande taille_ comme la s√©rie GPT ("Generative Pre-trained Transformer") d'OpenAI, entra√Æn√©s √† utiliser le langage naturel et le code.

Les utilisateurs peuvent d√©sormais interagir avec ces mod√®les en utilisant des paradigmes familiers comme le chat, sans n√©cessiter d'expertise technique ou de formation. Les mod√®les sont bas√©s sur des _prompts_ - les utilisateurs envoient une entr√©e textuelle (prompt) et obtiennent une r√©ponse de l'IA (compl√©tion). Ils peuvent ensuite "discuter avec l'IA" de mani√®re it√©rative, dans des conversations √† plusieurs tours, en affinant leur prompt jusqu'√† ce que la r√©ponse corresponde √† leurs attentes.

Les "prompts" deviennent d√©sormais l'interface de _programmation_ principale pour les applications d'IA g√©n√©rative, indiquant aux mod√®les ce qu'il faut faire et influen√ßant la qualit√© des r√©ponses retourn√©es. L'"Ing√©nierie des Prompts" est un domaine d'√©tude en pleine croissance qui se concentre sur la _conception et l'optimisation_ des prompts pour fournir des r√©ponses coh√©rentes et de qualit√© √† grande √©chelle.

## Objectifs d'apprentissage

Dans cette le√ßon, nous apprenons ce qu'est l'Ing√©nierie des Prompts, pourquoi elle est importante, et comment nous pouvons cr√©er des prompts plus efficaces pour un mod√®le et un objectif d'application donn√©s. Nous comprendrons les concepts de base et les meilleures pratiques pour l'ing√©nierie des prompts - et d√©couvrirons un environnement interactif de "bac √† sable" Jupyter Notebooks o√π nous pourrons voir ces concepts appliqu√©s √† des exemples r√©els.

√Ä la fin de cette le√ßon, nous serons capables de :

1. Expliquer ce qu'est l'ing√©nierie des prompts et pourquoi elle est importante.
2. D√©crire les composants d'un prompt et comment ils sont utilis√©s.
3. Apprendre les meilleures pratiques et techniques pour l'ing√©nierie des prompts.
4. Appliquer les techniques apprises √† des exemples r√©els, en utilisant un point de terminaison OpenAI.

## Termes cl√©s

Ing√©nierie des Prompts : La pratique de concevoir et affiner les entr√©es pour guider les mod√®les d'IA vers la production des sorties souhait√©es.
Tokenisation : Le processus de conversion du texte en unit√©s plus petites, appel√©es tokens, qu'un mod√®le peut comprendre et traiter.
LLMs ajust√©s par instruction : Mod√®les de Langage de Grande Taille (LLMs) qui ont √©t√© ajust√©s avec des instructions sp√©cifiques pour am√©liorer la pr√©cision et la pertinence de leurs r√©ponses.

## Bac √† sable d'apprentissage

L'ing√©nierie des prompts est actuellement plus un art qu'une science. La meilleure fa√ßon d'am√©liorer notre intuition est de _pratiquer davantage_ et d'adopter une approche d'essais et d'erreurs qui combine expertise du domaine d'application avec techniques recommand√©es et optimisations sp√©cifiques au mod√®le.

Le Jupyter Notebook accompagnant cette le√ßon fournit un environnement de _bac √† sable_ o√π vous pouvez essayer ce que vous apprenez - au fur et √† mesure ou dans le cadre du d√©fi de code √† la fin. Pour ex√©cuter les exercices, vous aurez besoin de :

1. **Une cl√© API Azure OpenAI** - le point de terminaison de service pour un LLM d√©ploy√©.
2. **Un environnement d'ex√©cution Python** - dans lequel le Notebook peut √™tre ex√©cut√©.
3. **Variables d'environnement locales** - _compl√©tez les √©tapes de [CONFIGURATION](./../00-course-setup/SETUP.md?WT.mc_id=academic-105485-koreyst) maintenant pour √™tre pr√™t_.

Le notebook est livr√© avec des exercices _de d√©marrage_ - mais vous √™tes encourag√© √† ajouter vos propres sections _Markdown_ (description) et _Code_ (demandes de prompts) pour essayer plus d'exemples ou d'id√©es - et d√©velopper votre intuition pour la conception de prompts.

## Guide illustr√©

Vous voulez avoir une vue d'ensemble de ce que cette le√ßon couvre avant de vous lancer? Consultez ce guide illustr√©, qui vous donne une id√©e des principaux sujets abord√©s et des points cl√©s √† garder √† l'esprit pour chacun d'eux. La feuille de route de la le√ßon vous emm√®ne de la compr√©hension des concepts de base et des d√©fis √† leur r√©solution avec des techniques d'ing√©nierie des prompts pertinentes et des meilleures pratiques. Notez que la section "Techniques Avanc√©es" de ce guide fait r√©f√©rence √† un contenu couvert dans le _prochain_ chapitre de ce programme.

![Guide illustr√© de l'ing√©nierie des prompts](../../../translated_images/04-prompt-engineering-sketchnote.png?WT.a936f69bc33c7a783015f6747ea56d0f0071349644cd9031f9b8d20a3eec8696.fr.mc_id=academic-105485-koreyst)

## Notre startup

Parlons maintenant de la fa√ßon dont _ce sujet_ est li√© √† notre mission de startup pour [apporter l'innovation de l'IA √† l'√©ducation](https://educationblog.microsoft.com/2023/06/collaborating-to-bring-ai-innovation-to-education?WT.mc_id=academic-105485-koreyst). Nous voulons construire des applications d'apprentissage personnalis√© aliment√©es par l'IA - alors r√©fl√©chissons √† la fa√ßon dont diff√©rents utilisateurs de notre application pourraient "concevoir" des prompts :

- **Les administrateurs** pourraient demander √† l'IA d'_analyser les donn√©es du programme pour identifier les lacunes de couverture_. L'IA peut r√©sumer les r√©sultats ou les visualiser avec du code.
- **Les √©ducateurs** pourraient demander √† l'IA de _g√©n√©rer un plan de le√ßon pour un public cible et un sujet_. L'IA peut construire le plan personnalis√© dans un format sp√©cifi√©.
- **Les √©tudiants** pourraient demander √† l'IA de les _tutorier dans un sujet difficile_. L'IA peut d√©sormais guider les √©tudiants avec des le√ßons, des indices et des exemples adapt√©s √† leur niveau.

Ce n'est que la partie √©merg√©e de l'iceberg. Consultez [Prompts Pour l'√âducation](https://github.com/microsoft/prompts-for-edu/tree/main?WT.mc_id=academic-105485-koreyst) - une biblioth√®que de prompts open-source organis√©e par des experts en √©ducation - pour avoir une id√©e plus large des possibilit√©s! _Essayez d'ex√©cuter certains de ces prompts dans le bac √† sable ou en utilisant le OpenAI Playground pour voir ce qui se passe!_

## Qu'est-ce que l'ing√©nierie des prompts?

Nous avons commenc√© cette le√ßon en d√©finissant **l'Ing√©nierie des Prompts** comme le processus de _conception et d'optimisation_ des entr√©es textuelles (prompts) pour fournir des r√©ponses coh√©rentes et de qualit√© (compl√©tions) pour un objectif d'application et un mod√®le donn√©s. Nous pouvons consid√©rer cela comme un processus en deux √©tapes :

- _concevoir_ le prompt initial pour un mod√®le et un objectif donn√©s
- _affiner_ le prompt de mani√®re it√©rative pour am√©liorer la qualit√© de la r√©ponse

C'est n√©cessairement un processus d'essais et d'erreurs qui n√©cessite de l'intuition et des efforts de la part de l'utilisateur pour obtenir des r√©sultats optimaux. Alors pourquoi est-ce important? Pour r√©pondre √† cette question, nous devons d'abord comprendre trois concepts :

- _Tokenisation_ = comment le mod√®le "voit" le prompt
- _Base LLMs_ = comment le mod√®le de base "traite" un prompt
- _LLMs ajust√©s par instruction_ = comment le mod√®le peut maintenant voir les "t√¢ches"

### Tokenisation

Un LLM voit les prompts comme une _s√©quence de tokens_ o√π diff√©rents mod√®les (ou versions d'un mod√®le) peuvent tokeniser le m√™me prompt de diff√©rentes mani√®res. √âtant donn√© que les LLMs sont entra√Æn√©s sur des tokens (et non sur du texte brut), la fa√ßon dont les prompts sont tokenis√©s a un impact direct sur la qualit√© de la r√©ponse g√©n√©r√©e.

Pour avoir une intuition sur le fonctionnement de la tokenisation, essayez des outils comme le [Tokeniseur OpenAI](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst) montr√© ci-dessous. Copiez votre prompt - et voyez comment il est converti en tokens, en pr√™tant attention √† la fa√ßon dont les caract√®res d'espace et les signes de ponctuation sont trait√©s. Notez que cet exemple montre un ancien LLM (GPT-3) - donc l'essayer avec un mod√®le plus r√©cent peut produire un r√©sultat diff√©rent.

![Tokenisation](../../../translated_images/04-tokenizer-example.png?WT.f5399316da400747ffe3af9c95e61dc1a85508d57378da23a77538270c4cabf1.fr.mc_id=academic-105485-koreyst)

### Concept : Mod√®les de base

Une fois qu'un prompt est tokenis√©, la fonction principale du ["Base LLM"](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) (ou mod√®le de base) est de pr√©dire le token dans cette s√©quence. √âtant donn√© que les LLMs sont entra√Æn√©s sur d'√©normes ensembles de donn√©es textuelles, ils ont une bonne id√©e des relations statistiques entre les tokens et peuvent faire cette pr√©diction avec une certaine confiance. Notez qu'ils ne comprennent pas le _sens_ des mots dans le prompt ou le token; ils voient juste un mod√®le qu'ils peuvent "compl√©ter" avec leur prochaine pr√©diction. Ils peuvent continuer √† pr√©dire la s√©quence jusqu'√† ce qu'elle soit termin√©e par l'intervention de l'utilisateur ou une condition pr√©√©tablie.

Vous voulez voir comment fonctionne la compl√©tion bas√©e sur les prompts? Entrez le prompt ci-dessus dans le [_Chat Playground_](https://oai.azure.com/playground?WT.mc_id=academic-105485-koreyst) de l'Azure OpenAI Studio avec les param√®tres par d√©faut. Le syst√®me est configur√© pour traiter les prompts comme des demandes d'information - vous devriez donc voir une compl√©tion qui satisfait ce contexte.

Mais que se passe-t-il si l'utilisateur voulait voir quelque chose de sp√©cifique qui r√©pondait √† certains crit√®res ou objectifs de t√¢che? C'est l√† que les LLMs _ajust√©s par instruction_ entrent en jeu.

![Base LLM Chat Completion](../../../translated_images/04-playground-chat-base.png?WT.7645a03d7989b1c410f2e9e6b503d18e4624f82d9cbf108dac999b8c8988f0ad.fr.mc_id=academic-105485-koreyst)

### Concept : LLMs ajust√©s par instruction

Un [LLM ajust√© par instruction](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) commence par le mod√®le de base et le peaufine avec des exemples ou des paires d'entr√©e/sortie (par exemple, des "messages" √† plusieurs tours) qui peuvent contenir des instructions claires - et la r√©ponse de l'IA tente de suivre cette instruction.

Cela utilise des techniques comme l'Apprentissage par Renforcement avec Feedback Humain (RLHF) qui peuvent entra√Æner le mod√®le √† _suivre des instructions_ et _apprendre du feedback_ afin qu'il produise des r√©ponses mieux adapt√©es aux applications pratiques et plus pertinentes pour les objectifs des utilisateurs.

Essayons-le - revisitez le prompt ci-dessus, mais changez maintenant le _message syst√®me_ pour fournir l'instruction suivante comme contexte :

> _R√©sumez le contenu qui vous est fourni pour un √©l√®ve de deuxi√®me ann√©e. Gardez le r√©sultat √† un paragraphe avec 3-5 puces._

Voyez comment le r√©sultat est maintenant ajust√© pour refl√©ter l'objectif et le format souhait√©s? Un √©ducateur peut d√©sormais utiliser directement cette r√©ponse dans ses diapositives pour cette classe.

![Instruction Tuned LLM Chat Completion](../../../translated_images/04-playground-chat-instructions.png?WT.d9c80b15e90815a83ce665bf4418e70205d30318a5a5bcf407b2c92769743593.fr.mc_id=academic-105485-koreyst)

## Pourquoi avons-nous besoin de l'ing√©nierie des prompts?

Maintenant que nous savons comment les prompts sont trait√©s par les LLMs, parlons de _pourquoi_ nous avons besoin de l'ing√©nierie des prompts. La r√©ponse r√©side dans le fait que les LLMs actuels posent un certain nombre de d√©fis qui rendent plus difficile d'obtenir des _compl√©tions fiables et coh√©rentes_ sans investir dans la construction et l'optimisation des prompts. Par exemple :

1. **Les r√©ponses des mod√®les sont stochastiques.** Le _m√™me prompt_ produira probablement des r√©ponses diff√©rentes avec diff√©rents mod√®les ou versions de mod√®les. Et il peut m√™me produire des r√©sultats diff√©rents avec le _m√™me mod√®le_ √† diff√©rents moments. _Les techniques d'ing√©nierie des prompts peuvent nous aider √† minimiser ces variations en fournissant de meilleures garde-fous_.

1. **Les mod√®les peuvent fabriquer des r√©ponses.** Les mod√®les sont pr√©-entra√Æn√©s avec des ensembles de donn√©es _grands mais finis_, ce qui signifie qu'ils manquent de connaissances sur les concepts en dehors de ce cadre d'entra√Ænement. En cons√©quence, ils peuvent produire des compl√©tions qui sont inexactes, imaginaires ou directement contradictoires avec des faits connus. _Les techniques d'ing√©nierie des prompts aident les utilisateurs √† identifier et √† att√©nuer ces fabrications, par exemple, en demandant √† l'IA des citations ou un raisonnement_.

1. **Les capacit√©s des mod√®les varieront.** Les mod√®les plus r√©cents ou les g√©n√©rations de mod√®les auront des capacit√©s plus riches mais apporteront √©galement des particularit√©s uniques et des compromis en termes de co√ªt et de complexit√©. _L'ing√©nierie des prompts peut nous aider √† d√©velopper des meilleures pratiques et des workflows qui abstraient les diff√©rences et s'adaptent aux exigences sp√©cifiques des mod√®les de mani√®re √©volutive et transparente_.

Voyons cela en action dans le OpenAI ou Azure OpenAI Playground :

- Utilisez le m√™me prompt avec diff√©rents d√©ploiements de LLM (par exemple, OpenAI, Azure OpenAI, Hugging Face) - avez-vous vu les variations?
- Utilisez le m√™me prompt √† plusieurs reprises avec le _m√™me_ d√©ploiement de LLM (par exemple, le playground Azure OpenAI) - comment ces variations ont-elles diff√©r√©?

### Exemple de fabrications

Dans ce cours, nous utilisons le terme **"fabrication"** pour faire r√©f√©rence au ph√©nom√®ne o√π les LLMs g√©n√®rent parfois des informations factuellement incorrectes en raison de limitations dans leur entra√Ænement ou d'autres contraintes. Vous avez peut-√™tre aussi entendu cela d√©sign√© sous le nom d'_hallucinations_ dans des articles populaires ou des articles de recherche. Cependant, nous recommandons fortement d'utiliser le terme _"fabrication"_ pour ne pas anthropomorphiser accidentellement le comportement en attribuant un trait humain √† un r√©sultat pilot√© par une machine. Cela renforce √©galement les [lignes directrices de l'IA Responsable](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-105485-koreyst) d'un point de vue terminologique, en supprimant les termes qui pourraient √©galement √™tre consid√©r√©s comme offensants ou non inclusifs dans certains contextes.

Vous voulez avoir une id√©e de comment fonctionnent les fabrications? Pensez √† un prompt qui demande √† l'IA de g√©n√©rer du contenu pour un sujet inexistant (pour s'assurer qu'il ne se trouve pas dans l'ensemble de donn√©es d'entra√Ænement). Par exemple - j'ai essay√© ce prompt :

> **Prompt :** g√©n√©rez un plan de le√ßon sur la Guerre Martienne de 2076.

Une recherche sur le web m'a montr√© qu'il existait des r√©cits fictifs (par exemple, des s√©ries t√©l√©vis√©es ou des livres) sur des guerres martiennes - mais aucune en 2076. Le bon sens nous dit √©galement que 2076 est _dans le futur_ et ne peut donc pas √™tre associ√© √† un √©v√©nement r√©el.

Alors que se passe-t-il lorsque nous ex√©cutons ce prompt avec diff√©rents fournisseurs de LLM?

> **R√©ponse 1** : OpenAI Playground (GPT-35)

![R√©ponse 1](../../../translated_images/04-fabrication-oai.png?WT.08cc3e01259a6b46725a800e67de50c37b7fdd6b1f932f027881cbe32f80bcf1.fr.mc_id=academic-105485-koreyst)

> **R√©ponse 2** : Azure OpenAI Playground (GPT-35)

![R√©ponse 2](../../../translated_images/04-fabrication-aoai.png?WT.81e0d286a351c87c804aaca6e5f8251a6deed5105d11bca035f8cead8c52d299.fr.mc_id=academic-105485-koreyst)

> **R√©ponse 3** : Hugging Face Chat Playground (LLama-2)

![R√©ponse 3](../../../translated_images/04-fabrication-huggingchat.png?WT.992b3a675cc7ed0dbe53e308b93df8165048fb7c4516e6bb00611d05c92e8fd5.fr.mc_id=academic-105485-koreyst)

Comme pr√©vu, chaque mod√®le (ou version de mod√®le) produit des r√©ponses l√©g√®rement diff√©rentes gr√¢ce au comportement stochastique et aux variations des capacit√©s du mod√®le. Par exemple, un mod√®le cible un public de 8e ann√©e tandis qu'un autre suppose un lyc√©en. Mais les trois mod√®les ont g√©n√©r√© des r√©ponses qui pourraient convaincre un utilisateur non inform√© que l'√©v√©nement √©tait r√©el.

Les techniques d'ing√©nierie des prompts comme le _m√©taprompting_ et la _configuration de temp√©rature_ peuvent r√©duire les fabrications de mod√®les dans une certaine mesure. Les nouvelles _architectures_ d'ing√©nierie des prompts int√®grent √©galement de nouveaux outils et techniques de mani√®re transparente dans le flux de prompts, pour att√©nuer ou r√©duire certains de ces effets.

## √âtude de cas : GitHub Copilot

Concluons cette section en comprenant comment l'ing√©nierie des prompts est utilis√©e dans des solutions du monde r√©el en examinant une √©tude de cas : [GitHub Copilot](https://github.com/features/copilot?WT.mc_id=academic-105485-koreyst).

GitHub Copilot est votre "programmeur en duo avec IA" - il convertit les prompts textuels en compl√©tions de code et est int√©gr√© dans votre environnement de d√©veloppement (par exemple, Visual Studio Code) pour une exp√©rience utilisateur fluide. Comme document√© dans la s√©rie de blogs ci-dessous, la premi√®re version √©tait bas√©e sur le mod√®le OpenAI Codex - avec des ing√©nieurs r√©alisant rapidement la n√©cessit√© de peaufiner le mod√®le et de d√©velopper de meilleures techniques d'ing√©nierie des prompts pour am√©liorer la qualit√© du code. En juillet, ils ont [d√©voil√© un mod√®le d'IA am√©lior√© qui va au-del√† de Codex](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-cop
La v√©ritable valeur des mod√®les r√©side dans la capacit√© de cr√©er et publier des _biblioth√®ques de prompts_ pour des domaines d'application verticaux - o√π le mod√®le de prompt est maintenant _optimis√©_ pour refl√©ter le contexte ou des exemples sp√©cifiques √† l'application, rendant les r√©ponses plus pertinentes et pr√©cises pour le public cible. Le d√©p√¥t [Prompts For Edu](https://github.com/microsoft/prompts-for-edu?WT.mc_id=academic-105485-koreyst) est un excellent exemple de cette approche, en proposant une biblioth√®que de prompts pour le domaine de l'√©ducation avec un accent sur des objectifs cl√©s tels que la planification des cours, la conception de programmes, le tutorat des √©tudiants, etc.

## Contenu de soutien

Si nous consid√©rons la construction de prompts comme ayant une instruction (t√¢che) et un objectif (contenu principal), alors le _contenu secondaire_ est comme un contexte suppl√©mentaire que nous fournissons pour **influencer la sortie d'une certaine mani√®re**. Il pourrait s'agir de param√®tres de r√©glage, d'instructions de formatage, de taxonomies de sujets, etc., qui peuvent aider le mod√®le √† _adapter_ sa r√©ponse pour convenir aux objectifs ou attentes souhait√©s de l'utilisateur.

Par exemple : √âtant donn√© un catalogue de cours avec des m√©tadonn√©es √©tendues (nom, description, niveau, balises de m√©tadonn√©es, instructeur, etc.) sur tous les cours disponibles dans le programme :

- nous pouvons d√©finir une instruction pour "r√©sumer le catalogue de cours pour l'automne 2023"
- nous pouvons utiliser le contenu principal pour fournir quelques exemples du r√©sultat souhait√©
- nous pouvons utiliser le contenu secondaire pour identifier les 5 principales "balises" d'int√©r√™t.

Maintenant, le mod√®le peut fournir un r√©sum√© dans le format montr√© par les quelques exemples - mais si un r√©sultat a plusieurs balises, il peut prioriser les 5 balises identifi√©es dans le contenu secondaire.

---

<!--
MOD√àLE DE LE√áON :
Cette unit√© doit couvrir le concept de base #1.
Renforcez le concept avec des exemples et des r√©f√©rences.

CONCEPT #3 :
Techniques d'ing√©nierie des prompts.
Quelles sont quelques techniques de base pour l'ing√©nierie des prompts ?
Illustrez-le avec quelques exercices.
-->

## Bonnes pratiques de cr√©ation de prompts

Maintenant que nous savons comment les prompts peuvent √™tre _construits_, nous pouvons commencer √† r√©fl√©chir √† comment les _concevoir_ pour refl√©ter les meilleures pratiques. Nous pouvons penser √† cela en deux parties - avoir le bon _√©tat d'esprit_ et appliquer les bonnes _techniques_.

### √âtat d'esprit pour l'ing√©nierie des prompts

L'ing√©nierie des prompts est un processus d'essais et d'erreurs, alors gardez trois facteurs directeurs √† l'esprit :

1. **La compr√©hension du domaine est importante.** La pr√©cision et la pertinence des r√©ponses d√©pendent du _domaine_ dans lequel cette application ou cet utilisateur op√®re. Appliquez votre intuition et votre expertise du domaine pour **personnaliser davantage les techniques**. Par exemple, d√©finissez des _personnalit√©s sp√©cifiques au domaine_ dans vos prompts syst√®me, ou utilisez des _mod√®les sp√©cifiques au domaine_ dans vos prompts utilisateur. Fournissez un contenu secondaire qui refl√®te les contextes sp√©cifiques au domaine, ou utilisez des _indices et exemples sp√©cifiques au domaine_ pour guider le mod√®le vers des mod√®les d'utilisation familiers.

2. **La compr√©hension du mod√®le est importante.** Nous savons que les mod√®les sont par nature stochastiques. Mais les impl√©mentations de mod√®les peuvent √©galement varier en termes de jeu de donn√©es d'entra√Ænement qu'ils utilisent (connaissances pr√©-entra√Æn√©es), des capacit√©s qu'ils offrent (par exemple, via API ou SDK) et du type de contenu pour lequel ils sont optimis√©s (par exemple, code vs. images vs. texte). Comprenez les forces et les limitations du mod√®le que vous utilisez, et utilisez ces connaissances pour _prioriser les t√¢ches_ ou cr√©er des _mod√®les personnalis√©s_ qui sont optimis√©s pour les capacit√©s du mod√®le.

3. **L'it√©ration et la validation sont importantes.** Les mod√®les √©voluent rapidement, tout comme les techniques d'ing√©nierie des prompts. En tant qu'expert du domaine, vous pouvez avoir d'autres contextes ou crit√®res pour _votre_ application sp√©cifique, qui peuvent ne pas s'appliquer √† la communaut√© plus large. Utilisez des outils et techniques d'ing√©nierie des prompts pour "acc√©l√©rer" la construction de prompts, puis it√©rez et validez les r√©sultats en utilisant votre propre intuition et expertise du domaine. Enregistrez vos insights et cr√©ez une **base de connaissances** (par exemple, des biblioth√®ques de prompts) qui peuvent √™tre utilis√©es comme nouvelle r√©f√©rence par d'autres, pour des it√©rations plus rapides √† l'avenir.

## Meilleures pratiques

Voyons maintenant les meilleures pratiques courantes recommand√©es par les praticiens de [OpenAI](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api?WT.mc_id=academic-105485-koreyst) et [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering#best-practices?WT.mc_id=academic-105485-koreyst).

| Quoi                             | Pourquoi                                                                                                                                                                                                                                               |
| :------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| √âvaluer les derniers mod√®les.    | Les nouvelles g√©n√©rations de mod√®les sont susceptibles d'avoir des fonctionnalit√©s et une qualit√© am√©lior√©es - mais peuvent aussi entra√Æner des co√ªts plus √©lev√©s. √âvaluez-les pour leur impact, puis prenez des d√©cisions de migration.                                                       |
| S√©parer instructions & contexte  | V√©rifiez si votre mod√®le/fournisseur d√©finit des _d√©limiteurs_ pour distinguer plus clairement les instructions, le contenu principal et secondaire. Cela peut aider les mod√®les √† attribuer des poids plus pr√©cis aux tokens.                                                                |
| Soyez sp√©cifique et clair        | Donnez plus de d√©tails sur le contexte souhait√©, le r√©sultat, la longueur, le format, le style, etc. Cela am√©liorera √† la fois la qualit√© et la coh√©rence des r√©ponses. Capturez les recettes dans des mod√®les r√©utilisables.                                                                |
| Soyez descriptif, utilisez des exemples | Les mod√®les peuvent mieux r√©pondre √† une approche "montrer et raconter". Commencez par une `zero-shot` approach where you give it an instruction (but no examples) then try `few-shot` as a refinement, providing a few examples of the desired output. Use analogies. |
| Use cues to jumpstart completions | Nudge it towards a desired outcome by giving it some leading words or phrases that it can use as a starting point for the response.                                                                                                               |
| Double Down                       | Sometimes you may need to repeat yourself to the model. Give instructions before and after your primary content, use an instruction and a cue, etc. Iterate & validate to see what works.                                                         |
| Order Matters                     | The order in which you present information to the model may impact the output, even in the learning examples, thanks to recency bias. Try different options to see what works best.                                                               |
| Give the model an ‚Äúout‚Äù           | Give the model a _fallback_ completion response it can provide if it cannot complete the task for any reason. This can reduce chances of models generating false or fabricated responses.                                                         |
|                                   |                                                                                                                                                                                                                                                   |

As with any best practice, remember that _your mileage may vary_ based on the model, the task and the domain. Use these as a starting point, and iterate to find what works best for you. Constantly re-evaluate your prompt engineering process as new models and tools become available, with a focus on process scalability and response quality.

<!--
LESSON TEMPLATE:
This unit should provide a code challenge if applicable

CHALLENGE:
Link to a Jupyter Notebook with only the code comments in the instructions (code sections are empty).

SOLUTION:
Link to a copy of that Notebook with the prompts filled in and run, showing what one example could be.
-->

## Assignment

Congratulations! You made it to the end of the lesson! It's time to put some of those concepts and techniques to the test with real examples!

For our assignment, we'll be using a Jupyter Notebook with exercises you can complete interactively. You can also extend the Notebook with your own Markdown and Code cells to explore ideas and techniques on your own.

### To get started, fork the repo, then

- (Recommended) Launch GitHub Codespaces
- (Alternatively) Clone the repo to your local device and use it with Docker Desktop
- (Alternatively) Open the Notebook with your preferred Notebook runtime environment.

### Next, configure your environment variables

- Copy the `.env.copy` file in repo root to `.env` and fill in the `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_DEPLOYMENT` valeurs. Revenez √† la [section Learning Sandbox](../../../04-prompt-engineering-fundamentals/04-prompt-engineering-fundamentals) pour apprendre comment.

### Ensuite, ouvrez le Jupyter Notebook

- S√©lectionnez le noyau d'ex√©cution. Si vous utilisez les options 1 ou 2, s√©lectionnez simplement le noyau Python 3.10.x par d√©faut fourni par le conteneur de d√©veloppement.

Vous √™tes pr√™t √† ex√©cuter les exercices. Notez qu'il n'y a pas de bonnes ou mauvaises r√©ponses ici - juste l'exploration d'options par essais et erreurs et la construction d'une intuition pour ce qui fonctionne pour un mod√®le et un domaine d'application donn√©s.

_Pour cette raison, il n'y a pas de segments de solution de code dans cette le√ßon. Au lieu de cela, le Notebook aura des cellules Markdown intitul√©es "Ma solution :" qui montrent un exemple de sortie pour r√©f√©rence._

 <!--
MOD√àLE DE LE√áON :
Concluez la section par un r√©sum√© et des ressources pour un apprentissage autonome.
-->

## V√©rification des connaissances

Lequel des √©l√©ments suivants est un bon prompt suivant certaines bonnes pratiques raisonnables ?

1. Montrez-moi une image de voiture rouge
2. Montrez-moi une image de voiture rouge de marque Volvo et mod√®le XC90 gar√©e pr√®s d'une falaise avec le soleil couchant
3. Montrez-moi une image de voiture rouge de marque Volvo et mod√®le XC90

R : 2, c'est le meilleur prompt car il donne des d√©tails sur "quoi" et va dans les sp√©cificit√©s (pas n'importe quelle voiture mais une marque et un mod√®le sp√©cifiques) et il d√©crit √©galement le cadre g√©n√©ral. 3 est le suivant car il contient √©galement beaucoup de descriptions.

## üöÄ D√©fi

Voyez si vous pouvez tirer parti de la technique de "l'indice" avec le prompt : Compl√©tez la phrase "Montrez-moi une image de voiture rouge de marque Volvo et ". Que r√©pond-il, et comment l'am√©lioreriez-vous ?

## Excellent travail ! Continuez votre apprentissage

Vous voulez en savoir plus sur les diff√©rents concepts d'ing√©nierie des prompts ? Allez sur la [page d'apprentissage continu](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pour trouver d'autres ressources int√©ressantes sur ce sujet.

Rendez-vous √† la le√ßon 5 o√π nous examinerons [des techniques avanc√©es de prompting](../05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst) !

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'intelligence artificielle. Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction professionnelle humaine. Nous ne sommes pas responsables des malentendus ou des mauvaises interpr√©tations r√©sultant de l'utilisation de cette traduction.