{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bouwen met Mistral-modellen\n",
    "\n",
    "## Introductie\n",
    "\n",
    "In deze les behandelen we:\n",
    "- Het verkennen van de verschillende Mistral-modellen\n",
    "- Inzicht krijgen in de toepassingsgebieden en scenario’s voor elk model\n",
    "- Codevoorbeelden die de unieke eigenschappen van elk model laten zien\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De Mistral-modellen\n",
    "\n",
    "In deze les gaan we drie verschillende Mistral-modellen bekijken: **Mistral Large**, **Mistral Small** en **Mistral Nemo**.\n",
    "\n",
    "Elk van deze modellen is gratis beschikbaar op de Github Model marketplace. De code in dit notebook maakt gebruik van deze modellen om de code uit te voeren. Hier vind je meer informatie over het gebruik van Github Models om te [prototypen met AI-modellen](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Large 2 (2407)\n",
    "Mistral Large 2 is momenteel het vlaggenschipmodel van Mistral en is ontworpen voor zakelijk gebruik.\n",
    "\n",
    "Het model is een upgrade van de originele Mistral Large en biedt:\n",
    "-  Groter contextvenster - 128k versus 32k\n",
    "-  Betere prestaties op wiskunde- en programmeertaken - 76,9% gemiddelde nauwkeurigheid versus 60,4%\n",
    "-  Verbeterde meertalige prestaties - ondersteunde talen zijn: Engels, Frans, Duits, Spaans, Italiaans, Portugees, Nederlands, Russisch, Chinees, Japans, Koreaans, Arabisch en Hindi.\n",
    "\n",
    "Met deze eigenschappen blinkt Mistral Large uit in:\n",
    "- *Retrieval Augmented Generation (RAG)* - dankzij het grotere contextvenster\n",
    "- *Function Calling* - dit model ondersteunt native function calling, waardoor integratie met externe tools en API’s mogelijk is. Deze oproepen kunnen zowel parallel als achter elkaar in een vaste volgorde worden uitgevoerd.\n",
    "- *Codegeneratie* - dit model presteert uitstekend bij het genereren van Python, Java, TypeScript en C++.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dit voorbeeld gebruiken we Mistral Large 2 om een RAG-patroon toe te passen op een tekstdocument. De vraag is in het Koreaans geschreven en gaat over de activiteiten van de auteur vóór de universiteit.\n",
    "\n",
    "Er wordt gebruikgemaakt van het Cohere Embeddings Model om embeddings te maken van zowel het tekstdocument als de vraag. Voor dit voorbeeld wordt het faiss Python-pakket gebruikt als vector store.\n",
    "\n",
    "De prompt die naar het Mistral-model wordt gestuurd bevat zowel de vragen als de opgehaalde tekstfragmenten die lijken op de vraag. Het model geeft vervolgens een antwoord in natuurlijke taal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /home/codespace/.python/current/lib/python3.12/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author primarily engaged in two activities before college: writing and programming. In terms of writing, they wrote short stories, albeit not very good ones, with minimal plot and characters expressing strong feelings. For programming, they started writing programs on the IBM 1401 used for data processing during their 9th grade, at the age of 13 or 14. They used an early version of Fortran and typed programs on punch cards, later loading them into the card reader to run the program.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference import EmbeddingsClient\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-large\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text\n",
    "\n",
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "len(chunks)\n",
    "\n",
    "embed_model_name = \"cohere-embed-v3-multilingual\" \n",
    "\n",
    "embed_client = EmbeddingsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(token)\n",
    ")\n",
    "\n",
    "embed_response = embed_client.embed(\n",
    "    input=chunks,\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "text_embeddings = []\n",
    "for item in embed_response.data:\n",
    "    length = len(item.embedding)\n",
    "    text_embeddings.append(item.embedding)\n",
    "text_embeddings = np.array(text_embeddings)\n",
    "\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)\n",
    "\n",
    "question = \"저자가 대학에 오기 전에 주로 했던 두 가지 일은 무엇이었나요?？\"\n",
    "\n",
    "question_embedding = embed_client.embed(\n",
    "    input=[question],\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "question_embeddings = np.array(question_embedding.data[0].embedding)\n",
    "\n",
    "\n",
    "D, I = index.search(question_embeddings.reshape(1, -1), k=2) # distance, index\n",
    "retrieved_chunks = [chunks[i] for i in I.tolist()[0]]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunks}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=prompt),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Small \n",
    "Mistral Small is een ander model uit de Mistral-familie, onder de premier/enterprise categorie. Zoals de naam al aangeeft, is dit een Small Language Model (SLM). De voordelen van Mistral Small zijn: \n",
    "- Kostenbesparend in vergelijking met Mistral LLMs zoals Mistral Large en NeMo – tot 80% goedkoper\n",
    "- Lage latentie – snellere reacties dan de LLMs van Mistral\n",
    "- Flexibel – kan in verschillende omgevingen worden ingezet met minder eisen aan benodigde resources\n",
    "\n",
    "Mistral Small is ideaal voor: \n",
    "- Tekstgebaseerde taken zoals samenvattingen, sentimentanalyse en vertalingen\n",
    "- Applicaties waar vaak verzoeken worden gedaan, dankzij de kostenefficiëntie\n",
    "- Taken met lage latentie zoals code review en suggesties voor code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Small en Mistral Large vergelijken\n",
    "\n",
    "Om de verschillen in latentie tussen Mistral Small en Large te laten zien, voer je de onderstaande cellen uit.\n",
    "\n",
    "Je zou een verschil in reactietijden van 3-5 seconden moeten zien. Let ook op de lengte en stijl van de antwoorden bij dezelfde prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-small\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-large\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral NeMo\n",
    "\n",
    "In vergelijking met de andere twee modellen die in deze les worden besproken, is Mistral NeMo het enige gratis model met een Apache2-licentie.\n",
    "\n",
    "Het wordt gezien als een upgrade van het eerdere open source LLM van Mistral, Mistral 7B.\n",
    "\n",
    "Andere kenmerken van het NeMo-model zijn:\n",
    "\n",
    "- *Efficiëntere tokenisatie:* Dit model gebruikt de Tekken-tokenizer in plaats van de meer gangbare tiktoken. Hierdoor presteert het beter in meerdere talen en bij code.\n",
    "\n",
    "- *Finetuning:* Het basismodel is beschikbaar voor finetuning. Dit biedt meer flexibiliteit voor situaties waarin finetuning nodig kan zijn.\n",
    "\n",
    "- *Native Function Calling* - Net als Mistral Large is dit model getraind op function calling. Dit maakt het bijzonder, omdat het een van de eerste open source modellen is die dit ondersteunt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral NeMo\n",
    "\n",
    "In vergelijking met de andere twee modellen die in deze les worden besproken, is Mistral NeMo het enige gratis model met een Apache2-licentie.\n",
    "\n",
    "Het wordt gezien als een upgrade van het eerdere open source LLM van Mistral, de Mistral 7B.\n",
    "\n",
    "Andere kenmerken van het NeMo-model zijn:\n",
    "\n",
    "- *Efficiëntere tokenisatie:* Dit model gebruikt de Tekken-tokenizer in plaats van de meer gangbare tiktoken. Hierdoor presteert het beter in meerdere talen en bij code.\n",
    "\n",
    "- *Finetuning:* Het basismodel is beschikbaar voor finetuning. Dit biedt meer flexibiliteit voor situaties waarin finetuning nodig kan zijn.\n",
    "\n",
    "- *Native Function Calling* - Net als Mistral Large is dit model getraind op function calling. Dit maakt het bijzonder, omdat het een van de eerste open source modellen is die dit ondersteunt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers vergelijken\n",
    "\n",
    "In dit voorbeeld kijken we naar hoe Mistral NeMo omgaat met tokenisatie in vergelijking met Mistral Large.\n",
    "\n",
    "Beide voorbeelden gebruiken dezelfde prompt, maar je zult zien dat NeMo minder tokens teruggeeft dan Mistral Large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistral-common\n",
      "  Downloading mistral_common-1.4.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (4.23.0)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from mistral-common) (2.9.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (2.32.3)\n",
      "Collecting sentencepiece==0.2.0 (from mistral-common)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from mistral-common)\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from mistral-common) (4.12.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (0.20.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.1->mistral-common) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.1->mistral-common) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (2024.8.30)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<0.8.0,>=0.7.0->mistral-common)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Downloading mistral_common-1.4.4-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, regex, tiktoken, mistral-common\n",
      "Successfully installed mistral-common-1.4.4 regex-2024.9.11 sentencepiece-0.2.0 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mistral-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"open-mistral-nemo\t\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"mistral-large-latest\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leren stopt hier niet, ga verder met de reis\n",
    "\n",
    "Na het afronden van deze les, bekijk onze [Generative AI Learning collectie](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) om je kennis over Generative AI verder uit te breiden!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nDit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet als de gezaghebbende bron worden beschouwd. Voor kritische informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "coopTranslator": {
   "original_hash": "03bc2561665a8b411acfc24a8aa0d442",
   "translation_date": "2025-08-25T22:25:50+00:00",
   "source_file": "20-mistral/python/githubmodels-assignment.ipynb",
   "language_code": "nl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}