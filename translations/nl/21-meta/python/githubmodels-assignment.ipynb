{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bouwen met de Meta Family-modellen\n",
    "\n",
    "## Introductie\n",
    "\n",
    "In deze les behandelen we:\n",
    "\n",
    "- Een verkenning van de twee belangrijkste Meta Family-modellen - Llama 3.1 en Llama 3.2\n",
    "- Inzicht in de gebruiksscenario’s en toepassingen van elk model\n",
    "- Een codevoorbeeld om de unieke eigenschappen van elk model te laten zien\n",
    "\n",
    "## De Meta Family-modellen\n",
    "\n",
    "In deze les bekijken we 2 modellen uit de Meta Family, ook wel de \"Llama Herd\" genoemd - Llama 3.1 en Llama 3.2\n",
    "\n",
    "Deze modellen zijn er in verschillende varianten en zijn beschikbaar op de Github Model marketplace. Meer informatie over het gebruik van Github Models om te [prototypen met AI-modellen](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Modelvarianten:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Let op: Llama 3 is ook beschikbaar op Github Models, maar wordt in deze les niet behandeld*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Met 405 miljard parameters valt Llama 3.1 in de categorie open source LLM’s.\n",
    "\n",
    "Deze versie is een upgrade ten opzichte van de eerdere release Llama 3 en biedt:\n",
    "\n",
    "- Groter contextvenster – 128k tokens tegenover 8k tokens\n",
    "- Groter maximaal aantal outputtokens – 4096 tegenover 2048\n",
    "- Betere meertalige ondersteuning – dankzij het grotere aantal trainingstokens\n",
    "\n",
    "Hierdoor kan Llama 3.1 complexere toepassingen aan bij het bouwen van GenAI-applicaties, waaronder:\n",
    "- Native Function Calling – de mogelijkheid om externe tools en functies aan te roepen buiten de LLM-werkstroom om\n",
    "- Betere RAG-prestaties – dankzij het grotere contextvenster\n",
    "- Genereren van synthetische data – de mogelijkheid om effectieve data te maken voor taken zoals fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Function Calling\n",
    "\n",
    "Llama 3.1 is verder getraind om beter om te gaan met het aanroepen van functies of tools. Het model heeft ook twee ingebouwde tools die het kan herkennen als nodig, afhankelijk van de vraag van de gebruiker. Deze tools zijn:\n",
    "\n",
    "- **Brave Search** - Kan gebruikt worden om actuele informatie op te zoeken, zoals het weer, door middel van een webzoekopdracht\n",
    "- **Wolfram Alpha** - Kan ingezet worden voor complexere wiskundige berekeningen, zodat je niet zelf functies hoeft te schrijven.\n",
    "\n",
    "Je kunt ook je eigen aangepaste tools maken die door de LLM aangeroepen kunnen worden.\n",
    "\n",
    "In het onderstaande codevoorbeeld:\n",
    "\n",
    "- Definiëren we de beschikbare tools (brave_search, wolfram_alpha) in de systeem prompt.\n",
    "- Sturen we een gebruikersvraag over het weer in een bepaalde stad.\n",
    "- De LLM zal reageren met een tool-aanroep naar de Brave Search tool, wat er zo uit zal zien: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Let op: Dit voorbeeld maakt alleen de tool-aanroep. Als je de resultaten wilt ontvangen, moet je een gratis account aanmaken op de Brave API-pagina en de functie zelf definiëren.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Ondanks dat het een LLM is, heeft Llama 3.1 als beperking dat het niet multimodaal is. Dat wil zeggen, het kan niet verschillende soorten input zoals afbeeldingen als prompt gebruiken en daarop reageren. Deze mogelijkheid is juist een van de belangrijkste kenmerken van Llama 3.2. Deze nieuwe functies zijn onder andere:\n",
    "\n",
    "- Multimodaliteit - kan zowel tekst- als afbeeldingsprompts verwerken\n",
    "- Kleine tot middelgrote varianten (11B en 90B) - dit biedt flexibele implementatiemogelijkheden,\n",
    "- Alleen-tekst varianten (1B en 3B) - hiermee kan het model op edge- of mobiele apparaten draaien en zorgt het voor lage vertraging\n",
    "\n",
    "De ondersteuning voor multimodaliteit is een grote stap in de wereld van open source modellen. In het onderstaande codevoorbeeld wordt zowel een afbeelding als een tekstprompt gebruikt om een analyse van de afbeelding te krijgen van Llama 3.2 90B.\n",
    "\n",
    "### Multimodale ondersteuning met Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leren stopt hier niet, ga verder met de reis\n",
    "\n",
    "Na het afronden van deze les, bekijk onze [Generative AI Learning collectie](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) om je kennis over Generative AI verder uit te breiden!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:\nDit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet als de gezaghebbende bron worden beschouwd. Voor kritische informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:45:23+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "nl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}