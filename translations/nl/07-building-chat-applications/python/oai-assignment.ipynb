{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Hoofdstuk 7: Chatapplicaties bouwen\n",
    "## OpenAI API Snelstart\n",
    "\n",
    "Dit notebook is aangepast van de [Azure OpenAI Samples Repository](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst) die notebooks bevat die toegang geven tot [Azure OpenAI](notebook-azure-openai.ipynb) diensten.\n",
    "\n",
    "De Python OpenAI API werkt ook met Azure OpenAI Modellen, met een paar aanpassingen. Lees hier meer over de verschillen: [Hoe je kunt wisselen tussen OpenAI en Azure OpenAI endpoints met Python](https://learn.microsoft.com/azure/ai-services/openai/how-to/switching-endpoints?WT.mc_id=academic-109527-jasmineg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Overzicht  \n",
    "\"Grote taalmodellen zijn functies die tekst naar tekst omzetten. Wanneer je een invoertekst geeft, probeert een groot taalmodel te voorspellen welke tekst er daarna zal komen\"(1). Deze \"quickstart\"-notebook geeft gebruikers een introductie tot de belangrijkste LLM-concepten, de kernvereisten om te starten met AML, een eenvoudige introductie tot promptontwerp, en enkele korte voorbeelden van verschillende toepassingsmogelijkheden.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Inhoudsopgave  \n",
    "\n",
    "[Overzicht](../../../../07-building-chat-applications/python)  \n",
    "[Hoe gebruik je OpenAI Service](../../../../07-building-chat-applications/python)  \n",
    "[1. Je OpenAI Service aanmaken](../../../../07-building-chat-applications/python)  \n",
    "[2. Installatie](../../../../07-building-chat-applications/python)    \n",
    "[3. Inloggegevens](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Toepassingen](../../../../07-building-chat-applications/python)    \n",
    "[1. Tekst samenvatten](../../../../07-building-chat-applications/python)  \n",
    "[2. Tekst classificeren](../../../../07-building-chat-applications/python)  \n",
    "[3. Nieuwe productnamen genereren](../../../../07-building-chat-applications/python)  \n",
    "[4. Een classifier verfijnen](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Referenties](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Bouw je eerste prompt  \n",
    "Deze korte oefening geeft een basisintroductie voor het indienen van prompts bij een OpenAI-model voor een eenvoudige taak: \"samenvatten\".\n",
    "\n",
    "\n",
    "**Stappen**:  \n",
    "1. Installeer de OpenAI-bibliotheek in je python-omgeving  \n",
    "2. Laad standaard hulplibraries en stel je gebruikelijke OpenAI-beveiligingsgegevens in voor de OpenAI Service die je hebt aangemaakt  \n",
    "3. Kies een model voor je taak  \n",
    "4. Maak een eenvoudige prompt voor het model  \n",
    "5. Dien je verzoek in bij de model-API!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\")\n",
    "assert API_KEY, \"ERROR: OpenAI Key is missing\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Het juiste model vinden  \n",
    "De GPT-3.5-turbo of GPT-4 modellen kunnen natuurlijke taal begrijpen en genereren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Promptontwerp  \n",
    "\n",
    "\"Het bijzondere aan grote taalmodellen is dat ze, door getraind te worden om de voorspellingsfout te minimaliseren op enorme hoeveelheden tekst, uiteindelijk concepten leren die nuttig zijn voor deze voorspellingen. Bijvoorbeeld, ze leren concepten zoals\"(1):\n",
    "\n",
    "* hoe je woorden spelt\n",
    "* hoe grammatica werkt\n",
    "* hoe je kunt parafraseren\n",
    "* hoe je vragen beantwoordt\n",
    "* hoe je een gesprek voert\n",
    "* hoe je in verschillende talen schrijft\n",
    "* hoe je codeert\n",
    "* enzovoort\n",
    "\n",
    "#### Hoe stuur je een groot taalmodel aan  \n",
    "\"Van alle invoer die een groot taalmodel krijgt, is de tekstprompt verreweg het meest bepalend(1).\n",
    "\n",
    "Grote taalmodellen kunnen op verschillende manieren worden aangespoord om output te genereren:\n",
    "\n",
    "Instructie: Vertel het model wat je wilt\n",
    "Aanvulling: Laat het model het begin van wat je wilt afmaken\n",
    "Demonstratie: Laat het model zien wat je wilt, met:\n",
    "Een paar voorbeelden in de prompt\n",
    "Honderden of duizenden voorbeelden in een fine-tuning trainingsdataset\"\n",
    "\n",
    "\n",
    "\n",
    "#### Er zijn drie basisrichtlijnen voor het maken van prompts:\n",
    "\n",
    "**Laat zien en vertel**. Maak duidelijk wat je wilt, door middel van instructies, voorbeelden, of een combinatie van beide. Als je wilt dat het model een lijst alfabetisch rangschikt of een alinea indeelt op sentiment, laat dan zien dat dat is wat je wilt.\n",
    "\n",
    "**Lever goede data aan**. Als je een classifier wilt bouwen of het model een patroon wilt laten volgen, zorg dan dat er genoeg voorbeelden zijn. Controleer je voorbeelden goed — het model is meestal slim genoeg om simpele spelfouten te doorzien en je toch een antwoord te geven, maar het kan ook denken dat dit expres is en dat beïnvloedt het antwoord.\n",
    "\n",
    "**Controleer je instellingen.** De temperatuur- en top_p-instellingen bepalen hoe voorspelbaar het model is bij het genereren van een antwoord. Als je een antwoord wilt waar maar één juist antwoord mogelijk is, zet deze dan lager. Als je juist meer variatie wilt, kun je ze hoger zetten. De grootste fout die mensen maken met deze instellingen is denken dat het \"slimheid\" of \"creativiteit\" regelaars zijn.\n",
    "\n",
    "\n",
    "Bron: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Tekst Samenvatten  \n",
    "#### Uitdaging  \n",
    "Vat tekst samen door 'tl;dr:' aan het einde van een tekstpassage toe te voegen. Let op hoe het model in staat is om verschillende taken uit te voeren zonder extra instructies. Je kunt experimenteren met meer beschrijvende prompts dan tl;dr om het gedrag van het model aan te passen en de samenvatting die je ontvangt te personaliseren(3).  \n",
    "\n",
    "Recent onderzoek heeft aanzienlijke vooruitgang laten zien op veel NLP-taken en benchmarks door eerst te pre-trainen op een grote hoeveelheid tekst en daarna fijn af te stemmen op een specifieke taak. Hoewel deze methode qua architectuur meestal taak-onafhankelijk is, zijn er nog steeds taak-specifieke datasets nodig met duizenden of tienduizenden voorbeelden voor de fine-tuning. Daarentegen kunnen mensen meestal een nieuwe taaltaak uitvoeren met slechts een paar voorbeelden of simpele instructies – iets waar huidige NLP-systemen nog steeds moeite mee hebben. Hier laten we zien dat het opschalen van taalmodellen de taak-onafhankelijke prestaties bij weinig voorbeelden sterk verbetert, soms zelfs tot het niveau van eerdere state-of-the-art fine-tuning methodes. \n",
    "\n",
    "\n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Oefeningen voor verschillende gebruikssituaties  \n",
    "1. Tekst samenvatten  \n",
    "2. Tekst classificeren  \n",
    "3. Nieuwe productnamen genereren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Tekst Classificeren  \n",
    "#### Uitdaging  \n",
    "Classificeer items in categorieën die tijdens de inferentie worden opgegeven. In het volgende voorbeeld geven we zowel de categorieën als de te classificeren tekst in de prompt (*playground_reference).\n",
    "\n",
    "Klantvraag: Hallo, een van de toetsen op mijn laptoptoetsenbord is onlangs kapot gegaan en ik heb een vervanging nodig:\n",
    "\n",
    "Geclassificeerde categorie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Genereer Nieuwe Productnamen\n",
    "#### Uitdaging\n",
    "Bedenk productnamen op basis van voorbeeldwoorden. In deze opdracht geven we informatie over het product waarvoor we namen gaan verzinnen. We geven ook een vergelijkbaar voorbeeld om het gewenste patroon te laten zien. Daarnaast hebben we de temperatuurwaarde hoog ingesteld om meer willekeur en creativiteit in de antwoorden te krijgen.\n",
    "\n",
    "Productbeschrijving: Een milkshakemachine voor thuis\n",
    "Zaadwoorden: snel, gezond, compact.\n",
    "Productnamen: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Productbeschrijving: Een paar schoenen die op elke voetmaat passen.\n",
    "Zaadwoorden: aanpasbaar, pasvorm, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Referenties  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [OpenAI Studio Voorbeelden](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Best practices voor het fijn afstemmen van GPT-3 om tekst te classificeren](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Voor meer hulp  \n",
    "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Bijdragers\n",
    "* Louis Li\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:\nDit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet als de gezaghebbende bron worden beschouwd. Voor kritische informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "1854bdde1dd56b366f1f6c9122f7d304",
   "translation_date": "2025-08-25T18:21:19+00:00",
   "source_file": "07-building-chat-applications/python/oai-assignment.ipynb",
   "language_code": "nl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}