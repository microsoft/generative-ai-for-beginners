{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Hoofdstuk 7: Chatapplicaties bouwen\n",
    "## Github Models API Snelstart\n",
    "\n",
    "Dit notebook is aangepast van de [Azure OpenAI Samples Repository](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst), waar je notebooks vindt die gebruikmaken van [Azure OpenAI](notebook-azure-openai.ipynb) diensten.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Overzicht  \n",
    "\"Grote taalmodellen zijn functies die tekst naar tekst omzetten. Wanneer je een invoerstring van tekst geeft, probeert een groot taalmodel te voorspellen welke tekst er daarna zal komen\"(1). Deze \"quickstart\"-notebook geeft gebruikers een introductie tot de belangrijkste LLM-concepten, de kernvereisten om te starten met AML, een eenvoudige introductie tot promptontwerp, en enkele korte voorbeelden van verschillende toepassingsmogelijkheden.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Inhoudsopgave  \n",
    "\n",
    "[Overzicht](../../../../07-building-chat-applications/python)  \n",
    "[Hoe gebruik je OpenAI Service](../../../../07-building-chat-applications/python)  \n",
    "[1. Je OpenAI Service aanmaken](../../../../07-building-chat-applications/python)  \n",
    "[2. Installatie](../../../../07-building-chat-applications/python)    \n",
    "[3. Inloggegevens](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Toepassingen](../../../../07-building-chat-applications/python)    \n",
    "[1. Tekst samenvatten](../../../../07-building-chat-applications/python)  \n",
    "[2. Tekst classificeren](../../../../07-building-chat-applications/python)  \n",
    "[3. Nieuwe productnamen genereren](../../../../07-building-chat-applications/python)  \n",
    "[4. Een classifier verfijnen](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Referenties](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Bouw je eerste prompt  \n",
    "Deze korte oefening geeft een basisintroductie voor het indienen van prompts bij een model in Github Models voor een eenvoudige taak: \"samenvatting\".\n",
    "\n",
    "**Stappen**:  \n",
    "1. Installeer de `azure-ai-inference` bibliotheek in je python-omgeving, als je dat nog niet hebt gedaan.  \n",
    "2. Laad standaard hulplibraries en stel de Github Models-gegevens in.  \n",
    "3. Kies een model voor je taak  \n",
    "4. Maak een eenvoudige prompt voor het model  \n",
    "5. Dien je verzoek in bij de model-API!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Installeer `azure-ai-inference`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Het juiste model vinden  \n",
    "De GPT-3.5-turbo of GPT-4 modellen kunnen natuurlijke taal begrijpen en genereren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Promptontwerp  \n",
    "\n",
    "\"De magie van grote taalmodellen is dat ze, door getraind te worden om deze voorspellingsfout te minimaliseren op enorme hoeveelheden tekst, uiteindelijk concepten leren die nuttig zijn voor deze voorspellingen. Ze leren bijvoorbeeld concepten zoals\"(1):\n",
    "\n",
    "* hoe je moet spellen\n",
    "* hoe grammatica werkt\n",
    "* hoe je kunt parafraseren\n",
    "* hoe je vragen beantwoordt\n",
    "* hoe je een gesprek voert\n",
    "* hoe je in veel talen schrijft\n",
    "* hoe je codeert\n",
    "* enzovoort\n",
    "\n",
    "#### Hoe stuur je een groot taalmodel aan  \n",
    "\"Van alle invoer voor een groot taalmodel is de tekstprompt veruit het meest bepalend(1).\n",
    "\n",
    "Grote taalmodellen kunnen op verschillende manieren worden aangespoord om output te genereren:\n",
    "\n",
    "Instructie: Vertel het model wat je wilt\n",
    "Aanvulling: Laat het model het begin van wat je wilt afmaken\n",
    "Demonstratie: Laat het model zien wat je wilt, met:\n",
    "Een paar voorbeelden in de prompt\n",
    "Honderden of duizenden voorbeelden in een fine-tuning trainingsdataset\"\n",
    "\n",
    "\n",
    "\n",
    "#### Er zijn drie basisrichtlijnen voor het maken van prompts:\n",
    "\n",
    "**Laat zien en vertel**. Maak duidelijk wat je wilt, via instructies, voorbeelden of een combinatie daarvan. Als je wilt dat het model een lijst alfabetisch rangschikt of een alinea op sentiment classificeert, laat dan zien dat dat is wat je wilt.\n",
    "\n",
    "**Lever kwalitatieve data aan**. Als je een classifier wilt bouwen of het model een patroon wilt laten volgen, zorg dan dat er genoeg voorbeelden zijn. Controleer je voorbeelden goed — het model is meestal slim genoeg om simpele spelfouten te doorzien en je toch een antwoord te geven, maar het kan ook aannemen dat dit expres is en dat beïnvloedt het antwoord.\n",
    "\n",
    "**Controleer je instellingen.** De temperatuur- en top_p-instellingen bepalen hoe voorspelbaar het model is bij het genereren van een antwoord. Als je een antwoord wilt waar maar één juist antwoord mogelijk is, zet deze dan lager. Wil je meer variatie in de antwoorden, zet ze dan hoger. De grootste fout die mensen maken met deze instellingen is denken dat het om \"slimheid\" of \"creativiteit\" gaat.\n",
    "\n",
    "\n",
    "Bron: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Tekst Samenvatten  \n",
    "#### Uitdaging  \n",
    "Vat tekst samen door 'tl;dr:' aan het einde van een tekstpassage toe te voegen. Let op hoe het model in staat is om verschillende taken uit te voeren zonder extra instructies. Je kunt experimenteren met meer beschrijvende prompts dan tl;dr om het gedrag van het model aan te passen en de samenvatting te personaliseren die je ontvangt(3).  \n",
    "\n",
    "Recent onderzoek heeft aanzienlijke vooruitgang laten zien op veel NLP-taken en benchmarks door eerst te pre-trainen op een grote hoeveelheid tekst en daarna fijn af te stemmen op een specifieke taak. Hoewel deze methode qua architectuur meestal taak-onafhankelijk is, zijn er nog steeds taak-specifieke datasets nodig met duizenden of tienduizenden voorbeelden voor de fine-tuning. Ter vergelijking: mensen kunnen meestal een nieuwe taaltaak uitvoeren met slechts een paar voorbeelden of simpele instructies – iets waar huidige NLP-systemen nog steeds moeite mee hebben. Hier laten we zien dat het opschalen van taalmodellen de taak-onafhankelijke prestaties bij weinig voorbeelden sterk verbetert, soms zelfs tot het niveau van eerdere state-of-the-art fine-tuning methodes. \n",
    "\n",
    "\n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Oefeningen voor verschillende gebruikssituaties  \n",
    "1. Tekst samenvatten  \n",
    "2. Tekst classificeren  \n",
    "3. Nieuwe productnamen genereren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Tekst Classificeren  \n",
    "#### Uitdaging  \n",
    "Classificeer items in categorieën die tijdens de inferentie worden opgegeven. In het volgende voorbeeld geven we zowel de categorieën als de te classificeren tekst mee in de prompt (*playground_reference).\n",
    "\n",
    "Klantvraag: Hallo, een van de toetsen op mijn laptoptoetsenbord is onlangs kapot gegaan en ik heb een vervanging nodig:\n",
    "\n",
    "Geclassificeerde categorie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Genereer Nieuwe Productnamen\n",
    "#### Uitdaging\n",
    "Bedenk productnamen op basis van voorbeeldwoorden. In de prompt geven we informatie over het product waarvoor we namen gaan verzinnen. We geven ook een vergelijkbaar voorbeeld om het gewenste patroon te laten zien. Daarnaast hebben we de temperatuurwaarde hoog gezet om meer willekeur en innovatieve antwoorden te krijgen.\n",
    "\n",
    "Productbeschrijving: Een milkshakemachine voor thuis\n",
    "Zaadwoorden: snel, gezond, compact.\n",
    "Productnamen: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Productbeschrijving: Een paar schoenen die op elke voetmaat passen.\n",
    "Zaadwoorden: aanpasbaar, pasvorm, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Referenties  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [OpenAI Studio Voorbeelden](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Best practices voor het fijn afstemmen van GPT-3 om tekst te classificeren](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Voor meer hulp  \n",
    "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Bijdragers\n",
    "* [Chew-Yean Yam](https://www.linkedin.com/in/cyyam/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:\nDit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet als de gezaghebbende bron worden beschouwd. Voor kritische informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "3eeb8e5cad61b52a8366f6259a53ed49",
   "translation_date": "2025-08-25T17:41:51+00:00",
   "source_file": "07-building-chat-applications/python/githubmodels-assignment.ipynb",
   "language_code": "nl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}