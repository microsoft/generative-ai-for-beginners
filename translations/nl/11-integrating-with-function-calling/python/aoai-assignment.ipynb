{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductie\n",
    "\n",
    "Deze les behandelt:\n",
    "- Wat is het aanroepen van functies en waarvoor wordt het gebruikt\n",
    "- Hoe maak je een functieaanroep met Azure OpenAI\n",
    "- Hoe integreer je een functieaanroep in een applicatie\n",
    "\n",
    "## Leerdoelen\n",
    "\n",
    "Na het afronden van deze les weet je hoe je:\n",
    "- Het doel van het gebruik van functieaanroepen begrijpt\n",
    "- Een functieaanroep opzet met de Azure Open AI Service\n",
    "- Effectieve functieaanroepen ontwerpt voor het gebruik in jouw applicatie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functie-aanroepen begrijpen\n",
    "\n",
    "Voor deze les willen we een functie bouwen voor onze educatieve startup waarmee gebruikers via een chatbot technische cursussen kunnen vinden. We raden cursussen aan die passen bij hun vaardigheidsniveau, huidige functie en technologie van interesse.\n",
    "\n",
    "Om dit te realiseren gebruiken we een combinatie van:\n",
    " - `Azure Open AI` om een chatervaring voor de gebruiker te creëren\n",
    " - `Microsoft Learn Catalog API` om gebruikers te helpen cursussen te vinden op basis van hun verzoek\n",
    " - `Function Calling` om de vraag van de gebruiker naar een functie te sturen die het API-verzoek uitvoert.\n",
    "\n",
    "Laten we eerst kijken waarom we functie-aanroepen überhaupt zouden willen gebruiken:\n",
    "\n",
    "print(\"Berichten in het volgende verzoek:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # haal een nieuw antwoord op van GPT waarbij het de functie-uitvoer kan zien\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waarom Function Calling\n",
    "\n",
    "Als je al een andere les in deze cursus hebt gevolgd, begrijp je waarschijnlijk de kracht van het gebruik van Large Language Models (LLMs). Hopelijk zie je ook enkele van hun beperkingen.\n",
    "\n",
    "Function Calling is een functie van de Azure Open AI Service om de volgende beperkingen te overwinnen:\n",
    "1) Consistente opmaak van antwoorden\n",
    "2) Mogelijkheid om gegevens uit andere bronnen van een applicatie te gebruiken in een chatcontext\n",
    "\n",
    "Voor function calling waren de antwoorden van een LLM ongestructureerd en inconsistent. Ontwikkelaars moesten complexe validatiecode schrijven om ervoor te zorgen dat ze met elke variatie van een antwoord konden omgaan.\n",
    "\n",
    "Gebruikers konden geen antwoorden krijgen zoals \"Wat is het huidige weer in Stockholm?\". Dit komt doordat modellen beperkt waren tot de periode waarin de data getraind was.\n",
    "\n",
    "Laten we eens kijken naar het onderstaande voorbeeld dat dit probleem illustreert:\n",
    "\n",
    "Stel dat we een database met studentgegevens willen maken zodat we hen het juiste vak kunnen aanraden. Hieronder staan twee beschrijvingen van studenten die erg op elkaar lijken qua gegevens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willen dit naar een LLM sturen om de gegevens te laten analyseren. Dit kan later in onze applicatie gebruikt worden om het naar een API te sturen of op te slaan in een database.\n",
    "\n",
    "Laten we twee identieke prompts maken waarmee we de LLM instrueren over welke informatie we willen hebben:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willen dit naar een LLM sturen om de onderdelen te analyseren die belangrijk zijn voor ons product. Zo kunnen we twee identieke prompts maken om de LLM te instrueren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na het aanmaken van deze twee prompts, sturen we ze naar de LLM met behulp van `openai.ChatCompletion`. We slaan de prompt op in de variabele `messages` en wijzen de rol toe aan `user`. Dit is om een bericht van een gebruiker aan een chatbot na te bootsen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoewel de prompts hetzelfde zijn en de beschrijvingen vergelijkbaar, kunnen we verschillende formaten krijgen voor de eigenschap `Grades`.\n",
    "\n",
    "Als je de bovenstaande cel meerdere keren uitvoert, kan het formaat bijvoorbeeld `3.7` of `3.7 GPA` zijn.\n",
    "\n",
    "Dit komt doordat het LLM ongestructureerde data ontvangt in de vorm van de geschreven prompt en ook ongestructureerde data teruggeeft. We hebben een gestructureerd formaat nodig zodat we weten wat we kunnen verwachten bij het opslaan of gebruiken van deze data.\n",
    "\n",
    "Door gebruik te maken van functioneel aanroepen, kunnen we ervoor zorgen dat we gestructureerde data terugkrijgen. Bij het gebruik van functioneel aanroepen voert het LLM eigenlijk geen functies uit. In plaats daarvan maken we een structuur waar het LLM zich aan moet houden bij het geven van antwoorden. Vervolgens gebruiken we die gestructureerde antwoorden om te bepalen welke functie we in onze applicaties moeten uitvoeren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Functie-aanroep stroomdiagram](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.nl.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gebruikssituaties voor het gebruik van functie-aanroepen\n",
    "\n",
    "**Externe tools aanroepen**  \n",
    "Chatbots zijn uitstekend in het geven van antwoorden op vragen van gebruikers. Door functie-aanroepen te gebruiken, kunnen chatbots berichten van gebruikers gebruiken om bepaalde taken uit te voeren. Bijvoorbeeld, een student kan de chatbot vragen: \"Stuur een e-mail naar mijn docent waarin staat dat ik meer hulp nodig heb met dit onderwerp.\" Dit kan een functie-aanroep doen naar `send_email(to: string, body: string)`\n",
    "\n",
    "**API- of databasequeries maken**  \n",
    "Gebruikers kunnen informatie vinden met natuurlijke taal die wordt omgezet in een geformatteerde query of API-verzoek. Een voorbeeld hiervan is een docent die vraagt: \"Wie zijn de studenten die de laatste opdracht hebben afgerond?\" Dit kan een functie aanroepen met de naam `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Gestructureerde data maken**  \n",
    "Gebruikers kunnen een tekstblok of CSV gebruiken en het LLM inzetten om belangrijke informatie eruit te halen. Bijvoorbeeld, een student kan een Wikipedia-artikel over vredesakkoorden omzetten om AI-flashcards te maken. Dit kan gedaan worden met een functie genaamd `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Je eerste functieaanroep maken\n",
    "\n",
    "Het proces van het maken van een functieaanroep bestaat uit 3 hoofd stappen:\n",
    "1. De Chat Completions API aanroepen met een lijst van je functies en een gebruikersbericht\n",
    "2. De reactie van het model lezen om een actie uit te voeren, bijvoorbeeld een functie of API-aanroep uitvoeren\n",
    "3. Nogmaals de Chat Completions API aanroepen met het antwoord van je functie om die informatie te gebruiken voor een reactie naar de gebruiker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stroom van een Functieaanroep](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.nl.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementen van een functie-aanroep\n",
    "\n",
    "#### Invoer van gebruikers\n",
    "\n",
    "De eerste stap is het maken van een gebruikersbericht. Dit kan dynamisch worden toegewezen door de waarde van een tekstveld te gebruiken, of je kunt hier een waarde instellen. Als je voor het eerst werkt met de Chat Completions API, moeten we de `role` en de `content` van het bericht definiëren.\n",
    "\n",
    "De `role` kan `system` zijn (regels opstellen), `assistant` (het model) of `user` (de eindgebruiker). Voor het aanroepen van een functie stellen we dit in als `user` en geven we een voorbeeldvraag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functies maken.\n",
    "\n",
    "Nu gaan we een functie definiëren en de parameters van die functie. We gebruiken hier slechts één functie genaamd `search_courses`, maar je kunt meerdere functies aanmaken.\n",
    "\n",
    "**Belangrijk** : Functies worden opgenomen in het systeembericht naar de LLM en tellen mee voor het aantal beschikbare tokens dat je hebt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definities**\n",
    "\n",
    "`name` - De naam van de functie die we willen laten aanroepen.\n",
    "\n",
    "`description` - Dit is de beschrijving van hoe de functie werkt. Hier is het belangrijk om specifiek en duidelijk te zijn.\n",
    "\n",
    "`parameters` - Een lijst van waarden en het formaat dat je wilt dat het model gebruikt in zijn antwoord.\n",
    "\n",
    "`type` - Het gegevenstype waarin de eigenschappen worden opgeslagen.\n",
    "\n",
    "`properties` - Lijst van de specifieke waarden die het model zal gebruiken voor zijn antwoord.\n",
    "\n",
    "`name` - De naam van de eigenschap die het model zal gebruiken in zijn geformatteerde antwoord.\n",
    "\n",
    "`type` - Het gegevenstype van deze eigenschap.\n",
    "\n",
    "`description` - Beschrijving van de specifieke eigenschap.\n",
    "\n",
    "**Optioneel**\n",
    "\n",
    "`required` - Vereiste eigenschap om de functieaanroep te voltooien\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De functie aanroepen\n",
    "Na het definiëren van een functie, moeten we deze nu opnemen in het verzoek aan de Chat Completion API. Dit doen we door `functions` toe te voegen aan het verzoek. In dit geval is dat `functions=functions`.\n",
    "\n",
    "Er is ook een optie om `function_call` op `auto` te zetten. Dit betekent dat we het LLM laten bepalen welke functie aangeroepen moet worden op basis van het bericht van de gebruiker, in plaats van dat we dit zelf toewijzen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kijken we naar de reactie en hoe deze is opgemaakt:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Je ziet dat de naam van de functie wordt aangeroepen en dat het LLM uit het bericht van de gebruiker de gegevens kon halen om de argumenten van de functie in te vullen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functie-aanroepen integreren in een applicatie.\n",
    "\n",
    "Nu we de geformatteerde reactie van het LLM hebben getest, kunnen we deze gaan integreren in een applicatie.\n",
    "\n",
    "### Het proces beheren\n",
    "\n",
    "Om dit in onze applicatie te verwerken, volgen we deze stappen:\n",
    "\n",
    "Als eerste doen we een aanvraag naar de Open AI-diensten en slaan we het bericht op in een variabele genaamd `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu zullen we de functie definiëren die de Microsoft Learn API aanroept om een lijst met cursussen op te halen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als best practice zullen we vervolgens kijken of het model een functie wil aanroepen. Daarna maken we een van de beschikbare functies aan en koppelen die aan de functie die wordt aangeroepen.\n",
    "\n",
    "Vervolgens nemen we de argumenten van de functie en koppelen deze aan de argumenten van het LLM.\n",
    "\n",
    "Tot slot voegen we het functie-aanroepbericht en de waarden die zijn teruggegeven door het `search_courses`-bericht toe. Dit geeft het LLM alle informatie die het nodig heeft om in natuurlijke taal op de gebruiker te reageren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code-uitdaging\n",
    "\n",
    "Goed gedaan! Om verder te leren over Azure Open AI Function Calling kun je aan de slag met: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Voeg meer parameters toe aan de functie die het voor gebruikers makkelijker maken om meer cursussen te vinden. Je vindt de beschikbare API-parameters hier:\n",
    " - Maak een extra functie-aanroep die meer informatie van de gebruiker vraagt, zoals hun moedertaal\n",
    " - Voeg foutafhandeling toe voor het geval de functie-aanroep en/of API-aanroep geen geschikte cursussen oplevert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nDit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet als de gezaghebbende bron worden beschouwd. Voor kritische informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-08-25T20:20:53+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "nl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}