{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductie\n",
    "\n",
    "Deze les behandelt:\n",
    "- Wat is het aanroepen van functies en waarvoor wordt het gebruikt\n",
    "- Hoe maak je een functie-aanroep met OpenAI\n",
    "- Hoe integreer je een functie-aanroep in een applicatie\n",
    "\n",
    "## Leerdoelen\n",
    "\n",
    "Na het afronden van deze les weet je hoe je:\n",
    "- Het doel van het gebruik van functie-aanroepen begrijpt\n",
    "- Een functie-aanroep opzet met de OpenAI Service\n",
    "- Effectieve functie-aanroepen ontwerpt voor het gebruik in jouw applicatie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functie-aanroepen begrijpen\n",
    "\n",
    "Voor deze les willen we een functie bouwen voor onze educatieve startup waarmee gebruikers via een chatbot technische cursussen kunnen vinden. We raden cursussen aan die passen bij hun vaardigheidsniveau, huidige functie en technologie waarin ze geïnteresseerd zijn.\n",
    "\n",
    "Om dit te realiseren gebruiken we een combinatie van:\n",
    " - `OpenAI` om een chatervaring voor de gebruiker te creëren\n",
    " - `Microsoft Learn Catalog API` om gebruikers te helpen cursussen te vinden op basis van hun verzoek\n",
    " - `Function Calling` om de vraag van de gebruiker naar een functie te sturen die het API-verzoek uitvoert.\n",
    "\n",
    "Laten we eerst kijken waarom we functie-aanroepen überhaupt zouden willen gebruiken:\n",
    "\n",
    "print(\"Berichten in het volgende verzoek:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # haal een nieuw antwoord op van GPT waarbij het het antwoord van de functie kan zien\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waarom Function Calling\n",
    "\n",
    "Als je al een andere les in deze cursus hebt gevolgd, begrijp je waarschijnlijk de kracht van het gebruik van Large Language Models (LLMs). Hopelijk zie je ook enkele van hun beperkingen.\n",
    "\n",
    "Function Calling is een functie van de OpenAI Service die is ontworpen om de volgende uitdagingen aan te pakken:\n",
    "\n",
    "Inconsistente responsformattering:\n",
    "- Voor function calling waren de antwoorden van een large language model ongestructureerd en inconsistent. Ontwikkelaars moesten complexe validatiecode schrijven om met elke variatie in de output om te gaan.\n",
    "\n",
    "Beperkte integratie met externe data:\n",
    "- Voor deze functie was het lastig om gegevens uit andere delen van een applicatie in een chatcontext te verwerken.\n",
    "\n",
    "Door het standaardiseren van responsformaten en het mogelijk maken van naadloze integratie met externe data, maakt function calling de ontwikkeling eenvoudiger en vermindert het de noodzaak voor extra validatielogica.\n",
    "\n",
    "Gebruikers konden geen antwoorden krijgen zoals \"Wat is het huidige weer in Stockholm?\". Dit komt doordat modellen beperkt waren tot de periode waarin de data getraind was.\n",
    "\n",
    "Laten we hieronder een voorbeeld bekijken dat dit probleem illustreert:\n",
    "\n",
    "Stel dat we een database willen maken met studentgegevens zodat we hen het juiste vak kunnen aanraden. Hieronder staan twee beschrijvingen van studenten die erg op elkaar lijken qua gegevens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willen dit naar een LLM sturen om de gegevens te laten analyseren. Dit kan later in onze applicatie gebruikt worden om het naar een API te sturen of op te slaan in een database.\n",
    "\n",
    "Laten we twee identieke prompts maken waarmee we de LLM instrueren over welke informatie we willen hebben:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willen dit naar een LLM sturen om de onderdelen te analyseren die belangrijk zijn voor ons product. Zo kunnen we twee identieke prompts maken om de LLM te instrueren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na het aanmaken van deze twee prompts, sturen we ze naar de LLM met behulp van `openai.ChatCompletion`. We slaan de prompt op in de variabele `messages` en wijzen de rol toe aan `user`. Dit is om een bericht van een gebruiker aan een chatbot na te bootsen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoewel de prompts hetzelfde zijn en de beschrijvingen op elkaar lijken, kunnen we verschillende formaten van de eigenschap `Grades` krijgen.\n",
    "\n",
    "Als je de bovenstaande cel meerdere keren uitvoert, kan het formaat `3.7` of `3.7 GPA` zijn.\n",
    "\n",
    "Dit komt doordat het LLM ongestructureerde data ontvangt in de vorm van de geschreven prompt en ook ongestructureerde data teruggeeft. We hebben een gestructureerd formaat nodig zodat we weten wat we kunnen verwachten bij het opslaan of gebruiken van deze data.\n",
    "\n",
    "Door gebruik te maken van functioneel aanroepen, kunnen we ervoor zorgen dat we gestructureerde data terugkrijgen. Bij functioneel aanroepen voert het LLM eigenlijk geen functies uit. In plaats daarvan maken we een structuur waar het LLM zich aan moet houden bij het geven van antwoorden. Vervolgens gebruiken we die gestructureerde antwoorden om te bepalen welke functie we in onze applicaties moeten uitvoeren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Functie-aanroep stroomdiagram](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.nl.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gebruiksscenario's voor het gebruik van functie-aanroepen\n",
    "\n",
    "**Externe tools aanroepen**  \n",
    "Chatbots zijn erg goed in het geven van antwoorden op vragen van gebruikers. Door functie-aanroepen te gebruiken, kunnen chatbots berichten van gebruikers gebruiken om bepaalde taken uit te voeren. Bijvoorbeeld, een student kan de chatbot vragen: \"Stuur een e-mail naar mijn docent dat ik meer hulp nodig heb bij dit vak.\" Dit kan een functie-aanroep doen naar `send_email(to: string, body: string)`\n",
    "\n",
    "**API- of databasequeries maken**  \n",
    "Gebruikers kunnen informatie vinden met natuurlijke taal die wordt omgezet in een geformatteerde query of API-verzoek. Een voorbeeld hiervan is een docent die vraagt: \"Wie zijn de studenten die de laatste opdracht hebben afgerond?\" Dit kan een functie aanroepen met de naam `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Gestructureerde data creëren**  \n",
    "Gebruikers kunnen een stuk tekst of een CSV-bestand nemen en het LLM gebruiken om belangrijke informatie eruit te halen. Bijvoorbeeld, een student kan een Wikipedia-artikel over vredesakkoorden omzetten om AI-flashcards te maken. Dit kan gedaan worden met een functie genaamd `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Je eerste functieaanroep maken\n",
    "\n",
    "Het proces van het maken van een functieaanroep bestaat uit 3 hoofd stappen:\n",
    "1. De Chat Completions API aanroepen met een lijst van je functies en een gebruikersbericht\n",
    "2. De reactie van het model lezen om een actie uit te voeren, bijvoorbeeld een functie of API-aanroep uitvoeren\n",
    "3. Nogmaals de Chat Completions API aanroepen met het antwoord van je functie om die informatie te gebruiken voor een reactie naar de gebruiker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stroom van een Functieaanroep](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.nl.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementen van een functie-aanroep\n",
    "\n",
    "#### Invoer van gebruikers\n",
    "\n",
    "De eerste stap is het maken van een gebruikersbericht. Dit kan dynamisch worden toegekend door de waarde van een tekstveld te gebruiken, of je kunt hier een waarde instellen. Als je voor het eerst werkt met de Chat Completions API, moeten we de `role` en de `content` van het bericht definiëren.\n",
    "\n",
    "De `role` kan `system` zijn (regels opstellen), `assistant` (het model) of `user` (de eindgebruiker). Voor het aanroepen van een functie stellen we dit in als `user` en geven we een voorbeeldvraag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functies maken.\n",
    "\n",
    "Nu gaan we een functie definiëren en de parameters van die functie. We gebruiken hier slechts één functie genaamd `search_courses`, maar je kunt meerdere functies aanmaken.\n",
    "\n",
    "**Belangrijk** : Functies worden opgenomen in het systeembericht naar de LLM en tellen mee voor het aantal beschikbare tokens dat je hebt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definities**\n",
    "\n",
    "De structuur van een functiedefinitie bestaat uit meerdere niveaus, elk met eigen eigenschappen. Hier volgt een overzicht van de geneste structuur:\n",
    "\n",
    "**Eigenschappen van de functie op het hoogste niveau:**\n",
    "\n",
    "`name` - De naam van de functie die we willen laten aanroepen.\n",
    "\n",
    "`description` - Dit is de beschrijving van hoe de functie werkt. Het is hier belangrijk om specifiek en duidelijk te zijn.\n",
    "\n",
    "`parameters` - Een lijst van waarden en het formaat die je wilt dat het model in zijn antwoord gebruikt.\n",
    "\n",
    "**Eigenschappen van het parameters-object:**\n",
    "\n",
    "`type` - Het gegevenstype van het parameters-object (meestal \"object\")\n",
    "\n",
    "`properties` - Lijst van de specifieke waarden die het model zal gebruiken voor zijn antwoord\n",
    "\n",
    "**Eigenschappen van individuele parameters:**\n",
    "\n",
    "`name` - Impliciet gedefinieerd door de sleutel van de eigenschap (bijvoorbeeld \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Het gegevenstype van deze specifieke parameter (bijvoorbeeld \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - Beschrijving van de specifieke parameter\n",
    "\n",
    "**Optionele eigenschappen:**\n",
    "\n",
    "`required` - Een array met daarin de parameters die verplicht zijn om de functieaanroep te voltooien\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De functie aanroepen\n",
    "Na het definiëren van een functie, moeten we deze nu opnemen in het verzoek aan de Chat Completion API. Dit doen we door `functions` toe te voegen aan het verzoek. In dit geval is dat `functions=functions`.\n",
    "\n",
    "Er is ook een optie om `function_call` op `auto` te zetten. Dit betekent dat we het LLM laten bepalen welke functie aangeroepen moet worden op basis van het bericht van de gebruiker, in plaats van dat we dit zelf toewijzen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kijken we naar de reactie en hoe deze is opgemaakt:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Je ziet dat de naam van de functie wordt aangeroepen en dat het LLM uit het bericht van de gebruiker de gegevens kon halen om de argumenten van de functie in te vullen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functie-aanroepen integreren in een applicatie.\n",
    "\n",
    "Nadat we de geformatteerde reactie van het LLM hebben getest, kunnen we deze nu integreren in een applicatie.\n",
    "\n",
    "### Het proces beheren\n",
    "\n",
    "Om dit in onze applicatie te integreren, volgen we de volgende stappen:\n",
    "\n",
    "Als eerste doen we een aanvraag naar de OpenAI-diensten en slaan we het bericht op in een variabele genaamd `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu zullen we de functie definiëren die de Microsoft Learn API aanroept om een lijst met cursussen op te halen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als best practice zullen we vervolgens kijken of het model een functie wil aanroepen. Daarna maken we een van de beschikbare functies aan en koppelen die aan de functie die wordt aangeroepen.  \n",
    "Vervolgens nemen we de argumenten van de functie en koppelen die aan de argumenten van het LLM.\n",
    "\n",
    "Tot slot voegen we het functie-aanroepbericht en de waarden die zijn teruggegeven door het `search_courses`-bericht toe. Dit geeft het LLM alle informatie die het nodig heeft om\n",
    "de gebruiker in natuurlijke taal te antwoorden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code-uitdaging\n",
    "\n",
    "Goed gedaan! Om verder te leren over OpenAI Function Calling kun je het volgende bouwen: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Meer parameters van de functie toevoegen die leerlingen kunnen helpen om meer cursussen te vinden. Je vindt de beschikbare API-parameters hier:\n",
    " - Maak een andere functie-aanroep die meer informatie van de leerling gebruikt, zoals hun moedertaal\n",
    " - Voeg foutafhandeling toe wanneer de functie-aanroep en/of API-aanroep geen geschikte cursussen oplevert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:\nDit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet als de gezaghebbende bron worden beschouwd. Voor kritische informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T21:09:52+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "nl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}