{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductie \n",
    "\n",
    "Deze les behandelt: \n",
    "- Wat functieaanroepen zijn en hun gebruiksgevallen \n",
    "- Hoe een functieaanroep te maken met OpenAI \n",
    "- Hoe een functieaanroep te integreren in een applicatie \n",
    "\n",
    "## Leerdoelen \n",
    "\n",
    "Na het voltooien van deze les weet je hoe en begrijp je: \n",
    "\n",
    "- Het doel van het gebruik van functieaanroepen \n",
    "- Het instellen van functieaanroepen met de OpenAI-service \n",
    "- Het ontwerpen van effectieve functieaanroepen voor het gebruiksgeval van je applicatie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functieaanroepen begrijpen\n",
    "\n",
    "Voor deze les willen we een functie bouwen voor onze educatieve startup waarmee gebruikers een chatbot kunnen gebruiken om technische cursussen te vinden. We zullen cursussen aanbevelen die passen bij hun vaardigheidsniveau, huidige rol en technologie van interesse.\n",
    "\n",
    "Om dit te voltooien gebruiken we een combinatie van:\n",
    " - `OpenAI` om een chatervaring voor de gebruiker te creëren\n",
    " - `Microsoft Learn Catalog API` om gebruikers te helpen cursussen te vinden op basis van het verzoek van de gebruiker\n",
    " - `Function Calling` om de query van de gebruiker te nemen en naar een functie te sturen om het API-verzoek te doen.\n",
    "\n",
    "Om te beginnen, laten we kijken waarom we functieaanroepen in de eerste plaats zouden willen gebruiken:\n",
    "\n",
    "print(\"Berichten in volgende aanvraag:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # krijg een nieuw antwoord van GPT waar het de functieantwoord kan zien\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waarom Functieaanroepen\n",
    "\n",
    "Als je een andere les in deze cursus hebt voltooid, begrijp je waarschijnlijk de kracht van het gebruik van Large Language Models (LLM's). Hopelijk zie je ook enkele van hun beperkingen.\n",
    "\n",
    "Functieaanroepen is een functie van de OpenAI Service die is ontworpen om de volgende uitdagingen aan te pakken:\n",
    "\n",
    "Inconsistente Responsopmaak:\n",
    "- Voor functieaanroepen waren de reacties van een groot taalmodel ongestructureerd en inconsistent. Ontwikkelaars moesten complexe validatiecode schrijven om elke variatie in de output af te handelen.\n",
    "\n",
    "Beperkte Integratie met Externe Gegevens:\n",
    "- Voor deze functie was het moeilijk om gegevens uit andere delen van een applicatie in een chatcontext te verwerken.\n",
    "\n",
    "Door responsformaten te standaardiseren en naadloze integratie met externe gegevens mogelijk te maken, vereenvoudigt functieaanroepen de ontwikkeling en vermindert het de noodzaak voor extra validatielogica.\n",
    "\n",
    "Gebruikers konden geen antwoorden krijgen zoals \"Wat is het huidige weer in Stockholm?\". Dit komt omdat modellen beperkt waren tot de tijd waarop de data getraind was.\n",
    "\n",
    "Laten we naar het onderstaande voorbeeld kijken dat dit probleem illustreert:\n",
    "\n",
    "Stel dat we een database van studentgegevens willen maken zodat we hen de juiste cursus kunnen voorstellen. Hieronder hebben we twee beschrijvingen van studenten die erg vergelijkbaar zijn in de gegevens die ze bevatten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willen dit naar een LLM sturen om de gegevens te parseren. Dit kan later in onze applicatie worden gebruikt om dit naar een API te sturen of op te slaan in een database.\n",
    "\n",
    "Laten we twee identieke prompts maken waarin we de LLM instrueren over welke informatie we geïnteresseerd zijn:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willen dit naar een LLM sturen om de onderdelen te analyseren die belangrijk zijn voor ons product. Zo kunnen we twee identieke prompts maken om de LLM te instrueren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na het maken van deze twee prompts sturen we ze naar de LLM met behulp van `openai.ChatCompletion`. We slaan de prompt op in de variabele `messages` en wijzen de rol toe aan `user`. Dit is om een bericht van een gebruiker dat naar een chatbot wordt geschreven na te bootsen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kunnen we beide verzoeken naar de LLM sturen en de reactie die we ontvangen onderzoeken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoewel de prompts hetzelfde zijn en de beschrijvingen vergelijkbaar, kunnen we verschillende formaten van de eigenschap `Grades` krijgen.\n",
    "\n",
    "Als je de bovenstaande cel meerdere keren uitvoert, kan het formaat `3.7` of `3.7 GPA` zijn.\n",
    "\n",
    "Dit komt omdat de LLM ongestructureerde gegevens in de vorm van de geschreven prompt neemt en ook ongestructureerde gegevens teruggeeft. We moeten een gestructureerd formaat hebben zodat we weten wat we kunnen verwachten bij het opslaan of gebruiken van deze gegevens.\n",
    "\n",
    "Door functioneel aan te roepen, kunnen we ervoor zorgen dat we gestructureerde gegevens terugkrijgen. Bij het gebruik van functioneel aanroepen roept de LLM eigenlijk geen functies aan of voert deze uit. In plaats daarvan creëren we een structuur die de LLM moet volgen voor zijn antwoorden. We gebruiken die gestructureerde antwoorden vervolgens om te weten welke functie we in onze toepassingen moeten uitvoeren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Function Calling Flow Diagram](../../../../translated_images/nl/Function-Flow.083875364af4f4bb.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kunnen dan nemen wat wordt geretourneerd door de functie en dit terugsturen naar de LLM. De LLM zal vervolgens reageren met natuurlijke taal om de vraag van de gebruiker te beantwoorden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gebruikssituaties voor het gebruik van functieaanroepen\n",
    "\n",
    "**Externe Hulpmiddelen Aanroepen**  \n",
    "Chatbots zijn uitstekend in het geven van antwoorden op vragen van gebruikers. Door gebruik te maken van functieaanroepen kunnen de chatbots berichten van gebruikers gebruiken om bepaalde taken uit te voeren. Bijvoorbeeld, een student kan de chatbot vragen om \"Stuur een e-mail naar mijn docent met de mededeling dat ik meer hulp nodig heb bij dit vak\". Dit kan een functieaanroep doen naar `send_email(to: string, body: string)`\n",
    "\n",
    "**API- of Databasequery's Maken**  \n",
    "Gebruikers kunnen informatie vinden met natuurlijke taal die wordt omgezet in een geformatteerde query of API-verzoek. Een voorbeeld hiervan kan een docent zijn die vraagt \"Wie zijn de studenten die de laatste opdracht hebben voltooid\" wat een functie kan aanroepen genaamd `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Gestructureerde Gegevens Maken**  \n",
    "Gebruikers kunnen een tekstblok of CSV nemen en de LLM gebruiken om belangrijke informatie eruit te halen. Bijvoorbeeld, een student kan een Wikipedia-artikel over vredesakkoorden omzetten om AI-flashcards te maken. Dit kan gedaan worden door een functie te gebruiken genaamd `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Je eerste functieaanroep maken\n",
    "\n",
    "Het proces van het maken van een functieaanroep omvat 3 hoofd stappen:  \n",
    "1. Het aanroepen van de Chat Completions API met een lijst van je functies en een gebruikersbericht  \n",
    "2. Het lezen van de reactie van het model om een actie uit te voeren, bijvoorbeeld het uitvoeren van een functie of API-aanroep  \n",
    "3. Nog een keer de Chat Completions API aanroepen met de reactie van je functie om die informatie te gebruiken om een antwoord aan de gebruiker te maken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stroom van een Functieaanroep](../../../../translated_images/nl/LLM-Flow.3285ed8caf4796d7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementen van een functieaanroep \n",
    "\n",
    "#### Gebruikersinvoer \n",
    "\n",
    "De eerste stap is het maken van een gebruikersbericht. Dit kan dynamisch worden toegewezen door de waarde van een tekstinvoer te nemen of je kunt hier een waarde toewijzen. Als dit je eerste keer is dat je met de Chat Completions API werkt, moeten we de `role` en de `content` van het bericht definiëren. \n",
    "\n",
    "De `role` kan `system` zijn (regels maken), `assistant` (het model) of `user` (de eindgebruiker). Voor functieaanroepen zullen we dit toewijzen als `user` en een voorbeeldvraag. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functies maken. \n",
    "\n",
    "Vervolgens definiëren we een functie en de parameters van die functie. We gebruiken hier slechts één functie genaamd `search_courses`, maar je kunt meerdere functies maken.\n",
    "\n",
    "**Belangrijk** : Functies worden opgenomen in het systeembericht aan de LLM en worden meegerekend in het aantal beschikbare tokens dat je hebt. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definities** \n",
    "\n",
    "De functiedefinitie-structuur heeft meerdere niveaus, elk met zijn eigen eigenschappen. Hier is een overzicht van de geneste structuur:\n",
    "\n",
    "**Topniveau Functie-eigenschappen:**\n",
    "\n",
    "`name` - De naam van de functie die we willen laten aanroepen.\n",
    "\n",
    "`description` - Dit is de beschrijving van hoe de functie werkt. Hier is het belangrijk om specifiek en duidelijk te zijn.\n",
    "\n",
    "`parameters` - Een lijst van waarden en het formaat dat je wilt dat het model produceert in zijn antwoord.\n",
    "\n",
    "**Eigenschappen van het Parameters-object:**\n",
    "\n",
    "`type` - Het datatype van het parameters-object (meestal \"object\").\n",
    "\n",
    "`properties` - Lijst van de specifieke waarden die het model zal gebruiken voor zijn antwoord.\n",
    "\n",
    "**Eigenschappen van individuele parameters:**\n",
    "\n",
    "`name` - Impliciet gedefinieerd door de eigenschapssleutel (bijv. \"role\", \"product\", \"level\").\n",
    "\n",
    "`type` - Het datatype van deze specifieke parameter (bijv. \"string\", \"number\", \"boolean\").\n",
    "\n",
    "`description` - Beschrijving van de specifieke parameter.\n",
    "\n",
    "**Optionele eigenschappen:**\n",
    "\n",
    "`required` - Een array die aangeeft welke parameters vereist zijn om de functie-aanroep te voltooien.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De functieaanroep maken  \n",
    "Na het definiëren van een functie moeten we deze nu opnemen in de aanroep naar de Chat Completion API. Dit doen we door `functions` toe te voegen aan het verzoek. In dit geval `functions=functions`.  \n",
    "\n",
    "Er is ook een optie om `function_call` in te stellen op `auto`. Dit betekent dat we de LLM laten beslissen welke functie moet worden aangeroepen op basis van het gebruikersbericht in plaats van dit zelf toe te wijzen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laten we nu naar de reactie kijken en zien hoe deze is opgemaakt:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Je kunt zien dat de naam van de functie wordt aangeroepen en dat de LLM op basis van het bericht van de gebruiker de gegevens heeft kunnen vinden die passen bij de argumenten van de functie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Integreren van Functieaanroepen in een Applicatie. \n",
    "\n",
    "\n",
    "Nadat we de geformatteerde reactie van de LLM hebben getest, kunnen we deze nu integreren in een applicatie. \n",
    "\n",
    "### De stroom beheren \n",
    "\n",
    "Om dit in onze applicatie te integreren, laten we de volgende stappen nemen: \n",
    "\n",
    "Eerst maken we de oproep naar de OpenAI-diensten en slaan het bericht op in een variabele genaamd `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu zullen we de functie definiëren die de Microsoft Learn API aanroept om een lijst met cursussen op te halen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als beste praktijk zullen we vervolgens kijken of het model een functie wil aanroepen. Daarna zullen we een van de beschikbare functies aanmaken en deze koppelen aan de functie die wordt aangeroepen.  \n",
    "Vervolgens nemen we de argumenten van de functie en koppelen deze aan de argumenten van het LLM.\n",
    "\n",
    "Ten slotte voegen we het functie-aanroepbericht en de waarden die zijn geretourneerd door het `search_courses`-bericht toe. Dit geeft het LLM alle informatie die het nodig heeft om op de gebruiker te reageren met natuurlijke taal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu zullen we het bijgewerkte bericht naar de LLM sturen zodat we een natuurlijke taalreactie kunnen ontvangen in plaats van een API JSON-geformatteerd antwoord.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code-uitdaging\n",
    "\n",
    "Goed gedaan! Om je kennis van OpenAI Function Calling voort te zetten, kun je bouwen: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    " - Meer parameters van de functie die leerlingen kunnen helpen meer cursussen te vinden. Je kunt de beschikbare API-parameters hier vinden:  \n",
    " - Maak een andere functieaanroep die meer informatie van de leerling vraagt, zoals hun moedertaal  \n",
    " - Maak foutafhandeling wanneer de functieaanroep en/of API-aanroep geen geschikte cursussen retourneert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:  \nDit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet als de gezaghebbende bron worden beschouwd. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T10:47:30+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "nl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}