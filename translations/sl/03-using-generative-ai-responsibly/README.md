<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "13084c6321a2092841b9a081b29497ba",
  "translation_date": "2025-05-19T14:53:18+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "sl"
}
-->
# Uporaba generativne umetne inteligence odgovorno

> _Kliknite na zgornjo sliko za ogled videa te lekcije_

Zlahka se je navdu코iti nad umetno inteligenco, 코e posebej generativno umetno inteligenco, vendar je treba razmisliti, kako jo uporabljati odgovorno. Treba je razmisliti o tem, kako zagotoviti, da je rezultat pravi캜en, ne코kodljiv in 코e ve캜. Ta poglavje vam 쬰li ponuditi omenjeni kontekst, kaj je treba upo코tevati in kako sprejeti aktivne korake za izbolj코anje va코e uporabe umetne inteligence.

## Uvod

Ta lekcija bo obravnavala:

- Zakaj bi morali pri gradnji aplikacij generativne umetne inteligence dati prednost odgovorni umetni inteligenci.
- Osnovna na캜ela odgovorne umetne inteligence in kako se nana코ajo na generativno umetno inteligenco.
- Kako uresni캜iti ta na캜ela odgovorne umetne inteligence s strategijo in orodji.

## Cilji u캜enja

Po kon캜ani lekciji boste vedeli:

- Pomembnost odgovorne umetne inteligence pri gradnji aplikacij generativne umetne inteligence.
- Kdaj razmisliti in uporabiti osnovna na캜ela odgovorne umetne inteligence pri gradnji aplikacij generativne umetne inteligence.
- Katera orodja in strategije so vam na voljo za uresni캜itev koncepta odgovorne umetne inteligence.

## Na캜ela odgovorne umetne inteligence

Navdu코enje nad generativno umetno inteligenco 코e nikoli ni bilo ve캜je. To navdu코enje je prineslo veliko novih razvijalcev, pozornosti in financiranja na tem podro캜ju. 캛eprav je to zelo pozitivno za vsakogar, ki 쬰li graditi izdelke in podjetja z uporabo generativne umetne inteligence, je pomembno, da ravnamo odgovorno.

V tem te캜aju se osredoto캜amo na gradnjo na코ega startupa in na코ega izobra쬰valnega produkta umetne inteligence. Uporabili bomo na캜ela odgovorne umetne inteligence: pravi캜nost, vklju캜enost, zanesljivost/varnost, varnost in zasebnost, preglednost in odgovornost. S temi na캜eli bomo raziskali, kako se nana코ajo na na코o uporabo generativne umetne inteligence v na코ih izdelkih.

## Zakaj bi morali dati prednost odgovorni umetni inteligenci

Pri gradnji izdelka pristop, ki je osredoto캜en na 캜loveka in upo코teva najbolj코e interese uporabnika, vodi do najbolj코ih rezultatov.

Edinstvenost generativne umetne inteligence je njena mo캜, da ustvari koristne odgovore, informacije, usmeritve in vsebino za uporabnike. To je mogo캜e storiti brez veliko ro캜nih korakov, kar lahko vodi do zelo impresivnih rezultatov. Brez ustreznega na캜rtovanja in strategij lahko 쬬l vodi tudi do nekaterih 코kodljivih rezultatov za va코e uporabnike, va코 izdelek in dru쬭o kot celoto.

Poglejmo si nekaj (vendar ne vseh) teh potencialno 코kodljivih rezultatov:

### Halucinacije

Halucinacije so izraz, ki opisuje, ko LLM ustvari vsebino, ki je bodisi popolnoma nesmiselna bodisi nekaj, kar vemo, da je dejansko napa캜no na podlagi drugih virov informacij.

Recimo, da zgradimo funkcijo za na코 startup, ki omogo캜a 코tudentom, da modelu zastavijo zgodovinska vpra코anja. 맚udent zastavi vpra코anje `Who was the sole survivor of Titanic?`

Model ustvari odgovor, kot je spodnji:

To je zelo samozavesten in temeljit odgovor. Na 쬬lost je napa캜en. Tudi z minimalno koli캜ino raziskav bi ugotovili, da je bilo ve캜 kot en pre쬴veli katastrofe Titanika. Za 코tudenta, ki 코ele za캜enja raziskovati to temo, je ta odgovor lahko dovolj prepri캜ljiv, da ga ne bo vpra코al in obravnaval kot dejstvo. Posledice tega lahko vodijo v nezanesljivost sistema umetne inteligence in negativno vplivajo na ugled na코ega startupa.

Z vsako iteracijo kateregakoli danega LLM smo videli izbolj코ave zmogljivosti pri zmanj코evanju halucinacij. Tudi s to izbolj코avo moramo kot graditelji aplikacij in uporabniki ostati pozorni na te omejitve.

### 맒odljiva vsebina

V prej코njem razdelku smo obravnavali, ko LLM ustvari napa캜ne ali nesmiselne odgovore. Drugo tveganje, na katerega moramo biti pozorni, je, ko model odgovori s 코kodljivo vsebino.

맒odljiva vsebina je lahko opredeljena kot:

- Nudenje navodil ali spodbujanje k samopo코kodbam ali 코kodi dolo캜enim skupinam.
- Sovra쬹a ali poni쬰valna vsebina.
- Usmerjanje k na캜rtovanju kakr코nih koli napadov ali nasilnih dejanj.
- Nudenje navodil, kako najti nezakonito vsebino ali storiti nezakonita dejanja.
- Prikazovanje spolno eksplicitne vsebine.

Za na코 startup 쬰limo zagotoviti, da imamo prava orodja in strategije, da prepre캜imo, da bi tovrstna vsebina bila videna s strani 코tudentov.

### Pomanjkanje pravi캜nosti

Pravi캜nost je opredeljena kot "zagotavljanje, da je sistem umetne inteligence brez pristranskosti in diskriminacije ter da obravnava vse po코teno in enakopravno." V svetu generativne umetne inteligence 쬰limo zagotoviti, da izklju캜ujo캜i pogledi marginaliziranih skupin niso okrepljeni z izhodom modela.

Tak코ne vrste izhodov niso le uni캜ujo캜e za gradnjo pozitivnih izku코enj izdelkov za na코e uporabnike, temve캜 povzro캜ajo tudi nadaljnjo dru쬭eno 코kodo. Kot graditelji aplikacij bi morali vedno imeti 코iroko in raznoliko bazo uporabnikov v mislih pri gradnji re코itev z generativno umetno inteligenco.

## Kako uporabljati generativno umetno inteligenco odgovorno

Sedaj, ko smo opredelili pomembnost odgovorne generativne umetne inteligence, si poglejmo 4 korake, ki jih lahko sprejmemo, da odgovorno gradimo na코e AI re코itve:

### Merjenje potencialnih 코kod

Pri testiranju programske opreme testiramo pri캜akovane akcije uporabnika na aplikaciji. Podobno je testiranje raznolikega nabora pozivov, ki jih uporabniki najverjetneje bodo uporabili, dober na캜in za merjenje potencialne 코kode.

Ker na코 startup gradi izobra쬰valni produkt, bi bilo dobro pripraviti seznam pozivov povezanih z izobra쬰vanjem. To bi lahko zajemalo dolo캜eno temo, zgodovinska dejstva in pozive o 코tudentskem 쬴vljenju.

### Zmanj코anje potencialnih 코kod

Sedaj je 캜as, da poi코캜emo na캜ine, kjer lahko prepre캜imo ali omejimo potencialno 코kodo, ki jo povzro캜a model in njegovi odgovori. To lahko obravnavamo v 4 razli캜nih slojih:

- **Model**. Izbira pravega modela za pravi primer uporabe. Ve캜ji in bolj kompleksni modeli, kot je GPT-4, lahko povzro캜ijo ve캜je tveganje za 코kodljivo vsebino, ko se uporabijo za manj코e in bolj specifi캜ne primere uporabe. Uporaba va코ih podatkov za fino nastavitev prav tako zmanj코a tveganje za 코kodljivo vsebino.

- **Varnostni sistem**. Varnostni sistem je niz orodij in konfiguracij na platformi, ki slu쬴 modelu in pomaga zmanj코ati 코kodo. Primer tega je sistem filtriranja vsebine na storitvi Azure OpenAI. Sistemi bi morali tudi zaznati napade z izogibanjem in neza쬰leno aktivnost, kot so zahteve od botov.

- **Metaprompt**. Metaprompti in utemeljitev so na캜ini, kako lahko usmerimo ali omejimo model na podlagi dolo캜enih vedenj in informacij. To bi lahko bilo uporaba sistemskih vnosov za dolo캜itev dolo캜enih omejitev modela. Poleg tega nudenje izhodov, ki so bolj relevantni za obseg ali domeno sistema.

Lahko se tudi uporabi tehnike, kot je Generacija z obogatitvijo pridobivanja (RAG), da model pridobi informacije samo iz izbora zaupanja vrednih virov. Kasneje v tem te캜aju je lekcija za [gradnjo iskalnih aplikacij](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **Uporabni코ka izku코nja**. Kon캜ni sloj je, kjer uporabnik neposredno sodeluje z modelom prek vmesnika na코e aplikacije na nek na캜in. Na ta na캜in lahko oblikujemo UI/UX, da omejimo uporabnika glede vrste vnosov, ki jih lahko po코lje modelu, kot tudi besedilo ali slike, prikazane uporabniku. Pri uvajanju AI aplikacije moramo biti tudi transparentni glede tega, kaj na코a aplikacija generativne umetne inteligence lahko in 캜esa ne more storiti.

Imamo celotno lekcijo, posve캜eno [Oblikovanju UX za AI aplikacije](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst).

- **Ocenjevanje modela**. Delo z LLM-ji je lahko zahtevno, ker nimamo vedno nadzora nad podatki, na katerih je bil model usposobljen. Kljub temu bi morali vedno oceniti zmogljivost in izhode modela. 만 vedno je pomembno meriti natan캜nost modela, podobnost, utemeljenost in relevantnost izhoda. To pomaga zagotoviti preglednost in zaupanje dele쬹ikom ter uporabnikom.

### Upravljanje odgovorne generativne AI re코itve

Gradnja operativne prakse okoli va코ih AI aplikacij je kon캜na faza. To vklju캜uje partnerstvo z drugimi deli na코ega startupa, kot sta pravni in varnostni oddelek, da zagotovimo skladnost z vsemi regulativnimi politikami. Pred lansiranjem 쬰limo tudi zgraditi na캜rte okoli dostave, obravnave incidentov in povratka, da prepre캜imo kakr코no koli 코kodo na코im uporabnikom pri rasti.

## Orodja

캛eprav se zdi delo razvoja re코itev odgovorne umetne inteligence veliko, je delo zelo vredno truda. Ko podro캜je generativne umetne inteligence raste, se bo ve캜 orodij za pomo캜 razvijalcem pri u캜inkovitem vklju캜evanju odgovornosti v njihove delovne tokove razvijalo. Na primer, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) lahko pomaga zaznati 코kodljivo vsebino in slike prek zahteve API.

## Preverjanje znanja

Na kaj morate biti pozorni, da zagotovite odgovorno uporabo umetne inteligence?

1. Da je odgovor pravilen.
2. 맒odljiva uporaba, da umetna inteligenca ni uporabljena za kriminalne namene.
3. Zagotavljanje, da je umetna inteligenca brez pristranskosti in diskriminacije.

A: 2 in 3 sta pravilna. Odgovorna umetna inteligenca vam pomaga razmisliti, kako zmanj코ati 코kodljive u캜inke in pristranskosti ter ve캜.

## 游 Izziv

Preberite o [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) in preverite, kaj lahko sprejmete za svojo uporabo.

## Odli캜no delo, nadaljujte z u캜enjem

Po kon캜ani lekciji si oglejte na코o [Generativno AI zbirko u캜enja](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), da nadaljujete z nadgradnjo svojega znanja o generativni umetni inteligenci!

Pojdite na Lekcijo 4, kjer si bomo ogledali [Osnove in쬰niringa pozivov](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

**Omejitev odgovornosti**: 
Ta dokument je bil preveden z uporabo storitve AI prevajanja [Co-op Translator](https://github.com/Azure/co-op-translator). 캛eprav si prizadevamo za natan캜nost, vas prosimo, da upo코tevate, da lahko avtomatski prevodi vsebujejo napake ali neto캜nosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za kriti캜ne informacije je priporo캜ljivo profesionalno 캜love코ko prevajanje. Ne odgovarjamo za morebitne nesporazume ali napa캜ne interpretacije, ki izhajajo iz uporabe tega prevoda.