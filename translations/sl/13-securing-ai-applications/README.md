<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:42:43+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "sl"
}
-->
# Zavarovanje vaÅ¡ih generativnih AI aplikacij

[![Zavarovanje vaÅ¡ih generativnih AI aplikacij](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.sl.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Uvod

Ta lekcija bo zajemala:

- Varnost v kontekstu AI sistemov.
- Pogoste tveganja in groÅ¾nje za AI sisteme.
- Metode in premisleke za zavarovanje AI sistemov.

## Cilji uÄenja

Po zakljuÄku te lekcije boste razumeli:

- GroÅ¾nje in tveganja za AI sisteme.
- Pogoste metode in prakse za zavarovanje AI sistemov.
- Kako lahko izvajanje varnostnega testiranja prepreÄi nepriÄakovane rezultate in zmanjÅ¡anje zaupanja uporabnikov.

## Kaj pomeni varnost v kontekstu generativne AI?

Ker tehnologije umetne inteligence (AI) in strojnega uÄenja (ML) vse bolj oblikujejo naÅ¡e Å¾ivljenje, je kljuÄnega pomena zaÅ¡Äititi ne le podatke strank, temveÄ tudi same AI sisteme. AI/ML se vse bolj uporablja pri podpori odloÄanju z visoko vrednostjo v panogah, kjer lahko napaÄna odloÄitev povzroÄi resne posledice.

KljuÄne toÄke za razmislek:

- **Vpliv AI/ML**: AI/ML moÄno vplivata na vsakdanje Å¾ivljenje, zato je njihova zaÅ¡Äita postala nujna.
- **Izzivi varnosti**: Ta vpliv zahteva ustrezno pozornost, da zaÅ¡Äitimo AI izdelke pred sofisticiranimi napadi, bodisi s strani trolov ali organiziranih skupin.
- **StrateÅ¡ke teÅ¾ave**: TehnoloÅ¡ka industrija mora proaktivno reÅ¡evati strateÅ¡ke izzive, da zagotovi dolgoroÄno varnost strank in zaÅ¡Äito podatkov.

Poleg tega modeli strojnega uÄenja veÄinoma ne loÄijo med zlonamernimi vnosi in neÅ¡kodljivimi anomalijami. Velik del uÄnih podatkov prihaja iz neurejenih, nereguliranih javnih zbirk podatkov, ki so odprte za prispevke tretjih oseb. Napadalci ne potrebujejo kompromitirati zbirk podatkov, Äe lahko vanje prosto prispevajo. SÄasoma nizkoverjetni zlonamerni podatki postanejo visokoverjetni zaupanja vredni podatki, Äe struktura/format podatkov ostane pravilen.

Zato je kljuÄnega pomena zagotoviti integriteto in zaÅ¡Äito podatkovnih skladiÅ¡Ä, ki jih vaÅ¡i modeli uporabljajo za sprejemanje odloÄitev.

## Razumevanje groÅ¾enj in tveganj AI

V kontekstu AI in sorodnih sistemov je zastrupitev podatkov danes ena najpomembnejÅ¡ih varnostnih groÅ¾enj. Zastrupitev podatkov pomeni, da nekdo namerno spremeni informacije, ki se uporabljajo za uÄenje AI, kar povzroÄi napake. To je posledica pomanjkanja standardiziranih metod zaznavanja in ublaÅ¾itve ter naÅ¡e odvisnosti od nezaupljivih ali neurejenih javnih zbirk podatkov za uÄenje. Za ohranjanje integritete podatkov in prepreÄevanje napaÄnega uÄnega procesa je kljuÄnega pomena slediti izvoru in poreklu podatkov. V nasprotnem primeru velja star rek "smeti noter, smeti ven", kar vodi do poslabÅ¡ane zmogljivosti modela.

Primeri, kako lahko zastrupitev podatkov vpliva na vaÅ¡e modele:

1. **Preobrat oznak**: Pri binarni klasifikaciji napadalec namerno spremeni oznake majhnega dela uÄnih podatkov. Na primer, neÅ¡kodljivi vzorci so oznaÄeni kot zlonamerni, kar povzroÄi, da se model nauÄi napaÄnih povezav.\
   **Primer**: Filter za neÅ¾eleno poÅ¡to napaÄno oznaÄi legitimna sporoÄila kot neÅ¾eleno zaradi manipuliranih oznak.
2. **Zastrupitev znaÄilnosti**: Napadalec subtilno spremeni znaÄilnosti v uÄnih podatkih, da vnese pristranskost ali zavaja model.\
   **Primer**: Dodajanje nepomembnih kljuÄnih besed v opise izdelkov za manipulacijo priporoÄilnih sistemov.
3. **Vbrizgavanje podatkov**: Vbrizgavanje zlonamernih podatkov v uÄni niz za vplivanje na vedenje modela.\
   **Primer**: Uvajanje laÅ¾nih uporabniÅ¡kih ocen za izkrivljanje rezultatov analize sentimenta.
4. **Napadi z zadnjimi vrati**: Napadalec v uÄne podatke vstavi skrit vzorec (zadnja vrata). Model se nauÄi prepoznati ta vzorec in se ob sproÅ¾itvi obnaÅ¡a zlonamerno.\
   **Primer**: Sistem za prepoznavanje obrazov, usposobljen z zadnjimi vrati, ki napaÄno prepozna doloÄeno osebo.

MITRE Corporation je ustvarila [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), zbirko znanja o taktikah in tehnikah, ki jih uporabljajo napadalci v resniÄnih napadih na AI sisteme.

> Å tevilo ranljivosti v sistemih, ki uporabljajo AI, naraÅ¡Äa, saj vkljuÄevanje AI poveÄuje povrÅ¡ino napada obstojeÄih sistemov onkraj tradicionalnih kibernetskih napadov. ATLAS smo razvili, da ozavestimo te edinstvene in razvijajoÄe se ranljivosti, saj globalna skupnost vse bolj vkljuÄuje AI v razliÄne sisteme. ATLAS je modeliran po okviru MITRE ATT&CKÂ® in njegove taktike, tehnike ter postopki (TTP) dopolnjujejo tiste v ATT&CK.

Podobno kot okvir MITRE ATT&CKÂ®, ki se Å¡iroko uporablja v tradicionalni kibernetski varnosti za naÄrtovanje scenarijev naprednega posnemanja groÅ¾enj, ATLAS ponuja enostavno iskalni nabor TTP, ki pomagajo bolje razumeti in se pripraviti na obrambo pred novimi napadi.

Poleg tega je Open Web Application Security Project (OWASP) ustvaril "[Top 10 seznam](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" najkritiÄnejÅ¡ih ranljivosti v aplikacijah, ki uporabljajo LLM-je. Seznam izpostavlja tveganja groÅ¾enj, kot je omenjena zastrupitev podatkov, ter druge, kot so:

- **Vbrizgavanje ukazov (Prompt Injection)**: tehnika, kjer napadalci manipulirajo z velikim jezikovnim modelom (LLM) z natanÄno oblikovanimi vnosi, zaradi Äesar model deluje izven svojega namena.
- **Ranljivosti v dobavni verigi**: Komponente in programska oprema, ki sestavljajo aplikacije, ki jih uporablja LLM, kot so Python moduli ali zunanji podatkovni nizi, so lahko same kompromitirane, kar vodi do nepriÄakovanih rezultatov, uvedenih pristranskosti in celo ranljivosti v osnovni infrastrukturi.
- **Prevelika zanaÅ¡anje**: LLM-ji so dovzetni za halucinacije, kar pomeni, da lahko podajo netoÄne ali nevarne rezultate. V veÄ dokumentiranih primerih so ljudje rezultate vzeli zares, kar je povzroÄilo neÅ¾elene negativne posledice v resniÄnem svetu.

Microsoft Cloud Advocate Rod Trent je napisal brezplaÄno e-knjigo, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), ki poglobljeno obravnava te in druge nastajajoÄe groÅ¾nje AI ter ponuja obseÅ¾na navodila, kako se najbolje spopasti s temi scenariji.

## Varnostno testiranje AI sistemov in LLM-jev

Umetna inteligenca (AI) spreminja razliÄna podroÄja in industrije ter ponuja nove moÅ¾nosti in koristi za druÅ¾bo. Vendar pa AI prinaÅ¡a tudi pomembne izzive in tveganja, kot so zasebnost podatkov, pristranskost, pomanjkanje razloÅ¾ljivosti in morebitna zloraba. Zato je kljuÄnega pomena zagotoviti, da so AI sistemi varni in odgovorni, kar pomeni, da spoÅ¡tujejo etiÄne in pravne standarde ter so zaupanja vredni za uporabnike in deleÅ¾nike.

Varnostno testiranje je proces ocenjevanja varnosti AI sistema ali LLM-ja z identifikacijo in izkoriÅ¡Äanjem njihovih ranljivosti. To lahko izvajajo razvijalci, uporabniki ali neodvisni revizorji, odvisno od namena in obsega testiranja. Nekatere najpogostejÅ¡e metode varnostnega testiranja AI sistemov in LLM-jev so:

- **ÄŒiÅ¡Äenje podatkov**: Proces odstranjevanja ali anonimizacije obÄutljivih ali zasebnih informacij iz uÄnih podatkov ali vhodov AI sistema ali LLM-ja. ÄŒiÅ¡Äenje podatkov pomaga prepreÄiti uhajanje podatkov in zlonamerno manipulacijo z zmanjÅ¡anjem izpostavljenosti zaupnih ali osebnih podatkov.
- **Adversarialno testiranje**: Proces ustvarjanja in uporabe nasprotujoÄih primerov na vhodu ali izhodu AI sistema ali LLM-ja za oceno njegove odpornosti in vzdrÅ¾ljivosti proti napadom. Adversarialno testiranje pomaga odkriti in ublaÅ¾iti ranljivosti in Å¡ibkosti AI sistema ali LLM-ja, ki jih lahko izkoristijo napadalci.
- **Preverjanje modela**: Proces preverjanja pravilnosti in popolnosti parametrov ali arhitekture modela AI sistema ali LLM-ja. Preverjanje modela pomaga zaznati in prepreÄiti krajo modela z zagotavljanjem zaÅ¡Äite in avtentiÄnosti modela.
- **Validacija izhoda**: Proces preverjanja kakovosti in zanesljivosti izhoda AI sistema ali LLM-ja. Validacija izhoda pomaga zaznati in popraviti zlonamerno manipulacijo z zagotavljanjem skladnosti in natanÄnosti izhoda.

OpenAI, vodilno podjetje na podroÄju AI sistemov, je vzpostavilo serijo _varnostnih ocen_ kot del svoje iniciative red teaming, namenjene testiranju izhodov AI sistemov z namenom prispevati k varnosti AI.

> Ocene lahko segajo od preprostih vpraÅ¡anj in odgovorov do bolj zapletenih simulacij. Kot konkretne primere predstavljamo nekaj ocen, ki jih je razvil OpenAI za ocenjevanje vedenja AI iz razliÄnih zornih kotov:

#### PrepriÄevanje

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Kako dobro lahko AI sistem prepriÄa drugega AI sistema, da izgovori skrivno besedo?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Kako dobro lahko AI sistem prepriÄa drugega AI sistema, da donira denar?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Kako dobro lahko AI sistem vpliva na podporo drugega AI sistema politiÄni pobudi?

#### Steganografija (skrito sporoÄanje)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Kako dobro lahko AI sistem prenese skrita sporoÄila, ne da bi ga drugi AI sistem odkril?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Kako dobro lahko AI sistem stisne in razÅ¡iri sporoÄila, da omogoÄi skrivanje skritih sporoÄil?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Kako dobro lahko AI sistem uskladi delovanje z drugim AI sistemom brez neposredne komunikacije?

### Varnost AI

Pomembno je, da si prizadevamo zaÅ¡Äititi AI sisteme pred zlonamernimi napadi, zlorabami ali nenamernimi posledicami. To vkljuÄuje ukrepe za zagotavljanje varnosti, zanesljivosti in zaupanja vrednosti AI sistemov, kot so:

- Zavarovanje podatkov in algoritmov, ki se uporabljajo za uÄenje in delovanje AI modelov
- PrepreÄevanje nepooblaÅ¡Äenega dostopa, manipulacije ali sabotaÅ¾e AI sistemov
- Zaznavanje in ublaÅ¾itev pristranskosti, diskriminacije ali etiÄnih vpraÅ¡anj v AI sistemih
- Zagotavljanje odgovornosti, preglednosti in razloÅ¾ljivosti AI odloÄitev in dejanj
- Usmerjanje ciljev in vrednot AI sistemov v skladu s cilji ljudi in druÅ¾be

Varnost AI je kljuÄna za zagotavljanje integritete, razpoloÅ¾ljivosti in zaupnosti AI sistemov in podatkov. Nekateri izzivi in priloÅ¾nosti varnosti AI so:

- PriloÅ¾nost: VkljuÄevanje AI v strategije kibernetske varnosti, saj lahko igra kljuÄno vlogo pri prepoznavanju groÅ¾enj in izboljÅ¡anju odzivnih Äasov. AI lahko pomaga avtomatizirati in izboljÅ¡ati zaznavanje ter ublaÅ¾itev kibernetskih napadov, kot so phishing, zlonamerna programska oprema ali ransomware.
- Izziv: AI lahko tudi napadalci uporabijo za izvajanje sofisticiranih napadov, kot so ustvarjanje laÅ¾nih ali zavajajoÄih vsebin, ponarejanje uporabnikov ali izkoriÅ¡Äanje ranljivosti AI sistemov. Zato imajo razvijalci AI edinstveno odgovornost, da oblikujejo sisteme, ki so robustni in odporni na zlorabe.

### ZaÅ¡Äita podatkov

LLM-ji lahko predstavljajo tveganja za zasebnost in varnost podatkov, ki jih uporabljajo. Na primer, LLM-ji lahko potencialno zapomnijo in razkrijejo obÄutljive informacije iz uÄnih podatkov, kot so osebna imena, naslovi, gesla ali Å¡tevilke kreditnih kartic. Prav tako jih lahko manipulirajo ali napadajo zlonamerni akterji, ki Å¾elijo izkoristiti njihove ranljivosti ali pristranskosti. Zato je pomembno biti seznanjen s temi tveganji in sprejeti ustrezne ukrepe za zaÅ¡Äito podatkov, ki se uporabljajo z LLM-ji. Nekaj korakov za zaÅ¡Äito podatkov, ki se uporabljajo z LLM-ji:

- **Omejitev koliÄine in vrste podatkov, ki jih delite z LLM-ji**: Delite le podatke, ki so potrebni in relevantni za namen, ter se izogibajte deljenju obÄutljivih, zaupnih ali osebnih podatkov. Uporabniki naj tudi anonimizirajo ali Å¡ifrirajo podatke, ki jih delijo z LLM-ji, na primer z odstranitvijo ali prikrivanjem identifikacijskih informacij ali z uporabo varnih komunikacijskih kanalov.
- **Preverjanje podatkov, ki jih LLM-ji generirajo**: Vedno preverite natanÄnost in kakovost izhodov, ki jih generirajo LLM-ji, da zagotovite, da ne vsebujejo nezaÅ¾elenih ali neprimernih informacij.
- **Prijava in opozarjanje na morebitne krÅ¡itve podatkov ali incidente**: Bodite pozorni na sumljive ali nenavadne aktivnosti ali vedenja LLM-jev, kot je generiranje besedil, ki so nepovezana, netoÄna, Å¾aljiva ali Å¡kodljiva. To je lahko znak krÅ¡itve podatkov ali varnostnega incidenta.

Varnost podatkov, upravljanje in skladnost so kljuÄni za vsako organizacijo, ki Å¾eli izkoristiti moÄ podatkov in AI v veÄoblaÄnem okolju. Zavarovanje in upravljanje vseh vaÅ¡ih podatkov je kompleksen in veÄplasten izziv. Potrebno je zavarovati in upravljati razliÄne vrste podatkov (strukturirane, nestrukturirane in podatke, ki jih generira AI) na razliÄnih lokacijah v veÄ oblakih ter upoÅ¡tevati obstojeÄe in prihodnje predpise o varnosti podatkov, upravljanju in AI. Za zaÅ¡Äito podatkov je priporoÄljivo sprejeti nekatere dobre prakse in previdnostne ukrepe, kot so:

- Uporaba oblaÄnih storitev ali platform, ki ponujajo funkcije zaÅ¡Äite podatkov in zasebnosti.
- Uporaba orodij za preverjanje kakovosti in validacijo podatkov za odkrivanje napak, neskladnosti ali anomalij.
- Uporaba okvirov za upravljanje podatkov in etiko, da zagotovite odgovorno in pregledno uporabo podatkov.

### Posnemanje groÅ¾enj iz resniÄnega sveta â€“ AI red teaming

Posnemanje groÅ¾enj iz resniÄnega sveta je danes standardna praksa pri gradnji odpornih AI sistemov, pri Äemer se uporabljajo podobna orodja, taktike in postopki za prepoznavanje tveganj sistemov ter testiranje odziva branilcev.
> Praksa AI red teaminga se je razvila in dobila Å¡irÅ¡i pomen: ne zajema le iskanja varnostnih ranljivosti, temveÄ tudi odkrivanja drugih sistemskih napak, kot je ustvarjanje potencialno Å¡kodljive vsebine. AI sistemi prinaÅ¡ajo nove tveganja, red teaming pa je kljuÄen za razumevanje teh novih tveganj, kot so vbrizgavanje ukazov in ustvarjanje neosnovane vsebine. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.sl.png)]()

Spodaj so kljuÄni vpogledi, ki so oblikovali Microsoftov program AI Red Team.

1. **ObseÅ¾en doseg AI Red Teaminga:**
   AI red teaming zdaj zajema tako varnostne kot tudi rezultate odgovorne umetne inteligence (RAI). Tradicionalno se je red teaming osredotoÄal na varnostne vidike, pri Äemer je model obravnaval kot vektor (npr. kraja osnovnega modela). Vendar pa AI sistemi prinaÅ¡ajo nove varnostne ranljivosti (npr. vbrizgavanje ukazov, zastrupljanje), ki zahtevajo posebno pozornost. Poleg varnosti AI red teaming preuÄuje tudi vpraÅ¡anja praviÄnosti (npr. stereotipiziranje) in Å¡kodljive vsebine (npr. poveliÄevanje nasilja). Zgodnje odkrivanje teh teÅ¾av omogoÄa prednostno usmeritev v obrambne ukrepe.
2. **Zlonamerne in nenevarne napake:**
   AI red teaming upoÅ¡teva napake tako z zlonamerne kot tudi z benignih vidikov. Na primer, pri red teamingu novega Binga raziskujemo ne le, kako lahko zlonamerni nasprotniki podredijo sistem, ampak tudi kako lahko obiÄajni uporabniki naletijo na problematiÄno ali Å¡kodljivo vsebino. V nasprotju s tradicionalnim varnostnim red teamingom, ki se osredotoÄa predvsem na zlonamerne akterje, AI red teaming upoÅ¡teva Å¡irÅ¡i spekter oseb in moÅ¾nih napak.
3. **DinamiÄna narava AI sistemov:**
   AI aplikacije se nenehno razvijajo. Pri aplikacijah velikih jezikovnih modelov se razvijalci prilagajajo spreminjajoÄim se zahtevam. Neprekinjen red teaming zagotavlja stalno budnost in prilagajanje spreminjajoÄim se tveganjem.

AI red teaming ni vseobsegajoÄ in ga je treba obravnavati kot dopolnilno dejavnost k dodatnim kontrolam, kot je [upravljanje dostopa na podlagi vlog (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) in celovite reÅ¡itve za upravljanje podatkov. Namenjen je dopolnitvi varnostne strategije, ki se osredotoÄa na uporabo varnih in odgovornih AI reÅ¡itev, ki upoÅ¡tevajo zasebnost in varnost ter si prizadevajo zmanjÅ¡ati pristranskosti, Å¡kodljivo vsebino in dezinformacije, ki lahko zmanjÅ¡ajo zaupanje uporabnikov.

Tukaj je seznam dodatnega branja, ki vam lahko pomaga bolje razumeti, kako lahko red teaming pomaga prepoznati in ublaÅ¾iti tveganja v vaÅ¡ih AI sistemih:

- [NaÄrtovanje red teaminga za velike jezikovne modele (LLM) in njihove aplikacije](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Kaj je OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - KljuÄna praksa za gradnjo varnejÅ¡ih in odgovornejÅ¡ih AI reÅ¡itev](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), zbirka znanja o taktikah in tehnikah, ki jih nasprotniki uporabljajo pri dejanskih napadih na AI sisteme.

## Preverjanje znanja

Kaj bi bil dober pristop za ohranjanje integritete podatkov in prepreÄevanje zlorabe?

1. Uporabiti moÄne kontrole dostopa do podatkov na podlagi vlog in upravljanje podatkov
1. Izvajati in pregledovati oznaÄevanje podatkov, da prepreÄimo napaÄno predstavitev ali zlorabo podatkov
1. Zagotoviti, da vaÅ¡a AI infrastruktura podpira filtriranje vsebine

Odgovor: 1, ÄŒeprav so vse tri odliÄna priporoÄila, bo zagotavljanje pravilnih dostopnih pravic uporabnikom veliko pripomoglo k prepreÄevanju manipulacije in napaÄne predstavitve podatkov, ki jih uporabljajo LLM.

## ğŸš€ Izziv

Preberite veÄ o tem, kako lahko [upravljate in zaÅ¡Äitite obÄutljive informacije](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) v dobi umetne inteligence.

## OdliÄno delo, nadaljujte z uÄenjem

Po zakljuÄku te lekcije si oglejte naÅ¡o [zbirko za uÄenje generativne umetne inteligence](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), da Å¡e naprej nadgrajujete svoje znanje o generativni AI!

Pojdite na Lekcijo 14, kjer bomo pogledali [Å¾ivljenjski cikel aplikacij generativne AI](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za avtomatski prevod AI [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas opozarjamo, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku velja za avtoritativni vir. Za kljuÄne informacije priporoÄamo strokovni ÄloveÅ¡ki prevod. Za morebitna nesporazume ali napaÄne interpretacije, ki izhajajo iz uporabe tega prevoda, ne odgovarjamo.