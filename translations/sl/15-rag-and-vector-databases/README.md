<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b4b0266fbadbba7ded891b6485adc66d",
  "translation_date": "2025-10-18T01:41:41+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "sl"
}
-->
# Pridobivanje z dodatno generacijo (RAG) in vektorske baze podatkov

[![Pridobivanje z dodatno generacijo (RAG) in vektorske baze podatkov](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.sl.png)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

V lekciji o aplikacijah za iskanje smo na kratko spoznali, kako vklju캜iti lastne podatke v velike jezikovne modele (LLM). V tej lekciji se bomo podrobneje posvetili konceptom utemeljevanja va코ih podatkov v aplikaciji LLM, mehaniki procesa in metodam za shranjevanje podatkov, vklju캜no z vektorskimi predstavitvami in besedilom.

> **Video prihaja kmalu**

## Uvod

V tej lekciji bomo obravnavali naslednje:

- Uvod v RAG, kaj je in zakaj se uporablja v umetni inteligenci (AI).

- Razumevanje, kaj so vektorske baze podatkov, in ustvarjanje ene za na코o aplikacijo.

- Prakti캜en primer, kako vklju캜iti RAG v aplikacijo.

## Cilji u캜enja

Po zaklju캜ku te lekcije boste lahko:

- Razlo쬴li pomen RAG pri pridobivanju in obdelavi podatkov.

- Nastavili aplikacijo RAG in utemeljili svoje podatke v LLM.

- U캜inkovito vklju캜ili RAG in vektorske baze podatkov v aplikacije LLM.

## Na코 scenarij: izbolj코anje na코ih LLM-jev z lastnimi podatki

V tej lekciji 쬰limo dodati lastne zapiske v izobra쬰valni startup, ki chatbotu omogo캜ajo pridobivanje ve캜 informacij o razli캜nih temah. Z uporabo na코ih zapiskov bodo u캜enci lahko bolje 코tudirali in razumeli razli캜ne teme, kar jim bo olaj코alo pripravo na izpite. Za ustvarjanje na코ega scenarija bomo uporabili:

- `Azure OpenAI:` LLM, ki ga bomo uporabili za ustvarjanje na코ega chatbota

- `Lekcija za za캜etnike o nevronskih mre쬬h:` to bodo podatki, na katerih bomo utemeljili na코 LLM

- `Azure AI Search` in `Azure Cosmos DB:` vektorska baza podatkov za shranjevanje na코ih podatkov in ustvarjanje iskalnega indeksa

Uporabniki bodo lahko ustvarili vadbene kvize iz svojih zapiskov, kartice za ponavljanje in jih povzemali v jedrnate preglede. Za za캜etek si poglejmo, kaj je RAG in kako deluje:

## Pridobivanje z dodatno generacijo (RAG)

Chatbot, ki ga poganja LLM, obdeluje uporabni코ke zahteve za generiranje odgovorov. Zasnovan je tako, da je interaktiven in se ukvarja z uporabniki na 코irokem spektru tem. Vendar pa so njegovi odgovori omejeni na kontekst, ki ga zagotavljajo osnovni podatki za usposabljanje. Na primer, GPT-4 ima omejitev znanja do septembra 2021, kar pomeni, da mu primanjkuje znanja o dogodkih, ki so se zgodili po tem obdobju. Poleg tega podatki, uporabljeni za usposabljanje LLM-jev, izklju캜ujejo zaupne informacije, kot so osebni zapiski ali priro캜nik za izdelke podjetja.

### Kako delujejo RAG (Pridobivanje z dodatno generacijo)

![diagram, ki prikazuje, kako delujejo RAG](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.sl.png)

Recimo, da 쬰lite namestiti chatbot, ki ustvarja kvize iz va코ih zapiskov, potrebovali boste povezavo z bazo znanja. Tukaj pride RAG na pomo캜. RAG deluje na naslednji na캜in:

- **Baza znanja:** Pred pridobivanjem je treba te dokumente uvoziti in predhodno obdelati, obi캜ajno z razdelitvijo velikih dokumentov na manj코e dele, pretvorbo v vektorske predstavitve in shranjevanje v bazo podatkov.

- **Uporabni코ka zahteva:** Uporabnik postavi vpra코anje.

- **Pridobivanje:** Ko uporabnik postavi vpra코anje, model za vektorsko predstavitev pridobi ustrezne informacije iz na코e baze znanja, da zagotovi ve캜 konteksta, ki bo vklju캜en v zahtevo.

- **Dodatna generacija:** LLM izbolj코a svoj odgovor na podlagi pridobljenih podatkov. To omogo캜a, da je generiran odgovor ne le temelje캜 na predhodno usposobljenih podatkih, temve캜 tudi na ustreznih informacijah iz dodanega konteksta. Pridobljeni podatki se uporabljajo za izbolj코anje odgovorov LLM. LLM nato vrne odgovor na uporabnikovo vpra코anje.

![diagram, ki prikazuje arhitekturo RAG](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.sl.png)

Arhitektura RAG je implementirana z uporabo transformatorjev, ki sestojijo iz dveh delov: kodirnika in dekodirnika. Na primer, ko uporabnik postavi vpra코anje, se vhodno besedilo 'kodira' v vektorje, ki zajemajo pomen besed, in vektorji se 'dekodirajo' v na코 indeks dokumentov ter generirajo novo besedilo na podlagi uporabni코ke zahteve. LLM uporablja tako kodirni-dekodirni model za generiranje izhoda.

Dva pristopa pri implementaciji RAG, kot je predlagano v 캜lanku: [Pridobivanje z dodatno generacijo za naloge NLP (obdelava naravnega jezika)](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst), sta:

- **_RAG-Sequence_** uporaba pridobljenih dokumentov za napovedovanje najbolj코ega mo쬹ega odgovora na uporabni코ko zahtevo

- **RAG-Token** uporaba dokumentov za generiranje naslednjega tokena, nato pa njihovo pridobivanje za odgovor na uporabni코ko zahtevo

### Zakaj bi uporabljali RAG?

- **Bogastvo informacij:** zagotavlja, da so besedilni odgovori posodobljeni in aktualni. Zato izbolj코uje zmogljivost pri nalogah, specifi캜nih za dolo캜eno podro캜je, z dostopom do notranje baze znanja.

- Zmanj코uje izmi코ljanje z uporabo **preverljivih podatkov** v bazi znanja za zagotavljanje konteksta uporabni코kim zahtevam.

- Je **stro코kovno u캜inkovit**, saj je bolj ekonomi캜en v primerjavi z dodatnim usposabljanjem LLM.

## Ustvarjanje baze znanja

Na코a aplikacija temelji na na코ih osebnih podatkih, tj. lekciji o nevronskih mre쬬h iz u캜nega na캜rta AI za za캜etnike.

### Vektorske baze podatkov

Vektorska baza podatkov, za razliko od tradicionalnih baz podatkov, je specializirana baza podatkov, zasnovana za shranjevanje, upravljanje in iskanje vektorskih predstavitev. Shranjuje numeri캜ne predstavitve dokumentov. Raz캜lenitev podatkov na numeri캜ne vektorske predstavitve olaj코a razumevanje in obdelavo podatkov na코emu AI sistemu.

Vektorske predstavitve shranjujemo v vektorskih bazah podatkov, saj imajo LLM-ji omejitev 코tevila tokenov, ki jih sprejmejo kot vhod. Ker ne morete posredovati celotnih vektorskih predstavitev LLM-ju, jih bomo morali razdeliti na dele, in ko uporabnik postavi vpra코anje, se vrnejo vektorske predstavitve, ki so najbolj podobne vpra코anju, skupaj z zahtevo. Razdelitev na dele tudi zmanj코a stro코ke glede 코tevila tokenov, ki jih posredujemo LLM-ju.

Nekatere priljubljene vektorske baze podatkov vklju캜ujejo Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant in DeepLake. Model Azure Cosmos DB lahko ustvarite z uporabo Azure CLI z naslednjim ukazom:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od besedila do vektorskih predstavitev

Preden shranimo na코e podatke, jih moramo pretvoriti v vektorske predstavitve, preden jih shranimo v bazo podatkov. 캛e delate z velikimi dokumenti ali dolgimi besedili, jih lahko razdelite glede na zahteve, ki jih pri캜akujete. Razdelitev lahko izvedete na ravni stavka ali odstavka. Ker razdelitev izhaja iz pomenov besed okoli njih, lahko dodate nekaj drugega konteksta k delu, na primer z dodajanjem naslova dokumenta ali vklju캜itvijo besedila pred ali po delu. Podatke lahko razdelite na naslednji na캜in:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Ko so podatki razdeljeni, jih lahko nato kodiramo z razli캜nimi modeli za vektorske predstavitve. Nekateri modeli, ki jih lahko uporabite, vklju캜ujejo: word2vec, ada-002 od OpenAI, Azure Computer Vision in mnoge druge. Izbira modela bo odvisna od jezikov, ki jih uporabljate, vrste kodirane vsebine (besedilo/slike/zvok), velikosti vhodnih podatkov, ki jih lahko kodira, in dol쬴ne izhoda vektorske predstavitve.

Primer kodiranega besedila z uporabo modela OpenAI `text-embedding-ada-002` je:
![vektorska predstavitev besede ma캜ka](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.sl.png)

## Pridobivanje in vektorsko iskanje

Ko uporabnik postavi vpra코anje, iskalnik to pretvori v vektor z uporabo kodirnika za zahteve, nato pa i코캜e po na코em iskalnem indeksu dokumentov za ustrezne vektorje v dokumentu, ki so povezani z vhodom. Ko je to opravljeno, pretvori tako vhodni vektor kot vektorje dokumentov v besedilo in jih posreduje LLM-ju.

### Pridobivanje

Pridobivanje se zgodi, ko sistem posku코a hitro najti dokumente iz indeksa, ki ustrezajo iskalnim kriterijem. Cilj iskalnika je pridobiti dokumente, ki bodo uporabljeni za zagotavljanje konteksta in utemeljitev LLM-ja na va코ih podatkih.

Obstaja ve캜 na캜inov za iskanje znotraj na코e baze podatkov, kot so:

- **Iskanje po klju캜nih besedah** - uporablja se za iskanje besedila.

- **Semanti캜no iskanje** - uporablja semanti캜ni pomen besed.

- **Vektorsko iskanje** - pretvori dokumente iz besedila v vektorske predstavitve z uporabo modelov za kodiranje. Pridobivanje se izvede z iskanjem dokumentov, katerih vektorske predstavitve so najbli쬵e uporabni코kemu vpra코anju.

- **Hibridno** - kombinacija iskanja po klju캜nih besedah in vektorskega iskanja.

Izziv pri pridobivanju nastane, ko v bazi podatkov ni podobnega odgovora na zahtevo, sistem pa nato vrne najbolj코e informacije, ki jih lahko najde. Vendar pa lahko uporabite taktike, kot so nastavitev najve캜je razdalje za ustreznost ali uporaba hibridnega iskanja, ki zdru쬿je iskanje po klju캜nih besedah in vektorsko iskanje. V tej lekciji bomo uporabili hibridno iskanje, kombinacijo vektorskega in iskanja po klju캜nih besedah. Na코e podatke bomo shranili v podatkovni okvir s stolpci, ki vsebujejo dele besedila in vektorske predstavitve.

### Vektorska podobnost

Iskalnik bo iskal po bazi znanja za vektorske predstavitve, ki so si med seboj blizu, najbli쬵i sosed, saj so to besedila, ki so si podobna. V scenariju, ko uporabnik postavi zahtevo, se ta najprej kodira, nato pa se ujema s podobnimi vektorskimi predstavitvami. Pogosta metoda za merjenje podobnosti med razli캜nimi vektorji je kosinusna podobnost, ki temelji na kotu med dvema vektorjema.

Za merjenje podobnosti lahko uporabimo tudi druge alternative, kot so Evklidska razdalja, ki je ravna 캜rta med kon캜nimi to캜kami vektorjev, in skalarni produkt, ki meri vsoto produktov ustreznih elementov dveh vektorjev.

### Iskalni indeks

Pri pridobivanju bomo morali zgraditi iskalni indeks za na코o bazo znanja, preden izvedemo iskanje. Indeks bo shranil na코e vektorske predstavitve in lahko hitro pridobil najbolj podobne dele, tudi v veliki bazi podatkov. Na코 indeks lahko lokalno ustvarimo z:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Ponovno razvr코캜anje

Ko ste poizvedovali po bazi podatkov, boste morda morali rezultate razvrstiti po ustreznosti. LLM za ponovno razvr코캜anje uporablja strojno u캜enje za izbolj코anje ustreznosti rezultatov iskanja z razvr코캜anjem od najbolj ustreznih. Z uporabo Azure AI Search se ponovno razvr코캜anje samodejno izvede za vas z uporabo semanti캜nega razvr코캜evalnika. Primer, kako deluje ponovno razvr코캜anje z uporabo najbli쬵ih sosedov:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Vse skupaj

Zadnji korak je dodajanje na코ega LLM v me코anico, da bi lahko dobili odgovore, ki temeljijo na na코ih podatkih. To lahko implementiramo na naslednji na캜in:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Vrednotenje na코e aplikacije

### Merila vrednotenja

- Kakovost podanih odgovorov, ki zagotavlja, da zvenijo naravno, teko캜e in 캜love코ko.

- Utemeljenost podatkov: ocenjevanje, ali je odgovor pri코el iz podanih dokumentov.

- Ustreznost: ocenjevanje, ali se odgovor ujema in je povezan z zastavljenim vpra코anjem.

- Teko캜nost - ali je odgovor slovni캜no smiseln.

## Primeri uporabe RAG (Pridobivanje z dodatno generacijo) in vektorskih baz podatkov

Obstaja veliko razli캜nih primerov uporabe, kjer lahko klici funkcij izbolj코ajo va코o aplikacijo, kot so:

- Vpra코anja in odgovori: utemeljitev podatkov va코ega podjetja v klepetu, ki ga lahko zaposleni uporabljajo za postavljanje vpra코anj.

- Sistemi priporo캜il: kjer lahko ustvarite sistem, ki ujema najbolj podobne vrednosti, npr. filme, restavracije in 코e veliko ve캜.

- Storitev chatbotov: lahko shranite zgodovino klepeta in prilagodite pogovor na podlagi uporabni코kih podatkov.

- Iskanje slik na podlagi vektorskih predstavitev, uporabno pri prepoznavanju slik in zaznavanju anomalij.

## Povzetek

Pokazali smo temeljna podro캜ja RAG od dodajanja na코ih podatkov v aplikacijo, uporabni코ke zahteve in izhoda. Za poenostavitev ustvarjanja RAG lahko uporabite ogrodja, kot so Semantic Kernel, Langchain ali Autogen.

## Naloga

Za nadaljevanje u캜enja o Pridobivanju z dodatno generacijo (RAG) lahko ustvarite:

- Ustvarite uporabni코ki vmesnik za aplikacijo z uporabo izbranega ogrodja.

- Uporabite ogrodje, bodisi LangChain ali Semantic Kernel, in ponovno ustvarite svojo aplikacijo.

캛estitke za zaklju캜ek lekcije 游녪.

## U캜enje se tukaj ne kon캜a, nadaljujte pot

Po zaklju캜ku te lekcije si oglejte na코o [Zbirko u캜enja o generativni umetni inteligenci](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), da nadaljujete z nadgradnjo svojega znanja o generativni umetni inteligenci!

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). 캛eprav si prizadevamo za natan캜nost, vas prosimo, da upo코tevate, da lahko avtomatizirani prevodi vsebujejo napake ali neto캜nosti. Izvirni dokument v njegovem maternem jeziku naj se 코teje za avtoritativni vir. Za klju캜ne informacije priporo캜amo profesionalni 캜love코ki prevod. Ne odgovarjamo za morebitne nesporazume ali napa캜ne razlage, ki izhajajo iz uporabe tega prevoda.