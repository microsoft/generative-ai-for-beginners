<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-05-20T01:46:50+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "sl"
}
-->
# Pove캜ana generacija pridobivanja (RAG) in vektorske baze podatkov

V lekciji o iskalnih aplikacijah smo na kratko spoznali, kako vklju캜iti svoje podatke v velike jezikovne modele (LLM). V tej lekciji se bomo poglobili v koncepte utemeljitve va코ih podatkov v va코i aplikaciji LLM, mehaniko procesa in metode za shranjevanje podatkov, vklju캜no z vektorskimi predstavitvami in besedilom.

> **Video prihaja kmalu**

## Uvod

V tej lekciji bomo obravnavali naslednje:

- Uvod v RAG, kaj je to in zakaj se uporablja v umetni inteligenci (AI).

- Razumevanje, kaj so vektorske baze podatkov, in ustvarjanje ene za na코o aplikacijo.

- Prakti캜en primer, kako integrirati RAG v aplikacijo.

## Cilji u캜enja

Po kon캜ani lekciji boste lahko:

- Razlo쬴li pomen RAG pri pridobivanju in obdelavi podatkov.

- Nastavili aplikacijo RAG in utemeljili svoje podatke na LLM.

- U캜inkovita integracija RAG in vektorskih baz podatkov v aplikacije LLM.

## Na코 scenarij: izbolj코anje na코ih LLM-ov z lastnimi podatki

Za to lekcijo 쬰limo dodati lastne zapiske v izobra쬰valni startup, kar omogo캜a klepetalnemu botu pridobivanje ve캜 informacij o razli캜nih predmetih. Z uporabo zapiskov, ki jih imamo, bodo u캜enci lahko bolje 코tudirali in razumeli razli캜ne teme, kar bo olaj코alo pripravo na izpite. Za ustvarjanje na코ega scenarija bomo uporabili:

- `Azure OpenAI:` LLM, ki ga bomo uporabili za ustvarjanje na코ega klepetalnega bota

- `AI for beginners' lesson on Neural Networks`: to bodo podatki, na katerih bomo utemeljili na코 LLM

- `Azure AI Search` in `Azure Cosmos DB:` vektorska baza podatkov za shranjevanje na코ih podatkov in ustvarjanje iskalnega indeksa

Uporabniki bodo lahko ustvarjali vadbene kvize iz svojih zapiskov, kartice za ponavljanje in jih povzemali v jedrnate preglede. Za캜nimo z razumevanjem, kaj je RAG in kako deluje:

## Pove캜ana generacija pridobivanja (RAG)

Klepetalni bot, ki ga poganja LLM, obdeluje uporabni코ke pozive za ustvarjanje odgovorov. Namenjen je interaktivnosti in vklju캜uje uporabnike v 코irok spekter tem. Vendar so njegovi odgovori omejeni na zagotovljeni kontekst in osnovne podatke o usposabljanju. Na primer, GPT-4 ima omejitev znanja do septembra 2021, kar pomeni, da mu manjka znanje o dogodkih, ki so se zgodili po tem obdobju. Poleg tega podatki, uporabljeni za usposabljanje LLM-ov, izklju캜ujejo zaupne informacije, kot so osebni zapiski ali priro캜nik za izdelke podjetja.

### Kako delujejo RAG-i (Pove캜ana generacija pridobivanja)

Predpostavimo, da 쬰lite uvesti klepetalni bot, ki ustvarja kvize iz va코ih zapiskov, potrebovali boste povezavo z bazo znanja. Tukaj pride na pomo캜 RAG. RAG-i delujejo na naslednji na캜in:

- **Baza znanja:** Pred pridobivanjem je treba te dokumente vnesti in predhodno obdelati, obi캜ajno razdeliti velike dokumente na manj코e dele, jih pretvoriti v vektorske predstavitve besedila in jih shraniti v bazo podatkov.

- **Uporabni코ko vpra코anje:** uporabnik postavi vpra코anje

- **Pridobivanje:** Ko uporabnik postavi vpra코anje, model vektorske predstavitve pridobi ustrezne informacije iz na코e baze znanja, da zagotovi ve캜 konteksta, ki bo vklju캜en v poziv.

- **Pove캜ana generacija:** LLM izbolj코a svoj odgovor na podlagi pridobljenih podatkov. Omogo캜a, da je ustvarjeni odgovor ne le na podlagi predhodno usposobljenih podatkov, temve캜 tudi na podlagi ustreznih informacij iz dodanega konteksta. Pridobljeni podatki se uporabljajo za pove캜anje odgovorov LLM-a. LLM nato vrne odgovor na uporabnikovo vpra코anje.

Arhitektura za RAG-e je implementirana z uporabo transformatorjev, ki sestojijo iz dveh delov: kodirnika in dekodirnika. Na primer, ko uporabnik postavi vpra코anje, se vhodno besedilo 'kodira' v vektorje, ki zajemajo pomen besed, in vektorji se 'dekodirajo' v na코 dokumentni indeks ter generirajo novo besedilo na podlagi uporabni코kega vpra코anja. LLM uporablja model kodirnik-dekodirnik za generiranje izhoda.

Dva pristopa pri implementaciji RAG-a po predlaganem 캜lanku: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sta:

- **_RAG-Sequence_** uporablja pridobljene dokumente za napovedovanje najbolj코ega mo쬹ega odgovora na uporabni코ko vpra코anje

- **RAG-Token** uporablja dokumente za generiranje naslednjega znaka, nato jih pridobi za odgovor na uporabni코ko vpra코anje

### Zakaj bi uporabljali RAG-e?

- **Bogastvo informacij:** zagotavlja, da so besedilni odgovori a쬿rni in aktualni. Zato izbolj코uje zmogljivost pri nalogah, specifi캜nih za dolo캜eno podro캜je, z dostopom do notranje baze znanja.

- Zmanj코uje izdelavo z uporabo **preverljivih podatkov** v bazi znanja za zagotavljanje konteksta uporabni코kim vpra코anjem.

- Je **stro코kovno u캜inkovit**, saj so bolj ekonomi캜ni v primerjavi z fino nastavitvijo LLM-a.

## Ustvarjanje baze znanja

Na코a aplikacija temelji na na코ih osebnih podatkih, tj. lekciji o nevronskih mre쬬h v u캜nem na캜rtu AI za za캜etnike.

### Vektorske baze podatkov

Vektorska baza podatkov, za razliko od tradicionalnih baz podatkov, je specializirana baza podatkov, zasnovana za shranjevanje, upravljanje in iskanje vektorskih predstavitev. Shranjuje 코tevil캜ne predstavitve dokumentov. Razdelitev podatkov na 코tevil캜ne vektorske predstavitve olaj코a na코emu AI sistemu razumevanje in obdelavo podatkov.

Na코e vektorske predstavitve shranjujemo v vektorskih bazah podatkov, saj imajo LLM-ji omejitev 코tevila znakov, ki jih sprejmejo kot vhod. Ker ne morete posredovati celotnih vektorskih predstavitev LLM-u, jih bomo morali razdeliti na dele, in ko uporabnik postavi vpra코anje, bodo vrnjene vektorske predstavitve, ki so najbolj podobne vpra코anju, skupaj s pozivom. Razdelitev na dele tudi zmanj코uje stro코ke glede na 코tevilo znakov, ki se posredujejo skozi LLM.

Nekatere priljubljene vektorske baze podatkov vklju캜ujejo Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant in DeepLake. Model Azure Cosmos DB lahko ustvarite z uporabo Azure CLI z naslednjim ukazom:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od besedila do vektorskih predstavitev

Preden shranimo na코e podatke, jih bomo morali pretvoriti v vektorske predstavitve, preden jih shranimo v bazo podatkov. 캛e delate z velikimi dokumenti ali dolgimi besedili, jih lahko razdelite na podlagi pri캜akovanih vpra코anj. Razdelitev lahko izvedete na ravni stavka ali odstavka. Ker razdelitev pridobiva pomene iz besed okoli njih, lahko dodate 코e nekaj konteksta k delu, na primer z dodajanjem naslova dokumenta ali vklju캜itvijo besedila pred ali po delu. Podatke lahko razdelite na naslednji na캜in:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Ko so razdeljeni, lahko nato vstavimo na코e besedilo z uporabo razli캜nih modelov vektorskih predstavitev. Nekateri modeli, ki jih lahko uporabite, vklju캜ujejo: word2vec, ada-002 s strani OpenAI, Azure Computer Vision in 코e ve캜. Izbira modela bo odvisna od jezikov, ki jih uporabljate, vrste kodirane vsebine (besedilo/slike/zvok), velikosti vnosa, ki ga lahko kodira, in dol쬴ne izhoda vektorske predstavitve.

Primer vstavljenega besedila z uporabo modela OpenAI `text-embedding-ada-002` je:
![vektorska predstavitev besede ma캜ka](../../../translated_images/cat.3db013cbca4fd5d90438ea7b312ad0364f7686cf79931ab15cd5922151aea53e.sl.png)

## Pridobivanje in vektorsko iskanje

Ko uporabnik postavi vpra코anje, ga retriver pretvori v vektor z uporabo kodirnika poizvedb, nato pa i코캜e po na코em indeksu iskanja dokumentov za ustrezne vektorje v dokumentu, ki so povezani z vhodom. Ko je to storjeno, pretvori tako vhodni vektor kot dokumentne vektorje v besedilo in jih posreduje skozi LLM.

### Pridobivanje

Pridobivanje se zgodi, ko sistem posku코a hitro najti dokumente iz indeksa, ki izpolnjujejo iskalna merila. Cilj retriverja je pridobiti dokumente, ki bodo uporabljeni za zagotavljanje konteksta in utemeljitev LLM-a na va코ih podatkih.

Obstaja ve캜 na캜inov za iskanje znotraj na코e baze podatkov, kot so:

- **Iskanje po klju캜nih besedah** - uporablja se za iskanje besedila

- **Semanti캜no iskanje** - uporablja semanti캜ni pomen besed

- **Vektorsko iskanje** - pretvarja dokumente iz besedila v vektorske predstavitve z uporabo modelov vektorskih predstavitev. Pridobivanje bo izvedeno z iskanjem dokumentov, katerih vektorske predstavitve so najbli쬵e uporabni코kemu vpra코anju.

- **Hibridno** - kombinacija iskanja po klju캜nih besedah in vektorskega iskanja.

Izziv pri pridobivanju se pojavi, ko v bazi podatkov ni podobnega odgovora na poizvedbo, sistem pa bo nato vrnil najbolj코e informacije, ki jih lahko dobi, vendar lahko uporabite taktike, kot so nastavitev najve캜je razdalje za ustreznost ali uporaba hibridnega iskanja, ki zdru쬿je tako iskanje po klju캜nih besedah kot vektorsko iskanje. V tej lekciji bomo uporabili hibridno iskanje, kombinacijo vektorskega in iskanja po klju캜nih besedah. Na코e podatke bomo shranili v podatkovni okvir s stolpci, ki vsebujejo dele in vektorske predstavitve.

### Vektorska podobnost

Retriver bo iskal po bazi znanja za vektorske predstavitve, ki so blizu skupaj, najbli쬵i sosed, saj so to besedila, ki so podobna. V scenariju, ko uporabnik postavi poizvedbo, se ta najprej vstavi, nato pa se ujema s podobnimi vektorskimi predstavitvami. Obi캜ajna meritev, ki se uporablja za ugotavljanje, kako podobni so razli캜ni vektorji, je kosinusna podobnost, ki temelji na kotu med dvema vektorjema.

Podobnost lahko merimo z drugimi alternativami, ki jih lahko uporabimo, so Evklidska razdalja, ki je ravna 캜rta med kon캜nimi to캜kami vektorjev, in skalarni produkt, ki meri vsoto produktov ustreznih elementov dveh vektorjev.

### Iskalni indeks

Ko izvajamo pridobivanje, bomo morali zgraditi iskalni indeks za na코o bazo znanja, preden izvedemo iskanje. Indeks bo shranil na코e vektorske predstavitve in lahko hitro pridobil najbolj podobne dele tudi v veliki bazi podatkov. Na코 indeks lahko ustvarimo lokalno z uporabo:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Ponovno rangiranje

Ko ste poizvedovali po bazi podatkov, boste morda morali rezultate razvrstiti od najbolj relevantnih. Ponovno rangiranje LLM uporablja strojno u캜enje za izbolj코anje ustreznosti iskalnih rezultatov z njihovim urejanjem od najbolj relevantnih. Z uporabo Azure AI Search je ponovno rangiranje samodejno izvedeno za vas z uporabo semanti캜nega ponovnega rangiranja. Primer, kako ponovno rangiranje deluje z uporabo najbli쬵ih sosedov:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Vse skupaj

Zadnji korak je dodajanje na코ega LLM-a v me코anico, da lahko dobimo odgovore, ki so utemeljeni na na코ih podatkih. To lahko implementiramo na naslednji na캜in:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Vrednotenje na코e aplikacije

### Evalvacijske metrike

- Kakovost dobavljenih odgovorov, ki zagotavlja, da zvenijo naravno, teko캜e in 캜love코ko.

- Utemeljenost podatkov: vrednotenje, ali je odgovor pri코el iz dobavljenih dokumentov.

- Ustreznost: vrednotenje, ali odgovor ustreza in je povezan z zastavljenim vpra코anjem.

- Teko캜nost - ali odgovor smiselno zveni slovni캜no.

## Primeri uporabe RAG (Pove캜ana generacija pridobivanja) in vektorskih baz podatkov

Obstaja veliko razli캜nih primerov uporabe, kjer lahko klici funkcij izbolj코ajo va코o aplikacijo, kot so:

- Vpra코anja in odgovori: utemeljevanje podatkov va코ega podjetja v klepetu, ki ga lahko uporabljajo zaposleni za postavljanje vpra코anj.

- Sistemi priporo캜il: kjer lahko ustvarite sistem, ki ujema najbolj podobne vrednosti, npr. filme, restavracije in 코e ve캜.

- Storitve klepetalnih botov: lahko shranite zgodovino klepeta in prilagodite pogovor na podlagi uporabni코kih podatkov.

- Iskanje slik na podlagi vektorskih predstavitev, uporabno pri prepoznavanju slik in odkrivanju anomalij.

## Povzetek

Pokazali smo osnovna podro캜ja RAG, od dodajanja na코ih podatkov v aplikacijo, uporabni코ke poizvedbe in izhoda. Za poenostavitev ustvarjanja RAG lahko uporabite ogrodja, kot so Semanti Kernel, Langchain ali Autogen.

## Naloga

Za nadaljevanje u캜enja o Pove캜ani generaciji pridobivanja (RAG) lahko zgradite:

- Zgradite sprednji del aplikacije z uporabo ogrodja po va코i izbiri.

- Uporabite ogrodje, bodisi LangChain ali Semanti Kernel, in ponovno ustvarite svojo aplikacijo.

캛estitamo za dokon캜anje lekcije 游녪.

## U캜enje se tukaj ne kon캜a, nadaljujte potovanje

Po kon캜ani lekciji si oglejte na코o [Generativno AI u캜no zbirko](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), da nadaljujete z nadgradnjo svojega znanja o generativni AI!

**Izjava o omejitvi odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). 캛eprav si prizadevamo za natan캜nost, vas opozarjamo, da lahko avtomatizirani prevodi vsebujejo napake ali neto캜nosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za kriti캜ne informacije je priporo캜ljivo profesionalno 캜love코ko prevajanje. Ne odgovarjamo za morebitne nesporazume ali napa캜ne razlage, ki izhajajo iz uporabe tega prevoda.