# Retrieval Augmented Generation (RAG) in vektorske baze podatkov

[![Retrieval Augmented Generation (RAG) in vektorske baze podatkov](../../../translated_images/sl/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

V lekciji o iskalnih aplikacijah smo na kratko spoznali, kako integrirati lastne podatke v velike jezikovne modele (LLM). V tej lekciji se bomo poglobili v koncepte utemeljitve vaÅ¡ih podatkov v aplikaciji LLM, mehaniko procesa in metode za shranjevanje podatkov, vkljuÄno z vkljuÄenimi predstavitvami (embeddings) in besedilom.

> **Video prihaja kmalu**

## Uvod

V tej lekciji bomo obravnavali naslednje:

- Uvod v RAG, kaj je in zakaj se uporablja v umetni inteligenci (AI).

- Razumevanje, kaj so vektorske baze podatkov in kako ustvariti eno za naÅ¡o aplikacijo.

- PraktiÄen primer, kako integrirati RAG v aplikacijo.

## Cilji uÄenja

Po zakljuÄku te lekcije boste znali:

- RazloÅ¾iti pomen RAG pri iskanju in obdelavi podatkov.

- Nastaviti RAG aplikacijo in utemeljiti vaÅ¡e podatke v LLM.

- UÄinkovito integrirati RAG in vektorske baze podatkov v aplikacije LLM.

## NaÅ¡ scenarij: izboljÅ¡anje naÅ¡ih LLM z lastnimi podatki

Za to lekcijo Å¾elimo dodati naÅ¡e lastne zapiske v izobraÅ¾evalni startup, kar omogoÄa klepetalnemu robotu, da pridobi veÄ informacij o razliÄnih predmetih. Uporabniki bodo lahko z uporabo teh zapiskov bolje Å¡tudirali in razumeli razliÄne teme, kar bo olajÅ¡alo pripravo na izpite. Za ustvarjanje naÅ¡ega scenarija bomo uporabili:

- `Azure OpenAI:` LLM, ki ga bomo uporabili za ustvarjanje klepetalnega bota

- `Lekcija AI za zaÄetnike o nevronskih mreÅ¾ah:` to bodo podatki, na katerih bomo utemeljili naÅ¡ LLM

- `Azure AI Search` in `Azure Cosmos DB:` vektorska baza podatkov za shranjevanje naÅ¡ih podatkov in ustvarjanje iskalnega indeksa

Uporabniki bodo lahko ustvarjali vajinske kvize iz svojih zapiskov, kartice za ponavljanje in jih povzeli v jedrnate preglede. Za zaÄetek si poglejmo, kaj je RAG in kako deluje:

## Retrieval Augmented Generation (RAG)

Klepetalni bot, ki ga poganja LLM, obdeluje uporabniÅ¡ke pozive za generiranje odgovorov. Namenjen je interakciji in sodelovanju z uporabniki na razliÄnih temah. Vendar so njegovi odgovori omejeni na kontekst, ki je podan, in na temeljne podatke za usposabljanje. Na primer, znanje GPT-4 je omejeno do septembra 2021, kar pomeni, da ne pozna dogodkov po tem obdobju. Poleg tega podatki, uporabljeni za usposabljanje LLM, ne vkljuÄujejo zaupnih informacij, kot so osebni zapiski ali priroÄniki izdelkov podjetja.

### Kako delujejo RAG (Retrieval Augmented Generation)

![risba, ki prikazuje, kako delujejo RAG](../../../translated_images/sl/how-rag-works.f5d0ff63942bd3a6.webp)

Recimo, da Å¾elite postaviti klepetalnega bota, ki ustvarja kvize iz vaÅ¡ih zapiskov; potrebovali boste povezavo z bazo znanja. Prav tu nastopi RAG. RAG deluje tako:

- **Baza znanja:** Pred pridobivanjem morajo biti ti dokumenti uvoÅ¾eni in predhodno obdelani, obiÄajno z razdelitvijo velikih dokumentov na manjÅ¡e koÅ¡Äke, pretvorbo v besedilne vkljuÄevalne predstavitve (text embeddings) in shranjevanjem v bazo podatkov.

- **UporabniÅ¡ki poizvedba:** uporabnik postavi vpraÅ¡anje

- **Pridobivanje:** ko uporabnik postavi vpraÅ¡anje, model vkljuÄevalne predstavitve poiÅ¡Äe ustrezne informacije iz naÅ¡e baze znanja, da zagotovi veÄ konteksta, ki se bo vkljuÄil v poziv.

- **Obogatena generacija:** LLM izboljÅ¡a svoj odgovor na podlagi pridobljenih podatkov. To omogoÄa generiranje odgovora, ki ni le na podlagi vnaprej usposobljenih podatkov, ampak tudi ustreznih informacij iz dodanega konteksta. Pridobljeni podatki se uporabijo za obogatitev odgovorov LLM. LLM nato odgovori na vpraÅ¡anje uporabnika.

![risba, ki prikazuje arhitekturo RAG](../../../translated_images/sl/encoder-decode.f2658c25d0eadee2.webp)

Arhitektura RAG je implementirana z uporabo transformatorjev, ki imajo dva dela: encoder in decoder. Na primer, ko uporabnik postavi vpraÅ¡anje, je vhodno besedilo 'zakodirano' v vektorje, ki zajemajo pomen besed, nato pa so ti vektorji 'dekodirani' v naÅ¡ indeks dokumentov in generirajo nov tekst glede na uporabnikovo poizvedbo. LLM uporablja model encoder-decoder za generiranje izhoda.

Dve pristopi pri implementaciji RAG po predlaganem Älanku: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sta:

- **_RAG-Sequence_** â€” uporaba pridobljenih dokumentov za napoved najbolj moÅ¾nega odgovora na uporabnikovo vpraÅ¡anje

- **RAG-Token** â€” uporaba dokumentov za generiranje naslednjega tokena, nato pa jih ponovno pridobi za odgovor na vpraÅ¡anje uporabnika

### Zakaj uporabiti RAG?Â 

- **Bogastvo informacij:** zagotavlja, da so besedilni odgovori aÅ¾urni in aktualni. S tem izboljÅ¡uje delovanje na specifiÄnih podroÄjih z dostopom do notranje baze znanja.

- ZmanjÅ¡uje izmiÅ¡ljevanje z uporabo **preverljivih podatkov** v bazi znanja za kontekst uporabniÅ¡kih poizvedb.

- Je **stroÅ¡kovno uÄinkovito**, saj je bolj ekonomiÄno v primerjavi z dodatnim usposabljanjem (fine-tuning) LLM.

## Ustvarjanje baze znanja

NaÅ¡a aplikacija temelji na naÅ¡ih osebnih podatkih, tj. lekciji o nevronskih mreÅ¾ah v kurikulumu AI za zaÄetnike.

### Vektorske baze podatkov

Vektorska baza podatkov, v nasprotju s tradicionalnimi bazami, je specializirana baza, zasnovana za shranjevanje, upravljanje in iskanje v vkljuÄenih vektorjih. Shranjuje numeriÄne predstavitve dokumentov. Razbijanje podatkov v numeriÄne vkljuÄevalne predstavitve omogoÄa naÅ¡em AI sistemu laÅ¾je razumevanje in obdelavo podatkov.

NaÅ¡e vkljuÄevalne predstavitve hranimo v vektorskih bazah, ker imajo LLM omejitev glede Å¡tevila tokenov, ki jih sprejmejo kot vhod. Ker ne morete posredovati celotnih vkljuÄevalnih predstavitev LLM, jih bomo razdelili na koÅ¡Äke, ko pa uporabnik postavi vpraÅ¡anje, se vrnejo vkljuÄevalne predstavitve, ki so najbolj podobne vpraÅ¡anju, skupaj s pozivom. Razbijanje na koÅ¡Äke tudi zniÅ¾uje stroÅ¡ke glede Å¡tevila tokenov, ki gredo skozi LLM.

Nekatere priljubljene vektorske baze vkljuÄujejo Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant in DeepLake. Model Azure Cosmos DB lahko ustvarite z Azure CLI z naslednjim ukazom:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Iz besedila v vkljuÄevalne predstavitve

Preden shranimo podatke, jih moramo pretvoriti v vektorske vkljuÄevalne predstavitve, preden jih shranimo v bazo. ÄŒe delate z velikimi dokumenti ali dolgimi besedili, jih lahko razdelite glede na priÄakovane poizvedbe. Razbijanje lahko izvedete na ravni stavkov ali odstavkov. Ker razbijanje izhaja iz pomena besed okoli njega, lahko dodate tudi drug kontekst h koÅ¡Äku, na primer, dodajanje naslova dokumenta ali vkljuÄitev nekaj besedila pred ali za koÅ¡Äkom. Podatke lahko razdelite takole:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # ÄŒe zadnji delÄek ni dosegel najmanjÅ¡e dolÅ¾ine, ga vseeno dodajte
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Ko razbijemo, lahko nato vstavimo naÅ¡e besedilo z razliÄnimi modeli za vkljuÄevalne predstavitve. Nekateri modeli, ki jih lahko uporabite, so: word2vec, ada-002 od OpenAI, Azure Computer Vision in Å¡e mnogi drugi. Izbira modela je odvisna od jezikov, ki jih uporabljate, tipa vsebine (besedilo/slike/zvok), velikosti vhoda, ki ga lahko kodira, in dolÅ¾ine izhoda vkljuÄevalne predstavitve.

Primer vkljuÄenega besedila z modelom `text-embedding-ada-002` OpenAI je:
![vkljuÄevalna predstavitev besede cat](../../../translated_images/sl/cat.74cbd7946bc9ca38.webp)

## Pridobivanje in vektorsko iskanje

Ko uporabnik postavi vpraÅ¡anje, ga iskalnik pretvori v vektor z uporabo kodirnika poizvedbe, nato iÅ¡Äe skozi naÅ¡ indeks dokumentov ustrezne vektorje v dokumentu, povezane z vhodom. Ko konÄa, pretvori tako vhodni vektor kot vektorje dokumentov nazaj v tekst in jih posreduje LLM.

### Pridobivanje

Pridobivanje se zgodi, ko sistem hitro najde dokumente iz indeksa, ki ustrezajo iskalnim kriterijem. Cilj iskalnika je pridobiti dokumente, ki bodo sluÅ¾ili kot kontekst in utemeljili LLM na vaÅ¡ih podatkih.

Obstajajo razliÄni naÄini za iskanje v naÅ¡i bazi, na primer:

- **Iskanje po kljuÄnih besedah** â€“ za tekstovno iskanje

- **Vektorsko iskanje** â€“ pretvori dokumente iz besedila v vektorske predstavitve z modeli za vkljuÄevalne predstavitve, kar omogoÄa **semantiÄno iskanje** s pomenom besed. Pridobivanje poteka z iskanjem dokumentov, katerih vektorske predstavitve so najbliÅ¾je uporabnikovemu vpraÅ¡anju.

- **Hibridno** â€“ kombinacija iskanja po kljuÄnih besedah in vektorskega iskanja.

TeÅ¾ava pri pridobivanju nastane, ko v bazi ni podobnega odgovora na poizvedbo; sistem potem vrne najboljÅ¡e informacije, ki jih ima. Vendar lahko uporabite taktike, npr. nastavitev maksimalne razdalje za relevantnost ali hibridno iskanje, ki zdruÅ¾uje kljuÄne besede in vektorsko iskanje. V tej lekciji bomo uporabili hibridno iskanje, kombinacijo vektorskega in iskanja po kljuÄnih besedah. Shranili bomo podatke v podatkovni okvir s stolpci, ki vsebujejo tako koÅ¡Äke kot vkljuÄevalne predstavitve.

### Vektorska podobnost

Iskalnik bo iskal v bazi znanja za vkljuÄevalne predstavitve, ki so si blizu, najbliÅ¾ji sosed, saj so to podobni teksti. V primeru, da uporabnik postavi poizvedbo, jo najprej vkljuÄi, nato pa jo primerja s podobnimi vkljuÄevalnimi predstavitvami. Pogosta mera, ki se uporablja za ugotavljanje podobnosti med vektorji, je kosinusna podobnost, ki temelji na kotu med dvema vektorjema.

Podobnost lahko merimo tudi z drugimi moÅ¾nostmi, kot so evklidska razdalja, ki predstavlja najkrajÅ¡o pot med konÄnima toÄkama dveh vektorjev, ali skalarni produkt, ki meri vsoto produktov pripadajoÄih elementov dveh vektorjev.

### Iskalni indeks

Pri pridobivanju bomo morali najprej zgraditi iskalni indeks za naÅ¡o bazo znanja, preden izvedemo iskanje. Indeks bo shranjeval naÅ¡e vkljuÄevalne predstavitve in lahko hitro pridobil najbolj podobne koÅ¡Äke, tudi v veliki bazi. Indeks lahko ustvarimo lokalno z uporabo:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Ustvari iskalni indeks
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Za poizvedbo indeksa lahko uporabite metodo kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```

### Ponovno razvrÅ¡Äanje

Ko ste poizvedovali bazo, boste morda Å¾eleli rezultate urediti po relevantnosti. Ponovno razvrÅ¡Äanje (reranking) z LLM uporablja strojno uÄenje za izboljÅ¡anje relevantnosti iskalnih rezultatov z njihovim urejanjem od najbolj relevantnih. Z uporabo Azure AI Search se reranking izvaja avtomatiÄno s pomoÄjo semantiÄnega prerazvrÅ¡Äevalnika. Primer delovanja rerankinga z uporabo najbliÅ¾jih sosedov:

```python
# Najdi najbolj podobne dokumente
distances, indices = nbrs.kneighbors([query_vector])

index = []
# IzpiÅ¡i najbolj podobne dokumente
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Vse skupaj zdruÅ¾imo

Zadnji korak je dodati naÅ¡ LLM v zmes, da lahko dobimo odgovore, ki so utemeljeni na naÅ¡ih podatkih. To lahko implementiramo takole:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Pretvori vpraÅ¡anje v vektor poizvedbe
    query_vector = create_embeddings(user_input)

    # Najdi najbolj podobne dokumente
    distances, indices = nbrs.kneighbors([query_vector])

    # dodaj dokumente k poizvedbi, da zagotoviÅ¡ kontekst
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # zdruÅ¾i zgodovino in uporabnikov vnos
    history.append(user_input)

    # ustvari objekt sporoÄila
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # uporabi dokonÄanje klepeta za generiranje odgovora
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Vrednotenje naÅ¡e aplikacije

### Evalvacijske metrike

- Kakovost danih odgovorov, ki zagotavlja, da zvenijo naravno, tekoÄe in ÄloveÅ¡ko

- Utemeljenost podatkov: ocena, ali je odgovor priÅ¡el iz priskrbljenih dokumentov

- Relevantnost: ocena, ali odgovor ustreza in je povezan z zastavljenim vpraÅ¡anjem

- TekoÄnost â€“ ali je odgovor slovniÄno smiseln

## Primeri uporabe RAG in vektorskih baz podatkov

Obstaja veliko primerov uporabe, kjer lahko klici funkcij izboljÅ¡ajo vaÅ¡o aplikacijo, kot so:

- VpraÅ¡anja in odgovori: utemeljitev podatkov vaÅ¡ega podjetja v klepetalnik, ki ga zaposleni lahko uporabljajo za zastavljanje vpraÅ¡anj.

- PriporoÄilni sistemi: kjer lahko ustvarite sistem, ki najde najbolj podobne vrednosti, npr. filme, restavracije in Å¡e veÄ.

- Storitve klepetalnikov: lahko shranjujete zgodovino pogovorov in prilagajate pogovor glede na uporabniÅ¡ke podatke.

- Iskanje slik na podlagi vektorskih vkljuÄevalnih predstavitev, uporabno pri prepoznavanju slik in odkrivanju anomalij.

## Povzetek

Pokril smo osnovna podroÄja RAG, od dodajanja podatkov v aplikacijo, uporabniÅ¡ke poizvedbe do izhoda. Za poenostavitev ustvarjanja RAG lahko uporabite ogrodja, kot so Semanti Kernel, Langchain ali Autogen.

## Naloga

Za nadaljevanje uÄenja Retrieval Augmented Generation (RAG) lahko:

- ustvarite uporabniÅ¡ki vmesnik za aplikacijo z ogrodjem po vaÅ¡i izbiri

- uporabite ogrodje, bodisi LangChain ali Semantic Kernel, in ponovno ustvarite vaÅ¡o aplikacijo.

ÄŒestitke za dokonÄanje lekcije ğŸ‘.

## UÄenje se tukaj ne ustavi, nadaljujte pot

Po zakljuÄku te lekcije si oglejte naÅ¡o [kolekcijo za uÄenje generativne AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), da Å¡e naprej nadgrajujete svoje znanje o generativni umetni inteligenci!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Opozorilo**:
Ta dokument je bil preveden z uporabo storitve za avtomatski prevod AI [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, upoÅ¡tevajte, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvorni jeziku velja za avtoritativni vir. Za kljuÄne informacije priporoÄamo strokovni ÄloveÅ¡ki prevod. Ne odgovarjamo za morebitna nesporazume ali napaÄne interpretacije, ki izhajajo iz uporabe tega prevoda.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->