<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-05-20T06:44:43+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "sl"
}
-->
# Uvod v nevronske mreÅ¾e: Perceptron

Eden prvih poskusov za izvedbo neÄesa podobnega sodobni nevronski mreÅ¾i je izvedel Frank Rosenblatt iz Cornell Aeronautical Laboratory leta 1957. Å lo je za strojno izvedbo, imenovano "Mark-1", ki je bila zasnovana za prepoznavanje primitivnih geometrijskih likov, kot so trikotniki, kvadrati in krogi.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='Perceptron Mark 1' />|

> Slike iz Wikipedije

Vhodna slika je bila predstavljena z 20x20 matriko fotocelic, zato je imela nevronska mreÅ¾a 400 vhodov in en binarni izhod. Enostavna mreÅ¾a je vsebovala en nevron, imenovan tudi **enota logiÄnega praga**. TeÅ¾e nevronske mreÅ¾e so delovale kot potenciometri, ki so zahtevali roÄno nastavitev med fazo uÄenja.

> âœ… Potenciometer je naprava, ki omogoÄa uporabniku nastavitev upornosti vezja.

> New York Times je takrat pisal o perceptronu: *zarodek elektronskega raÄunalnika, ki [morje] priÄakuje, da bo lahko hodil, govoril, videl, pisal, se reproduciral in bil zavesten svoje eksistence.*

## Model perceptrona

Predpostavimo, da imamo v naÅ¡em modelu N znaÄilnosti, v tem primeru bi bil vhodni vektor vektor velikosti N. Perceptron je model **binarne klasifikacije**, kar pomeni, da lahko razlikuje med dvema razredoma vhodnih podatkov. Predpostavili bomo, da bi bil za vsak vhodni vektor x izhod naÅ¡ega perceptrona bodisi +1 ali -1, odvisno od razreda. Izhod se bo izraÄunal po formuli:

y(x) = f(w<sup>T</sup>x)

kjer je f stopniÄasta aktivacijska funkcija

## UÄenje perceptrona

Za uÄenje perceptrona moramo najti vektorsko teÅ¾o w, ki veÄino vrednosti pravilno klasificira, torej rezultira v najmanjÅ¡i **napaki**. Ta napaka je opredeljena z **perceptronskim kriterijem** na naslednji naÄin:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

kjer:

* se vsota vzame na tistih toÄkah uÄnih podatkov i, ki rezultirajo v napaÄni klasifikaciji
* x<sub>i</sub> so vhodni podatki, in t<sub>i</sub> je bodisi -1 ali +1 za negativne in pozitivne primere ustrezno.

Ta kriterij se Å¡teje kot funkcija teÅ¾e w, ki jo moramo minimizirati. Pogosto se uporablja metoda, imenovana **gradientni spust**, pri kateri zaÄnemo z zaÄetnimi teÅ¾ami w<sup>(0)</sup>, in nato v vsakem koraku posodobimo teÅ¾e po formuli:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

Tu je Î· tako imenovana **hitrost uÄenja**, in âˆ‡E(w) oznaÄuje **gradient** E. Ko izraÄunamo gradient, dobimo

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Algoritem v Pythonu izgleda takole:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## ZakljuÄek

V tej lekciji ste se nauÄili o perceptronu, ki je model binarne klasifikacije, in kako ga trenirati z uporabo vektorske teÅ¾e.

## ğŸš€ Izziv

ÄŒe Å¾elite poskusiti zgraditi svoj perceptron, poskusite ta laboratorij na Microsoft Learn, ki uporablja Azure ML designer.

## Pregled in samostojno uÄenje

Da vidite, kako lahko uporabimo perceptron za reÅ¡evanje enostavnih problemov kot tudi realnih problemov, in da nadaljujete z uÄenjem - pojdite na zvezek Perceptron.

Tukaj je zanimiv Älanek o perceptronih.

## Naloga

V tej lekciji smo izvedli perceptron za nalogo binarne klasifikacije, in uporabili smo ga za klasifikacijo med dvema roÄno napisanima Å¡tevilkama. V tem laboratoriju ste pozvani, da popolnoma reÅ¡ite problem klasifikacije Å¡tevilk, torej doloÄite, katera Å¡tevilka najverjetneje ustreza dani sliki.

* Navodila
* Zvezek

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas opozarjamo, da lahko avtomatski prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot verodostojen vir. Za kljuÄne informacije priporoÄamo strokovno ÄloveÅ¡ko prevajanje. Ne odgovarjamo za morebitna nesporazumevanja ali napaÄne razlage, ki bi nastale zaradi uporabe tega prevoda.