<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:52:12+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "sl"
}
-->
# Uvod v nevronske mreÅ¾e. VeÄplastni perceptron

V prejÅ¡njem poglavju ste spoznali najpreprostejÅ¡i model nevronske mreÅ¾e â€“ enoplastni perceptron, linearen model za dvo-razredno klasifikacijo.

V tem poglavju bomo ta model razÅ¡irili v bolj prilagodljiv okvir, ki nam omogoÄa:

* izvajanje **veÄrazredne klasifikacije** poleg dvo-razredne
* reÅ¡evanje **regresijskih problemov** poleg klasifikacije
* loÄevanje razredov, ki niso linearno loÄljivi

Razvili bomo tudi lasten modularni okvir v Pythonu, ki nam bo omogoÄil sestavljanje razliÄnih arhitektur nevronskih mreÅ¾.

## Formalizacija strojnega uÄenja

ZaÄnimo s formalizacijo problema strojnega uÄenja. Predpostavimo, da imamo uÄni niz podatkov **X** z oznakami **Y** in Å¾elimo zgraditi model *f*, ki bo dajal Äim natanÄnejÅ¡e napovedi. Kakovost napovedi merimo z **funkcijo izgube** â„’. Pogosto uporabljene funkcije izgube so:

* Za regresijski problem, ko napovedujemo Å¡tevilo, lahko uporabimo **absolutno napako** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| ali **kvadratno napako** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Za klasifikacijo uporabljamo **0-1 izgubo** (kar je v bistvu enako kot **natanÄnost** modela) ali **logistiÄno izgubo**.

Za enoplastni perceptron je bila funkcija *f* definirana kot linearna funkcija *f(x)=wx+b* (tukaj je *w* matrika uteÅ¾i, *x* vektor vhodnih znaÄilk, *b* pa vektor pristranskosti). Pri razliÄnih arhitekturah nevronskih mreÅ¾ lahko ta funkcija zavzame bolj zapleteno obliko.

> Pri klasifikaciji je pogosto zaÅ¾eleno, da kot izhod mreÅ¾e dobimo verjetnosti pripadajoÄih razredov. Za pretvorbo poljubnih Å¡tevil v verjetnosti (npr. za normalizacijo izhoda) pogosto uporabimo **softmax** funkcijo Ïƒ, in funkcija *f* postane *f(x)=Ïƒ(wx+b)*

V zgornji definiciji *f* sta *w* in *b* imenovana **parametra** Î¸=âŸ¨*w,b*âŸ©. Glede na podatkovni niz âŸ¨**X**,**Y**âŸ© lahko izraÄunamo skupno napako na celotnem naboru kot funkcijo parametrov Î¸.

> âœ… **Cilj uÄenja nevronske mreÅ¾e je minimizirati napako z variiranjem parametrov Î¸**

## Optimizacija z gradientnim spustom

Obstaja dobro znana metoda optimizacije funkcij, imenovana **gradientni spust**. Ideja je, da lahko izraÄunamo odvod (v veÄdimenzionalnem primeru **gradient**) funkcije izgube glede na parametre in parametre spreminjamo tako, da se napaka zmanjÅ¡a. To lahko formaliziramo takole:

* Inicializiramo parametre z nakljuÄnimi vrednostmi w<sup>(0)</sup>, b<sup>(0)</sup>
* VeÄkrat ponovimo naslednji korak:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Med uÄenjem naj bi optimizacijske korake raÄunali na celotnem naboru podatkov (spomnimo se, da je izguba izraÄunana kot vsota preko vseh uÄnih primerov). V praksi pa vzamemo majhne dele podatkov, imenovane **minibatches**, in izraÄunamo gradient glede na podmnoÅ¾ico podatkov. Ker je podmnoÅ¾ica vsakiÄ izbrana nakljuÄno, temu pravimo **stohastiÄni gradientni spust** (SGD).

## VeÄplastni perceptroni in povratno Å¡irjenje napake

Enoplastna mreÅ¾a, kot smo videli zgoraj, zmore klasificirati linearno loÄljive razrede. Za gradnjo bogatejÅ¡ega modela lahko zdruÅ¾imo veÄ plasti mreÅ¾e. MatematiÄno to pomeni, da bo funkcija *f* imela bolj zapleteno obliko in se bo izraÄunala v veÄ korakih:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Tukaj je Î± **nenelinearna aktivacijska funkcija**, Ïƒ pa softmax funkcija, parametri pa Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritem gradientnega spusta ostane enak, a izraÄun gradientov je zahtevnejÅ¡i. Glede na pravilo veriÅ¾ne diferenciacije lahko odvode izraÄunamo kot:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Za izraÄun odvodov funkcije izgube glede na parametre uporabimo pravilo veriÅ¾ne diferenciacije.

Opazimo, da je levi del vseh izrazov enak, zato lahko odvode uÄinkovito raÄunamo, zaÄenÅ¡i pri funkciji izgube in se premikamo "nazaj" skozi raÄunski graf. Metoda uÄenja veÄplastnega perceptrona se zato imenuje **povratno Å¡irjenje napake** ali 'backprop'.

> TODO: navedba slike

> âœ… Povratno Å¡irjenje bomo podrobneje obravnavali v naÅ¡em primerku zvezka.

## ZakljuÄek

V tej lekciji smo zgradili lastno knjiÅ¾nico nevronskih mreÅ¾ in jo uporabili za preprosto dvodimenzionalno klasifikacijsko nalogo.

## ğŸš€ Izziv

V spremljajoÄem zvezku boste implementirali lasten okvir za gradnjo in uÄenje veÄplastnih perceptronov. Podrobno boste spoznali, kako delujejo sodobne nevronske mreÅ¾e.

Nadaljujte v OwnFramework zvezek in ga preglejte.

## Pregled in samostojno uÄenje

Povratno Å¡irjenje je pogost algoritem v AI in ML, ki ga je vredno podrobneje preuÄiti.

## Naloga

V tej vaji uporabite okvir, ki ste ga zgradili v tej lekciji, za reÅ¡evanje klasifikacije roÄno pisanih Å¡tevilk MNIST.

* Navodila
* Zvezek

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za avtomatski prevod AI [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas opozarjamo, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku velja za avtoritativni vir. Za kljuÄne informacije priporoÄamo strokovni ÄloveÅ¡ki prevod. Za morebitna nesporazume ali napaÄne interpretacije, ki izhajajo iz uporabe tega prevoda, ne odgovarjamo.