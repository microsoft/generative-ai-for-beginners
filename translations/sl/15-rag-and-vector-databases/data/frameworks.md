<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:38:26+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "sl"
}
-->
# Okviri za nevronske mreÅ¾e

Kot smo Å¾e spoznali, za uÄinkovito uÄenje nevronskih mreÅ¾ moramo narediti dve stvari:

* Operirati s tenzorji, npr. mnoÅ¾iti, seÅ¡tevati in izraÄunati nekatere funkcije, kot sta sigmoid ali softmax
* IzraÄunati gradient vseh izrazov, da lahko izvedemo optimizacijo z metodo gradientnega spusta

Medtem ko knjiÅ¾nica `numpy` zmore prvo nalogo, potrebujemo mehanizem za izraÄun gradientov. V naÅ¡em okviru, ki smo ga razvili v prejÅ¡njem poglavju, smo morali vse odvode roÄno programirati znotraj metode `backward`, ki izvaja backpropagation. Idealno bi bilo, da nam okvir omogoÄa izraÄun gradientov *kateregakoli izraza*, ki ga lahko definiramo.

Pomembno je tudi, da lahko izvajamo izraÄune na GPU ali drugih specializiranih procesnih enotah, kot je TPU. Globoko uÄenje nevronskih mreÅ¾ zahteva *veliko* izraÄunov, zato je zelo pomembno, da jih lahko paraleliziramo na GPU-jih.

> âœ… Izraz 'paralelizirati' pomeni razporediti izraÄune na veÄ naprav.

Trenutno sta najbolj priljubljena okvira za nevronske mreÅ¾e TensorFlow in PyTorch. Oba nudita nizkonivojski API za delo s tenzorji tako na CPU kot na GPU. Nad nizkonivojskim API-jem pa obstaja tudi viÅ¡jenivojski API, imenovan Keras oziroma PyTorch Lightning.

Nizkonivojski API | TensorFlow | PyTorch
-----------------|------------|---------
ViÅ¡jenivojski API| Keras      | PyTorch

**Nizkonivojski API-ji** v obeh okvirih omogoÄajo gradnjo t.i. **raÄunalniÅ¡kih grafov**. Ta graf doloÄa, kako izraÄunati izhod (obiÄajno funkcijo izgube) za dane vhodne parametre in ga je mogoÄe poslati v izraÄun na GPU, Äe je na voljo. Obstajajo funkcije za diferenciranje tega raÄunalniÅ¡kega grafa in izraÄun gradientov, ki jih nato uporabimo za optimizacijo parametrov modela.

**ViÅ¡jenivojski API-ji** obravnavajo nevronske mreÅ¾e kot **zaporedje plasti** in poenostavijo gradnjo veÄine nevronskih mreÅ¾. UÄenje modela obiÄajno zahteva pripravo podatkov in nato klic funkcije `fit`, ki opravi delo.

ViÅ¡jenivojski API omogoÄa hitro sestavo tipiÄnih nevronskih mreÅ¾ brez skrbi za podrobnosti. Hkrati nizkonivojski API ponuja veÄji nadzor nad procesom uÄenja, zato se pogosto uporablja v raziskavah, ko delamo z novimi arhitekturami nevronskih mreÅ¾.

Pomembno je razumeti, da lahko oba API-ja uporabljamo skupaj, npr. lahko razvijete lastno arhitekturo plasti z nizkonivojskim API-jem in jo nato uporabite znotraj veÄje mreÅ¾e, zgrajene in nauÄene z viÅ¡jenivojskim API-jem. Ali pa definirate mreÅ¾o z viÅ¡jenivojskim API-jem kot zaporedje plasti in nato uporabite svoj nizkonivojski uÄni zanko za optimizacijo. Oba API-ja temeljita na istih osnovnih konceptih in sta zasnovana za dobro medsebojno delovanje.

## UÄenje

V tem teÄaju ponujamo veÄino vsebin tako za PyTorch kot za TensorFlow. Izberete lahko svoj priljubljeni okvir in sledite ustreznim zvezkom. ÄŒe niste prepriÄani, kateri okvir izbrati, preberite nekaj razprav na spletu o **PyTorch vs. TensorFlow**. Lahko si ogledate tudi oba okvirja, da bolje razumete.

Kjer je mogoÄe, bomo zaradi enostavnosti uporabljali viÅ¡jenivojske API-je. Vendar menimo, da je pomembno razumeti, kako nevronske mreÅ¾e delujejo od temeljev, zato zaÄnemo z nizkonivojskim API-jem in tenzorji. ÄŒe pa Å¾elite hitro zaÄeti in se ne Å¾elite ukvarjati s podrobnostmi, lahko te dele preskoÄite in se takoj lotite viÅ¡jenivojskih API zvezkov.

## âœï¸ Vaje: Okviri

Nadaljujte z uÄenjem v naslednjih zvezkih:

Nizkonivojski API | TensorFlow+Keras zvezek | PyTorch
-----------------|--------------------------|---------
ViÅ¡jenivojski API| Keras                    | *PyTorch Lightning*

Ko boste obvladali okvire, si osveÅ¾imo pojem prekomernega prileganja.

# Prekomerno prileganje (Overfitting)

Prekomerno prileganje je izjemno pomemben koncept v strojni inteligenci in zelo pomembno je, da ga pravilno razumemo!

Razmislimo o problemu pribliÅ¾evanja 5 toÄk (na grafih spodaj oznaÄenih z `x`):

!linear | overfit
-------------------------|--------------------------
**Linearen model, 2 parametra** | **Nelinearen model, 7 parametrov**
Napaka uÄenja = 5.3 | Napaka uÄenja = 0
Napaka validacije = 5.1 | Napaka validacije = 20

* Na levi vidimo dobro linearno pribliÅ¾anje. Ker je Å¡tevilo parametrov ustrezno, model pravilno zajame vzorec razporeditve toÄk.
* Na desni je model premoÄan. Ker imamo le 5 toÄk, model s 7 parametri lahko prilagodi funkcijo tako, da gre skozi vse toÄke, zaradi Äesar je napaka uÄenja 0. Vendar to prepreÄuje modelu, da bi razumel pravi vzorec podatkov, zato je napaka validacije zelo visoka.

Zelo pomembno je najti pravo ravnovesje med kompleksnostjo modela (Å¡tevilom parametrov) in Å¡tevilom uÄnih vzorcev.

## Zakaj pride do prekomernega prileganja

  * Premalo uÄnih podatkov
  * Model je premoÄan
  * PreveÄ Å¡uma v vhodnih podatkih

## Kako zaznati prekomerno prileganje

Kot lahko vidite na zgornjem grafu, lahko prekomerno prileganje zaznamo po zelo nizki napaki uÄenja in visoki napaki validacije. ObiÄajno med uÄenjem vidimo, da se napaki uÄenja in validacije sprva zmanjÅ¡ujeta, nato pa se napaka validacije ustavi in zaÄne naraÅ¡Äati. To je znak prekomernega prileganja in indikator, da bi morali uÄenje verjetno ustaviti (ali vsaj narediti posnetek modela).

prekomerno prileganje

## Kako prepreÄiti prekomerno prileganje

ÄŒe opazite, da prihaja do prekomernega prileganja, lahko naredite eno od naslednjega:

 * PoveÄajte koliÄino uÄnih podatkov
 * ZmanjÅ¡ajte kompleksnost modela
 * Uporabite tehniko regularizacije, kot je Dropout, ki jo bomo obravnavali kasneje.

## Prekomerno prileganje in kompromis med pristranskostjo in varianco

Prekomerno prileganje je pravzaprav primer bolj sploÅ¡nega problema v statistiki, imenovanega kompromis med pristranskostjo in varianco (Bias-Variance Tradeoff). ÄŒe pogledamo moÅ¾ne vire napak v naÅ¡em modelu, lahko razlikujemo dve vrsti napak:

* **Napake zaradi pristranskosti (Bias errors)** nastanejo, ker naÅ¡ algoritem ne more pravilno zajeti odnosa med uÄnimi podatki. To se lahko zgodi, Äe model ni dovolj zmogljiv (**podprileganje**).
* **Napake zaradi variance (Variance errors)** nastanejo, ko model pribliÅ¾uje Å¡um v vhodnih podatkih namesto smiselnih vzorcev (**prekomerno prileganje**).

Med uÄenjem se napaka zaradi pristranskosti zmanjÅ¡uje (model se uÄi pribliÅ¾evati podatke), napaka zaradi variance pa naraÅ¡Äa. Pomembno je, da uÄenje ustavimo â€“ bodisi roÄno (ko zaznamo prekomerno prileganje) ali samodejno (z uvedbo regularizacije) â€“ da prepreÄimo prekomerno prileganje.

## ZakljuÄek

V tej lekciji ste spoznali razlike med razliÄnimi API-ji za dva najbolj priljubljena AI okvira, TensorFlow in PyTorch. Poleg tega ste se nauÄili o zelo pomembni temi, prekomernem prileganju.

## ğŸš€ Izziv

V spremljajoÄih zvezkih boste na dnu naÅ¡li 'naloge'; preglejte zvezke in jih dokonÄajte.

## Pregled in samostojno uÄenje

Raziskujte naslednje teme:

- TensorFlow
- PyTorch
- Prekomerno prileganje

VpraÅ¡ajte se:

- KakÅ¡na je razlika med TensorFlow in PyTorch?
- KakÅ¡na je razlika med prekomernim prileganjem in podprileganjem?

## Naloga

V tej laboratorijski vaji morate reÅ¡iti dva problema klasifikacije z uporabo enoplastnih in veÄplastnih popolnoma povezanih mreÅ¾ z uporabo PyTorch ali TensorFlow.

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za avtomatski prevod AI [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas opozarjamo, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku velja za avtoritativni vir. Za kljuÄne informacije priporoÄamo strokovni ÄloveÅ¡ki prevod. Za morebitna nesporazume ali napaÄne interpretacije, ki izhajajo iz uporabe tega prevoda, ne odgovarjamo.