<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T02:09:33+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "sl"
}
-->
# Okviri za nevronske mreÅ¾e

Kot smo Å¾e spoznali, za uÄinkovito treniranje nevronskih mreÅ¾ moramo narediti dve stvari:

* Upravljati s tenzorji, npr. mnoÅ¾iti, seÅ¡tevati in izraÄunavati nekatere funkcije, kot so sigmoid ali softmax
* IzraÄunati gradient vseh izrazov, da lahko izvedemo optimizacijo z gradientnim spustom

ÄŒeprav knjiÅ¾nica `numpy` lahko opravi prvi del, potrebujemo nek mehanizem za izraÄun gradientov. V naÅ¡em okviru, ki smo ga razvili v prejÅ¡njem poglavju, smo morali roÄno programirati vse funkcije derivacij znotraj metode `backward`, ki izvaja povratno propagacijo. Idealno bi bilo, da bi nam okvir omogoÄal izraÄun gradientov *kateregakoli izraza*, ki ga lahko definiramo.

Druga pomembna stvar je zmoÅ¾nost izvajanja izraÄunov na GPU ali drugih specializiranih enotah, kot je TPU. Treniranje globokih nevronskih mreÅ¾ zahteva *veliko* izraÄunov, zato je zelo pomembno, da lahko te izraÄune paraleliziramo na GPU.

> âœ… Izraz 'paralelizirati' pomeni razdeliti izraÄune na veÄ naprav.

Trenutno sta dva najbolj priljubljena okvira za nevronske mreÅ¾e: TensorFlow in PyTorch. Oba nudita nizkonivojski API za delo s tenzorji na CPU in GPU. Poleg nizkonivojskega API-ja obstaja tudi viÅ¡jenivojski API, imenovan Keras in PyTorch Lightning.

Nizkonivojski API | TensorFlow| PyTorch
------------------|-------------------------------------|--------------------------------
ViÅ¡jenivojski API| Keras| Pytorch

**Nizkonivojski API-ji** v obeh okvirih vam omogoÄajo gradnjo tako imenovanih **raÄunalniÅ¡kih grafov**. Ta graf definira, kako izraÄunati izhod (obiÄajno funkcijo izgube) z danimi vhodnimi parametri, in ga je mogoÄe prenesti na GPU za izraÄun, Äe je na voljo. Obstajajo funkcije za diferenciacijo tega raÄunalniÅ¡kega grafa in izraÄun gradientov, ki se lahko nato uporabijo za optimizacijo parametrov modela.

**ViÅ¡jenivojski API-ji** obravnavajo nevronske mreÅ¾e kot **zaporedje slojev**, kar olajÅ¡a konstrukcijo veÄine nevronskih mreÅ¾. Treniranje modela obiÄajno zahteva pripravo podatkov in nato klic funkcije `fit`, da opravi delo.

ViÅ¡jenivojski API vam omogoÄa hitro konstrukcijo tipiÄnih nevronskih mreÅ¾, ne da bi se morali ukvarjati z veliko podrobnostmi. Hkrati pa nizkonivojski API nudi veliko veÄ nadzora nad procesom treniranja, zato se veliko uporablja v raziskavah, ko se ukvarjate z novimi arhitekturami nevronskih mreÅ¾.

Pomembno je tudi razumeti, da lahko oba API-ja uporabljate skupaj, npr. lahko razvijete svojo arhitekturo slojev mreÅ¾e z nizkonivojskim API-jem in jo nato uporabite znotraj veÄje mreÅ¾e, zgrajene in trenirane z viÅ¡jenivojskim API-jem. Ali pa lahko definirate mreÅ¾o z viÅ¡jenivojskim API-jem kot zaporedje slojev in nato uporabite svoj nizkonivojski cikel treniranja za optimizacijo. Oba API-ja uporabljata iste osnovne koncepte in sta zasnovana tako, da dobro delujeta skupaj.

## UÄenje

V tem teÄaju ponujamo veÄino vsebine tako za PyTorch kot za TensorFlow. Lahko izberete svoj najljubÅ¡i okvir in preuÄite samo ustrezne zvezke. ÄŒe niste prepriÄani, kateri okvir izbrati, preberite nekaj razprav na internetu o **PyTorch vs. TensorFlow**. Prav tako si lahko ogledate oba okvira, da bolje razumete.

Kjer je mogoÄe, bomo uporabljali viÅ¡jenivojske API-je zaradi enostavnosti. Vendar verjamemo, da je pomembno razumeti, kako nevronske mreÅ¾e delujejo od zaÄetka, zato bomo na zaÄetku zaÄeli z delom z nizkonivojskim API-jem in tenzorji. Vendar, Äe Å¾elite hitro zaÄeti in ne Å¾elite porabiti veliko Äasa za uÄenje teh podrobnosti, jih lahko preskoÄite in se takoj posvetite zvezkom z viÅ¡jenivojskim API-jem.

## âœï¸ Vaje: Okviri

Nadaljujte z uÄenjem v naslednjih zvezkih:

Nizkonivojski API | TensorFlow+Keras Zvezek | PyTorch
------------------|-------------------------------------|--------------------------------
ViÅ¡jenivojski API| Keras | *PyTorch Lightning*

Ko obvladate okvire, ponovimo koncept prenauÄenja.

# PrenauÄenje

PrenauÄenje je izjemno pomemben koncept v strojnem uÄenju in zelo pomembno je, da ga pravilno razumemo!

Razmislite o naslednjem problemu pribliÅ¾evanja 5 toÄk (predstavljenih z `x` na grafih spodaj):

!linear | prenauÄenje
----------------------|--------------------------
**Linearen model, 2 parametra** | **Nelinearen model, 7 parametrov**
Napaka pri treniranju = 5.3 | Napaka pri treniranju = 0
Napaka pri validaciji = 5.1 | Napaka pri validaciji = 20

* Na levi vidimo dobro pribliÅ¾anje s premico. Ker je Å¡tevilo parametrov ustrezno, model pravilno razume razporeditev toÄk.
* Na desni je model preveÄ zmogljiv. Ker imamo le 5 toÄk in model ima 7 parametrov, se lahko prilagodi tako, da gre skozi vse toÄke, kar povzroÄi, da je napaka pri treniranju 0. Vendar to prepreÄi modelu, da bi razumel pravi vzorec podatkov, zato je napaka pri validaciji zelo visoka.

Zelo pomembno je doseÄi pravilno ravnoteÅ¾je med bogastvom modela (Å¡tevilo parametrov) in Å¡tevilom vzorcev za treniranje.

## Zakaj pride do prenauÄenja

  * Premalo podatkov za treniranje
  * PreveÄ zmogljiv model
  * PreveÄ Å¡uma v vhodnih podatkih

## Kako zaznati prenauÄenje

Kot lahko vidite iz zgornjega grafa, lahko prenauÄenje zaznamo z zelo nizko napako pri treniranju in visoko napako pri validaciji. ObiÄajno med treniranjem vidimo, da se napake pri treniranju in validaciji zaÄneta zmanjÅ¡evati, nato pa se lahko na neki toÄki napaka pri validaciji preneha zmanjÅ¡evati in zaÄne naraÅ¡Äati. To bo znak prenauÄenja in indikator, da bi morali verjetno prenehati s treniranjem na tej toÄki (ali vsaj narediti posnetek modela).

prenauÄenje

## Kako prepreÄiti prenauÄenje

ÄŒe opazite, da se pojavlja prenauÄenje, lahko storite eno od naslednjega:

 * PoveÄajte koliÄino podatkov za treniranje
 * ZmanjÅ¡ajte kompleksnost modela
 * Uporabite tehniko regularizacije, kot je Dropout, ki jo bomo obravnavali kasneje.

## PrenauÄenje in kompromis med pristranskostjo in varianco

PrenauÄenje je pravzaprav primer bolj sploÅ¡nega problema v statistiki, imenovanega kompromis med pristranskostjo in varianco. ÄŒe razmislimo o moÅ¾nih virih napak v naÅ¡em modelu, lahko vidimo dve vrsti napak:

* **Napake pristranskosti** so povzroÄene zaradi nezmoÅ¾nosti naÅ¡ega algoritma, da pravilno zajame razmerje med podatki za treniranje. To lahko izhaja iz dejstva, da naÅ¡ model ni dovolj zmogljiv (**premalo prileganje**).
* **Napake variance**, ki jih povzroÄa model, ki pribliÅ¾uje Å¡um v vhodnih podatkih namesto smiselnega razmerja (**prenauÄenje**).

Med treniranjem se napaka pristranskosti zmanjÅ¡uje (ko naÅ¡ model uÄi pribliÅ¾evati podatke), medtem ko se napaka variance poveÄuje. Pomembno je prenehati s treniranjem - bodisi roÄno (ko zaznamo prenauÄenje) ali samodejno (z uvedbo regularizacije) - da prepreÄimo prenauÄenje.

## ZakljuÄek

V tej lekciji ste spoznali razlike med razliÄnimi API-ji za dva najbolj priljubljena AI okvira, TensorFlow in PyTorch. Poleg tega ste spoznali zelo pomembno temo, prenauÄenje.

## ğŸš€ Izziv

V spremljajoÄih zvezkih boste naÅ¡li 'naloge' na dnu; preglejte zvezke in dokonÄajte naloge.

## Pregled & Samostojno uÄenje

Raziskujte naslednje teme:

- TensorFlow
- PyTorch
- PrenauÄenje

VpraÅ¡ajte se naslednja vpraÅ¡anja:

- KakÅ¡na je razlika med TensorFlow in PyTorch?
- KakÅ¡na je razlika med prenauÄenjem in premalo prileganjem?

## Naloga

V tej delavnici morate reÅ¡iti dve klasifikacijski nalogi z uporabo enoplastnih in veÄplastnih popolnoma povezanih mreÅ¾ z uporabo PyTorch ali TensorFlow.

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas opozarjamo, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za kritiÄne informacije je priporoÄljiv strokovni ÄloveÅ¡ki prevod. Ne prevzemamo odgovornosti za morebitne nesporazume ali napaÄne razlage, ki izhajajo iz uporabe tega prevoda.