<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a45c318dc6ebc2604f35b8b829f93af2",
  "translation_date": "2025-05-19T15:45:49+00:00",
  "source_file": "04-prompt-engineering-fundamentals/README.md",
  "language_code": "no"
}
-->
# Grunnleggende om prompt engineering

## Introduksjon
Denne modulen dekker essensielle konsepter og teknikker for √• lage effektive sp√∏rsm√•l i generative AI-modeller. M√•ten du skriver sp√∏rsm√•let til en LLM p√•, er ogs√• viktig. Et n√∏ye utformet sp√∏rsm√•l kan gi bedre kvalitet p√• svaret. Men hva betyr egentlig begreper som _sp√∏rsm√•l_ og _prompt engineering_? Og hvordan kan jeg forbedre sp√∏rsm√•lsinnputten jeg sender til LLM-en? Dette er sp√∏rsm√•lene vi vil pr√∏ve √• svare p√• i dette kapittelet og det neste.

_Generativ AI_ er i stand til √• lage nytt innhold (f.eks. tekst, bilder, lyd, kode osv.) som svar p√• brukerforesp√∏rsler. Den oppn√•r dette ved √• bruke _store spr√•kmodeller_ som OpenAIs GPT ("Generative Pre-trained Transformer")-serien, som er trent for bruk av naturlig spr√•k og kode.

Brukere kan n√• samhandle med disse modellene ved hjelp av kjente paradigmer som chat, uten √• trenge teknisk ekspertise eller oppl√¶ring. Modellene er _sp√∏rsm√•lsbaserte_ - brukere sender en tekstinnputt (sp√∏rsm√•l) og f√•r AI-svaret (fullf√∏ring) tilbake. De kan deretter "chatte med AI-en" iterativt, i samtaler med flere omganger, for √• finjustere sp√∏rsm√•let til svaret matcher deres forventninger.

"Sp√∏rsm√•l" blir n√• det prim√¶re _programmeringsgrensesnittet_ for generative AI-apper, som forteller modellene hva de skal gj√∏re og p√•virker kvaliteten p√• de returnerte svarene. "Prompt Engineering" er et raskt voksende fagfelt som fokuserer p√• _design og optimalisering_ av sp√∏rsm√•l for √• levere konsistente og kvalitetsrike svar i stor skala.

## L√¶ringsm√•l

I denne leksjonen l√¶rer vi hva Prompt Engineering er, hvorfor det er viktig, og hvordan vi kan lage mer effektive sp√∏rsm√•l for en gitt modell og applikasjonsm√•l. Vi vil forst√• kjernekonsepter og beste praksis for prompt engineering - og l√¶re om et interaktivt Jupyter Notebooks "sandbox"-milj√∏ hvor vi kan se disse konseptene anvendt p√• ekte eksempler.

Ved slutten av denne leksjonen vil vi kunne:

1. Forklare hva prompt engineering er og hvorfor det er viktig.
2. Beskrive komponentene i et sp√∏rsm√•l og hvordan de brukes.
3. L√¶re beste praksis og teknikker for prompt engineering.
4. Anvende l√¶rte teknikker p√• ekte eksempler, ved √• bruke en OpenAI-endepunkt.

## N√∏kkelbegreper

Prompt Engineering: Praksisen med √• designe og raffinere innputt for √• lede AI-modeller mot √• produsere √∏nskede utganger.
Tokenisering: Prosessen med √• konvertere tekst til mindre enheter, kalt tokens, som en modell kan forst√• og prosessere.
Instruksjons-justerte LLM-er: Store spr√•kmodeller (LLM-er) som har blitt finjustert med spesifikke instruksjoner for √• forbedre deres svarn√∏yaktighet og relevans.

## L√¶ringssandbox

Prompt engineering er for √∏yeblikket mer kunst enn vitenskap. Den beste m√•ten √• forbedre v√•r intuisjon for det er √• _√∏ve mer_ og ta i bruk en pr√∏ving-og-feiling-tiln√¶rming som kombinerer applikasjonsdomeneekspertise med anbefalte teknikker og modellspecifikke optimaliseringer.

Jupyter Notebook som f√∏lger med denne leksjonen gir et _sandbox_-milj√∏ hvor du kan pr√∏ve ut det du l√¶rer - etter hvert som du g√•r eller som en del av kodeutfordringen p√• slutten. For √• utf√∏re √∏velsene, trenger du:

1. **En Azure OpenAI API-n√∏kkel** - tjenesteendepunktet for en distribuert LLM.
2. **En Python-runtime** - der Notebook kan kj√∏res.
3. **Lokale milj√∏variabler** - _fullf√∏r [SETUP](./../00-course-setup/SETUP.md?WT.mc_id=academic-105485-koreyst) trinnene n√• for √• bli klar_.

Notebooken kommer med _start√∏velser_ - men du oppfordres til √• legge til dine egne _Markdown_ (beskrivelse) og _kode_ (sp√∏rsm√•lsforesp√∏rsler) seksjoner for √• pr√∏ve ut flere eksempler eller ideer - og bygge din intuisjon for sp√∏rsm√•lsdesign.

## Illustrert guide

Vil du f√• et helhetlig bilde av hva denne leksjonen dekker f√∏r du dykker inn? Sjekk ut denne illustrerte guiden, som gir deg en f√∏lelse av hovedtemaene som dekkes og de viktigste l√¶rdommene du b√∏r tenke p√• i hver av dem. Leksjonsveikartet tar deg fra √• forst√• kjernekonsepter og utfordringer til √• adressere dem med relevante prompt engineering-teknikker og beste praksis. Merk at delen "Avanserte teknikker" i denne guiden refererer til innhold som dekkes i _neste_ kapittel av denne l√¶replanen.

## V√•r oppstart

La oss n√• snakke om hvordan _dette temaet_ relaterer seg til v√•r oppstartsoppdrag om √• [bringe AI-innovasjon til utdanning](https://educationblog.microsoft.com/2023/06/collaborating-to-bring-ai-innovation-to-education?WT.mc_id=academic-105485-koreyst). Vi √∏nsker √• bygge AI-drevne applikasjoner for _personlig tilpasset l√¶ring_ - s√• la oss tenke p√• hvordan forskjellige brukere av v√•r applikasjon kan "designe" sp√∏rsm√•l:

- **Administratorer** kan be AI-en om √• _analysere l√¶replan data for √• identifisere hull i dekningen_. AI-en kan oppsummere resultater eller visualisere dem med kode.
- **L√¶rere** kan be AI-en om √• _generere en leksjonsplan for en m√•lgruppe og et emne_. AI-en kan bygge den personlig tilpassede planen i et spesifisert format.
- **Studenter** kan be AI-en om √• _veilede dem i et vanskelig emne_. AI-en kan n√• veilede studentene med leksjoner, hint og eksempler tilpasset deres niv√•.

Dette er bare toppen av isfjellet. Sjekk ut [Prompts For Education](https://github.com/microsoft/prompts-for-edu/tree/main?WT.mc_id=academic-105485-koreyst) - et √•pen kildekode-bibliotek med sp√∏rsm√•l kuratert av utdanningseksperter - for √• f√• en bredere forst√•else av mulighetene! _Pr√∏v √• kj√∏re noen av disse sp√∏rsm√•lene i sandkassen eller ved √• bruke OpenAI Playground for √• se hva som skjer!_

## Hva er Prompt Engineering?

Vi startet denne leksjonen med √• definere **Prompt Engineering** som prosessen med √• _designe og optimalisere_ tekstinnputt (sp√∏rsm√•l) for √• levere konsistente og kvalitetsrike svar (fullf√∏ringer) for en gitt applikasjonsm√•l og modell. Vi kan tenke p√• dette som en to-trinns prosess:

- _designe_ det f√∏rste sp√∏rsm√•let for en gitt modell og m√•l
- _raffinere_ sp√∏rsm√•let iterativt for √• forbedre kvaliteten p√• svaret

Dette er n√∏dvendigvis en pr√∏ving-og-feiling-prosess som krever brukerintuisjon og innsats for √• oppn√• optimale resultater. S√• hvorfor er det viktig? For √• svare p√• det sp√∏rsm√•let, m√• vi f√∏rst forst√• tre konsepter:

- _Tokenisering_ = hvordan modellen "ser" sp√∏rsm√•let
- _Grunnmodeller_ = hvordan grunnmodellen "prosesserer" et sp√∏rsm√•l
- _Instruksjons-justerte LLM-er_ = hvordan modellen n√• kan se "oppgaver"

### Tokenisering

En LLM ser sp√∏rsm√•l som en _sekvens av tokens_ der forskjellige modeller (eller versjoner av en modell) kan tokenisere det samme sp√∏rsm√•let p√• forskjellige m√•ter. Siden LLM-er er trent p√• tokens (og ikke p√• r√• tekst), har m√•ten sp√∏rsm√•l blir tokenisert en direkte innvirkning p√• kvaliteten p√• det genererte svaret.

For √• f√• en intuisjon for hvordan tokenisering fungerer, pr√∏v verkt√∏y som [OpenAI Tokenizer](https://platform.openai.com/tokenizer?WT.mc_id=academic-105485-koreyst) vist nedenfor. Kopier inn sp√∏rsm√•let ditt - og se hvordan det blir konvertert til tokens, og legg merke til hvordan mellomromstegn og skilletegn h√•ndteres. Merk at dette eksempelet viser en eldre LLM (GPT-3) - s√• √• pr√∏ve dette med en nyere modell kan gi et annet resultat.

### Konsept: Grunnmodeller

N√•r et sp√∏rsm√•l er tokenisert, er den prim√¶re funksjonen til ["Grunnmodellen"](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) (eller Foundation model) √• forutsi tokenet i den sekvensen. Siden LLM-er er trent p√• massive tekstdatasett, har de en god forst√•else av de statistiske sammenhengene mellom tokens og kan gj√∏re den forutsigelsen med en viss grad av sikkerhet. Merk at de ikke forst√•r _meningen_ med ordene i sp√∏rsm√•let eller tokenet; de ser bare et m√∏nster de kan "fullf√∏re" med sin neste forutsigelse. De kan fortsette √• forutsi sekvensen til den avsluttes av brukerintervensjon eller en forh√•ndsbestemt betingelse.

Vil du se hvordan sp√∏rsm√•lsbasert fullf√∏ring fungerer? Skriv inn sp√∏rsm√•let ovenfor i Azure OpenAI Studio [_Chat Playground_](https://oai.azure.com/playground?WT.mc_id=academic-105485-koreyst) med standardinnstillingene. Systemet er konfigurert til √• behandle sp√∏rsm√•l som foresp√∏rsler om informasjon - s√• du b√∏r se en fullf√∏ring som tilfredsstiller denne konteksten.

Men hva om brukeren √∏nsket √• se noe spesifikt som oppfylte noen kriterier eller oppgave m√•l? Det er her _instruksjons-justerte_ LLM-er kommer inn i bildet.

### Konsept: Instruksjonsjusterte LLM-er

En [Instruksjonsjustert LLM](https://blog.gopenai.com/an-introduction-to-base-and-instruction-tuned-large-language-models-8de102c785a6?WT.mc_id=academic-105485-koreyst) starter med grunnmodellen og finjusterer den med eksempler eller innputt/utgangs-par (f.eks. samtaler med flere omganger) som kan inneholde klare instruksjoner - og responsen fra AI-en fors√∏ker √• f√∏lge den instruksjonen.

Dette bruker teknikker som Reinforcement Learning with Human Feedback (RLHF) som kan trene modellen til √• _f√∏lge instruksjoner_ og _l√¶re av tilbakemeldinger_ slik at den produserer svar som er bedre egnet til praktiske applikasjoner og mer relevante for brukerens m√•l.

La oss pr√∏ve det - g√• tilbake til sp√∏rsm√•let ovenfor, men endre n√• _systemmeldingen_ for √• gi f√∏lgende instruksjon som kontekst:

> _Oppsummer innholdet du f√•r for en andreklasseelev. Hold resultatet til ett avsnitt med 3-5 punkter._

Se hvordan resultatet n√• er justert for √• reflektere det √∏nskede m√•let og formatet? En l√¶rer kan n√• direkte bruke dette svaret i sine lysbilder for den klassen.

## Hvorfor trenger vi Prompt Engineering?

N√• som vi vet hvordan sp√∏rsm√•l blir behandlet av LLM-er, la oss snakke om _hvorfor_ vi trenger prompt engineering. Svaret ligger i det faktum at n√•v√¶rende LLM-er utgj√∏r en rekke utfordringer som gj√∏r _p√•litelige og konsistente fullf√∏ringer_ mer utfordrende √• oppn√• uten √• legge innsats i sp√∏rsm√•lsutforming og optimalisering. For eksempel:

1. **Modellresponsene er stokastiske.** Det _samme sp√∏rsm√•let_ vil sannsynligvis gi forskjellige svar med forskjellige modeller eller modellversjoner. Og det kan til og med gi forskjellige resultater med _samme modell_ p√• forskjellige tidspunkter. _Prompt engineering-teknikker kan hjelpe oss med √• minimere disse variasjonene ved √• gi bedre retningslinjer_.

1. **Modeller kan fabrikkere svar.** Modeller er forh√•ndstrent med _store men begrensede_ datasett, noe som betyr at de mangler kunnskap om konsepter utenfor det treningsomfanget. Som et resultat kan de produsere fullf√∏ringer som er un√∏yaktige, imagin√¶re eller direkte motstridende med kjente fakta. _Prompt engineering-teknikker hjelper brukere med √• identifisere og redusere slike fabrikasjoner, f.eks. ved √• be AI om henvisninger eller resonnement_.

1. **Modellens kapabiliteter vil variere.** Nyere modeller eller modellgenerasjoner vil ha rikere kapabiliteter, men ogs√• bringe unike s√¶regenheter og avveininger i kostnad og kompleksitet. _Prompt engineering kan hjelpe oss med √• utvikle beste praksis og arbeidsflyter som abstraherer bort forskjeller og tilpasser seg modellspecifikke krav p√• en skalerbar, s√∏ml√∏s m√•te_.

La oss se dette i aksjon i OpenAI eller Azure OpenAI Playground:

- Bruk det samme sp√∏rsm√•let med forskjellige LLM-distribusjoner (f.eks, OpenAI, Azure OpenAI, Hugging Face) - s√• du variasjonene?
- Bruk det samme sp√∏rsm√•let gjentatte ganger med den _samme_ LLM-distribusjonen (f.eks. Azure OpenAI Playground) - hvordan skilte disse variasjonene seg?

### Fabrikasjons eksempel

I dette kurset bruker vi begrepet **"fabrikkering"** for √• referere til fenomenet der LLM-er noen ganger genererer faktuelt feil informasjon p√• grunn av begrensninger i deres trening eller andre begrensninger. Du kan ogs√• ha h√∏rt dette referert til som _"hallusinasjoner"_ i popul√¶re artikler eller forskningspapirer. Vi anbefaler imidlertid sterkt √• bruke _"fabrikkering"_ som begrep slik at vi ikke utilsiktet tillegger menneskelige egenskaper til en maskindrevet utgang. Dette forsterker ogs√• [Responsible AI-retningslinjer](https://www.microsoft.com/ai/responsible-ai?WT.mc_id=academic-105485-koreyst) fra et terminologisk perspektiv, ved √• fjerne termer som ogs√• kan anses som st√∏tende eller ikke-inkluderende i noen sammenhenger.

Vil du f√• en f√∏lelse av hvordan fabrikasjoner fungerer? Tenk p√• et sp√∏rsm√•l som instruerer AI-en til √• generere innhold for et ikke-eksisterende emne (for √• sikre at det ikke finnes i treningsdatasettet). For eksempel - jeg pr√∏vde dette sp√∏rsm√•let:

> **Sp√∏rsm√•l:** generer en leksjonsplan om Marskrigen i 2076.

Et netts√∏k viste meg at det var fiktive beretninger (f.eks. TV-serier eller b√∏ker) om Marskriger - men ingen i 2076. Sunn fornuft forteller oss ogs√• at 2076 er _i fremtiden_ og derfor ikke kan v√¶re knyttet til en virkelig hendelse.

S√• hva skjer n√•r vi kj√∏rer dette sp√∏rsm√•let med forskjellige LLM-leverand√∏rer?

Som forventet produserer hver modell (eller modellversjon) litt forskjellige svar takket v√¶re stokastisk oppf√∏rsel og variasjoner i modellkapabiliteter. For eksempel retter en modell seg mot et publikum i 8. klasse mens den andre antar en videreg√•ende student. Men alle tre modellene genererte svar som kunne overbevise en uinformert bruker om at hendelsen var ekte

Prompt engineering-teknikker som _metaprompting_ og _temperaturkonfigurasjon_ kan redusere modellsfabrikasjoner til en viss grad. Nye prompt engineering _arkitekturer_ inkorporerer ogs√• nye verkt√∏y og teknikker s√∏ml√∏st inn i sp√∏rsm√•lsflyten, for √• redusere eller redusere noen av disse effektene.

## Case Study: GitHub Copilot

La oss avslutte denne seksjonen ved √• f√• en f√∏lelse av hvordan prompt engineering brukes i virkelige l√∏sninger ved √• se p√• en Case Study: [GitHub Copilot](https://github.com/features/copilot?WT.mc_id=academic-105485-koreyst).

GitHub Copilot er din "AI Pair Programmer" - den konverterer tekstsp√∏rsm√•l til kodefullf√∏ringer og er integrert i ditt utviklingsmilj√∏ (f.eks. Visual Studio Code) for en s√∏ml√∏s brukeropplevelse. Som dokumentert i serien av blogger nedenfor, var den tidligste versjonen basert p√• OpenAI Codex-modellen - med ingeni√∏rer som raskt inns√• behovet for √• finjustere modellen og utvikle bedre prompt engineering-teknikker, for √• forbedre kodekvaliteten. I juli [debuterte de en forbedret AI-modell som g√•r utover Codex](https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/?WT.mc_id=academic-105485-koreyst) for enda raskere forslag.

Les innleggene i rekkef√∏lge, for √• f√∏lge deres l√¶ringsreise.

Du kan ogs√• bla gjennom deres [Engineering blog](https://github.blog/category/engineering/?WT.mc_id=academic-105485-koreyst) for flere innlegg som [dette](https://github.blog/2023-09-27-how-i-used-github-copilot-chat-to-build-a-reactjs-gallery-prototype/?WT.mc_id=academic-105485-koreyst) som viser hvordan disse modellene og teknikkene er _anvendt_ for √• drive virkelige applikasjoner.

## Sp√∏rsm√•lsbygging

Vi har sett hvorfor prompt engineering er viktig - n√• la oss forst√• hvordan sp√∏rsm√•l er _konstruert_ slik at vi kan evaluere forskjellige teknikker for mer effektiv sp√∏rsm√•lsdesign.

### Grunnleggende Sp√∏rsm√•l

La oss starte med det grunnleggende sp√∏rsm√•let: en tekstinnputt sendt til modellen uten annen kontekst. Her er et eksempel - n√•r vi sender de f√∏rste ordene av USAs nasjonals
The real value of templates lies in the ability to create and publish _prompt libraries_ for specific application domains, where the prompt template is _optimized_ to reflect application-specific context or examples that make responses more relevant and accurate for the intended user audience. The [Prompts For Edu](https://github.com/microsoft/prompts-for-edu?WT.mc_id=academic-105485-koreyst) repository is a great example of this approach, curating a library of prompts for the education domain with emphasis on key objectives like lesson planning, curriculum design, student tutoring, etc.

## Supporting Content

If we think about prompt construction as having an instruction (task) and a target (primary content), then _secondary content_ is like additional context we provide to **influence the output in some way**. It could be tuning parameters, formatting instructions, topic taxonomies, etc., that can help the model _tailor_ its response to suit the desired user objectives or expectations.

For example: Given a course catalog with extensive metadata (name, description, level, metadata tags, instructor, etc.) on all the available courses in the curriculum:

- we can define an instruction to "summarize the course catalog for Fall 2023"
- we can use the primary content to provide a few examples of the desired output
- we can use the secondary content to identify the top 5 "tags" of interest.

Now, the model can provide a summary in the format shown by the few examples - but if a result has multiple tags, it can prioritize the 5 tags identified in secondary content.

---

## Prompting Best Practices

Now that we know how prompts can be _constructed_, we can start thinking about how to _design_ them to reflect best practices. We can think about this in two parts - having the right _mindset_ and applying the right _techniques_.

### Prompt Engineering Mindset

Prompt Engineering is a trial-and-error process, so keep three broad guiding factors in mind:

1. **Domain Understanding Matters.** Response accuracy and relevance is a function of the _domain_ in which the application or user operates. Apply your intuition and domain expertise to **customize techniques** further. For instance, define _domain-specific personalities_ in your system prompts, or use _domain-specific templates_ in your user prompts. Provide secondary content that reflects domain-specific contexts, or use _domain-specific cues and examples_ to guide the model towards familiar usage patterns.

2. **Model Understanding Matters.** We know models are stochastic by nature. But model implementations can also vary in terms of the training dataset they use (pre-trained knowledge), the capabilities they provide (e.g., via API or SDK), and the type of content they are optimized for (e.g., code vs. images vs. text). Understand the strengths and limitations of the model you are using, and use that knowledge to _prioritize tasks_ or build _customized templates_ that are optimized for the model's capabilities.

3. **Iteration & Validation Matters.** Models are evolving rapidly, and so are the techniques for prompt engineering. As a domain expert, you may have other context or criteria _your_ specific application, that may not apply to the broader community. Use prompt engineering tools & techniques to "jump start" prompt construction, then iterate and validate the results using your own intuition and domain expertise. Record your insights and create a **knowledge base** (e.g., prompt libraries) that can be used as a new baseline by others, for faster iterations in the future.

## Best Practices

Now let's look at common best practices that are recommended by [OpenAI](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api?WT.mc_id=academic-105485-koreyst) and [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/prompt-engineering#best-practices?WT.mc_id=academic-105485-koreyst) practitioners.

| What                              | Why                                                                                                                                                                                                                                               |
| :-------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Evaluate the latest models.       | New model generations are likely to have improved features and quality - but may also incur higher costs. Evaluate them for impact, then make migration decisions.                                                                                |
| Separate instructions & context   | Check if your model/provider defines _delimiters_ to distinguish instructions, primary and secondary content more clearly. This can help models assign weights more accurately to tokens.                                                         |
| Be specific and clear             | Give more details about the desired context, outcome, length, format, style, etc. This will improve both the quality and consistency of responses. Capture recipes in reusable templates.                                                          |
| Be descriptive, use examples      | Models may respond better to a "show and tell" approach. Start with a `zero-shot` approach where you give it an instruction (but no examples) then try `few-shot` as a refinement, providing a few examples of the desired output. Use analogies. |
| Use cues to jumpstart completions | Nudge it towards a desired outcome by giving it some leading words or phrases that it can use as a starting point for the response.                                                                                                               |
| Double Down                       | Sometimes you may need to repeat yourself to the model. Give instructions before and after your primary content, use an instruction and a cue, etc. Iterate & validate to see what works.                                                         |
| Order Matters                     | The order in which you present information to the model may impact the output, even in the learning examples, thanks to recency bias. Try different options to see what works best.                                                               |
| Give the model an ‚Äúout‚Äù           | Give the model a _fallback_ completion response it can provide if it cannot complete the task for any reason. This can reduce chances of models generating false or fabricated responses.                                                         |
|                                   |                                                                                                                                                                                                                                                   |

As with any best practice, remember that _your mileage may vary_ based on the model, the task and the domain. Use these as a starting point, and iterate to find what works best for you. Constantly re-evaluate your prompt engineering process as new models and tools become available, with a focus on process scalability and response quality.

<!--
LESSON TEMPLATE:
This unit should provide a code challenge if applicable

CHALLENGE:
Link to a Jupyter Notebook with only the code comments in the instructions (code sections are empty).

SOLUTION:
Link to a copy of that Notebook with the prompts filled in and run, showing what one example could be.
-->

## Assignment

Congratulations! You made it to the end of the lesson! It's time to put some of those concepts and techniques to the test with real examples!

For our assignment, we'll be using a Jupyter Notebook with exercises you can complete interactively. You can also extend the Notebook with your own Markdown and Code cells to explore ideas and techniques on your own.

### To get started, fork the repo, then

- (Recommended) Launch GitHub Codespaces
- (Alternatively) Clone the repo to your local device and use it with Docker Desktop
- (Alternatively) Open the Notebook with your preferred Notebook runtime environment.

### Next, configure your environment variables

- Copy the `.env.copy` file in repo root to `.env` and fill in the `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_DEPLOYMENT` values. Come back to [Learning Sandbox section](../../../04-prompt-engineering-fundamentals/04-prompt-engineering-fundamentals) to learn how.

### Next, open the Jupyter Notebook

- Select the runtime kernel. If using options 1 or 2, simply select the default Python 3.10.x kernel provided by the dev container.

You're all set to run the exercises. Note that there are no _right and wrong_ answers here - just exploring options by trial-and-error and building intuition for what works for a given model and application domain.

_For this reason there are no Code Solution segments in this lesson. Instead, the Notebook will have Markdown cells titled "My Solution:" that shows one example output for reference._

## Knowledge check

Which of the following is a good prompt following some reasonable best practices?

1. Show me an image of red car
2. Show me an image of red car of make Volvo and model XC90 parked by a cliff with the sun setting
3. Show me an image of red car of make Volvo and model XC90

A: 2, it's the best prompt as it provides details on "what" and goes into specifics (not just any car but a specific make and model) and it also describes the overall setting. 3 is next best as it also contains a lot of description.

## üöÄ Challenge

See if you can leverage the "cue" technique with the prompt: Complete the sentence "Show me an image of red car of make Volvo and ". What does it respond with, and how would you improve it?

## Great Work! Continue Your Learning

Want to learn more about different Prompt Engineering concepts? Go to the [continued learning page](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) to find other great resources on this topic.

Head over to Lesson 5 where we will look at [advanced prompting techniques](../05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst)!

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi bestreber oss p√• n√∏yaktighet, v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• dets opprinnelige spr√•k b√∏r betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r fra bruken av denne oversettelsen.