{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bygging med Meta-familiemodellene\n",
    "\n",
    "## Introduksjon\n",
    "\n",
    "Denne leksjonen vil dekke:\n",
    "\n",
    "- Utforske de to hovedmodellene i Meta-familien – Llama 3.1 og Llama 3.2\n",
    "- Forstå bruksområder og scenarier for hver modell\n",
    "- Kodeeksempel som viser de unike egenskapene til hver modell\n",
    "\n",
    "## Meta-familiens modeller\n",
    "\n",
    "I denne leksjonen skal vi se nærmere på 2 modeller fra Meta-familien, eller \"Llama-flokken\" – Llama 3.1 og Llama 3.2\n",
    "\n",
    "Disse modellene finnes i ulike varianter og er tilgjengelige på Github Model marketplace. Her finner du mer informasjon om hvordan du kan bruke Github Models til å [prototype med AI-modeller](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Modellvarianter:\n",
    "- Llama 3.1 – 70B Instruct\n",
    "- Llama 3.1 – 405B Instruct\n",
    "- Llama 3.2 – 11B Vision Instruct\n",
    "- Llama 3.2 – 90B Vision Instruct\n",
    "\n",
    "*Merk: Llama 3 er også tilgjengelig på Github Models, men vil ikke bli gjennomgått i denne leksjonen*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Med 405 milliarder parametere faller Llama 3.1 inn under kategorien åpne kildekode-LLM-er.\n",
    "\n",
    "Modellen er en oppgradering fra den tidligere utgaven Llama 3 ved å tilby:\n",
    "\n",
    "- Større kontekstvindu – 128k tokens mot 8k tokens\n",
    "- Større maks antall utdata-tokens – 4096 mot 2048\n",
    "- Bedre flerspråklig støtte – takket være økning i antall treningstokens\n",
    "\n",
    "Dette gjør at Llama 3.1 kan håndtere mer komplekse bruksområder når man bygger GenAI-applikasjoner, inkludert:\n",
    "- Naturlig funksjonskall – muligheten til å kalle eksterne verktøy og funksjoner utenfor LLM-arbeidsflyten\n",
    "- Bedre RAG-ytelse – på grunn av det større kontekstvinduet\n",
    "- Generering av syntetiske data – muligheten til å lage effektiv data for oppgaver som finjustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native funksjonskalling\n",
    "\n",
    "Llama 3.1 har blitt finjustert for å være mer effektiv til å gjøre funksjons- eller verktøykall. Den har også to innebygde verktøy som modellen kan identifisere at bør brukes basert på brukerens prompt. Disse verktøyene er:\n",
    "\n",
    "- **Brave Search** – Kan brukes for å hente oppdatert informasjon som været ved å gjøre et nettsøk\n",
    "- **Wolfram Alpha** – Kan brukes for mer avanserte matematiske utregninger, slik at du slipper å skrive egne funksjoner.\n",
    "\n",
    "Du kan også lage dine egne tilpassede verktøy som LLM kan kalle på.\n",
    "\n",
    "I kodeeksempelet under:\n",
    "\n",
    "- Vi definerer tilgjengelige verktøy (brave_search, wolfram_alpha) i systemprompten.\n",
    "- Sender en brukerprompt som spør om været i en bestemt by.\n",
    "- LLM vil svare med et verktøykall til Brave Search-verktøyet, som vil se slik ut: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Merk: Dette eksempelet gjør kun verktøykallet. Hvis du ønsker å få resultatene, må du opprette en gratis konto på Brave API-siden og definere selve funksjonen.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Selv om Llama 3.1 er en LLM, har den en begrensning når det gjelder multimodalitet. Det vil si, å kunne bruke ulike typer input som bilder som prompt og gi svar. Denne evnen er en av hovedfunksjonene i Llama 3.2. Disse funksjonene inkluderer også:\n",
    "\n",
    "- Multimodalitet – har mulighet til å evaluere både tekst- og bildeprompter\n",
    "- Små til mellomstore varianter (11B og 90B) – dette gir fleksible muligheter for utrulling,\n",
    "- Kun tekst-varianter (1B og 3B) – dette gjør det mulig å bruke modellen på edge- og mobile enheter og gir lav ventetid\n",
    "\n",
    "Støtten for multimodalitet er et stort steg i verden av åpne kildekode-modeller. Eksempelet under tar både et bilde og en tekstprompt for å få en analyse av bildet fra Llama 3.2 90B.\n",
    "\n",
    "### Multimodal støtte med Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Læringen stopper ikke her, fortsett reisen\n",
    "\n",
    "Etter at du har fullført denne leksjonen, ta en titt på vår [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for å fortsette å utvikle kunnskapen din om Generativ AI!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber nøyaktighet, vær oppmerksom på at automatiske oversettelser kan inneholde feil eller unøyaktigheter. Det opprinnelige dokumentet på sitt originale språk skal anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell, menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:44:32+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "no"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}