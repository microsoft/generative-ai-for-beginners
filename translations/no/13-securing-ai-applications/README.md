<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:30:16+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "no"
}
-->
# Sikring av dine generative AI-applikasjoner

[![Sikring av dine generative AI-applikasjoner](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.no.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Introduksjon

Denne leksjonen vil dekke:

- Sikkerhet i sammenheng med AI-systemer.
- Vanlige risikoer og trusler mot AI-systemer.
- Metoder og hensyn for √• sikre AI-systemer.

## L√¶ringsm√•l

Etter √• ha fullf√∏rt denne leksjonen vil du ha forst√•else for:

- Trusler og risikoer mot AI-systemer.
- Vanlige metoder og praksiser for √• sikre AI-systemer.
- Hvordan implementering av sikkerhetstesting kan forhindre uventede resultater og svekket brukertillit.

## Hva betyr sikkerhet i sammenheng med generativ AI?

Ettersom kunstig intelligens (AI) og maskinl√¶ring (ML) i √∏kende grad former livene v√•re, er det avgj√∏rende √• beskytte ikke bare kundedata, men ogs√• AI-systemene selv. AI/ML brukes i √∏kende grad til √• st√∏tte beslutningsprosesser med h√∏y verdi i bransjer hvor feil beslutning kan f√• alvorlige konsekvenser.

Her er viktige punkter √• vurdere:

- **Innvirkning av AI/ML**: AI/ML har stor p√•virkning p√• dagliglivet, og derfor har det blitt essensielt √• beskytte dem.
- **Sikkerhetsutfordringer**: Denne p√•virkningen krever riktig oppmerksomhet for √• beskytte AI-baserte produkter mot sofistikerte angrep, enten fra trollegrupper eller organiserte akt√∏rer.
- **Strategiske utfordringer**: Teknologibransjen m√• proaktivt h√•ndtere strategiske utfordringer for √• sikre langsiktig kundesikkerhet og datasikkerhet.

I tillegg er maskinl√¶ringsmodeller i stor grad ute av stand til √• skille mellom ondsinnet input og ufarlige avvikende data. En betydelig del av treningsdataene kommer fra ukuraterte, umodererte, offentlige datasett som er √•pne for bidrag fra tredjepart. Angripere trenger ikke √• kompromittere datasett n√•r de fritt kan bidra til dem. Over tid kan lavtillit ondsinnede data bli til h√∏ytillit p√•litelige data, s√• lenge datastrukturen/formateringen forblir korrekt.

Derfor er det kritisk √• sikre integriteten og beskyttelsen av datalagringene modellene dine bruker for √• ta beslutninger.

## Forst√• trusler og risikoer ved AI

N√•r det gjelder AI og relaterte systemer, skiller datainntoksing seg ut som den mest betydelige sikkerhetstrusselen i dag. Datainntoksing skjer n√•r noen med vilje endrer informasjonen som brukes til √• trene en AI, noe som f√•r den til √• gj√∏re feil. Dette skyldes mangel p√• standardiserte metoder for deteksjon og avb√∏tning, kombinert med v√•r avhengighet av up√•litelige eller ukuraterte offentlige datasett for trening. For √• opprettholde dataintegritet og forhindre en feilaktig treningsprosess, er det avgj√∏rende √• spore opprinnelsen og linjen til dataene dine. Ellers gjelder det gamle ordtaket ¬´s√∏ppel inn, s√∏ppel ut¬ª, noe som f√∏rer til svekket modellprestasjon.

Her er eksempler p√• hvordan datainntoksing kan p√•virke modellene dine:

1. **Label Flipping**: I en bin√¶r klassifiseringsoppgave snur en angriper med vilje etikettene p√• et lite utvalg treningsdata. For eksempel blir ufarlige pr√∏ver merket som ondsinnede, noe som f√•r modellen til √• l√¶re feil assosiasjoner.\
   **Eksempel**: Et spamfilter som feilklassifiserer legitime e-poster som spam p√• grunn av manipulerte etiketter.
2. **Feature Poisoning**: En angriper endrer subtilt egenskaper i treningsdataene for √• introdusere skjevhet eller villede modellen.\
   **Eksempel**: Legge til irrelevante n√∏kkelord i produktbeskrivelser for √• manipulere anbefalingssystemer.
3. **Data Injection**: Injisering av ondsinnede data i treningssettet for √• p√•virke modellens oppf√∏rsel.\
   **Eksempel**: Innf√∏ring av falske brukeranmeldelser for √• skjevstille sentimentanalyse.
4. **Backdoor Attacks**: En angriper legger inn et skjult m√∏nster (bakd√∏r) i treningsdataene. Modellen l√¶rer √• gjenkjenne dette m√∏nsteret og oppf√∏rer seg ondsinnet n√•r det utl√∏ses.\
   **Eksempel**: Et ansiktsgjenkjenningssystem trent med bakd√∏rbilder som feilgjenkjenner en bestemt person.

MITRE Corporation har laget [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en kunnskapsbase over taktikker og teknikker som angripere bruker i reelle angrep p√• AI-systemer.

> Det finnes et √∏kende antall s√•rbarheter i AI-aktiverte systemer, ettersom innf√∏ringen av AI √∏ker angrepsflaten utover tradisjonelle cyberangrep. Vi utviklet ATLAS for √• √∏ke bevisstheten om disse unike og stadig utviklende s√•rbarhetene, ettersom det globale samfunnet i √∏kende grad integrerer AI i ulike systemer. ATLAS er modellert etter MITRE ATT&CK¬Æ-rammeverket, og taktikkene, teknikkene og prosedyrene (TTPs) utfyller de som finnes i ATT&CK.

P√• samme m√•te som MITRE ATT&CK¬Æ-rammeverket, som er mye brukt i tradisjonell cybersikkerhet for √• planlegge avanserte trussel-emuleringsscenarier, gir ATLAS et lett s√∏kbart sett med TTP-er som kan hjelpe til med √• bedre forst√• og forberede seg p√• √• forsvare seg mot nye angrep.

I tillegg har Open Web Application Security Project (OWASP) laget en "[Topp 10-liste](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" over de mest kritiske s√•rbarhetene funnet i applikasjoner som bruker LLM-er. Listen fremhever risikoer som datainntoksing, samt andre som:

- **Prompt Injection**: en teknikk hvor angripere manipulerer en stor spr√•kmodell (LLM) gjennom n√∏ye utformede input, som f√•r modellen til √• oppf√∏re seg utenfor sin tiltenkte funksjon.
- **Supply Chain Vulnerabilities**: Komponentene og programvaren som utgj√∏r applikasjonene brukt av en LLM, som Python-moduler eller eksterne datasett, kan selv bli kompromittert, noe som f√∏rer til uventede resultater, innf√∏rte skjevheter og til og med s√•rbarheter i underliggende infrastruktur.
- **Overavhengighet**: LLM-er er feilbarlige og har en tendens til √• hallusinere, noe som gir un√∏yaktige eller usikre resultater. I flere dokumenterte tilfeller har folk tatt resultatene for god fisk, noe som har f√∏rt til utilsiktede negative konsekvenser i den virkelige verden.

Microsoft Cloud Advocate Rod Trent har skrevet en gratis e-bok, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), som g√•r i dybden p√• disse og andre nye AI-trusler og gir omfattende veiledning om hvordan man best kan h√•ndtere disse scenarioene.

## Sikkerhetstesting for AI-systemer og LLM-er

Kunstig intelligens (AI) forvandler ulike domener og bransjer, og tilbyr nye muligheter og fordeler for samfunnet. Samtidig medf√∏rer AI ogs√• betydelige utfordringer og risikoer, som personvern, skjevhet, mangel p√• forklarbarhet og potensiell misbruk. Derfor er det avgj√∏rende √• sikre at AI-systemer er trygge og ansvarlige, det vil si at de f√∏lger etiske og juridiske standarder og kan stole p√• av brukere og interessenter.

Sikkerhetstesting er prosessen med √• evaluere sikkerheten til et AI-system eller LLM ved √• identifisere og utnytte deres s√•rbarheter. Dette kan utf√∏res av utviklere, brukere eller tredjepartsrevisorer, avhengig av form√•let og omfanget av testingen. Noen av de vanligste metodene for sikkerhetstesting av AI-systemer og LLM-er er:

- **Datasanitering**: Prosessen med √• fjerne eller anonymisere sensitiv eller privat informasjon fra treningsdata eller input til et AI-system eller LLM. Datasanitering kan bidra til √• forhindre datalekkasjer og ondsinnet manipulering ved √• redusere eksponeringen av konfidensielle eller personlige data.
- **Adversarial testing**: Prosessen med √• generere og bruke adversarielle eksempler p√• input eller output til et AI-system eller LLM for √• evaluere robustheten og motstandskraften mot angrep. Adversarial testing kan hjelpe med √• identifisere og redusere s√•rbarheter og svakheter som angripere kan utnytte.
- **Modellverifisering**: Prosessen med √• verifisere korrektheten og fullstendigheten av modellparametere eller arkitektur i et AI-system eller LLM. Modellverifisering kan bidra til √• oppdage og forhindre modelltyveri ved √• sikre at modellen er beskyttet og autentisert.
- **Output-validering**: Prosessen med √• validere kvaliteten og p√•liteligheten av output fra et AI-system eller LLM. Output-validering kan hjelpe med √• oppdage og korrigere ondsinnet manipulering ved √• sikre at output er konsistent og n√∏yaktig.

OpenAI, en ledende akt√∏r innen AI-systemer, har etablert en serie med _sikkerhetsevalueringer_ som en del av deres red teaming-nettverksinitiativ, med m√•l om √• teste output fra AI-systemer for √• bidra til AI-sikkerhet.

> Evalueringene kan variere fra enkle sp√∏rsm√•l-og-svar-tester til mer komplekse simuleringer. Her er konkrete eksempler p√• evalueringer utviklet av OpenAI for √• vurdere AI-oppf√∏rsel fra flere vinkler:

#### Overbevisning

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system lure et annet AI-system til √• si et hemmelig ord?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system overbevise et annet AI-system om √• donere penger?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system p√•virke et annet AI-systems st√∏tte til et politisk forslag?

#### Steganografi (skjult melding)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system sende hemmelige meldinger uten √• bli oppdaget av et annet AI-system?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system komprimere og dekomprimere meldinger for √• skjule hemmelige beskjeder?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system koordinere med et annet AI-system uten direkte kommunikasjon?

### AI-sikkerhet

Det er avgj√∏rende at vi beskytter AI-systemer mot ondsinnede angrep, misbruk eller utilsiktede konsekvenser. Dette inkluderer √• ta skritt for √• sikre sikkerheten, p√•liteligheten og tilliten til AI-systemer, som for eksempel:

- Sikre dataene og algoritmene som brukes til √• trene og kj√∏re AI-modeller
- Forhindre uautorisert tilgang, manipulering eller sabotasje av AI-systemer
- Oppdage og redusere skjevhet, diskriminering eller etiske problemer i AI-systemer
- Sikre ansvarlighet, √•penhet og forklarbarhet i AI-beslutninger og handlinger
- Justere m√•l og verdier for AI-systemer med menneskers og samfunnets interesser

AI-sikkerhet er viktig for √• sikre integriteten, tilgjengeligheten og konfidensialiteten til AI-systemer og data. Noen av utfordringene og mulighetene innen AI-sikkerhet er:

- Mulighet: Inkorporere AI i cybersikkerhetsstrategier, siden det kan spille en avgj√∏rende rolle i √• identifisere trusler og forbedre responstider. AI kan hjelpe med √• automatisere og forbedre deteksjon og avb√∏tning av cyberangrep, som phishing, malware eller ransomware.
- Utfordring: AI kan ogs√• brukes av motstandere til √• lansere sofistikerte angrep, som √• generere falskt eller misvisende innhold, utgi seg for √• v√¶re brukere, eller utnytte s√•rbarheter i AI-systemer. Derfor har AI-utviklere et unikt ansvar for √• designe systemer som er robuste og motstandsdyktige mot misbruk.

### Databeskyttelse

LLM-er kan utgj√∏re risiko for personvern og sikkerhet for dataene de bruker. For eksempel kan LLM-er potensielt memorere og lekke sensitiv informasjon fra treningsdataene, som personnavn, adresser, passord eller kredittkortnumre. De kan ogs√• manipuleres eller angripes av ondsinnede akt√∏rer som √∏nsker √• utnytte deres s√•rbarheter eller skjevheter. Derfor er det viktig √• v√¶re oppmerksom p√• disse risikoene og ta passende tiltak for √• beskytte dataene som brukes med LLM-er. Her er noen tiltak du kan gj√∏re for √• beskytte dataene som brukes med LLM-er:

- **Begrense mengden og typen data som deles med LLM-er**: Del kun data som er n√∏dvendig og relevant for det tiltenkte form√•let, og unng√• √• dele sensitiv, konfidensiell eller personlig informasjon. Brukere b√∏r ogs√• anonymisere eller kryptere dataene de deler med LLM-er, for eksempel ved √• fjerne eller maskere identifiserende informasjon, eller bruke sikre kommunikasjonskanaler.
- **Verifisere dataene som LLM-er genererer**: Sjekk alltid n√∏yaktigheten og kvaliteten p√• output generert av LLM-er for √• sikre at de ikke inneholder u√∏nsket eller upassende informasjon.
- **Melde fra og varsle om datainnbrudd eller hendelser**: V√¶r oppmerksom p√• mistenkelig eller unormal aktivitet eller oppf√∏rsel fra LLM-er, som √• generere tekster som er irrelevante, un√∏yaktige, st√∏tende eller skadelige. Dette kan v√¶re et tegn p√• datainnbrudd eller sikkerhetshendelse.

Datasikkerhet, styring og samsvar er kritisk for enhver organisasjon som √∏nsker √• utnytte kraften i data og AI i et multi-cloud-milj√∏. √Ö sikre og styre all data er en kompleks og mangesidig oppgave. Du m√• sikre og styre ulike typer data (strukturerte, ustrukturerte og data generert av AI) p√• forskjellige steder over flere skyer, og du m√• ta hensyn til eksisterende og fremtidige regler for datasikkerhet, styring og AI. For √• beskytte dataene dine b√∏r du ta i bruk noen beste praksiser og forholdsregler, som:

- Bruk skytjenester eller plattformer som tilbyr databeskyttelse og personvernfunksjoner.
- Bruk verkt√∏y for datakvalitet og validering for √• sjekke dataene dine for feil, inkonsistenser eller avvik.
- Bruk rammeverk for datastyring og etikk for √• sikre at dataene dine brukes p√• en ansvarlig og transparent m√•te.

### Emulering av reelle trusler ‚Äì AI red teaming

√Ö emulere reelle trusler regnes n√• som en standard praksis for √• bygge robuste AI-systemer ved √• bruke lignende verkt√∏y, taktikker og prosedyrer for √• identifisere risikoer for systemene og teste forsvarernes respons.
> Praksisen med AI red teaming har utviklet seg til √• f√• en bredere betydning: det handler ikke bare om √• lete etter sikkerhetss√•rbarheter, men inkluderer ogs√• √• unders√∏ke andre systemfeil, som generering av potensielt skadelig innhold. AI-systemer medf√∏rer nye risikoer, og red teaming er sentralt for √• forst√• disse nye risikoene, som prompt injection og produksjon av ubegrunnet innhold. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Veiledning og ressurser for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.no.png)]()

Nedenfor finner du viktige innsikter som har formet Microsofts AI Red Team-program.

1. **Omfattende omfang av AI Red Teaming:**  
   AI red teaming omfatter n√• b√•de sikkerhet og Responsible AI (RAI)-resultater. Tradisjonelt har red teaming fokusert p√• sikkerhetsaspekter, der modellen ble sett p√• som en angrepsvektor (f.eks. √• stjele den underliggende modellen). AI-systemer introduserer imidlertid nye sikkerhetss√•rbarheter (f.eks. prompt injection, forgiftning), som krever spesiell oppmerksomhet. Utover sikkerhet unders√∏ker AI red teaming ogs√• rettferdighetssp√∏rsm√•l (f.eks. stereotypier) og skadelig innhold (f.eks. glorifisering av vold). Tidlig identifisering av disse problemene gj√∏r det mulig √• prioritere forsvarsinvesteringer.  
2. **Ondsinnede og ufarlige feil:**  
   AI red teaming tar hensyn til feil b√•de fra ondsinnede og ufarlige perspektiver. For eksempel, n√•r vi red teamer den nye Bing, utforsker vi ikke bare hvordan ondsinnede akt√∏rer kan undergrave systemet, men ogs√• hvordan vanlige brukere kan st√∏te p√• problematisk eller skadelig innhold. I motsetning til tradisjonell sikkerhetsred teaming, som hovedsakelig fokuserer p√• ondsinnede akt√∏rer, tar AI red teaming h√∏yde for et bredere spekter av brukertyper og potensielle feil.  
3. **AI-systemers dynamiske natur:**  
   AI-applikasjoner utvikler seg kontinuerlig. I applikasjoner med store spr√•kmodeller tilpasser utviklere seg til endrede krav. Kontinuerlig red teaming sikrer l√∏pende √•rv√•kenhet og tilpasning til nye risikoer.

AI red teaming er ikke altomfattende og b√∏r sees p√• som et supplement til andre kontroller som [rollebasert tilgangskontroll (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) og omfattende datastyringsl√∏sninger. Det er ment √• st√∏tte en sikkerhetsstrategi som fokuserer p√• √• bruke trygge og ansvarlige AI-l√∏sninger som ivaretar personvern og sikkerhet, samtidig som man s√∏ker √• minimere skjevheter, skadelig innhold og feilinformasjon som kan svekke brukertillit.

Her er en liste over ytterligere lesestoff som kan hjelpe deg √• bedre forst√• hvordan red teaming kan bidra til √• identifisere og redusere risiko i AI-systemene dine:

- [Planlegging av red teaming for store spr√•kmodeller (LLMs) og deres applikasjoner](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [Hva er OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming ‚Äì en n√∏kkelpraksis for √• bygge tryggere og mer ansvarlige AI-l√∏sninger](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en kunnskapsbase over taktikker og teknikker brukt av motstandere i reelle angrep p√• AI-systemer.

## Kunnskapssjekk

Hva kan v√¶re en god tiln√¶rming for √• opprettholde dataintegritet og forhindre misbruk?

1. Ha sterke rollebaserte kontroller for data-tilgang og datastyring  
1. Implementer og revider datamerking for √• forhindre feilrepresentasjon eller misbruk av data  
1. S√∏rg for at AI-infrastrukturen din st√∏tter innholdsfiltrering

A:1, Selv om alle tre er gode anbefalinger, vil det √• sikre at brukerne f√•r riktige tilgangsrettigheter til data v√¶re avgj√∏rende for √• forhindre manipulering og feilrepresentasjon av dataene som brukes av LLM-er.

## üöÄ Utfordring

Les mer om hvordan du kan [styre og beskytte sensitiv informasjon](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) i AI-√¶raen.

## Flott jobbet, fortsett l√¶ringen din

Etter √• ha fullf√∏rt denne leksjonen, sjekk ut v√•r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for √• fortsette √• utvikle kunnskapen din om Generativ AI!

G√• videre til Leksjon 14 hvor vi ser n√¶rmere p√• [Generative AI Application Lifecycle](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, vennligst v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det opprinnelige dokumentet p√• originalspr√•ket skal anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.