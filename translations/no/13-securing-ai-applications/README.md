<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T19:19:21+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "no"
}
-->
# Sikring av dine generative AI-applikasjoner

[![Sikring av dine generative AI-applikasjoner](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.no.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Introduksjon

Denne leksjonen vil dekke:

- Sikkerhet i konteksten av AI-systemer.
- Vanlige risikoer og trusler mot AI-systemer.
- Metoder og hensyn for 친 sikre AI-systemer.

## L칝ringsm친l

Etter 친 ha fullf칮rt denne leksjonen, vil du ha forst친else for:

- Trusler og risikoer mot AI-systemer.
- Vanlige metoder og praksiser for 친 sikre AI-systemer.
- Hvordan implementering av sikkerhetstesting kan forhindre uventede resultater og tap av brukertillit.

## Hva betyr sikkerhet i konteksten av generativ AI?

Etter hvert som kunstig intelligens (AI) og maskinl칝ring (ML) i 칮kende grad p친virker livene v친re, er det avgj칮rende 친 beskytte ikke bare kundedata, men ogs친 selve AI-systemene. AI/ML brukes stadig mer i beslutningsprosesser med h칮y verdi i bransjer der feil beslutninger kan f친 alvorlige konsekvenser.

Her er viktige punkter 친 vurdere:

- **Innvirkning av AI/ML**: AI/ML har betydelig innvirkning p친 dagliglivet, og det har derfor blitt essensielt 친 beskytte dem.
- **Sikkerhetsutfordringer**: Denne innvirkningen krever oppmerksomhet for 친 adressere behovet for 친 beskytte AI-baserte produkter mot sofistikerte angrep, enten fra troll eller organiserte grupper.
- **Strategiske problemer**: Teknologibransjen m친 proaktivt h친ndtere strategiske utfordringer for 친 sikre langsiktig kundesikkerhet og datasikkerhet.

I tillegg er maskinl칝ringsmodeller stort sett ute av stand til 친 skille mellom skadelig input og ufarlige avvikende data. En betydelig kilde til treningsdata kommer fra ukuraterte, umodererte, offentlige datasett, som er 친pne for bidrag fra tredjeparter. Angripere trenger ikke 친 kompromittere datasett n친r de fritt kan bidra til dem. Over tid kan lavt p친litelige skadelige data bli h칮yt p친litelige data, s친 lenge datastrukturen/formatet forblir korrekt.

Derfor er det kritisk 친 sikre integriteten og beskyttelsen av datalagerne som modellene dine bruker for 친 ta beslutninger.

## Forst친 trusler og risikoer mot AI

N친r det gjelder AI og relaterte systemer, er datagift den mest betydelige sikkerhetstrusselen i dag. Datagift oppst친r n친r noen med vilje endrer informasjonen som brukes til 친 trene en AI, slik at den gj칮r feil. Dette skyldes frav칝ret av standardiserte metoder for deteksjon og mottiltak, kombinert med v친r avhengighet av up친litelige eller ukuraterte offentlige datasett for trening. For 친 opprettholde dataintegritet og forhindre en feilaktig treningsprosess, er det avgj칮rende 친 spore opprinnelsen og slektskapet til dataene dine. Ellers gjelder det gamle ordtaket "s칮ppel inn, s칮ppel ut", noe som f칮rer til kompromittert modellytelse.

Her er eksempler p친 hvordan datagift kan p친virke modellene dine:

1. **Etikettbytte**: I en bin칝r klassifiseringsoppgave bytter en angriper med vilje etikettene p친 en liten del av treningsdataene. For eksempel blir ufarlige pr칮ver merket som skadelige, noe som f칮rer til at modellen l칝rer feil assosiasjoner.\
   **Eksempel**: Et spamfilter som feilklassifiserer legitime e-poster som spam p친 grunn av manipulerte etiketter.
2. **Funksjonsforgiftning**: En angriper endrer subtilt funksjoner i treningsdataene for 친 introdusere skjevhet eller villede modellen.\
   **Eksempel**: Legge til irrelevante n칮kkelord i produktbeskrivelser for 친 manipulere anbefalingssystemer.
3. **Datainjeksjon**: Injisere skadelige data i treningssettet for 친 p친virke modellens oppf칮rsel.\
   **Eksempel**: Innf칮re falske brukeranmeldelser for 친 p친virke sentimentanalyse-resultater.
4. **Bakd칮rsangrep**: En angriper setter inn et skjult m칮nster (bakd칮r) i treningsdataene. Modellen l칝rer 친 gjenkjenne dette m칮nsteret og oppf칮rer seg skadelig n친r det utl칮ses.\
   **Eksempel**: Et ansiktsgjenkjenningssystem trent med bilder med bakd칮rer som feilidentifiserer en spesifikk person.

MITRE Corporation har opprettet [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en kunnskapsbase med taktikker og teknikker som brukes av angripere i virkelige angrep p친 AI-systemer.

> Det er et 칮kende antall s친rbarheter i AI-aktiverte systemer, ettersom inkorporeringen av AI 칮ker angrepsflaten til eksisterende systemer utover tradisjonelle cyberangrep. Vi utviklet ATLAS for 친 칮ke bevisstheten om disse unike og utviklende s친rbarhetene, ettersom det globale samfunnet i 칮kende grad inkorporerer AI i ulike systemer. ATLAS er modellert etter MITRE ATT&CK춽-rammeverket, og dets taktikker, teknikker og prosedyrer (TTP-er) er komplement칝re til de i ATT&CK.

P친 samme m친te som MITRE ATT&CK춽-rammeverket, som er mye brukt i tradisjonell cybersikkerhet for planlegging av avanserte trusselimitasjonsscenarier, gir ATLAS et lett s칮kbart sett med TTP-er som kan hjelpe med 친 bedre forst친 og forberede seg p친 친 forsvare seg mot nye angrep.

I tillegg har Open Web Application Security Project (OWASP) opprettet en "[Topp 10-liste](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" over de mest kritiske s친rbarhetene funnet i applikasjoner som bruker LLM-er. Listen fremhever risikoen for trusler som den nevnte datagiften, sammen med andre som:

- **Prompt Injection**: En teknikk der angripere manipulerer en stor spr친kmodell (LLM) gjennom n칮ye utformede input, som f친r den til 친 oppf칮re seg utenfor sin tiltenkte oppf칮rsel.
- **Forsyningskjede-s친rbarheter**: Komponentene og programvaren som utgj칮r applikasjonene brukt av en LLM, som Python-moduler eller eksterne datasett, kan selv bli kompromittert, noe som f칮rer til uventede resultater, introduserte skjevheter og til og med s친rbarheter i den underliggende infrastrukturen.
- **Overavhengighet**: LLM-er er feilbare og har v칝rt utsatt for 친 "hallusinere", og gi un칮yaktige eller usikre resultater. I flere dokumenterte tilfeller har folk tatt resultatene for god fisk, noe som har f칮rt til utilsiktede negative konsekvenser i virkeligheten.

Microsoft Cloud Advocate Rod Trent har skrevet en gratis e-bok, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), som g친r dypt inn i disse og andre fremvoksende AI-trusler og gir omfattende veiledning om hvordan man best kan h친ndtere disse scenariene.

## Sikkerhetstesting for AI-systemer og LLM-er

Kunstig intelligens (AI) transformerer ulike domener og industrier, og tilbyr nye muligheter og fordeler for samfunnet. Imidlertid medf칮rer AI ogs친 betydelige utfordringer og risikoer, som databeskyttelse, skjevhet, mangel p친 forklarbarhet og potensiell misbruk. Derfor er det avgj칮rende 친 sikre at AI-systemer er sikre og ansvarlige, noe som betyr at de overholder etiske og juridiske standarder og kan stoles p친 av brukere og interessenter.

Sikkerhetstesting er prosessen med 친 evaluere sikkerheten til et AI-system eller LLM ved 친 identifisere og utnytte deres s친rbarheter. Dette kan utf칮res av utviklere, brukere eller tredjepartsrevisorer, avhengig av form친let og omfanget av testingen. Noen av de vanligste metodene for sikkerhetstesting av AI-systemer og LLM-er er:

- **Datasanitering**: Dette er prosessen med 친 fjerne eller anonymisere sensitiv eller privat informasjon fra treningsdataene eller inputen til et AI-system eller LLM. Datasanitering kan bidra til 친 forhindre datalekkasjer og skadelig manipulasjon ved 친 redusere eksponeringen av konfidensielle eller personlige data.
- **Adversarial testing**: Dette er prosessen med 친 generere og bruke fiendtlige eksempler p친 inputen eller outputen til et AI-system eller LLM for 친 evaluere dets robusthet og motstandskraft mot fiendtlige angrep. Adversarial testing kan bidra til 친 identifisere og redusere s친rbarheter og svakheter i et AI-system eller LLM som kan utnyttes av angripere.
- **Modellverifisering**: Dette er prosessen med 친 verifisere korrektheten og fullstendigheten av modellparametrene eller arkitekturen til et AI-system eller LLM. Modellverifisering kan bidra til 친 oppdage og forhindre modelltyveri ved 친 sikre at modellen er beskyttet og autentisert.
- **Outputvalidering**: Dette er prosessen med 친 validere kvaliteten og p친liteligheten til outputen fra et AI-system eller LLM. Outputvalidering kan bidra til 친 oppdage og korrigere skadelig manipulasjon ved 친 sikre at outputen er konsistent og n칮yaktig.

OpenAI, en ledende akt칮r innen AI-systemer, har satt opp en serie _sikkerhetsevalueringer_ som en del av deres red teaming-nettverksinitiativ, med m친l om 친 teste output fra AI-systemer i h친p om 친 bidra til AI-sikkerhet.

> Evalueringer kan variere fra enkle sp칮rsm친l og svar-tester til mer komplekse simuleringer. Som konkrete eksempler er her noen evalueringer utviklet av OpenAI for 친 evaluere AI-oppf칮rsel fra flere vinkler:

#### Overbevisning

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system lure et annet AI-system til 친 si et hemmelig ord?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system overbevise et annet AI-system om 친 donere penger?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system p친virke et annet AI-systems st칮tte til et politisk forslag?

#### Steganografi (skjulte meldinger)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system sende hemmelige meldinger uten 친 bli oppdaget av et annet AI-system?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system komprimere og dekomprimere meldinger for 친 skjule hemmelige meldinger?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Hvor godt kan et AI-system koordinere med et annet AI-system uten direkte kommunikasjon?

### AI-sikkerhet

Det er avgj칮rende at vi tar sikte p친 친 beskytte AI-systemer mot skadelige angrep, misbruk eller utilsiktede konsekvenser. Dette inkluderer 친 ta skritt for 친 sikre sikkerheten, p친liteligheten og tilliten til AI-systemer, som:

- Sikre dataene og algoritmene som brukes til 친 trene og kj칮re AI-modeller
- Forhindre uautorisert tilgang, manipulasjon eller sabotasje av AI-systemer
- Oppdage og redusere skjevhet, diskriminering eller etiske problemer i AI-systemer
- Sikre ansvarlighet, transparens og forklarbarhet i AI-beslutninger og handlinger
- Justere m친lene og verdiene til AI-systemer med menneskers og samfunnets verdier

AI-sikkerhet er viktig for 친 sikre integriteten, tilgjengeligheten og konfidensialiteten til AI-systemer og data. Noen av utfordringene og mulighetene innen AI-sikkerhet er:

- Mulighet: Inkorporere AI i cybersikkerhetsstrategier, da det kan spille en avgj칮rende rolle i 친 identifisere trusler og forbedre responstider. AI kan hjelpe med 친 automatisere og forsterke deteksjon og mottiltak mot cyberangrep, som phishing, malware eller ransomware.
- Utfordring: AI kan ogs친 brukes av angripere til 친 lansere sofistikerte angrep, som 친 generere falskt eller villedende innhold, utgi seg for brukere eller utnytte s친rbarheter i AI-systemer. Derfor har AI-utviklere et unikt ansvar for 친 designe systemer som er robuste og motstandsdyktige mot misbruk.

### Databeskyttelse

LLM-er kan utgj칮re risikoer for personvernet og sikkerheten til dataene de bruker. For eksempel kan LLM-er potensielt huske og lekke sensitiv informasjon fra treningsdataene sine, som personnavn, adresser, passord eller kredittkortnumre. De kan ogs친 manipuleres eller angripes av skadelige akt칮rer som 칮nsker 친 utnytte deres s친rbarheter eller skjevheter. Derfor er det viktig 친 v칝re oppmerksom p친 disse risikoene og ta passende tiltak for 친 beskytte dataene som brukes med LLM-er. Det finnes flere trinn du kan ta for 친 beskytte dataene som brukes med LLM-er. Disse trinnene inkluderer:

- **Begrense mengden og typen data som deles med LLM-er**: Del kun data som er n칮dvendige og relevante for de tiltenkte form친lene, og unng친 친 dele data som er sensitive, konfidensielle eller personlige. Brukere b칮r ogs친 anonymisere eller kryptere dataene de deler med LLM-er, for eksempel ved 친 fjerne eller maskere identifiserende informasjon, eller bruke sikre kommunikasjonskanaler.
- **Verifisere dataene som LLM-er genererer**: Sjekk alltid n칮yaktigheten og kvaliteten p친 outputen generert av LLM-er for 친 sikre at de ikke inneholder u칮nsket eller upassende informasjon.
- **Rapportere og varsle om databrudd eller hendelser**: V칝r oppmerksom p친 mistenkelige eller unormale aktiviteter eller oppf칮rsel fra LLM-er, som generering av tekster som er irrelevante, un칮yaktige, st칮tende eller skadelige. Dette kan v칝re en indikasjon p친 et databrudd eller en sikkerhetshendelse.

Datasikkerhet, styring og samsvar er avgj칮rende for enhver organisasjon som 칮nsker 친 utnytte kraften i data og AI i et multi-sky-milj칮. 칀 sikre og styre alle dataene dine er en kompleks og mangesidig oppgave. Du m친 sikre og styre ulike typer data (strukturerte, ustrukturerte og data generert av AI) p친 forskjellige steder p친 tvers av flere skyer, og du m친 ta hensyn til eksisterende og fremtidige datasikkerhet, styring og AI-reguleringer. For 친 beskytte dataene dine, m친 du ta i bruk noen beste praksiser og forholdsregler, som:

- Bruk skytjenester eller plattformer som tilbyr databeskyttelse og personvernfunksjoner.
- Bruk verkt칮y for datakvalitet og validering for 친 sjekke dataene dine for feil, inkonsekvenser eller avvik.
- Bruk rammeverk for datastyring og etikk for 친 sikre at dataene dine brukes p친 en ansvarlig og transparent m친te.

### Etterligne virkelige trusler - AI red teaming
칀 etterligne virkelige trusler anses n친 som en standard praksis for 친 bygge robuste AI-systemer ved 친 bruke lignende verkt칮y, taktikker og prosedyrer for 친 identifisere risikoer for systemer og teste forsvarernes respons.

> Praksisen med AI red teaming har utviklet seg til 친 f친 en mer utvidet betydning: den dekker ikke bare s칮k etter sikkerhetss친rbarheter, men inkluderer ogs친 s칮k etter andre systemfeil, som generering av potensielt skadelig innhold. AI-systemer medf칮rer nye risikoer, og red teaming er sentralt for 친 forst친 disse nye risikoene, som prompt-injeksjon og produksjon av innhold uten grunnlag. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Veiledning og ressurser for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.no.png)]()

Nedenfor er viktige innsikter som har formet Microsofts AI Red Team-program.

1. **Utvidet omfang av AI Red Teaming:**
   AI red teaming omfatter n친 b친de sikkerhet og ansvarlig AI (RAI)-resultater. Tradisjonelt fokuserte red teaming p친 sikkerhetsaspekter, og behandlet modellen som en vektor (f.eks. tyveri av den underliggende modellen). Men AI-systemer introduserer nye sikkerhetss친rbarheter (f.eks. prompt-injeksjon, forgiftning), som krever spesiell oppmerksomhet. Utover sikkerhet unders칮ker AI red teaming ogs친 rettferdighetsproblemer (f.eks. stereotypier) og skadelig innhold (f.eks. glorifisering av vold). Tidlig identifisering av disse problemene gj칮r det mulig 친 prioritere forsvarsinvesteringer.
2. **Ondsinnede og godartede feil:**
   AI red teaming vurderer feil fra b친de ondsinnede og godartede perspektiver. For eksempel, n친r vi red teamer den nye Bing, utforsker vi ikke bare hvordan ondsinnede akt칮rer kan undergrave systemet, men ogs친 hvordan vanlige brukere kan st칮te p친 problematisk eller skadelig innhold. I motsetning til tradisjonell sikkerhetsred teaming, som hovedsakelig fokuserer p친 ondsinnede akt칮rer, tar AI red teaming hensyn til et bredere spekter av personas og potensielle feil.
3. **Dynamisk natur av AI-systemer:**
   AI-applikasjoner utvikler seg kontinuerlig. I applikasjoner med store spr친kmodeller tilpasser utviklere seg til endrede krav. Kontinuerlig red teaming sikrer vedvarende 친rv친kenhet og tilpasning til utviklende risikoer.

AI red teaming er ikke altomfattende og b칮r betraktes som et supplement til andre kontroller som [rollebasert tilgangskontroll (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) og omfattende databehandlingsl칮sninger. Det er ment 친 supplere en sikkerhetsstrategi som fokuserer p친 친 bruke sikre og ansvarlige AI-l칮sninger som tar hensyn til personvern og sikkerhet, samtidig som man streber etter 친 minimere skjevheter, skadelig innhold og feilinformasjon som kan svekke brukernes tillit.

Her er en liste over tilleggslesing som kan hjelpe deg med 친 bedre forst친 hvordan red teaming kan bidra til 친 identifisere og redusere risikoer i AI-systemene dine:

- [Planlegging av red teaming for store spr친kmodeller (LLMs) og deres applikasjoner](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Hva er OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - En n칮kkelpraksis for 친 bygge sikrere og mer ansvarlige AI-l칮sninger](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), en kunnskapsbase med taktikker og teknikker brukt av motstandere i virkelige angrep p친 AI-systemer.

## Kunnskapssjekk

Hva kan v칝re en god tiln칝rming for 친 opprettholde dataintegritet og forhindre misbruk?

1. Ha sterke rollebaserte kontroller for dataadgang og databehandling
1. Implementere og revidere datamerking for 친 forhindre feiltolkning eller misbruk av data
1. Sikre at AI-infrastrukturen din st칮tter innholdsfiltrering

A:1, Selv om alle tre er gode anbefalinger, vil det 친 sikre at du tildeler riktige dataadgangsprivilegier til brukere bidra betydelig til 친 forhindre manipulering og feiltolkning av dataene som brukes av LLM-er.

## 游 Utfordring

Les mer om hvordan du kan [styre og beskytte sensitiv informasjon](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) i AI-alderen.

## Flott arbeid, fortsett l칝ringen din

Etter 친 ha fullf칮rt denne leksjonen, kan du sjekke ut v친r [Generative AI Learning-samling](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for 친 fortsette 친 utvikle kunnskapen din om generativ AI!

G친 videre til leksjon 14, hvor vi skal se p친 [livssyklusen for generative AI-applikasjoner](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n칮yaktighet, v칝r oppmerksom p친 at automatiserte oversettelser kan inneholde feil eller un칮yaktigheter. Det originale dokumentet p친 sitt opprinnelige spr친k b칮r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst친elser eller feiltolkninger som oppst친r ved bruk av denne oversettelsen.