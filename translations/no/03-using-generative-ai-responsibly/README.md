<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "13084c6321a2092841b9a081b29497ba",
  "translation_date": "2025-05-19T14:42:24+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "no"
}
-->
# Bruke generativ AI p친 en ansvarlig m친te

> _Klikk p친 bildet over for 친 se videoen av denne leksjonen_

Det er lett 친 bli fascinert av AI og spesielt generativ AI, men du m친 vurdere hvordan du bruker det ansvarlig. Du m친 tenke p친 ting som hvordan du sikrer at resultatene er rettferdige, ikke-skadelige og mer. Dette kapittelet har som m친l 친 gi deg den nevnte konteksten, hva du b칮r vurdere, og hvordan du kan ta aktive steg for 친 forbedre din AI-bruk.

## Introduksjon

Denne leksjonen vil dekke:

- Hvorfor du b칮r prioritere ansvarlig AI n친r du bygger generative AI-applikasjoner.
- Kjerneprinsippene for ansvarlig AI og hvordan de relaterer seg til generativ AI.
- Hvordan sette disse prinsippene for ansvarlig AI ut i praksis gjennom strategi og verkt칮y.

## L칝ringsm친l

Etter 친 ha fullf칮rt denne leksjonen vil du vite:

- Viktigheten av ansvarlig AI n친r du bygger generative AI-applikasjoner.
- N친r du skal tenke p친 og anvende kjerneprinsippene for ansvarlig AI n친r du bygger generative AI-applikasjoner.
- Hvilke verkt칮y og strategier som er tilgjengelige for deg for 친 sette konseptet ansvarlig AI ut i praksis.

## Prinsipper for ansvarlig AI

Spenningsniv친et rundt generativ AI har aldri v칝rt h칮yere. Denne spenningen har brakt mange nye utviklere, oppmerksomhet og finansiering til dette omr친det. Selv om dette er veldig positivt for alle som 칮nsker 친 bygge produkter og selskaper ved hjelp av generativ AI, er det ogs친 viktig at vi g친r frem p친 en ansvarlig m친te.

Gjennom dette kurset fokuserer vi p친 친 bygge v친r oppstart og v친rt AI-utdanningsprodukt. Vi vil bruke prinsippene for ansvarlig AI: Rettferdighet, Inkludering, P친litelighet/Sikkerhet, Sikkerhet og Personvern, 칀penhet og Ansvarlighet. Med disse prinsippene vil vi utforske hvordan de relaterer seg til v친r bruk av generativ AI i v친re produkter.

## Hvorfor b칮r du prioritere ansvarlig AI

N친r du bygger et produkt, vil det 친 ta en menneskesentrert tiln칝rming ved 친 ha brukerens beste interesse i tankene f칮re til de beste resultatene.

Det unike ved generativ AI er dens evne til 친 skape nyttige svar, informasjon, veiledning og innhold for brukerne. Dette kan gj칮res uten mange manuelle steg som kan f칮re til sv칝rt imponerende resultater. Uten riktig planlegging og strategier kan det dessverre ogs친 f칮re til noen skadelige resultater for brukerne dine, produktet ditt og samfunnet som helhet.

La oss se p친 noen (men ikke alle) av disse potensielt skadelige resultatene:

### Hallusinasjoner

Hallusinasjoner er et begrep som brukes for 친 beskrive n친r en LLM produserer innhold som enten er helt meningsl칮st eller noe vi vet er faktuelt feil basert p친 andre informasjonskilder.

La oss for eksempel si at vi bygger en funksjon for v친r oppstart som lar studenter stille historiske sp칮rsm친l til en modell. En student stiller sp칮rsm친let `Who was the sole survivor of Titanic?`

Modellen produserer et svar som det nedenfor:

Dette er et veldig selvsikkert og grundig svar. Dessverre er det feil. Selv med en minimal mengde forskning, ville man oppdage at det var mer enn 칠n overlevende fra Titanic-katastrofen. For en student som nettopp begynner 친 forske p친 dette emnet, kan dette svaret v칝re overbevisende nok til 친 ikke bli stilt sp칮rsm친l ved og behandlet som en fakta. Konsekvensene av dette kan f칮re til at AI-systemet blir up친litelig og p친virker oppstartens omd칮mme negativt.

Med hver iterasjon av en gitt LLM, har vi sett ytelsesforbedringer rundt 친 minimere hallusinasjoner. Selv med denne forbedringen, m친 vi som applikasjonsbyggere og brukere fortsatt v칝re oppmerksomme p친 disse begrensningene.

### Skadelig innhold

Vi dekket i den tidligere delen n친r en LLM produserer feil eller meningsl칮se svar. En annen risiko vi m친 v칝re klar over er n친r en modell svarer med skadelig innhold.

Skadelig innhold kan defineres som:

- 칀 gi instruksjoner eller oppmuntre til selvskading eller skade p친 visse grupper.
- Hatefulle eller nedsettende innhold.
- Veiledning i planlegging av angrep eller voldelige handlinger.
- 칀 gi instruksjoner om hvordan man finner ulovlig innhold eller beg친r ulovlige handlinger.
- Vise seksuelt eksplisitt innhold.

For v친r oppstart, 칮nsker vi 친 sikre at vi har de riktige verkt칮yene og strategiene p친 plass for 친 forhindre at denne typen innhold blir sett av studenter.

### Mangel p친 rettferdighet

Rettferdighet er definert som "친 sikre at et AI-system er fritt for skjevhet og diskriminering og at de behandler alle rettferdig og likt." I verden av generativ AI 칮nsker vi 친 sikre at ekskluderende verdenssyn av marginaliserte grupper ikke blir forsterket av modellens output.

Disse typer output er ikke bare destruktive for 친 bygge positive produktopplevelser for brukerne v친re, men de for친rsaker ogs친 ytterligere samfunnsskader. Som applikasjonsbyggere b칮r vi alltid ha et bredt og mangfoldig brukergrunnlag i tankene n친r vi bygger l칮sninger med generativ AI.

## Hvordan bruke generativ AI ansvarlig

N친 som vi har identifisert viktigheten av ansvarlig generativ AI, la oss se p친 4 steg vi kan ta for 친 bygge v친re AI-l칮sninger ansvarlig:

### M친le potensielle skader

I programvaretesting tester vi de forventede handlingene til en bruker p친 en applikasjon. Tilsvarende er det 친 teste et mangfoldig sett med foresp칮rsler brukere mest sannsynlig vil bruke en god m친te 친 m친le potensielle skader p친.

Siden v친r oppstart bygger et utdanningsprodukt, ville det v칝re bra 친 forberede en liste over utdanningsrelaterte foresp칮rsler. Dette kan v칝re for 친 dekke et bestemt emne, historiske fakta, og foresp칮rsler om studentlivet.

### Begrense potensielle skader

Det er n친 p친 tide 친 finne m친ter hvor vi kan forhindre eller begrense den potensielle skaden for친rsaket av modellen og dens svar. Vi kan se p친 dette i 4 forskjellige lag:

- **Modell**. Velge riktig modell for riktig brukstilfelle. St칮rre og mer komplekse modeller som GPT-4 kan for친rsake mer risiko for skadelig innhold n친r de brukes p친 mindre og mer spesifikke brukstilfeller. 칀 bruke treningsdataene dine til finjustering reduserer ogs친 risikoen for skadelig innhold.

- **Sikkerhetssystem**. Et sikkerhetssystem er et sett med verkt칮y og konfigurasjoner p친 plattformen som serverer modellen som hjelper til med 친 begrense skade. Et eksempel p친 dette er innholdsfiltreringssystemet p친 Azure OpenAI-tjenesten. Systemer b칮r ogs친 oppdage jailbreak-angrep og u칮nsket aktivitet som foresp칮rsler fra roboter.

- **Metaprompt**. Metaprompts og grunnlag er m친ter vi kan dirigere eller begrense modellen basert p친 visse atferder og informasjon. Dette kan v칝re 친 bruke systeminnganger til 친 definere visse grenser for modellen. I tillegg kan det v칝re 친 gi output som er mer relevant for omfanget eller domenet til systemet.

Det kan ogs친 v칝re 친 bruke teknikker som Retrieval Augmented Generation (RAG) for 친 f친 modellen til 친 kun hente informasjon fra et utvalg av p친litelige kilder. Det er en leksjon senere i dette kurset for [친 bygge s칮keapplikasjoner](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Brukeropplevelse**. Det siste laget er der brukeren samhandler direkte med modellen gjennom v친r applikasjonsgrensesnitt p친 en eller annen m친te. P친 denne m친ten kan vi designe UI/UX for 친 begrense brukeren p친 typene input de kan sende til modellen, samt tekst eller bilder vist til brukeren. N친r vi distribuerer AI-applikasjonen, m친 vi ogs친 v칝re transparente om hva v친r generative AI-applikasjon kan og ikke kan gj칮re.

Vi har en hel leksjon dedikert til [친 designe UX for AI-applikasjoner](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Evaluere modell**. 칀 jobbe med LLM-er kan v칝re utfordrende fordi vi ikke alltid har kontroll over dataene modellen ble trent p친. Uansett b칮r vi alltid evaluere modellens ytelse og output. Det er fortsatt viktig 친 m친le modellens n칮yaktighet, likhet, jordn칝rhet og relevans av output. Dette bidrar til 친 gi 친penhet og tillit til interessenter og brukere.

### Drifte en ansvarlig generativ AI-l칮sning

칀 bygge en operasjonell praksis rundt dine AI-applikasjoner er det siste stadiet. Dette inkluderer 친 samarbeide med andre deler av v친r oppstart som juridisk og sikkerhet for 친 sikre at vi er i samsvar med alle regulatoriske retningslinjer. F칮r lansering 칮nsker vi ogs친 친 bygge planer rundt levering, h친ndtering av hendelser og tilbakef칮ring for 친 forhindre enhver skade for v친re brukere fra 친 vokse.

## Verkt칮y

Selv om arbeidet med 친 utvikle ansvarlige AI-l칮sninger kan virke som mye, er det arbeid vel verdt innsatsen. Etter hvert som omr친det generativ AI vokser, vil flere verkt칮y for 친 hjelpe utviklere med 친 effektivt integrere ansvarlighet i sine arbeidsflyter modnes. For eksempel kan [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) hjelpe med 친 oppdage skadelig innhold og bilder via en API-foresp칮rsel.

## Kunnskapssjekk

Hva er noen ting du m친 bry deg om for 친 sikre ansvarlig AI-bruk?

1. At svaret er riktig.
1. Skadelig bruk, at AI ikke brukes til kriminelle form친l.
1. Sikre at AI er fri for skjevhet og diskriminering.

A: 2 og 3 er riktige. Ansvarlig AI hjelper deg med 친 vurdere hvordan du kan begrense skadelige effekter og skjevheter og mer.

## 游 Utfordring

Les om [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) og se hva du kan ta i bruk for din bruk.

## Flott arbeid, fortsett l칝ringen din

Etter 친 ha fullf칮rt denne leksjonen, sjekk ut v친r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for 친 fortsette 친 heve din kunnskap om generativ AI!

G친 videre til leksjon 4 hvor vi vil se p친 [Grunnleggende om prompt engineering](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

**Ansvarsfraskrivelse**:  
Dette dokumentet har blitt oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber n칮yaktighet, v칝r oppmerksom p친 at automatiserte oversettelser kan inneholde feil eller un칮yaktigheter. Det originale dokumentet p친 sitt opprinnelige spr친k b칮r betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller feiltolkninger som oppst친r ved bruk av denne oversettelsen.