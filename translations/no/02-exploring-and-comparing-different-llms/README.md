<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6b7629b8ee4d7d874a27213e903d86a7",
  "translation_date": "2025-10-17T19:21:11+00:00",
  "source_file": "02-exploring-and-comparing-different-llms/README.md",
  "language_code": "no"
}
-->
# Utforsking og sammenligning av ulike LLM-er

[![Utforsking og sammenligning av ulike LLM-er](../../../translated_images/02-lesson-banner.ef94c84979f97f60f07e27d905e708cbcbdf78707120553ccab27d91c947805b.no.png)](https://youtu.be/KIRUeDKscfI?si=8BHX1zvwzQBn-PlK)

> _Klikk p√• bildet over for √• se videoen til denne leksjonen_

I forrige leksjon s√• vi hvordan generativ AI endrer teknologilandskapet, hvordan store spr√•kmodeller (LLM-er) fungerer, og hvordan en bedrift ‚Äì som v√•r oppstart ‚Äì kan bruke dem til sine form√•l og vokse! I dette kapittelet skal vi sammenligne og kontrastere ulike typer store spr√•kmodeller (LLM-er) for √• forst√• deres fordeler og ulemper.

Neste steg i v√•r oppstartsreise er √• utforske det n√•v√¶rende landskapet av LLM-er og forst√• hvilke som passer for v√•rt brukstilfelle.

## Introduksjon

Denne leksjonen vil dekke:

- Ulike typer LLM-er i dagens landskap.
- Testing, iterering og sammenligning av ulike modeller for ditt brukstilfelle i Azure.
- Hvordan distribuere en LLM.

## L√¶ringsm√•l

Etter √• ha fullf√∏rt denne leksjonen, vil du kunne:

- Velge riktig modell for ditt brukstilfelle.
- Forst√• hvordan du tester, itererer og forbedrer ytelsen til modellen din.
- Vite hvordan bedrifter distribuerer modeller.

## Forst√• ulike typer LLM-er

LLM-er kan kategoriseres p√• flere m√•ter basert p√• deres arkitektur, treningsdata og brukstilfelle. √Ö forst√• disse forskjellene vil hjelpe v√•r oppstart med √• velge riktig modell for scenarioet, samt forst√• hvordan man tester, itererer og forbedrer ytelsen.

Det finnes mange forskjellige typer LLM-modeller, og valget av modell avhenger av hva du √∏nsker √• bruke dem til, dataene dine, hvor mye du er villig til √• betale, og mer.

Avhengig av om du √∏nsker √• bruke modellene til tekst, lyd, video, bildegenerering og s√• videre, kan du velge en annen type modell.

- **Lyd- og talegjenkjenning**. For dette form√•let er Whisper-typen modeller et godt valg, da de er allsidige og rettet mot talegjenkjenning. De er trent p√• variert lyd og kan utf√∏re flerspr√•klig talegjenkjenning. L√¶r mer om [Whisper-typen modeller her](https://platform.openai.com/docs/models/whisper?WT.mc_id=academic-105485-koreyst).

- **Bildegenerering**. For bildegenerering er DALL-E og Midjourney to sv√¶rt kjente valg. DALL-E tilbys av Azure OpenAI. [Les mer om DALL-E her](https://platform.openai.com/docs/models/dall-e?WT.mc_id=academic-105485-koreyst) og ogs√• i kapittel 9 av dette pensumet.

- **Tekstgenerering**. De fleste modeller er trent p√• tekstgenerering, og du har et stort utvalg av alternativer fra GPT-3.5 til GPT-4. De har ulike kostnader, der GPT-4 er den dyreste. Det er verdt √• se p√• [Azure OpenAI-lekeplassen](https://oai.azure.com/portal/playground?WT.mc_id=academic-105485-koreyst) for √• evaluere hvilke modeller som best passer dine behov n√•r det gjelder kapasitet og kostnad.

- **Multimodalitet**. Hvis du √∏nsker √• h√•ndtere flere typer data i input og output, kan du vurdere modeller som [gpt-4 turbo med visjon eller gpt-4o](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models?WT.mc_id=academic-105485-koreyst) ‚Äì de nyeste utgivelsene av OpenAI-modeller ‚Äì som er i stand til √• kombinere naturlig spr√•kbehandling med visuell forst√•else, og muliggj√∏r interaksjoner gjennom multimodale grensesnitt.

√Ö velge en modell gir deg noen grunnleggende funksjoner, men det er kanskje ikke nok. Ofte har du bedriftsspesifikke data som du p√• en eller annen m√•te m√• informere LLM-en om. Det finnes flere ulike tiln√¶rminger til dette, mer om det i de kommende seksjonene.

### Grunnmodeller versus LLM-er

Begrepet Grunnmodell ble [skapt av forskere ved Stanford](https://arxiv.org/abs/2108.07258?WT.mc_id=academic-105485-koreyst) og definert som en AI-modell som oppfyller visse kriterier, som:

- **De er trent ved bruk av usupervisert l√¶ring eller selv-supervisert l√¶ring**, noe som betyr at de er trent p√• umerkede multimodale data og ikke krever menneskelig annotering eller merking av data for treningsprosessen.
- **De er sv√¶rt store modeller**, basert p√• sv√¶rt dype nevrale nettverk trent p√• milliarder av parametere.
- **De er vanligvis ment √• tjene som et ‚Äògrunnlag‚Äô for andre modeller**, noe som betyr at de kan brukes som et utgangspunkt for √• bygge andre modeller, som kan gj√∏res ved finjustering.

![Grunnmodeller versus LLM-er](../../../translated_images/FoundationModel.e4859dbb7a825c94b284f17eae1c186aabc21d4d8644331f5b007d809cf8d0f2.no.png)

Bildekilde: [Essential Guide to Foundation Models and Large Language Models | by Babar M Bhatti | Medium
](https://thebabar.medium.com/essential-guide-to-foundation-models-and-large-language-models-27dab58f7404)

For √• tydeliggj√∏re dette skillet, la oss ta ChatGPT som et eksempel. For √• bygge den f√∏rste versjonen av ChatGPT, fungerte en modell kalt GPT-3.5 som grunnmodellen. Dette betyr at OpenAI brukte noen chat-spesifikke data for √• lage en finjustert versjon av GPT-3.5 som var spesialisert p√• √• prestere godt i samtalescenarier, som chatboter.

![Grunnmodell](../../../translated_images/Multimodal.2c389c6439e0fc51b0b7b226d95d7d900d372ae66902d71b8ce5ec4951b8efbe.no.png)

Bildekilde: [2108.07258.pdf (arxiv.org)](https://arxiv.org/pdf/2108.07258.pdf?WT.mc_id=academic-105485-koreyst)

### √Öpen kildekode versus propriet√¶re modeller

En annen m√•te √• kategorisere LLM-er p√• er om de er √•pen kildekode eller propriet√¶re.

Modeller med √•pen kildekode er modeller som gj√∏res tilgjengelige for offentligheten og kan brukes av hvem som helst. De gj√∏res ofte tilgjengelige av selskapet som opprettet dem, eller av forskningsmilj√∏et. Disse modellene kan inspiseres, modifiseres og tilpasses for ulike brukstilfeller i LLM-er. De er imidlertid ikke alltid optimalisert for produksjonsbruk og kan v√¶re mindre ytelsesdyktige enn propriet√¶re modeller. I tillegg kan finansiering for modeller med √•pen kildekode v√¶re begrenset, og de kan ikke vedlikeholdes p√• lang sikt eller oppdateres med den nyeste forskningen. Eksempler p√• popul√¶re modeller med √•pen kildekode inkluderer [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html?WT.mc_id=academic-105485-koreyst), [Bloom](https://huggingface.co/bigscience/bloom) og [LLaMA](https://llama.meta.com).

Propriet√¶re modeller er modeller som eies av et selskap og ikke gj√∏res tilgjengelige for offentligheten. Disse modellene er ofte optimalisert for produksjonsbruk. De kan imidlertid ikke inspiseres, modifiseres eller tilpasses for ulike brukstilfeller. I tillegg er de ikke alltid gratis tilgjengelige og kan kreve abonnement eller betaling for bruk. Brukere har heller ikke kontroll over dataene som brukes til √• trene modellen, noe som betyr at de m√• stole p√• at modelleieren sikrer forpliktelse til databeskyttelse og ansvarlig bruk av AI. Eksempler p√• popul√¶re propriet√¶re modeller inkluderer [OpenAI-modeller](https://platform.openai.com/docs/models/overview?WT.mc_id=academic-105485-koreyst), [Google Bard](https://sapling.ai/llm/bard?WT.mc_id=academic-105485-koreyst) eller [Claude 2](https://www.anthropic.com/index/claude-2?WT.mc_id=academic-105485-koreyst).

### Innebygging versus bildegenerering versus tekst- og kodegenerering

LLM-er kan ogs√• kategoriseres etter outputen de genererer.

Innebygginger er et sett med modeller som kan konvertere tekst til en numerisk form, kalt innebygging, som er en numerisk representasjon av input-teksten. Innebygginger gj√∏r det enklere for maskiner √• forst√• forholdet mellom ord eller setninger og kan brukes som input av andre modeller, som klassifiseringsmodeller eller klyngemodeller som har bedre ytelse p√• numeriske data. Innebyggingsmodeller brukes ofte til overf√∏ringsl√¶ring, der en modell bygges for en surrogatoppgave som det finnes rikelig med data for, og deretter gjenbrukes modellvektene (innebyggingene) for andre oppgaver. Et eksempel p√• denne kategorien er [OpenAI innebygginger](https://platform.openai.com/docs/models/embeddings?WT.mc_id=academic-105485-koreyst).

![Innebygging](../../../translated_images/Embedding.c3708fe988ccf76073d348483dbb7569f622211104f073e22e43106075c04800.no.png)

Bildegenereringsmodeller er modeller som genererer bilder. Disse modellene brukes ofte til bildebehandling, bildesyntese og bildetransformasjon. Bildegenereringsmodeller trenes ofte p√• store datasett av bilder, som [LAION-5B](https://laion.ai/blog/laion-5b/?WT.mc_id=academic-105485-koreyst), og kan brukes til √• generere nye bilder eller redigere eksisterende bilder med teknikker som inpainting, superoppl√∏sning og fargelegging. Eksempler inkluderer [DALL-E-3](https://openai.com/dall-e-3?WT.mc_id=academic-105485-koreyst) og [Stable Diffusion-modeller](https://github.com/Stability-AI/StableDiffusion?WT.mc_id=academic-105485-koreyst).

![Bildegenerering](../../../translated_images/Image.349c080266a763fd255b840a921cd8fc526ed78dc58708fa569ff1873d302345.no.png)

Tekst- og kodegenereringsmodeller er modeller som genererer tekst eller kode. Disse modellene brukes ofte til tekstsammendrag, oversettelse og sp√∏rsm√•l-svar. Tekstgenereringsmodeller trenes ofte p√• store datasett av tekst, som [BookCorpus](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html?WT.mc_id=academic-105485-koreyst), og kan brukes til √• generere ny tekst eller svare p√• sp√∏rsm√•l. Kodegenereringsmodeller, som [CodeParrot](https://huggingface.co/codeparrot?WT.mc_id=academic-105485-koreyst), trenes ofte p√• store datasett av kode, som GitHub, og kan brukes til √• generere ny kode eller fikse feil i eksisterende kode.

![Tekst- og kodegenerering](../../../translated_images/Text.a8c0cf139e5cc2a0cd3edaba8d675103774e6ddcb3c9fc5a98bb17c9a450e31d.no.png)

### Encoder-Decoder versus kun Decoder

For √• snakke om de ulike typene arkitekturer for LLM-er, la oss bruke en analogi.

Tenk deg at sjefen din ga deg en oppgave med √• lage en quiz for studentene. Du har to kolleger; √©n som har ansvar for √• lage innholdet og en annen som har ansvar for √• gjennomg√• det.

Innholdsskaperen er som en modell som kun er en Decoder, de kan se p√• emnet og det du allerede har skrevet, og deretter skrive et kurs basert p√• det. De er veldig gode til √• skrive engasjerende og informativt innhold, men de er ikke veldig gode til √• forst√• emnet og l√¶ringsm√•lene. Noen eksempler p√• Decoder-modeller er GPT-familien, som GPT-3.

Gjennomleseren er som en modell som kun er en Encoder, de ser p√• kurset som er skrevet og svarene, legger merke til forholdet mellom dem og forst√•r konteksten, men de er ikke gode til √• generere innhold. Et eksempel p√• en Encoder-modell ville v√¶re BERT.

Tenk deg at vi ogs√• kan ha noen som b√•de kan lage og gjennomg√• quizen, dette er en Encoder-Decoder-modell. Noen eksempler ville v√¶re BART og T5.

### Tjeneste versus Modell

La oss n√• snakke om forskjellen mellom en tjeneste og en modell. En tjeneste er et produkt som tilbys av en skyleverand√∏r, og er ofte en kombinasjon av modeller, data og andre komponenter. En modell er kjernen i en tjeneste, og er ofte en grunnmodell, som en LLM.

Tjenester er ofte optimalisert for produksjonsbruk og er ofte enklere √• bruke enn modeller, via et grafisk brukergrensesnitt. Tjenester er imidlertid ikke alltid gratis tilgjengelige og kan kreve abonnement eller betaling for bruk, i bytte mot √• utnytte tjenesteeierens utstyr og ressurser, optimalisere utgifter og skalere enkelt. Et eksempel p√• en tjeneste er [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/overview?WT.mc_id=academic-105485-koreyst), som tilbyr en betalingsplan basert p√• bruk, noe som betyr at brukere belastes proporsjonalt med hvor mye de bruker tjenesten. I tillegg tilbyr Azure OpenAI Service sikkerhet p√• bedriftsniv√• og et ansvarlig AI-rammeverk p√• toppen av modellens funksjoner.

Modeller er bare det nevrale nettverket, med parametere, vekter og andre komponenter. Dette gj√∏r det mulig for selskaper √• kj√∏re dem lokalt, men de m√• kj√∏pe utstyr, bygge en struktur for skalering og kj√∏pe en lisens eller bruke en modell med √•pen kildekode. En modell som LLaMA er tilgjengelig for bruk, men krever datakraft for √• kj√∏re modellen.

## Hvordan teste og iterere med ulike modeller for √• forst√• ytelse i Azure

N√•r teamet v√•rt har utforsket det n√•v√¶rende LLM-landskapet og identifisert noen gode kandidater for sine scenarier, er neste steg √• teste dem p√• deres data og arbeidsbelastning. Dette er en iterativ prosess, utf√∏rt gjennom eksperimenter og m√•linger.
De fleste av modellene vi nevnte i tidligere avsnitt (OpenAI-modeller, √•pen kildekode-modeller som Llama2 og Hugging Face-transformers) er tilgjengelige i [Model Catalog](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview?WT.mc_id=academic-105485-koreyst) i [Azure AI Studio](https://ai.azure.com/?WT.mc_id=academic-105485-koreyst).

[Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/what-is-ai-studio?WT.mc_id=academic-105485-koreyst) er en skyplattform designet for utviklere som √∏nsker √• bygge generative AI-applikasjoner og administrere hele utviklingssyklusen ‚Äì fra eksperimentering til evaluering ‚Äì ved √• kombinere alle Azure AI-tjenester i ett enkelt knutepunkt med et brukervennlig grensesnitt. Modellkatalogen i Azure AI Studio gj√∏r det mulig for brukeren √•:

- Finne Foundation-modellen av interesse i katalogen ‚Äì enten propriet√¶r eller √•pen kildekode ‚Äì ved √• filtrere etter oppgave, lisens eller navn. For √• forbedre s√∏kbarheten er modellene organisert i samlinger, som Azure OpenAI-samlingen, Hugging Face-samlingen og mer.

![Model catalog](../../../translated_images/AzureAIStudioModelCatalog.3cf8a499aa8ba0314f2c73d4048b3225d324165f547525f5b7cfa5f6c9c68941.no.png)

- G√• gjennom modellkortet, som inkluderer en detaljert beskrivelse av tiltenkt bruk og treningsdata, kodeeksempler og evalueringsresultater fra det interne evalueringsbiblioteket.

![Model card](../../../translated_images/ModelCard.598051692c6e400d681a713ba7717e8b6e5e65f08d12131556fcec0f1789459b.no.png)

- Sammenligne benchmarks p√• tvers av modeller og datasett tilgjengelig i bransjen for √• vurdere hvilken som passer best til forretningsscenariet, via [Model Benchmarks](https://learn.microsoft.com/azure/ai-studio/how-to/model-benchmarks?WT.mc_id=academic-105485-koreyst)-panelet.

![Model benchmarks](../../../translated_images/ModelBenchmarks.254cb20fbd06c03a4ca53994585c5ea4300a88bcec8eff0450f2866ee2ac5ff3.no.png)

- Finjustere modellen p√• egendefinerte treningsdata for √• forbedre modellens ytelse i en spesifikk arbeidsbelastning, ved √• utnytte eksperimenterings- og sporingsfunksjonene i Azure AI Studio.

![Model fine-tuning](../../../translated_images/FineTuning.aac48f07142e36fddc6571b1f43ea2e003325c9c6d8e3fc9d8834b771e308dbf.no.png)

- Distribuere den originale forh√•ndstrente modellen eller den finjusterte versjonen til en ekstern sanntidsinference ‚Äì administrert databehandling ‚Äì eller serverl√∏s API-endepunkt ‚Äì [pay-as-you-go](https://learn.microsoft.com/azure/ai-studio/how-to/model-catalog-overview#model-deployment-managed-compute-and-serverless-api-pay-as-you-go?WT.mc_id=academic-105485-koreyst) ‚Äì for √• gj√∏re det mulig for applikasjoner √• bruke den.

![Model deployment](../../../translated_images/ModelDeploy.890da48cbd0bccdb4abfc9257f3d884831e5d41b723e7d1ceeac9d60c3c4f984.no.png)

> [!NOTE]
> Ikke alle modeller i katalogen er for √∏yeblikket tilgjengelige for finjustering og/eller pay-as-you-go-distribusjon. Sjekk modellkortet for detaljer om modellens funksjoner og begrensninger.

## Forbedring av LLM-resultater

Vi har utforsket med v√•rt oppstartsteam ulike typer LLM-er og en skyplattform (Azure Machine Learning) som gj√∏r det mulig for oss √• sammenligne forskjellige modeller, evaluere dem p√• testdata, forbedre ytelsen og distribuere dem p√• inference-endepunkter.

Men n√•r b√∏r de vurdere √• finjustere en modell i stedet for √• bruke en forh√•ndstrent? Finnes det andre tiln√¶rminger for √• forbedre modellens ytelse p√• spesifikke arbeidsbelastninger?

Det finnes flere tiln√¶rminger en bedrift kan bruke for √• oppn√• de resultatene de trenger fra en LLM. Du kan velge forskjellige typer modeller med ulike grader av trening n√•r du distribuerer en LLM i produksjon, med forskjellige niv√•er av kompleksitet, kostnad og kvalitet. Her er noen forskjellige tiln√¶rminger:

- **Prompt engineering med kontekst**. Ideen er √• gi nok kontekst n√•r du gir en prompt for √• sikre at du f√•r de svarene du trenger.

- **Retrieval Augmented Generation, RAG**. Dataene dine kan for eksempel finnes i en database eller et web-endepunkt. For √• sikre at disse dataene, eller en del av dem, inkluderes ved prompting, kan du hente relevante data og gj√∏re dem til en del av brukerens prompt.

- **Finjustert modell**. Her trener du modellen videre p√• dine egne data, noe som gj√∏r modellen mer presis og responsiv til dine behov, men det kan v√¶re kostbart.

![LLMs deployment](../../../translated_images/Deploy.18b2d27412ec8c02871386cbe91097c7f2190a8c6e2be88f66392b411609a48c.no.png)

Bildekilde: [Four Ways that Enterprises Deploy LLMs | Fiddler AI Blog](https://www.fiddler.ai/blog/four-ways-that-enterprises-deploy-llms?WT.mc_id=academic-105485-koreyst)

### Prompt Engineering med Kontekst

Forh√•ndstrente LLM-er fungerer veldig godt p√• generelle oppgaver innen naturlig spr√•k, selv ved √• kalle dem med en kort prompt, som en setning som skal fullf√∏res eller et sp√∏rsm√•l ‚Äì det s√•kalte "zero-shot"-l√¶ring.

Jo mer brukeren kan ramme inn foresp√∏rselen sin med en detaljert foresp√∏rsel og eksempler ‚Äì konteksten ‚Äì desto mer n√∏yaktig og n√¶rmere brukerens forventninger vil svaret v√¶re. I dette tilfellet snakker vi om "one-shot"-l√¶ring hvis prompten inneholder bare ett eksempel, og "few-shot"-l√¶ring hvis den inneholder flere eksempler. Prompt engineering med kontekst er den mest kostnadseffektive tiln√¶rmingen √• starte med.

### Retrieval Augmented Generation (RAG)

LLM-er har begrensningen at de kun kan bruke dataene som ble brukt under treningen for √• generere et svar. Dette betyr at de ikke vet noe om hendelser som skjedde etter treningsprosessen, og de kan ikke f√• tilgang til ikke-offentlig informasjon (som bedriftsdata).
Dette kan overvinnes gjennom RAG, en teknikk som utvider prompten med eksterne data i form av dokumentbiter, med tanke p√• begrensninger i promptlengde. Dette st√∏ttes av verkt√∏y for vektordatabaser (som [Azure Vector Search](https://learn.microsoft.com/azure/search/vector-search-overview?WT.mc_id=academic-105485-koreyst)) som henter nyttige biter fra ulike forh√•ndsdefinerte datakilder og legger dem til promptens kontekst.

Denne teknikken er sv√¶rt nyttig n√•r en bedrift ikke har nok data, nok tid eller ressurser til √• finjustere en LLM, men likevel √∏nsker √• forbedre ytelsen p√• en spesifikk arbeidsbelastning og redusere risikoen for fabrikasjoner, det vil si forvrengning av virkeligheten eller skadelig innhold.

### Finjustert modell

Finjustering er en prosess som utnytter overf√∏ringsl√¶ring for √• "tilpasse" modellen til en nedstr√∏msoppgave eller for √• l√∏se et spesifikt problem. I motsetning til few-shot-l√¶ring og RAG resulterer det i en ny modell som genereres, med oppdaterte vekter og skjevheter. Det krever et sett med treningseksempler som best√•r av en enkelt input (prompten) og dens tilh√∏rende output (fullf√∏ringen).
Dette vil v√¶re den foretrukne tiln√¶rmingen hvis:

- **Bruk av finjusterte modeller**. En bedrift √∏nsker √• bruke finjusterte mindre kapable modeller (som embedding-modeller) i stedet for h√∏yytelsesmodeller, noe som resulterer i en mer kostnadseffektiv og rask l√∏sning.

- **Vurdering av latens**. Latens er viktig for en spesifikk brukstilfelle, s√• det er ikke mulig √• bruke veldig lange promter eller antallet eksempler som modellen skal l√¶re av, passer ikke med promptens lengdebegrensning.

- **Holde seg oppdatert**. En bedrift har mye data av h√∏y kvalitet og sannhetsdata, samt ressursene som kreves for √• holde disse dataene oppdatert over tid.

### Trenet modell

√Ö trene en LLM fra bunnen av er uten tvil den mest krevende og komplekse tiln√¶rmingen √• ta i bruk, og krever enorme mengder data, dyktige ressurser og passende datakraft. Dette alternativet b√∏r kun vurderes i et scenario der en bedrift har en domenespesifikk brukstilfelle og en stor mengde domenesentriske data.

## Kunnskapssjekk

Hva kan v√¶re en god tiln√¶rming for √• forbedre LLM-fullf√∏ringsresultater?

1. Prompt engineering med kontekst
1. RAG
1. Finjustert modell

A:3, hvis du har tid og ressurser og data av h√∏y kvalitet, er finjustering det bedre alternativet for √• holde seg oppdatert. Men hvis du ser p√• √• forbedre ting og mangler tid, er det verdt √• vurdere RAG f√∏rst.

## üöÄ Utfordring

Les mer om hvordan du kan [bruke RAG](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview?WT.mc_id=academic-105485-koreyst) for din bedrift.

## Flott arbeid, fortsett l√¶ringen din

Etter √• ha fullf√∏rt denne leksjonen, sjekk ut v√•r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for √• fortsette √• √∏ke kunnskapen din om Generative AI!

G√• videre til Leksjon 3 hvor vi skal se p√• hvordan man [bygger med Generative AI p√• en ansvarlig m√•te](../03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber n√∏yaktighet, v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.