# Retrieval Augmented Generation (RAG) og vektordatabaser

[![Retrieval Augmented Generation (RAG) og vektordatabaser](../../../translated_images/no/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

I leksjonen om s√∏keapplikasjoner l√¶rte vi kort hvordan du integrerer dine egne data i store spr√•kmodeller (LLM-er). I denne leksjonen vil vi dykke dypere inn i konseptene for √• forankre dine data i LLM-applikasjonen, mekanismene i prosessen og metodene for lagring av data, inkludert b√•de innebygde representasjoner og tekst.

> **Video kommer snart**

## Introduksjon

I denne leksjonen vil vi dekke f√∏lgende:

- En introduksjon til RAG, hva det er og hvorfor det brukes i KI (kunstig intelligens).

- Forst√• hva vektordatabaser er og opprette en slik for v√•r applikasjon.

- Et praktisk eksempel p√• hvordan integrere RAG i en applikasjon.

## L√¶ringsm√•l

Etter √• ha fullf√∏rt denne leksjonen, vil du kunne:

- Forklare betydningen av RAG i datahenting og behandling.

- Sette opp en RAG-applikasjon og forankre dine data til en LLM

- Effektiv integrering av RAG og vektordatabaser i LLM-applikasjoner.

## V√•rt scenario: forbedre v√•re LLM-er med v√•re egne data

For denne leksjonen √∏nsker vi √• legge til egne notater i utdanningsstartupen, som gj√∏r at chatbotten f√•r mer informasjon om de ulike fagene. Ved √• bruke notatene vi har, vil l√¶rere kunne studere bedre og forst√• de forskjellige temaene, noe som gj√∏r det lettere √• repetere til eksamener. For √• skape v√•rt scenario vil vi bruke:

- `Azure OpenAI:` LLM-en vi bruker for √• lage chatbotten v√•r

- `AI for beginners' lesson on Neural Networks`: dette vil v√¶re dataene vi forankrer v√•r LLM p√•

- `Azure AI Search` og `Azure Cosmos DB:` vektordatabaser for √• lagre data og lage en s√∏keindeks

Brukere vil kunne lage √∏vingsquizzer fra notatene sine, repetisjonsflashkort og oppsummere til konsise oversikter. For √• starte, la oss se p√• hva RAG er og hvordan det fungerer:

## Retrieval Augmented Generation (RAG)

En LLM-drevet chatbot behandler brukerforesp√∏rsler for √• generere svar. Den er designet for √• v√¶re interaktiv og engasjerer brukere i et bredt spekter av emner. Likevel er svarene begrenset til konteksten som gis og det grunnleggende treningsmaterialet. For eksempel har GPT-4 kunnskapsavskj√¶ring i september 2021, noe som betyr at den mangler kjennskap til hendelser som har skjedd etter denne perioden. I tillegg ekskluderer dataene som brukes til √• trene LLM-er konfidensiell informasjon som personlige notater eller en bedrifts produktmanual.

### Hvordan RAGs (Retrieval Augmented Generation) fungerer

![drawing showing how RAGs work](../../../translated_images/no/how-rag-works.f5d0ff63942bd3a6.webp)

Anta at du vil lansere en chatbot som lager quizzer fra notatene dine, da trenger du en tilkobling til kunnskapsbasen. Her kommer RAG inn som en l√∏sning. RAGs fungerer p√• f√∏lgende m√•te:

- **Kunnskapsbase:** F√∏r uthenting m√• dokumentene inntas og forh√•ndsbehandles, vanligvis ved √• dele store dokumenter inn i mindre biter, transformere dem til tekst-embedding og lagre dem i en database.

- **Brukersp√∏rsm√•l:** brukeren stiller et sp√∏rsm√•l

- **Uthenting:** N√•r en bruker stiller et sp√∏rsm√•l, henter embedding-modellen relevant informasjon fra kunnskapsbasen v√•r for √• gi mer kontekst som blir inkorporert i prompten.

- **Augmented Generation:** LLM-en forbedrer svaret sitt basert p√• de hentede dataene. Det gj√∏r at svaret som genereres ikke bare baseres p√• forh√•ndstrent data, men ogs√• p√• relevant informasjon fra den tilf√∏rte konteksten. De innhentede dataene brukes til √• forbedre LLM-ens svar. LLM-en returnerer s√• et svar p√• brukerens sp√∏rsm√•l.

![drawing showing how RAGs architecture](../../../translated_images/no/encoder-decode.f2658c25d0eadee2.webp)

Arkitekturen for RAGs implementeres med transformere som best√•r av to deler: en encoder og en decoder. For eksempel, n√•r en bruker stiller et sp√∏rsm√•l, "kodes" inngangsteksten til vektorer som fanger betydningen av ordene, og vektorene "dekodes" til v√•r dokumentindeks og genererer ny tekst basert p√• brukerforesp√∏rselen. LLM-en bruker b√•de en encoder-decoder-modell for √• generere output.

To tiln√¶rminger n√•r man implementerer RAG if√∏lge det foresl√•tte papiret: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) er:

- **_RAG-Sequence_** bruker hentede dokumenter for √• forutsi det beste mulige svaret p√• en brukersp√∏rsm√•l

- **RAG-Token** bruker dokumenter for √• generere neste token, og henter deretter dem for √• svare p√• brukerens sp√∏rsm√•l

### Hvorfor bruke RAGs?¬†

- **Rike informasjonskilder:** sikrer at tekstsvar er oppdaterte og aktuelle. Det forbedrer dermed ytelsen p√• domenespesifikke oppgaver ved √• f√• tilgang til intern kunnskapsbase.

- Reduserer fabrikkering ved √• bruke **verifiserbare data** i kunnskapsbasen for √• gi kontekst til brukerforesp√∏rslene.

- Det er **kostnadseffektivt** siden de er rimeligere sammenlignet med finjustering av en LLM

## Lage en kunnskapsbase

Applikasjonen v√•r baseres p√• v√•re personlige data, alts√• leksjonen om nevrale nettverk i AI For Beginners-kurset.

### Vektordatabaser

En vektordatabasen, i motsetning til tradisjonelle databaser, er en spesialisert database designet for √• lagre, administrere og s√∏ke innebygde vektorer. Den lagrer numeriske representasjoner av dokumenter. √Ö bryte data ned til numeriske embedding gj√∏r det lettere for v√•rt KI-system √• forst√• og behandle dataene.

Vi lagrer embeddingene v√•re i vektordatabaser da LLM-er har en begrensning p√• antall tokens de kan akseptere som input. Siden du ikke kan sende hele embeddingene til en LLM, m√• vi dele dem opp i biter, og n√•r en bruker stiller et sp√∏rsm√•l, vil embeddingene som ligner mest p√• sp√∏rsm√•let bli returnert sammen med prompten. Oppdeling reduserer ogs√• kostnader per tokens som sendes gjennom en LLM.

Noen popul√¶re vektordatabaser inkluderer Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant og DeepLake. Du kan opprette en Azure Cosmos DB-modell ved bruk av Azure CLI med f√∏lgende kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fra tekst til embedding

F√∏r vi lagrer dataene v√•re, m√• vi konvertere dem til vektor-embedding f√∏r de lagres i databasen. Hvis du jobber med store dokumenter eller lange tekster, kan du dele dem opp basert p√• forventede sp√∏rsm√•l. Oppdeling kan gj√∏res p√• setningsniv√•, eller p√• avsnittsniv√•. Siden oppdeling henter mening fra ordene rundt, kan du legge til annen kontekst til en bit, for eksempel ved √• legge til dokumentets tittel eller inkludere noe tekst f√∏r eller etter biten. Du kan dele opp dataene som f√∏lger:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Hvis den siste biten ikke n√•dde minimum lengde, legg den til uansett
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N√•r dataene er delt, kan vi deretter bygge embedding av teksten ved √• bruke ulike embedding-modeller. Noen modeller du kan bruke inkluderer: word2vec, ada-002 fra OpenAI, Azure Computer Vision og mange flere. Valg av modell avhenger av spr√•kene du bruker, typen innhold som kodes (tekst/bilder/lyd), st√∏rrelsen p√• input den kan kode og lengden p√• embedding-output.

Et eksempel p√• embedding av tekst ved bruk av OpenAIs `text-embedding-ada-002`-modell er:
![an embedding of the word cat](../../../translated_images/no/cat.74cbd7946bc9ca38.webp)

## Henting og vektors√∏k

N√•r en bruker stiller et sp√∏rsm√•l, transformerer henteren det til en vektor ved bruk av sp√∏rsm√•lsenkoderen, den s√∏ker s√• gjennom dokumentindeksen v√•r etter relevante vektorer i dokumentet som er relatert til input. N√•r dette er gjort, konverterer den b√•de input-vektoren og dokumentvektorene til tekst og sender det til LLM-en.

### Henting

Henting skjer n√•r systemet pr√∏ver √• raskt finne dokumentene i indeksen som oppfyller s√∏kekriteriene. M√•let med henteren er √• finne dokumenter som skal brukes til √• gi kontekst og forankre LLM-en p√• dine data.

Det finnes flere m√•ter √• utf√∏re s√∏k i databasen v√•r p√•, som for eksempel:

- **N√∏kkelordss√∏k** - for teksts√∏k

- **Vektors√∏k** - konverterer dokumenter fra tekst til vektorreprensentasjoner ved bruk av embedding-modeller, som tillater et **semantisk s√∏k** basert p√• betydningen av ord. Henting gj√∏res ved √• s√∏ke etter dokumenter hvis vektorreprensentasjoner er n√¶rmest brukerens sp√∏rsm√•l.

- **Hybrid** - en kombinasjon av b√•de n√∏kkelordss√∏k og vektors√∏k.

En utfordring med henting er n√•r det ikke finnes noe lignende svar p√• sp√∏rsm√•let i databasen, systemet returnerer da den beste informasjonen det kan f√• tak i, men du kan bruke taktikker som √• sette maksimal avstand for relevans eller bruke hybrid-s√∏k som kombinerer b√•de n√∏kkelord og vektors√∏k. I denne leksjonen vil vi bruke hybrid s√∏k, en kombinasjon av b√•de vektor- og n√∏kkelordss√∏k. Vi vil lagre dataene i en dataframe med kolonner som inneholder b√•de bitene og embeddingene.

### Vektorlignendehet

Henteren vil s√∏ke i kunnskapsdatabasen etter embedding som er n√¶r hverandre, n√¶rmeste nabo, ettersom det er tekster som er like. I scenarioet hvor en bruker stiller et sp√∏rsm√•l, blir det f√∏rst embeddet og deretter matchet med lignende embedding. Den vanlige metoden som brukes for √• finne hvor like forskjellige vektorer er, er cosinuslikhet som er basert p√• vinkelen mellom to vektorer.

Vi kan m√•le likhet ved hjelp av andre alternativer som vi kan bruke, for eksempel euklidisk avstand som er den rette linjen mellom vektorenes endepunkter, og prikkprodukt som m√•ler summen av produktene av tilsvarende elementer i to vektorer.

### S√∏keindeks

N√•r vi gj√∏r henting, m√• vi bygge en s√∏keindeks for kunnskapsbasen v√•r f√∏r vi utf√∏rer s√∏k. En indeks lagrer embeddingene v√•re og kan raskt hente de mest lignende bitene selv i en stor database. Vi kan opprette indeksen lokalt ved √• bruke:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Opprett s√∏keindeksen
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# For √• s√∏ke i indeksen kan du bruke kneighbors-metoden
distances, indices = nbrs.kneighbors(embeddings)
```

### Omranging

N√•r du har hentet data fra databasen, m√• du kanskje sortere resultatene fra mest relevante. En omodererende LLM bruker maskinl√¶ring for √• forbedre relevansen av s√∏kresultater ved √• rangere dem fra mest relevante. Ved bruk av Azure AI Search gj√∏res omrangering automatisk for deg ved hjelp av en semantisk omrangering. Et eksempel p√• hvordan omrangering fungerer ved hjelp av n√¶rmeste naboer:

```python
# Finn de mest lignende dokumentene
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Skriv ut de mest lignende dokumentene
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## √Ö sette det hele sammen

Det siste steget er √• legge til v√•r LLM i miksen for √• kunne f√• svar som er forankret i v√•re data. Vi kan implementere det som f√∏lger:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Konverter sp√∏rsm√•let til en sp√∏rrevektor
    query_vector = create_embeddings(user_input)

    # Finn de mest lignende dokumentene
    distances, indices = nbrs.kneighbors([query_vector])

    # legg til dokumenter i sp√∏rsm√•let for √• gi kontekst
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # kombiner historikken og brukerens inndata
    history.append(user_input)

    # opprett et meldingsobjekt
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # bruk chatteferdighet for √• generere et svar
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluering av applikasjonen v√•r

### Evalueringsmetoder

- Kvaliteten p√• svarene som leveres, slik at det h√∏res naturlig, flytende og menneskelig ut

- Forankring av dataene: evaluere om svaret kom fra de leverte dokumentene

- Relevans: evaluere om svaret samsvarer med og er relatert til det stilte sp√∏rsm√•let

- Flyt ‚Äì hvorvidt svaret er grammatisk meningsfylt

## Brukstilfeller for RAG (Retrieval Augmented Generation) og vektordatabaser

Det finnes mange forskjellige brukstilfeller hvor funksjonskall kan forbedre appen din, for eksempel:

- Sp√∏rsm√•l og svar: forankre bedriftsdata til en chat som ansatte kan bruke for √• stille sp√∏rsm√•l.

- Anbefalingssystemer: der du kan lage et system som matcher de mest lignende verdiene f.eks. filmer, restauranter og mye mer.

- Chattetjenester: du kan lagre chatthistorikk og tilpasse samtalen basert p√• brukerdata.

- Bildes√∏k basert p√• vektor-embedding, nyttig ved bildeidentifikasjon og anomali-deteksjon.

## Oppsummering

Vi har dekket grunnleggende omr√•der i RAG fra √• legge til dataene v√•re i applikasjonen, brukerforesp√∏rselen og output. For √• forenkle opprettelse av RAG kan du bruke rammeverk som Semantic Kernel, Langchain eller Autogen.

## Oppgave

For √• fortsette l√¶ringen din av Retrieval Augmented Generation (RAG) kan du bygge:

- Bygg et frontend for applikasjonen med rammeverket du foretrekker

- Bruk et rammeverk, enten LangChain eller Semantic Kernel, og gjenskap applikasjonen din.

Gratulerer med √• ha fullf√∏rt leksjonen üëè.

## L√¶ring stopper ikke her, fortsett reisen

Etter √• ha fullf√∏rt denne leksjonen, sjekk ut v√•r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for √• fortsette √• heve din kunnskap om Generative AI!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfraskrivelse**:
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, vennligst v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k skal betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->