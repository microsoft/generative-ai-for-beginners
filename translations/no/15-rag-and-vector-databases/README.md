<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-05-20T01:33:41+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "no"
}
-->
# Henteforsterket generering (RAG) og vektordatabaser

I leksjonen om s칮keapplikasjoner l칝rte vi kort hvordan du kan integrere dine egne data i store spr친kmodeller (LLMs). I denne leksjonen vil vi g친 dypere inn i konseptene om 친 forankre dataene dine i LLM-applikasjonen din, mekanikken i prosessen og metodene for 친 lagre data, inkludert b친de innebygde elementer og tekst.

> **Video kommer snart**

## Introduksjon

I denne leksjonen vil vi dekke f칮lgende:

- En introduksjon til RAG, hva det er og hvorfor det brukes i kunstig intelligens (AI).

- Forst친 hva vektordatabaser er og lage en for v친r applikasjon.

- Et praktisk eksempel p친 hvordan du integrerer RAG i en applikasjon.

## L칝ringsm친l

Etter 친 ha fullf칮rt denne leksjonen, vil du kunne:

- Forklare betydningen av RAG i datainnhenting og behandling.

- Sette opp RAG-applikasjonen og forankre dataene dine til en LLM.

- Effektiv integrasjon av RAG og vektordatabaser i LLM-applikasjoner.

## V친rt scenario: forbedre v친re LLM-er med v친re egne data

For denne leksjonen 칮nsker vi 친 legge til v친re egne notater i oppstarten av utdanning, som lar chatboten f친 mer informasjon om de forskjellige emnene. Ved 친 bruke notatene vi har, vil l칝rende kunne studere bedre og forst친 de forskjellige temaene, noe som gj칮r det lettere 친 forberede seg til eksamenene sine. For 친 lage v친rt scenario, vil vi bruke:

- `Azure OpenAI:` LLM vi vil bruke til 친 lage v친r chatbot

- `AI for beginners' lesson on Neural Networks`: dette vil v칝re dataene vi forankrer v친r LLM p친

- `Azure AI Search` og `Azure Cosmos DB:` vektordatabase for 친 lagre v친re data og lage et s칮keindeks

Brukere vil kunne lage praksisquizer fra sine notater, revisjonskort og oppsummere dem til konsise oversikter. For 친 komme i gang, la oss se p친 hva RAG er og hvordan det fungerer:

## Henteforsterket generering (RAG)

En LLM-drevet chatbot behandler brukerens foresp칮rsler for 친 generere svar. Den er designet for 친 v칝re interaktiv og engasjerer seg med brukere om et bredt spekter av emner. Imidlertid er svarene begrenset til konteksten som gis og dens grunnleggende treningsdata. For eksempel er GPT-4s kunnskapsavgrensning september 2021, noe som betyr at den mangler kunnskap om hendelser som har skjedd etter denne perioden. I tillegg utelukker dataene som brukes til 친 trene LLM-er konfidensiell informasjon som personlige notater eller en bedrifts produktmanual.

### Hvordan RAGs (Henteforsterket generering) fungerer

Anta at du vil distribuere en chatbot som lager quizer fra notatene dine, du vil kreve en tilkobling til kunnskapsbasen. Dette er hvor RAG kommer til unnsetning. RAGs opererer som f칮lger:

- **Kunnskapsbase:** F칮r innhenting m친 disse dokumentene bli inntatt og forh친ndsbehandlet, vanligvis ved 친 bryte ned store dokumenter i mindre deler, transformere dem til tekstinnbygging og lagre dem i en database.

- **Brukerforesp칮rsel:** brukeren stiller et sp칮rsm친l

- **Innhenting:** N친r en bruker stiller et sp칮rsm친l, henter innebyggingsmodellen relevant informasjon fra v친r kunnskapsbase for 친 gi mer kontekst som vil bli innlemmet i foresp칮rselen.

- **Forsterket generering:** LLM forbedrer sitt svar basert p친 de innhentede dataene. Det lar det genererte svaret v칝re basert ikke bare p친 forh친ndstrente data, men ogs친 relevant informasjon fra den tilf칮rte konteksten. De innhentede dataene brukes til 친 forsterke LLMs svar. LLM returnerer deretter et svar p친 brukerens sp칮rsm친l.

Arkitekturen for RAGs implementeres ved bruk av transformatorer best친ende av to deler: en koder og en dekoder. For eksempel, n친r en bruker stiller et sp칮rsm친l, blir inputteksten 'kodet' inn i vektorer som fanger betydningen av ord, og vektorene blir 'dekodet' inn i v친rt dokumentindeks og genererer ny tekst basert p친 brukerens foresp칮rsel. LLM bruker b친de en koder-dekoder-modell for 친 generere output.

To tiln칝rminger ved implementering av RAG if칮lge det foresl친tte papiret: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) er:

- **_RAG-Sequence_** bruker innhentede dokumenter for 친 forutsi det beste mulige svaret p친 en brukerforesp칮rsel

- **RAG-Token** bruker dokumenter til 친 generere neste token, deretter innhenter dem for 친 svare p친 brukerens foresp칮rsel

### Hvorfor ville du bruke RAGs?

- **Informasjonsrikhet:** sikrer at tekstsvar er oppdaterte og aktuelle. Det forbedrer derfor ytelsen p친 domene-spesifikke oppgaver ved 친 f친 tilgang til den interne kunnskapsbasen.

- Reduserer fabrikasjon ved 친 bruke **verifiserbare data** i kunnskapsbasen for 친 gi kontekst til brukerens foresp칮rsler.

- Det er **kostnadseffektivt** da de er mer 칮konomiske sammenlignet med finjustering av en LLM.

## Lage en kunnskapsbase

V친r applikasjon er basert p친 v친re personlige data, dvs. leksjonen om nevrale nettverk i AI for nybegynnere l칝replanen.

### Vektordatabaser

En vektordatabase, i motsetning til tradisjonelle databaser, er en spesialisert database designet for 친 lagre, administrere og s칮ke innebygde vektorer. Den lagrer numeriske representasjoner av dokumenter. 칀 bryte ned data til numeriske innebygginger gj칮r det lettere for v친rt AI-system 친 forst친 og behandle dataene.

Vi lagrer v친re innebygginger i vektordatabaser ettersom LLM-er har en grense for antall tokens de aksepterer som input. Ettersom du ikke kan sende hele innebyggingene til en LLM, vil vi trenge 친 bryte dem ned i deler, og n친r en bruker stiller et sp칮rsm친l, vil innebyggingene som ligner mest p친 sp칮rsm친let bli returnert sammen med foresp칮rselen. 칀 dele opp reduserer ogs친 kostnadene for antall tokens som sendes gjennom en LLM.

Noen popul칝re vektordatabaser inkluderer Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant og DeepLake. Du kan lage en Azure Cosmos DB-modell ved hjelp av Azure CLI med f칮lgende kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fra tekst til innebygginger

F칮r vi lagrer v친re data, m친 vi konvertere dem til vektorinnebygginger f칮r de lagres i databasen. Hvis du arbeider med store dokumenter eller lange tekster, kan du dele dem opp basert p친 foresp칮rsler du forventer. Oppdeling kan gj칮res p친 setningsniv친 eller p친 avsnittsniv친. Ettersom oppdeling utleder betydninger fra ordene rundt dem, kan du legge til litt annen kontekst til en del, for eksempel ved 친 legge til dokumenttittelen eller inkludere noe tekst f칮r eller etter delen. Du kan dele opp dataene som f칮lger:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N친r de er delt opp, kan vi deretter bygge inn teksten v친r ved hjelp av forskjellige innebyggingsmodeller. Noen modeller du kan bruke inkluderer: word2vec, ada-002 av OpenAI, Azure Computer Vision og mange flere. Valg av modell 친 bruke vil avhenge av spr친kene du bruker, typen innhold som kodes (tekst/bilder/lyd), st칮rrelsen p친 input det kan kode og lengden p친 innebyggingsoutputen.

Et eksempel p친 innebygd tekst ved bruk av OpenAIs `text-embedding-ada-002`-modell er:
![en innebygging av ordet katt](../../../translated_images/cat.3db013cbca4fd5d90438ea7b312ad0364f7686cf79931ab15cd5922151aea53e.no.png)

## Innhenting og vektors칮k

N친r en bruker stiller et sp칮rsm친l, transformerer innhenteren det til en vektor ved hjelp av foresp칮rselskoderen, den s칮ker deretter gjennom v친rt dokumentindeks for relevante vektorer i dokumentet som er relatert til input. N친r det er gjort, konverterer det b친de inputvektoren og dokumentvektorene til tekst og sender det gjennom LLM.

### Innhenting

Innhenting skjer n친r systemet pr칮ver 친 raskt finne dokumentene fra indeksen som tilfredsstiller s칮kekriteriene. M친let med innhenteren er 친 f친 dokumenter som vil bli brukt til 친 gi kontekst og forankre LLM p친 dine data.

Det er flere m친ter 친 utf칮re s칮k innen v친r database p친, som:

- **N칮kkelordss칮k** - brukt for teksts칮k

- **Semantisk s칮k** - bruker den semantiske betydningen av ord

- **Vektors칮k** - konverterer dokumenter fra tekst til vektorrepresentasjoner ved bruk av innebyggingsmodeller. Innhenting vil bli gjort ved 친 sp칮rre dokumentene hvis vektorrepresentasjoner er n칝rmest brukerens sp칮rsm친l.

- **Hybrid** - en kombinasjon av b친de n칮kkelord og vektors칮k.

En utfordring med innhenting kommer n친r det ikke er noe lignende svar p친 foresp칮rselen i databasen, systemet vil da returnere den beste informasjonen de kan f친, men du kan bruke taktikker som 친 sette opp maksimal avstand for relevans eller bruke hybrid s칮k som kombinerer b친de n칮kkelord og vektors칮k. I denne leksjonen vil vi bruke hybrid s칮k, en kombinasjon av b친de vektor- og n칮kkelordss칮k. Vi vil lagre v친re data i en dataramme med kolonner som inneholder delene samt innebyggingene.

### Vektorlignhet

Innhenteren vil s칮ke gjennom kunnskapsdatabasen for innebygginger som er n칝r hverandre, de n칝rmeste naboene, ettersom de er tekster som er like. I scenarioet hvor en bruker stiller en foresp칮rsel, blir den f칮rst innebygd og deretter matchet med lignende innebygginger. Det vanlige m친let som brukes for 친 finne hvor like forskjellige vektorer er, er cosinuslikhet som er basert p친 vinkelen mellom to vektorer.

Vi kan m친le likhet ved 친 bruke andre alternativer som vi kan bruke, som euklidisk avstand, som er den rette linjen mellom vektorendepunkter, og prikkprodukt, som m친ler summen av produktene av tilsvarende elementer av to vektorer.

### S칮keindeks

N친r du gj칮r innhenting, vil vi trenge 친 bygge en s칮keindeks for v친r kunnskapsbase f칮r vi utf칮rer s칮k. En indeks vil lagre v친re innebygginger og kan raskt hente de mest like delene selv i en stor database. Vi kan lage v친r indeks lokalt ved 친 bruke:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Re-rangering

N친r du har spurt databasen, kan det v칝re n칮dvendig 친 sortere resultatene fra de mest relevante. En re-rangerings LLM bruker maskinl칝ring for 친 forbedre relevansen av s칮keresultater ved 친 ordne dem fra de mest relevante. Ved 친 bruke Azure AI Search, gj칮res re-rangering automatisk for deg ved 친 bruke en semantisk re-rangerer. Et eksempel p친 hvordan re-rangering fungerer ved bruk av n칝rmeste naboer:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## 칀 samle alt sammen

Det siste trinnet er 친 legge til v친r LLM i miksen for 친 kunne f친 svar som er forankret p친 v친re data. Vi kan implementere det som f칮lger:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluering av v친r applikasjon

### Evalueringsmetrikker

- Kvaliteten p친 svarene som leveres, sikrer at det h칮res naturlig, flytende og menneskelig ut.

- Forankring av dataene: evaluering av om svaret kom fra de leverte dokumentene.

- Relevans: evaluering av om svaret matcher og er relatert til sp칮rsm친let som ble stilt.

- Flyt - om svaret gir mening grammatisk.

## Brukstilfeller for bruk av RAG (Henteforsterket generering) og vektordatabaser

Det er mange forskjellige brukstilfeller der funksjonskall kan forbedre appen din, for eksempel:

- Sp칮rsm친l og svar: forankre dine bedriftsdata til en chat som kan brukes av ansatte til 친 stille sp칮rsm친l.

- Anbefalingssystemer: hvor du kan lage et system som matcher de mest like verdiene, f.eks. filmer, restauranter og mange flere.

- Chatbot-tjenester: du kan lagre chat-historikk og tilpasse samtalen basert p친 brukerens data.

- Bildes칮k basert p친 vektorinnebygginger, nyttig n친r du gj칮r bilderegistrering og avviksdeteksjon.

## Oppsummering

Vi har dekket de grunnleggende omr친dene av RAG fra 친 legge til v친re data til applikasjonen, brukerens foresp칮rsel og output. For 친 forenkle opprettelsen av RAG kan du bruke rammeverk som Semanti Kernel, Langchain eller Autogen.

## Oppgave

For 친 fortsette l칝ringen din om Henteforsterket generering (RAG) kan du bygge:

- Bygg en front-end for applikasjonen ved hjelp av rammeverket du velger.

- Bruk et rammeverk, enten LangChain eller Semantic Kernel, og gjenskap applikasjonen din.

Gratulerer med 친 ha fullf칮rt leksjonen 游녪.

## L칝ring stopper ikke her, fortsett reisen

Etter 친 ha fullf칮rt denne leksjonen, sjekk ut v친r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for 친 fortsette 친 칮ke din kunnskap om generativ AI!

**Ansvarsfraskrivelse**:  
Dette dokumentet har blitt oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n칮yaktighet, v칝r oppmerksom p친 at automatiserte oversettelser kan inneholde feil eller un칮yaktigheter. Det originale dokumentet p친 sitt opprinnelige spr친k b칮r betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller feiltolkninger som oppst친r ved bruk av denne oversettelsen.