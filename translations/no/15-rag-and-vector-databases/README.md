<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b4b0266fbadbba7ded891b6485adc66d",
  "translation_date": "2025-10-17T19:20:47+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "no"
}
-->
# Retrieval Augmented Generation (RAG) og Vektordatabaser

[![Retrieval Augmented Generation (RAG) og Vektordatabaser](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.no.png)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

I leksjonen om s칮keapplikasjoner l칝rte vi kort hvordan du kan integrere dine egne data i store spr친kmodeller (LLMs). I denne leksjonen skal vi g친 dypere inn i konseptene rundt 친 forankre dine data i LLM-applikasjoner, mekanismene bak prosessen og metodene for lagring av data, inkludert b친de embeddings og tekst.

> **Video kommer snart**

## Introduksjon

I denne leksjonen skal vi dekke f칮lgende:

- En introduksjon til RAG, hva det er og hvorfor det brukes i kunstig intelligens (AI).

- Forst친 hva vektordatabaser er og opprette en for v친r applikasjon.

- Et praktisk eksempel p친 hvordan man integrerer RAG i en applikasjon.

## L칝ringsm친l

Etter 친 ha fullf칮rt denne leksjonen, vil du kunne:

- Forklare betydningen av RAG i datainnhenting og behandling.

- Sette opp en RAG-applikasjon og forankre dine data til en LLM.

- Effektiv integrering av RAG og vektordatabaser i LLM-applikasjoner.

## V친rt scenario: forbedre v친re LLM-er med egne data

I denne leksjonen 칮nsker vi 친 legge til v친re egne notater i utdanningsstart-upen, som lar chatboten f친 mer informasjon om de ulike emnene. Ved 친 bruke notatene vi har, vil l칝rende kunne studere bedre og forst친 de ulike temaene, noe som gj칮r det enklere 친 forberede seg til eksamen. For 친 lage v친rt scenario, vil vi bruke:

- `Azure OpenAI:` LLM-en vi skal bruke for 친 lage v친r chatbot.

- `AI for nybegynnere-leksjon om nevrale nettverk:` dette vil v칝re dataene vi forankrer v친r LLM p친.

- `Azure AI Search` og `Azure Cosmos DB:` vektordatabase for 친 lagre v친re data og opprette et s칮keindeks.

Brukere vil kunne lage 칮vingsquizer fra sine notater, repetisjonskort og oppsummere dem til konsise oversikter. For 친 komme i gang, la oss se p친 hva RAG er og hvordan det fungerer:

## Retrieval Augmented Generation (RAG)

En LLM-drevet chatbot behandler brukerforesp칮rsler for 친 generere svar. Den er designet for 친 v칝re interaktiv og engasjerer seg med brukere om et bredt spekter av temaer. Imidlertid er dens svar begrenset til konteksten som er gitt og dens grunnleggende treningsdata. For eksempel har GPT-4 en kunnskapsgrense fra september 2021, noe som betyr at den mangler kunnskap om hendelser som har skjedd etter denne perioden. I tillegg utelukker dataene som brukes til 친 trene LLM-er konfidensiell informasjon som personlige notater eller en bedrifts produktmanual.

### Hvordan RAGs (Retrieval Augmented Generation) fungerer

![tegning som viser hvordan RAGs fungerer](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.no.png)

Anta at du 칮nsker 친 distribuere en chatbot som lager quizer fra dine notater, da vil du trenge en tilkobling til kunnskapsbasen. Her kommer RAG til unnsetning. RAGs fungerer som f칮lger:

- **Kunnskapsbase:** F칮r innhenting m친 disse dokumentene behandles og forberedes, vanligvis ved 친 dele opp store dokumenter i mindre deler, transformere dem til tekstembeddings og lagre dem i en database.

- **Brukerforesp칮rsel:** brukeren stiller et sp칮rsm친l.

- **Innhenting:** N친r en bruker stiller et sp칮rsm친l, henter embedding-modellen relevant informasjon fra v친r kunnskapsbase for 친 gi mer kontekst som vil bli innlemmet i foresp칮rselen.

- **Forsterket generering:** LLM-en forbedrer sitt svar basert p친 de innhentede dataene. Dette gj칮r at det genererte svaret ikke bare er basert p친 forh친ndstrente data, men ogs친 relevant informasjon fra den ekstra konteksten. De innhentede dataene brukes til 친 forsterke LLM-ens svar. LLM-en returnerer deretter et svar p친 brukerens sp칮rsm친l.

![tegning som viser RAGs arkitektur](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.no.png)

Arkitekturen for RAGs implementeres ved hjelp av transformatorer som best친r av to deler: en encoder og en decoder. For eksempel, n친r en bruker stiller et sp칮rsm친l, blir input-teksten 'kodet' til vektorer som fanger meningen med ordene, og vektorene blir 'dekodet' inn i v친rt dokumentindeks og genererer ny tekst basert p친 brukerforesp칮rselen. LLM-en bruker b친de en encoder-decoder-modell for 친 generere output.

To tiln칝rminger ved implementering av RAG if칮lge den foresl친tte artikkelen: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) er:

- **_RAG-Sequence_** bruker innhentede dokumenter for 친 forutsi det beste mulige svaret p친 en brukerforesp칮rsel.

- **RAG-Token** bruker dokumenter for 친 generere neste token, deretter henter dem for 친 svare p친 brukerens foresp칮rsel.

### Hvorfor bruke RAGs?

- **Informasjonsrikdom:** sikrer at tekstsvar er oppdaterte og aktuelle. Det forbedrer derfor ytelsen p친 domene-spesifikke oppgaver ved 친 f친 tilgang til den interne kunnskapsbasen.

- Reduserer fabrikasjon ved 친 bruke **verifiserbare data** i kunnskapsbasen for 친 gi kontekst til brukerforesp칮rsler.

- Det er **kostnadseffektivt** da de er mer 칮konomiske sammenlignet med finjustering av en LLM.

## Opprette en kunnskapsbase

V친r applikasjon er basert p친 v친re personlige data, dvs. leksjonen om nevrale nettverk fra AI For Beginners-kurset.

### Vektordatabaser

En vektordatabase, i motsetning til tradisjonelle databaser, er en spesialisert database designet for 친 lagre, administrere og s칮ke i embedded vektorer. Den lagrer numeriske representasjoner av dokumenter. 칀 bryte ned data til numeriske embeddings gj칮r det enklere for v친rt AI-system 친 forst친 og behandle dataene.

Vi lagrer v친re embeddings i vektordatabaser da LLM-er har en grense for antall tokens de aksepterer som input. Siden du ikke kan sende hele embeddings til en LLM, m친 vi dele dem opp i mindre deler, og n친r en bruker stiller et sp칮rsm친l, vil embeddings som ligner mest p친 sp칮rsm친let bli returnert sammen med foresp칮rselen. Oppdeling reduserer ogs친 kostnadene for antall tokens som sendes gjennom en LLM.

Noen popul칝re vektordatabaser inkluderer Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant og DeepLake. Du kan opprette en Azure Cosmos DB-modell ved hjelp av Azure CLI med f칮lgende kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fra tekst til embeddings

F칮r vi lagrer v친re data, m친 vi konvertere dem til vektor-embeddings f칮r de lagres i databasen. Hvis du arbeider med store dokumenter eller lange tekster, kan du dele dem opp basert p친 foresp칮rsler du forventer. Oppdeling kan gj칮res p친 setningsniv친 eller avsnittsniv친. Siden oppdeling henter mening fra ordene rundt dem, kan du legge til litt annen kontekst til en del, for eksempel ved 친 legge til dokumenttittelen eller inkludere litt tekst f칮r eller etter delen. Du kan dele opp dataene som f칮lger:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N친r de er delt opp, kan vi deretter embedde teksten v친r ved hjelp av forskjellige embedding-modeller. Noen modeller du kan bruke inkluderer: word2vec, ada-002 fra OpenAI, Azure Computer Vision og mange flere. Valg av modell avhenger av spr친kene du bruker, typen innhold som kodes (tekst/bilder/lyd), st칮rrelsen p친 input den kan kode og lengden p친 embedding-output.

Et eksempel p친 embedded tekst ved bruk av OpenAIs `text-embedding-ada-002`-modell er:
![en embedding av ordet katt](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.no.png)

## Innhenting og vektors칮k

N친r en bruker stiller et sp칮rsm친l, transformerer innhenteren det til en vektor ved hjelp av foresp칮rselskoderen, og s칮ker deretter gjennom v친rt dokumentindeks for relevante vektorer i dokumentet som er relatert til input. N친r dette er gjort, konverterer den b친de input-vektoren og dokumentvektorene til tekst og sender det gjennom LLM-en.

### Innhenting

Innhenting skjer n친r systemet pr칮ver 친 raskt finne dokumentene fra indeksen som tilfredsstiller s칮kekriteriene. M친let med innhenteren er 친 f친 dokumenter som vil bli brukt til 친 gi kontekst og forankre LLM-en p친 dine data.

Det finnes flere m친ter 친 utf칮re s칮k i v친r database, som:

- **N칮kkelordss칮k** - brukes for teksts칮k.

- **Semantisk s칮k** - bruker den semantiske betydningen av ord.

- **Vektors칮k** - konverterer dokumenter fra tekst til vektorrepresentasjoner ved hjelp av embedding-modeller. Innhenting vil bli gjort ved 친 s칮ke etter dokumenter hvis vektorrepresentasjoner er n칝rmest brukerens sp칮rsm친l.

- **Hybrid** - en kombinasjon av b친de n칮kkelord og vektors칮k.

En utfordring med innhenting oppst친r n친r det ikke finnes et lignende svar p친 foresp칮rselen i databasen. Systemet vil da returnere den beste informasjonen de kan finne. Du kan imidlertid bruke taktikker som 친 sette opp maksimal avstand for relevans eller bruke hybrid s칮k som kombinerer b친de n칮kkelord og vektors칮k. I denne leksjonen vil vi bruke hybrid s칮k, en kombinasjon av b친de vektor- og n칮kkelordss칮k. Vi vil lagre v친re data i en dataframe med kolonner som inneholder delene samt embeddings.

### Vektorsimilaritet

Innhenteren vil s칮ke gjennom kunnskapsdatabasen etter embeddings som er n칝r hverandre, den n칝rmeste naboen, da de er tekster som er like. I scenarioet der en bruker stiller en foresp칮rsel, blir den f칮rst embeddet og deretter matchet med lignende embeddings. Den vanlige m친lingen som brukes for 친 finne hvor like forskjellige vektorer er, er cosinus-similaritet, som er basert p친 vinkelen mellom to vektorer.

Vi kan m친le similaritet ved hjelp av andre alternativer som Euklidisk avstand, som er den rette linjen mellom vektorendepunkter, og prikkprodukt, som m친ler summen av produktene av tilsvarende elementer i to vektorer.

### S칮keindeks

N친r vi utf칮rer innhenting, m친 vi bygge en s칮keindeks for v친r kunnskapsbase f칮r vi utf칮rer s칮k. En indeks vil lagre v친re embeddings og kan raskt hente de mest lignende delene selv i en stor database. Vi kan opprette v친r indeks lokalt ved hjelp av:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Re-ranking

N친r du har spurt databasen, kan det v칝re n칮dvendig 친 sortere resultatene fra de mest relevante. En re-ranking LLM bruker maskinl칝ring for 친 forbedre relevansen av s칮keresultater ved 친 ordne dem fra de mest relevante. Ved bruk av Azure AI Search, blir re-ranking gjort automatisk for deg ved hjelp av en semantisk re-ranker. Et eksempel p친 hvordan re-ranking fungerer ved bruk av n칝rmeste naboer:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Sette alt sammen

Det siste steget er 친 legge til v친r LLM i miksen for 친 kunne f친 svar som er forankret i v친re data. Vi kan implementere det som f칮lger:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluering av v친r applikasjon

### Evalueringsmetoder

- Kvaliteten p친 de leverte svarene, og sikre at de h칮res naturlige, flytende og menneskelige ut.

- Forankring av data: evaluere om svaret kom fra de leverte dokumentene.

- Relevans: evaluere om svaret samsvarer med og er relatert til det stilte sp칮rsm친let.

- Flyt - om svaret gir mening grammatisk.

## Bruksomr친der for RAG (Retrieval Augmented Generation) og vektordatabaser

Det finnes mange ulike bruksomr친der der funksjonskall kan forbedre din app, som:

- Sp칮rsm친l og svar: forankre dine bedriftsdata til en chat som kan brukes av ansatte til 친 stille sp칮rsm친l.

- Anbefalingssystemer: der du kan lage et system som matcher de mest lignende verdiene, f.eks. filmer, restauranter og mye mer.

- Chatbot-tjenester: du kan lagre chathistorikk og tilpasse samtalen basert p친 brukerdata.

- Bildes칮k basert p친 vektor-embeddings, nyttig ved bildegjenkjenning og avviksdeteksjon.

## Oppsummering

Vi har dekket de grunnleggende omr친dene av RAG fra 친 legge til v친re data i applikasjonen, brukerforesp칮rselen og output. For 친 forenkle opprettelsen av RAG, kan du bruke rammeverk som Semantic Kernel, Langchain eller Autogen.

## Oppgave

For 친 fortsette din l칝ring om Retrieval Augmented Generation (RAG) kan du bygge:

- Lag en front-end for applikasjonen ved hjelp av rammeverket du 칮nsker.

- Bruk et rammeverk, enten LangChain eller Semantic Kernel, og gjenskap applikasjonen din.

Gratulerer med 친 ha fullf칮rt leksjonen 游녪.

## L칝ring stopper ikke her, fortsett reisen

Etter 친 ha fullf칮rt denne leksjonen, sjekk ut v친r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for 친 fortsette 친 utvikle din kunnskap om generativ AI!

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n칮yaktighet, v칝r oppmerksom p친 at automatiske oversettelser kan inneholde feil eller un칮yaktigheter. Det originale dokumentet p친 sitt opprinnelige spr친k b칮r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst친elser eller feiltolkninger som oppst친r ved bruk av denne oversettelsen.