<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-07-09T16:14:30+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "no"
}
-->
# Retrieval Augmented Generation (RAG) og vektordatabaser

[![Retrieval Augmented Generation (RAG) and Vector Databases](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.no.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

I leksjonen om s√∏keapplikasjoner l√¶rte vi kort hvordan du kan integrere dine egne data i store spr√•kmodeller (LLMs). I denne leksjonen skal vi g√• dypere inn i konseptene rundt √• forankre data i LLM-applikasjonen din, mekanismene i prosessen og metodene for lagring av data, inkludert b√•de embeddings og tekst.

> **Video kommer snart**

## Introduksjon

I denne leksjonen vil vi dekke f√∏lgende:

- En introduksjon til RAG, hva det er og hvorfor det brukes i AI (kunstig intelligens).

- Forst√• hva vektordatabaser er og hvordan man oppretter en for applikasjonen v√•r.

- Et praktisk eksempel p√• hvordan man integrerer RAG i en applikasjon.

## L√¶ringsm√•l

Etter √• ha fullf√∏rt denne leksjonen vil du kunne:

- Forklare betydningen av RAG i datahenting og -behandling.

- Sette opp en RAG-applikasjon og forankre dataene dine til en LLM.

- Effektiv integrering av RAG og vektordatabaser i LLM-applikasjoner.

## V√•rt scenario: forbedre LLM-ene v√•re med egne data

I denne leksjonen √∏nsker vi √• legge til egne notater i utdanningsstartuppen, slik at chatboten kan f√• mer informasjon om ulike fagomr√•der. Ved √• bruke notatene vi har, vil elever kunne studere bedre og forst√• de forskjellige temaene, noe som gj√∏r det enklere √• repetere til eksamen. For √• lage v√•rt scenario vil vi bruke:

- `Azure OpenAI:` LLM-en vi bruker for √• lage chatboten v√•r

- `AI for beginners' lesson on Neural Networks:` dette blir dataene vi forankrer LLM-en v√•r p√•

- `Azure AI Search` og `Azure Cosmos DB:` vektordatabaser for √• lagre dataene v√•re og opprette en s√∏keindeks

Brukere vil kunne lage √∏vingsquizzer basert p√• notatene sine, repetisjonsflashcards og oppsummere dem til korte oversikter. For √• komme i gang, la oss se p√• hva RAG er og hvordan det fungerer:

## Retrieval Augmented Generation (RAG)

En LLM-drevet chatbot behandler brukerforesp√∏rsler for √• generere svar. Den er designet for √• v√¶re interaktiv og engasjere brukere i et bredt spekter av temaer. Likevel er svarene begrenset til konteksten som gis og den grunnleggende treningsdataen. For eksempel har GPT-4 kunnskapsavgrensning i september 2021, noe som betyr at den mangler kunnskap om hendelser som har skjedd etter dette tidspunktet. I tillegg ekskluderer treningsdataene for LLM-er konfidensiell informasjon som personlige notater eller en bedrifts produktmanual.

### Hvordan RAG (Retrieval Augmented Generation) fungerer

![drawing showing how RAGs work](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.no.png)

Anta at du √∏nsker √• lansere en chatbot som lager quizzer basert p√• notatene dine, da trenger du en kobling til kunnskapsbasen. Det er her RAG kommer inn. RAG fungerer slik:

- **Kunnskapsbase:** F√∏r henting m√• dokumentene importeres og forh√•ndsbehandles, vanligvis ved √• dele opp store dokumenter i mindre biter, konvertere dem til tekst-embedding og lagre dem i en database.

- **Brukersp√∏rsm√•l:** brukeren stiller et sp√∏rsm√•l

- **Henting:** N√•r en bruker stiller et sp√∏rsm√•l, henter embedding-modellen relevant informasjon fra kunnskapsbasen for √• gi mer kontekst som inkluderes i prompten.

- **Forsterket generering:** LLM-en forbedrer svaret sitt basert p√• de hentede dataene. Dette gj√∏r at svaret ikke bare baseres p√• forh√•ndstrent data, men ogs√• p√• relevant informasjon fra den ekstra konteksten. De hentede dataene brukes til √• forsterke LLM-ens svar. LLM-en returnerer deretter et svar p√• brukerens sp√∏rsm√•l.

![drawing showing how RAGs architecture](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.no.png)

Arkitekturen for RAG implementeres ved hjelp av transformere som best√•r av to deler: en encoder og en decoder. For eksempel, n√•r en bruker stiller et sp√∏rsm√•l, blir inndata-teksten "kodet" til vektorer som fanger meningen i ordene, og vektorene "dekodes" til dokumentindeksen v√•r og genererer ny tekst basert p√• brukerens sp√∏rsm√•l. LLM-en bruker b√•de en encoder-decoder-modell for √• generere output.

To tiln√¶rminger ved implementering av RAG if√∏lge den foresl√•tte artikkelen: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) er:

- **_RAG-Sequence_** som bruker hentede dokumenter for √• forutsi det beste mulige svaret p√• et brukerforesp√∏rsel

- **RAG-Token** som bruker dokumenter for √• generere neste token, og deretter hente dem for √• svare p√• brukerens sp√∏rsm√•l

### Hvorfor bruke RAG?

- **Informasjonsrikdom:** sikrer at tekstsvar er oppdaterte og aktuelle. Det forbedrer derfor ytelsen p√• domene-spesifikke oppgaver ved √• f√• tilgang til intern kunnskapsbase.

- Reduserer fabrikasjon ved √• bruke **verifiserbare data** i kunnskapsbasen for √• gi kontekst til brukerforesp√∏rsler.

- Det er **kostnadseffektivt** siden det er rimeligere enn √• finjustere en LLM

## Opprette en kunnskapsbase

Applikasjonen v√•r baseres p√• v√•re personlige data, alts√• leksjonen om nevrale nettverk i AI For Beginners-kurset.

### Vektordatabaser

En vektordatabasen, i motsetning til tradisjonelle databaser, er en spesialisert database designet for √• lagre, h√•ndtere og s√∏ke i innebygde vektorer. Den lagrer numeriske representasjoner av dokumenter. √Ö bryte ned data til numeriske embeddings gj√∏r det enklere for AI-systemet v√•rt √• forst√• og behandle dataene.

Vi lagrer embeddingene v√•re i vektordatabaser fordi LLM-er har en grense for hvor mange tokens de kan ta inn som input. Siden du ikke kan sende hele embeddingene til en LLM, m√• vi dele dem opp i biter, og n√•r en bruker stiller et sp√∏rsm√•l, vil embeddingene som ligner mest p√• sp√∏rsm√•let bli returnert sammen med prompten. Oppdeling reduserer ogs√• kostnader knyttet til antall tokens som sendes gjennom en LLM.

Noen popul√¶re vektordatabaser inkluderer Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant og DeepLake. Du kan opprette en Azure Cosmos DB-modell ved √• bruke Azure CLI med f√∏lgende kommando:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Fra tekst til embeddings

F√∏r vi lagrer dataene v√•re, m√• vi konvertere dem til vektor-embeddings f√∏r de lagres i databasen. Hvis du jobber med store dokumenter eller lange tekster, kan du dele dem opp basert p√• forventede sp√∏rsm√•l. Oppdeling kan gj√∏res p√• setningsniv√• eller avsnittsniv√•. Siden oppdeling henter mening fra ordene rundt, kan du legge til ekstra kontekst til en bit, for eksempel ved √• legge til dokumenttittel eller inkludere noe tekst f√∏r eller etter biten. Du kan dele opp dataene slik:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

N√•r dataene er delt opp, kan vi deretter embedde teksten ved hjelp av ulike embedding-modeller. Noen modeller du kan bruke inkluderer: word2vec, ada-002 fra OpenAI, Azure Computer Vision og mange flere. Valg av modell avhenger av spr√•kene du bruker, typen innhold som kodes (tekst/bilder/lyd), st√∏rrelsen p√• input den kan kode og lengden p√• embedding-output.

Et eksempel p√• embedded tekst ved bruk av OpenAIs `text-embedding-ada-002` modell er:
![an embedding of the word cat](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.no.png)

## Henting og vektors√∏k

N√•r en bruker stiller et sp√∏rsm√•l, konverterer retrieveren det til en vektor ved hjelp av query-encoderen, og s√∏ker deretter gjennom dokumentindeksen v√•r etter relevante vektorer i dokumentet som er relatert til input. N√•r dette er gjort, konverteres b√•de input-vektoren og dokumentvektorene til tekst og sendes gjennom LLM-en.

### Henting

Henting skjer n√•r systemet pr√∏ver √• raskt finne dokumenter fra indeksen som oppfyller s√∏kekriteriene. M√•let med retrieveren er √• hente dokumenter som skal brukes for √• gi kontekst og forankre LLM-en p√• dataene dine.

Det finnes flere m√•ter √• utf√∏re s√∏k i databasen v√•r p√•, for eksempel:

- **N√∏kkelordss√∏k** - brukt for teksts√∏k

- **Semantisk s√∏k** - bruker den semantiske betydningen av ord

- **Vektors√∏k** - konverterer dokumenter fra tekst til vektorrepresentasjoner ved hjelp av embedding-modeller. Henting gj√∏res ved √• s√∏ke i dokumenter hvis vektorrepresentasjoner er n√¶rmest brukerens sp√∏rsm√•l.

- **Hybrid** - en kombinasjon av b√•de n√∏kkelord- og vektors√∏k.

En utfordring med henting oppst√•r n√•r det ikke finnes noe lignende svar p√• sp√∏rsm√•let i databasen. Systemet vil da returnere den beste informasjonen det kan finne, men du kan bruke taktikker som √• sette maksimal avstand for relevans eller bruke hybrid s√∏k som kombinerer b√•de n√∏kkelord- og vektors√∏k. I denne leksjonen vil vi bruke hybrid s√∏k, en kombinasjon av b√•de vektor- og n√∏kkelordss√∏k. Vi lagrer dataene v√•re i en dataframe med kolonner som inneholder b√•de bitene og embeddingene.

### Vektorsimilaritet

Retrieveren s√∏ker gjennom kunnskapsdatabasen etter embeddings som ligger n√¶r hverandre, den n√¶rmeste naboen, siden de er tekster som ligner. I scenarioet hvor en bruker stiller et sp√∏rsm√•l, embeddes det f√∏rst og matches deretter med lignende embeddings. Den vanlige m√•lingen som brukes for √• finne hvor like forskjellige vektorer er, er cosinuslikhet, som baseres p√• vinkelen mellom to vektorer.

Vi kan ogs√• m√•le likhet med andre alternativer som Euclidean distance, som er den rette linjen mellom vektorendepunkter, og dot product som m√•ler summen av produktene av tilsvarende elementer i to vektorer.

### S√∏keindeks

N√•r vi gj√∏r henting, m√• vi bygge en s√∏keindeks for kunnskapsbasen v√•r f√∏r vi utf√∏rer s√∏k. En indeks lagrer embeddingene v√•re og kan raskt hente de mest like bitene selv i en stor database. Vi kan opprette indeksen lokalt ved √• bruke:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Omrangering

N√•r du har spurt databasen, kan det hende du m√• sortere resultatene fra mest relevante. En omrangering-LLM bruker maskinl√¶ring for √• forbedre relevansen av s√∏keresultatene ved √• ordne dem fra mest relevante. Ved bruk av Azure AI Search gj√∏res omrangering automatisk for deg ved hjelp av en semantisk omrangering. Et eksempel p√• hvordan omrangering fungerer ved bruk av n√¶rmeste naboer:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## √Ö sette det hele sammen

Det siste steget er √• legge til LLM-en v√•r i miksen for √• kunne f√• svar som er forankret i dataene v√•re. Vi kan implementere det slik:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluering av applikasjonen v√•r

### Evalueringsmetoder

- Kvalitet p√• svarene som gis, slik at de h√∏res naturlige, flytende og menneskelige ut

- Forankring av data: evaluere om svaret kommer fra de leverte dokumentene

- Relevans: evaluere om svaret samsvarer med og er relatert til sp√∏rsm√•let som ble stilt

- Flyt ‚Äì om svaret gir grammatisk mening

## Bruksomr√•der for RAG (Retrieval Augmented Generation) og vektordatabaser

Det finnes mange ulike bruksomr√•der hvor funksjonskall kan forbedre appen din, som for eksempel:

- Sp√∏rsm√•l og svar: forankre bedriftsdata til en chat som ansatte kan bruke til √• stille sp√∏rsm√•l.

- Anbefalingssystemer: hvor du kan lage et system som matcher de mest like verdiene, f.eks. filmer, restauranter og mye mer.

- Chatbot-tjenester: du kan lagre chatthistorikk og personalisere samtalen basert p√• brukerdata.

- Bildes√∏k basert p√• vektor-embeddings, nyttig ved bildeanalyse og anomali-deteksjon.

## Oppsummering

Vi har dekket grunnleggende omr√•der av RAG, fra √• legge til data i applikasjonen, brukerforesp√∏rsel og output. For √• forenkle opprettelsen av RAG kan du bruke rammeverk som Semantic Kernel, Langchain eller Autogen.

## Oppgave

For √• fortsette l√¶ringen din om Retrieval Augmented Generation (RAG) kan du bygge:

- Lag et frontend for applikasjonen ved √• bruke rammeverket du foretrekker

- Bruk et rammeverk, enten LangChain eller Semantic Kernel, og gjenskap applikasjonen din.

Gratulerer med √• ha fullf√∏rt leksjonen üëè.

## L√¶ringen stopper ikke her, fortsett reisen

Etter √• ha fullf√∏rt denne leksjonen, sjekk ut v√•r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for √• fortsette √• utvikle kunnskapen din om Generativ AI!

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, vennligst v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det opprinnelige dokumentet p√• originalspr√•ket skal anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.