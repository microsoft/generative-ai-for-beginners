<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:33:14+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "no"
}
-->
# Nevrale nettverksrammeverk

Som vi allerede har l√¶rt, for √• kunne trene nevrale nettverk effektivt m√• vi gj√∏re to ting:

* √Ö operere p√• tensorer, f.eks. √• multiplisere, legge til, og beregne noen funksjoner som sigmoid eller softmax
* √Ö beregne gradienter av alle uttrykk, for √• kunne utf√∏re gradientnedstigningsoptimalisering

Mens `numpy`-biblioteket kan gj√∏re den f√∏rste delen, trenger vi en mekanisme for √• beregne gradienter. I rammeverket vi utviklet i forrige seksjon m√•tte vi programmere alle deriverte funksjoner manuelt inne i `backward`-metoden, som utf√∏rer backpropagation. Ideelt sett b√∏r et rammeverk gi oss muligheten til √• beregne gradienter av *hvilket som helst uttrykk* vi kan definere.

En annen viktig ting er √• kunne utf√∏re beregninger p√• GPU, eller andre spesialiserte beregningsenheter, som TPU. Dyp nevralt nettverkstrening krever *mye* beregninger, og det er sv√¶rt viktig √• kunne parallellisere disse beregningene p√• GPUer.

> ‚úÖ Begrepet 'parallellisere' betyr √• fordele beregningene over flere enheter.

For √∏yeblikket er de to mest popul√¶re nevrale rammeverkene: TensorFlow og PyTorch. Begge tilbyr et lavniv√•-API for √• operere med tensorer p√• b√•de CPU og GPU. I tillegg til lavniv√•-API finnes det ogs√• h√∏yere niv√• API, kalt Keras og PyTorch Lightning henholdsvis.

Lavniv√•-API | TensorFlow | PyTorch  
------------|-------------------------------|-----------------------------  
H√∏yniv√•-API | Keras | PyTorch

**Lavniv√•-APIer** i begge rammeverkene lar deg bygge s√•kalte **beregningsgrafer**. Denne grafen definerer hvordan man beregner output (vanligvis tapsfunksjonen) med gitte inputparametere, og kan kj√∏res p√• GPU hvis det er tilgjengelig. Det finnes funksjoner for √• differensiere denne beregningsgrafen og beregne gradienter, som deretter kan brukes til √• optimalisere modellparametere.

**H√∏yniv√•-APIer** ser p√• nevrale nettverk som en **sekvens av lag**, og gj√∏r det mye enklere √• bygge de fleste nevrale nettverk. Trening av modellen krever vanligvis √• forberede dataene og deretter kalle en `fit`-funksjon for √• utf√∏re treningen.

H√∏yniv√•-API lar deg raskt bygge typiske nevrale nettverk uten √• bekymre deg for mange detaljer. Samtidig gir lavniv√•-API mye mer kontroll over treningsprosessen, og brukes derfor mye i forskning n√•r man jobber med nye nevrale nettverksarkitekturer.

Det er ogs√• viktig √• forst√• at du kan bruke begge APIene sammen, f.eks. kan du utvikle din egen nettverkslagarkitektur med lavniv√•-API, og deretter bruke den i et st√∏rre nettverk bygget og trent med h√∏yniv√•-API. Eller du kan definere et nettverk med h√∏yniv√•-API som en sekvens av lag, og deretter bruke din egen lavniv√• treningssl√∏yfe for √• utf√∏re optimalisering. Begge APIene bruker de samme grunnleggende konseptene, og er designet for √• fungere godt sammen.

## L√¶ring

I dette kurset tilbyr vi mesteparten av innholdet b√•de for PyTorch og TensorFlow. Du kan velge ditt foretrukne rammeverk og kun g√• gjennom de tilh√∏rende notatb√∏kene. Hvis du er usikker p√• hvilket rammeverk du skal velge, kan du lese noen diskusjoner p√• internett om **PyTorch vs. TensorFlow**. Du kan ogs√• se p√• begge rammeverkene for √• f√• bedre forst√•else.

Der det er mulig, vil vi bruke h√∏yniv√•-API for enkelhets skyld. Vi mener imidlertid det er viktig √• forst√• hvordan nevrale nettverk fungerer helt fra bunnen av, s√• i starten jobber vi med lavniv√•-API og tensorer. Men hvis du vil komme raskt i gang og ikke √∏nsker √• bruke mye tid p√• √• l√¶re disse detaljene, kan du hoppe over dem og g√• rett til h√∏yniv√•-API-notatb√∏kene.

## ‚úçÔ∏è √òvelser: Rammeverk

Fortsett l√¶ringen i f√∏lgende notatb√∏ker:

Lavniv√•-API | TensorFlow+Keras Notebook | PyTorch  
------------|-------------------------------|-----------------------------  
H√∏yniv√•-API | Keras | *PyTorch Lightning*

Etter √• ha mestret rammeverkene, la oss oppsummere begrepet overtilpasning.

# Overtilpasning

Overtilpasning er et ekstremt viktig konsept innen maskinl√¶ring, og det er veldig viktig √• f√• det riktig!

Se for deg f√∏lgende problem med √• tiln√¶rme 5 punkter (representert med `x` p√• grafene under):

!linear | overfit  
-------------------------|--------------------------  
**Line√¶r modell, 2 parametere** | **Ikke-line√¶r modell, 7 parametere**  
Treningsfeil = 5.3 | Treningsfeil = 0  
Valideringsfeil = 5.1 | Valideringsfeil = 20

* Til venstre ser vi en god rett linje-tiln√¶rming. Fordi antallet parametere er passende, fanger modellen opp fordelingen av punktene riktig.
* Til h√∏yre er modellen for kraftig. Fordi vi bare har 5 punkter og modellen har 7 parametere, kan den justere seg slik at den g√•r gjennom alle punktene, og treningsfeilen blir 0. Dette hindrer modellen i √• forst√• det riktige m√∏nsteret bak dataene, og valideringsfeilen blir derfor veldig h√∏y.

Det er sv√¶rt viktig √• finne en riktig balanse mellom modellens kompleksitet (antall parametere) og antall treningspr√∏ver.

## Hvorfor overtilpasning oppst√•r

  * Ikke nok treningsdata
  * For kraftig modell
  * For mye st√∏y i inputdata

## Hvordan oppdage overtilpasning

Som du kan se fra grafen over, kan overtilpasning oppdages ved veldig lav treningsfeil og h√∏y valideringsfeil. Vanligvis under trening vil b√•de trenings- og valideringsfeil begynne √• synke, men p√• et tidspunkt kan valideringsfeilen slutte √• synke og begynne √• √∏ke. Dette er et tegn p√• overtilpasning, og en indikasjon p√• at vi sannsynligvis b√∏r stoppe treningen p√• dette tidspunktet (eller i det minste ta et √∏yeblikksbilde av modellen).

overfitting

## Hvordan forhindre overtilpasning

Hvis du ser at overtilpasning oppst√•r, kan du gj√∏re en av f√∏lgende:

 * √òke mengden treningsdata
 * Redusere modellens kompleksitet
 * Bruke en form for regularisering, som Dropout, som vi vil se n√¶rmere p√• senere.

## Overtilpasning og Bias-Variance Tradeoff

Overtilpasning er egentlig et tilfelle av et mer generelt problem i statistikk kalt Bias-Variance Tradeoff. Hvis vi ser p√• mulige feilkilder i modellen v√•r, kan vi skille to typer feil:

* **Bias-feil** skyldes at algoritmen v√•r ikke klarer √• fange forholdet i treningsdataene riktig. Dette kan komme av at modellen ikke er kraftig nok (**undertilpasning**).
* **Variansfeil** skyldes at modellen tilpasser seg st√∏y i inputdataene i stedet for meningsfulle sammenhenger (**overtilpasning**).

Under trening minker bias-feilen (etter hvert som modellen l√¶rer √• tiln√¶rme dataene), mens variansfeilen √∏ker. Det er viktig √• stoppe treningen ‚Äì enten manuelt (n√•r vi oppdager overtilpasning) eller automatisk (ved √• innf√∏re regularisering) ‚Äì for √• forhindre overtilpasning.

## Konklusjon

I denne leksjonen har du l√¶rt om forskjellene mellom de ulike APIene for de to mest popul√¶re AI-rammeverkene, TensorFlow og PyTorch. I tillegg har du l√¶rt om et sv√¶rt viktig tema, overtilpasning.

## üöÄ Utfordring

I de medf√∏lgende notatb√∏kene finner du 'oppgaver' nederst; jobb deg gjennom notatb√∏kene og fullf√∏r oppgavene.

## Gjennomgang & Selvstudium

Gj√∏r litt research p√• f√∏lgende temaer:

- TensorFlow  
- PyTorch  
- Overtilpasning

Still deg selv f√∏lgende sp√∏rsm√•l:

- Hva er forskjellen mellom TensorFlow og PyTorch?  
- Hva er forskjellen mellom overtilpasning og undertilpasning?

## Oppgave

I dette laboratoriet skal du l√∏se to klassifiseringsproblemer ved hjelp av enkelt- og flerlags fulltilkoblede nettverk ved bruk av PyTorch eller TensorFlow.

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, vennligst v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det opprinnelige dokumentet p√• originalspr√•ket skal anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.