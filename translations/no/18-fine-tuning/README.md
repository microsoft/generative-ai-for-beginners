<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-07-09T17:44:46+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "no"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.no.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Finjustering av LLM-en din

Bruk av store spr√•kmodeller for √• bygge generative AI-applikasjoner medf√∏rer nye utfordringer. En sentral problemstilling er √• sikre kvaliteten p√• svarene (n√∏yaktighet og relevans) i innholdet modellen genererer for en gitt brukerforesp√∏rsel. I tidligere leksjoner har vi diskutert teknikker som prompt engineering og retrieval-augmented generation, som pr√∏ver √• l√∏se problemet ved √• _endre prompt-inputen_ til den eksisterende modellen.

I dagens leksjon skal vi se p√• en tredje teknikk, **finjustering**, som fors√∏ker √• m√∏te utfordringen ved √• _trene modellen p√• nytt_ med ekstra data. La oss g√• n√¶rmere inn p√• detaljene.

## L√¶ringsm√•l

Denne leksjonen introduserer konseptet finjustering for forh√•ndstrente spr√•kmodeller, utforsker fordeler og utfordringer med denne tiln√¶rmingen, og gir veiledning om n√•r og hvordan du kan bruke finjustering for √• forbedre ytelsen til dine generative AI-modeller.

Etter denne leksjonen skal du kunne svare p√• f√∏lgende sp√∏rsm√•l:

- Hva er finjustering for spr√•kmodeller?
- N√•r og hvorfor er finjustering nyttig?
- Hvordan kan jeg finjustere en forh√•ndstrent modell?
- Hva er begrensningene ved finjustering?

Klar? La oss sette i gang.

## Illustrert guide

Vil du f√• en oversikt over hva vi skal dekke f√∏r vi g√•r i gang? Sjekk ut denne illustrerte guiden som beskriver l√¶ringsreisen for denne leksjonen ‚Äì fra √• l√¶re kjernebegrepene og motivasjonen for finjustering, til √• forst√• prosessen og beste praksis for √• utf√∏re finjusteringsoppgaven. Dette er et spennende tema √• utforske, s√• ikke glem √• sjekke [Ressurser](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) for flere lenker som st√∏tter din selvstyrte l√¶ringsreise!

![Illustrert guide til finjustering av spr√•kmodeller](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.no.png)

## Hva er finjustering for spr√•kmodeller?

Per definisjon er store spr√•kmodeller _forh√•ndstrent_ p√• store mengder tekst hentet fra ulike kilder, inkludert internett. Som vi har l√¶rt i tidligere leksjoner, trenger vi teknikker som _prompt engineering_ og _retrieval-augmented generation_ for √• forbedre kvaliteten p√• modellens svar p√• brukerens sp√∏rsm√•l ("prompter").

En popul√¶r prompt-engineering-teknikk inneb√¶rer √• gi modellen mer veiledning om hva som forventes i svaret, enten ved √• gi _instruksjoner_ (eksplisitt veiledning) eller _noen f√• eksempler_ (implisitt veiledning). Dette kalles _few-shot learning_, men har to begrensninger:

- Modellens token-grenser kan begrense antall eksempler du kan gi, og dermed redusere effektiviteten.
- Kostnader knyttet til tokens kan gj√∏re det dyrt √• legge til eksempler i hver prompt, og begrense fleksibiliteten.

Finjustering er en vanlig praksis i maskinl√¶ringssystemer hvor man tar en forh√•ndstrent modell og trener den p√• nytt med nye data for √• forbedre ytelsen p√• en spesifikk oppgave. I sammenheng med spr√•kmodeller kan vi finjustere den forh√•ndstrente modellen _med et kuratert sett av eksempler for en gitt oppgave eller bruksomr√•de_ for √• lage en **tilpasset modell** som kan v√¶re mer n√∏yaktig og relevant for akkurat den oppgaven eller domenet. En ekstra fordel med finjustering er at det ogs√• kan redusere antall eksempler som trengs for few-shot learning ‚Äì noe som reduserer token-bruk og tilh√∏rende kostnader.

## N√•r og hvorfor b√∏r vi finjustere modeller?

I _denne_ sammenhengen, n√•r vi snakker om finjustering, refererer vi til **supervised** finjustering hvor ny trening gj√∏res ved √• **legge til nye data** som ikke var en del av det opprinnelige treningssettet. Dette skiller seg fra en usupervised finjustering der modellen trenes p√• nytt med de opprinnelige dataene, men med andre hyperparametere.

Det viktigste √• huske er at finjustering er en avansert teknikk som krever et visst niv√• av ekspertise for √• oppn√• √∏nskede resultater. Gj√∏r man det feil, kan det hende man ikke f√•r de forventede forbedringene, og det kan til og med forringe modellens ytelse for det aktuelle domenet.

S√•, f√∏r du l√¶rer "hvordan" du finjusterer spr√•kmodeller, m√• du vite "hvorfor" du b√∏r velge denne veien, og "n√•r" du skal starte finjusteringsprosessen. Start med √• stille deg selv disse sp√∏rsm√•lene:

- **Bruksomr√•de**: Hva er ditt _bruksomr√•de_ for finjustering? Hvilke aspekter ved den n√•v√¶rende forh√•ndstrente modellen √∏nsker du √• forbedre?
- **Alternativer**: Har du pr√∏vd _andre teknikker_ for √• oppn√• √∏nskede resultater? Bruk dem for √• lage en referanse for sammenligning.
  - Prompt engineering: Pr√∏v teknikker som few-shot prompting med eksempler p√• relevante prompt-svar. Evaluer kvaliteten p√• svarene.
  - Retrieval Augmented Generation: Pr√∏v √• utvide promptene med s√∏keresultater hentet fra dine data. Evaluer kvaliteten p√• svarene.
- **Kostnader**: Har du identifisert kostnadene ved finjustering?
  - Mulighet for finjustering ‚Äì er den forh√•ndstrente modellen tilgjengelig for finjustering?
  - Innsats ‚Äì for √• forberede treningsdata, evaluere og forbedre modellen.
  - Beregningsressurser ‚Äì for √• kj√∏re finjusteringsjobber og distribuere den finjusterte modellen.
  - Data ‚Äì tilgang til tilstrekkelig kvalitetsdata for at finjusteringen skal ha effekt.
- **Fordeler**: Har du bekreftet fordelene ved finjustering?
  - Kvalitet ‚Äì overgikk den finjusterte modellen referansemodellen?
  - Kostnad ‚Äì reduserer det token-bruken ved √• forenkle promptene?
  - Utvidbarhet ‚Äì kan du gjenbruke basismodellen for nye domener?

Ved √• svare p√• disse sp√∏rsm√•lene b√∏r du kunne avgj√∏re om finjustering er riktig tiln√¶rming for ditt bruksomr√•de. Ideelt sett er tiln√¶rmingen gyldig bare hvis fordelene oppveier kostnadene. N√•r du har bestemt deg for √• g√• videre, er det p√• tide √• tenke p√• _hvordan_ du kan finjustere den forh√•ndstrente modellen.

Vil du ha flere innsikter om beslutningsprosessen? Se [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Hvordan kan vi finjustere en forh√•ndstrent modell?

For √• finjustere en forh√•ndstrent modell trenger du:

- en forh√•ndstrent modell som kan finjusteres
- et datasett som skal brukes til finjustering
- et treningsmilj√∏ for √• kj√∏re finjusteringsjobben
- et hostingmilj√∏ for √• distribuere den finjusterte modellen

## Finjustering i praksis

F√∏lgende ressurser gir trinnvise veiledninger som tar deg gjennom et ekte eksempel med en valgt modell og et kuratert datasett. For √• jobbe med disse veiledningene trenger du en konto hos den aktuelle leverand√∏ren, samt tilgang til den relevante modellen og datasettene.

| Leverand√∏r   | Veiledning                                                                                                                                                                   | Beskrivelse                                                                                                                                                                                                                                                                                                                                                                                                                      |
| ------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)              | L√¶r hvordan du finjusterer en `gpt-35-turbo` for et spesifikt domene ("oppskriftassistent") ved √• forberede treningsdata, kj√∏re finjusteringsjobben og bruke den finjusterte modellen til inferens.                                                                                                                                                                                                                              |
| Azure OpenAI | [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | L√¶r hvordan du finjusterer en `gpt-35-turbo-0613` modell **p√• Azure** ved √• lage og laste opp treningsdata, kj√∏re finjusteringsjobben, distribuere og bruke den nye modellen.                                                                                                                                                                                                                                                   |
| Hugging Face | [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                             | Dette blogginnlegget viser hvordan du finjusterer en _√•pen LLM_ (f.eks. `CodeLlama 7B`) ved hjelp av [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteket og [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) med √•pne [datasett](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) p√• Hugging Face. |
|              |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                |
| ü§ó AutoTrain | [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                       | AutoTrain (eller AutoTrain Advanced) er et Python-bibliotek utviklet av Hugging Face som lar deg finjustere mange forskjellige oppgaver, inkludert LLM-finjustering. AutoTrain er en kodefri l√∏sning, og finjustering kan gj√∏res i din egen sky, p√• Hugging Face Spaces eller lokalt. Den st√∏tter b√•de web-basert GUI, CLI og trening via yaml-konfigurasjonsfiler.                                                                 |
|              |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                |

## Oppgave

Velg en av veiledningene ovenfor og g√• gjennom den. _Vi kan komme til √• lage en versjon av disse veiledningene i Jupyter Notebooks i dette repoet kun som referanse. Bruk de originale kildene direkte for √• f√• de nyeste versjonene_.

## Flott jobba! Fortsett l√¶ringen din.

Etter √• ha fullf√∏rt denne leksjonen, sjekk ut v√•r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for √• fortsette √• utvikle din kunnskap om Generativ AI!

Gratulerer!! Du har fullf√∏rt den siste leksjonen i v2-serien for dette kurset! Ikke slutt √• l√¶re og bygge. \*\*Sjekk ut [RESSURSER](RESOURCES.md?WT.mc_id=academic-105485-koreyst) for en liste over flere forslag om akkurat dette temaet.

V√•r v1-serie med leksjoner er ogs√• oppdatert med flere oppgaver og konsepter. Ta deg et √∏yeblikk til √• friske opp kunnskapen din ‚Äì og vennligst [del dine sp√∏rsm√•l og tilbakemeldinger](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) for √• hjelpe oss med √• forbedre disse leksjonene for fellesskapet.

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, vennligst v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det opprinnelige dokumentet p√• originalspr√•ket skal anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.