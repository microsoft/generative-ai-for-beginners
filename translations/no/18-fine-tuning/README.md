<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "807f0d9fc1747e796433534e1be6a98a",
  "translation_date": "2025-10-17T19:23:18+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "no"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.no.png)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# Finjustering av din LLM

칀 bruke store spr친kmodeller for 친 bygge generative AI-applikasjoner kommer med nye utfordringer. En viktig problemstilling er 친 sikre kvaliteten p친 svarene (n칮yaktighet og relevans) i innholdet som genereres av modellen for en gitt brukerforesp칮rsel. I tidligere leksjoner diskuterte vi teknikker som prompt engineering og retrieval-augmented generation som fors칮ker 친 l칮se problemet ved 친 _modifisere prompt-inndata_ til den eksisterende modellen.

I dagens leksjon diskuterer vi en tredje teknikk, **finjustering**, som pr칮ver 친 takle utfordringen ved 친 _trene modellen p친 nytt_ med ekstra data. La oss dykke ned i detaljene.

## L칝ringsm친l

Denne leksjonen introduserer konseptet finjustering for forh친ndstrente spr친kmodeller, utforsker fordelene og utfordringene ved denne tiln칝rmingen, og gir veiledning om n친r og hvordan man kan bruke finjustering for 친 forbedre ytelsen til dine generative AI-modeller.

Ved slutten av denne leksjonen b칮r du kunne svare p친 f칮lgende sp칮rsm친l:

- Hva er finjustering for spr친kmodeller?
- N친r, og hvorfor, er finjustering nyttig?
- Hvordan kan jeg finjustere en forh친ndstrent modell?
- Hva er begrensningene ved finjustering?

Klar? La oss komme i gang.

## Illustrert guide

Vil du f친 et overblikk over hva vi skal dekke f칮r vi dykker inn? Sjekk ut denne illustrerte guiden som beskriver l칝ringsreisen for denne leksjonen - fra 친 l칝re de grunnleggende konseptene og motivasjonen for finjustering, til 친 forst친 prosessen og beste praksis for 친 utf칮re finjusteringsoppgaven. Dette er et fascinerende tema 친 utforske, s친 ikke glem 친 sjekke ut [Ressurser](./RESOURCES.md?WT.mc_id=academic-105485-koreyst)-siden for ekstra lenker som st칮tter din selvstyrte l칝ringsreise!

![Illustrert guide til finjustering av spr친kmodeller](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.no.png)

## Hva er finjustering for spr친kmodeller?

Per definisjon er store spr친kmodeller _forh친ndstrente_ p친 store mengder tekst hentet fra ulike kilder, inkludert internett. Som vi har l칝rt i tidligere leksjoner, trenger vi teknikker som _prompt engineering_ og _retrieval-augmented generation_ for 친 forbedre kvaliteten p친 modellens svar p친 brukerens sp칮rsm친l ("prompts").

En popul칝r prompt-engineering-teknikk inneb칝rer 친 gi modellen mer veiledning om hva som forventes i svaret, enten ved 친 gi _instruksjoner_ (eksplisitt veiledning) eller _gi den noen eksempler_ (implisitt veiledning). Dette kalles _few-shot learning_, men det har to begrensninger:

- Modellens token-grenser kan begrense antall eksempler du kan gi, og dermed effektiviteten.
- Kostnadene for tokens kan gj칮re det dyrt 친 legge til eksempler i hver prompt, og begrense fleksibiliteten.

Finjustering er en vanlig praksis i maskinl칝ringssystemer der vi tar en forh친ndstrent modell og trener den p친 nytt med nye data for 친 forbedre ytelsen p친 en spesifikk oppgave. I konteksten av spr친kmodeller kan vi finjustere den forh친ndstrente modellen _med et kuratert sett av eksempler for en gitt oppgave eller applikasjonsdomene_ for 친 lage en **tilpasset modell** som kan v칝re mer n칮yaktig og relevant for den spesifikke oppgaven eller domenet. En sidefordel med finjustering er at det ogs친 kan redusere antall eksempler som trengs for few-shot learning - og dermed redusere tokenbruk og relaterte kostnader.

## N친r og hvorfor b칮r vi finjustere modeller?

I _denne_ konteksten, n친r vi snakker om finjustering, refererer vi til **supervisert** finjustering der treningen gj칮res ved 친 **legge til nye data** som ikke var en del av det opprinnelige treningsdatasettet. Dette er forskjellig fra en usupervisert finjusteringstiln칝rming der modellen trenes p친 nytt med det opprinnelige datasettet, men med forskjellige hyperparametere.

Det viktigste 친 huske er at finjustering er en avansert teknikk som krever et visst niv친 av ekspertise for 친 oppn친 칮nskede resultater. Hvis det gj칮res feil, kan det hende at det ikke gir de forventede forbedringene, og det kan til og med forringe modellens ytelse for ditt m친lrettede domene.

S친 f칮r du l칝rer "hvordan" du kan finjustere spr친kmodeller, m친 du vite "hvorfor" du b칮r ta denne veien, og "n친r" du skal starte prosessen med finjustering. Begynn med 친 stille deg selv disse sp칮rsm친lene:

- **Brukstilfelle**: Hva er ditt _brukstilfelle_ for finjustering? Hvilket aspekt ved den n친v칝rende forh친ndstrente modellen 칮nsker du 친 forbedre?
- **Alternativer**: Har du pr칮vd _andre teknikker_ for 친 oppn친 칮nskede resultater? Bruk dem til 친 lage en baseline for sammenligning.
  - Prompt engineering: Pr칮v teknikker som few-shot prompting med eksempler p친 relevante prompt-svar. Evaluer kvaliteten p친 svarene.
  - Retrieval Augmented Generation: Pr칮v 친 utvide prompts med sp칮rringsresultater hentet fra s칮k i dine data. Evaluer kvaliteten p친 svarene.
- **Kostnader**: Har du identifisert kostnadene for finjustering?
  - Justerbarhet - er den forh친ndstrente modellen tilgjengelig for finjustering?
  - Innsats - for 친 forberede treningsdata, evaluere og forbedre modellen.
  - Datakraft - for 친 kj칮re finjusteringsjobber og distribuere finjustert modell.
  - Data - tilgang til tilstrekkelige kvalitets-eksempler for finjusteringsp친virkning.
- **Fordeler**: Har du bekreftet fordelene ved finjustering?
  - Kvalitet - overgikk den finjusterte modellen baseline?
  - Kostnad - reduserer det tokenbruk ved 친 forenkle prompts?
  - Utvidbarhet - kan du gjenbruke basismodellen for nye domener?

Ved 친 svare p친 disse sp칮rsm친lene, b칮r du kunne avgj칮re om finjustering er riktig tiln칝rming for ditt brukstilfelle. Ideelt sett er tiln칝rmingen gyldig bare hvis fordelene oppveier kostnadene. N친r du bestemmer deg for 친 g친 videre, er det p친 tide 친 tenke p친 _hvordan_ du kan finjustere den forh친ndstrente modellen.

Vil du ha mer innsikt i beslutningsprosessen? Se [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Hvordan kan vi finjustere en forh친ndstrent modell?

For 친 finjustere en forh친ndstrent modell, trenger du:

- en forh친ndstrent modell 친 finjustere
- et datasett 친 bruke for finjustering
- et treningsmilj칮 for 친 kj칮re finjusteringsjobben
- et hostingmilj칮 for 친 distribuere den finjusterte modellen

## Finjustering i praksis

F칮lgende ressurser gir steg-for-steg veiledninger for 친 g친 gjennom et reelt eksempel ved bruk av en valgt modell med et kuratert datasett. For 친 jobbe gjennom disse veiledningene, trenger du en konto hos den spesifikke leverand칮ren, sammen med tilgang til relevante modeller og datasett.

| Leverand칮r   | Veiledning                                                                                                                                                                     | Beskrivelse                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                | L칝r 친 finjustere en `gpt-35-turbo` for et spesifikt domene ("oppskriftsassistent") ved 친 forberede treningsdata, kj칮re finjusteringsjobben, og bruke den finjusterte modellen for inferens.                                                                                                                                                                                                                                          |
| Azure OpenAI | [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | L칝r 친 finjustere en `gpt-35-turbo-0613` modell **p친 Azure** ved 친 ta steg for 친 lage og laste opp treningsdata, kj칮re finjusteringsjobben. Distribuer og bruk den nye modellen.                                                                                                                                                                                                                                                      |
| Hugging Face | [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Denne bloggposten guider deg gjennom finjustering av en _친pen LLM_ (eks: `CodeLlama 7B`) ved bruk av [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst)-biblioteket & [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) med 친pne [datasett](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) p친 Hugging Face. |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 游뱅 AutoTrain | [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                         | AutoTrain (eller AutoTrain Advanced) er et Python-bibliotek utviklet av Hugging Face som tillater finjustering for mange forskjellige oppgaver, inkludert LLM-finjustering. AutoTrain er en kodefri l칮sning, og finjustering kan gj칮res i din egen sky, p친 Hugging Face Spaces eller lokalt. Det st칮tter b친de en web-basert GUI, CLI og trening via yaml-konfigurasjonsfiler.                                                       |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Oppgave

Velg en av veiledningene ovenfor og g친 gjennom dem. _Vi kan replikere en versjon av disse veiledningene i Jupyter Notebooks i dette repoet kun for referanse. Vennligst bruk de originale kildene direkte for 친 f친 de nyeste versjonene_.

## Flott arbeid! Fortsett l칝ringen din.

Etter 친 ha fullf칮rt denne leksjonen, sjekk ut v친r [Generative AI Learning-samling](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for 친 fortsette 친 bygge opp din kunnskap om generativ AI!

Gratulerer!! Du har fullf칮rt den siste leksjonen fra v2-serien for dette kurset! Ikke slutt 친 l칝re og bygge. \*\*Sjekk ut [RESSURSER](RESOURCES.md?WT.mc_id=academic-105485-koreyst)-siden for en liste over ekstra forslag for akkurat dette temaet.

V친r v1-serie med leksjoner har ogs친 blitt oppdatert med flere oppgaver og konsepter. S친 ta et 칮yeblikk for 친 friske opp kunnskapen din - og vennligst [del dine sp칮rsm친l og tilbakemeldinger](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) for 친 hjelpe oss med 친 forbedre disse leksjonene for fellesskapet.

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n칮yaktighet, v칝r oppmerksom p친 at automatiserte oversettelser kan inneholde feil eller un칮yaktigheter. Det originale dokumentet p친 sitt opprinnelige spr친k b칮r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst친elser eller feiltolkninger som oppst친r ved bruk av denne oversettelsen.