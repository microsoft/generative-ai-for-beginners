<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-05-20T07:50:46+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "no"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.8487555c3e3225eefc1dc84e72c8e00bce1ee76db867a080628fb0fbb04aa0d2.no.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Tilpasse din LLM

Bruk av store spr친kmodeller for 친 bygge generative AI-applikasjoner inneb칝rer nye utfordringer. En viktig problemstilling er 친 sikre responskvaliteten (n칮yaktighet og relevans) i innholdet som genereres av modellen for en gitt brukerforesp칮rsel. I tidligere leksjoner har vi diskutert teknikker som prompt engineering og retrieval-augmented generation som pr칮ver 친 l칮se problemet ved 친 _modifisere prompt-inngangen_ til den eksisterende modellen.

I dagens leksjon diskuterer vi en tredje teknikk, **tilpasning**, som pr칮ver 친 takle utfordringen ved _친 trene modellen p친 nytt_ med ekstra data. La oss dykke inn i detaljene.

## L칝ringsm친l

Denne leksjonen introduserer konseptet med tilpasning for forh친ndstrente spr친kmodeller, utforsker fordelene og utfordringene ved denne tiln칝rmingen, og gir veiledning om n친r og hvordan du kan bruke tilpasning for 친 forbedre ytelsen til dine generative AI-modeller.

Ved slutten av denne leksjonen b칮r du kunne svare p친 f칮lgende sp칮rsm친l:

- Hva er tilpasning for spr친kmodeller?
- N친r, og hvorfor, er tilpasning nyttig?
- Hvordan kan jeg tilpasse en forh친ndstrent modell?
- Hva er begrensningene ved tilpasning?

Klar? La oss komme i gang.

## Illustrert guide

Vil du f친 det store bildet av hva vi skal dekke f칮r vi dykker inn? Sjekk ut denne illustrerte guiden som beskriver l칝ringsreisen for denne leksjonen - fra 친 l칝re de grunnleggende konseptene og motivasjonen for tilpasning, til 친 forst친 prosessen og beste praksis for 친 utf칮re tilpasningsoppgaven. Dette er et fascinerende emne 친 utforske, s친 ikke glem 친 sjekke ut [Resources](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) siden for ekstra lenker som st칮tter din selvguidede l칝ringsreise!

![Illustrert guide til tilpasning av spr친kmodeller](../../../translated_images/18-fine-tuning-sketchnote.92733966235199dd260184b1aae3a84b877c7496bc872d8e63ad6fa2dd96bafc.no.png)

## Hva er tilpasning for spr친kmodeller?

Per definisjon er store spr친kmodeller _forh친ndstrente_ p친 store mengder tekst hentet fra ulike kilder, inkludert internett. Som vi har l칝rt i tidligere leksjoner, trenger vi teknikker som _prompt engineering_ og _retrieval-augmented generation_ for 친 forbedre kvaliteten p친 modellens svar p친 brukerens sp칮rsm친l ("prompts").

En popul칝r prompt-engineering teknikk involverer 친 gi modellen mer veiledning om hva som forventes i svaret enten ved 친 gi _instruksjoner_ (eksplisitt veiledning) eller _gi den noen f친 eksempler_ (implisitt veiledning). Dette kalles _few-shot learning_, men det har to begrensninger:

- Modellens tokenbegrensninger kan begrense antall eksempler du kan gi, og begrense effektiviteten.
- Modellens tokenkostnader kan gj칮re det dyrt 친 legge til eksempler i hver prompt, og begrense fleksibiliteten.

Tilpasning er en vanlig praksis i maskinl칝ringssystemer hvor vi tar en forh친ndstrent modell og trener den p친 nytt med nye data for 친 forbedre ytelsen p친 en spesifikk oppgave. I sammenheng med spr친kmodeller kan vi tilpasse den forh친ndstrente modellen _med et kuratert sett av eksempler for en gitt oppgave eller applikasjonsdomene_ for 친 lage en **tilpasset modell** som kan v칝re mer n칮yaktig og relevant for den spesifikke oppgaven eller domenet. En sidefordel med tilpasning er at det ogs친 kan redusere antall eksempler som trengs for few-shot learning - redusere tokenbruk og relaterte kostnader.

## N친r og hvorfor b칮r vi tilpasse modeller?

I _denne_ sammenhengen, n친r vi snakker om tilpasning, refererer vi til **supervised** tilpasning hvor treningen gj칮res ved **친 legge til nye data** som ikke var en del av det opprinnelige treningsdatasettet. Dette er forskjellig fra en unsupervised tilpasningsmetode hvor modellen trenes p친 nytt p친 de opprinnelige dataene, men med forskjellige hyperparametere.

Det viktige 친 huske er at tilpasning er en avansert teknikk som krever et visst niv친 av ekspertise for 친 oppn친 de 칮nskede resultatene. Hvis det gj칮res feil, kan det hende at det ikke gir de forventede forbedringene, og det kan til og med forringe ytelsen til modellen for ditt m친lrettede domene.

S친, f칮r du l칝rer "hvordan" du skal tilpasse spr친kmodeller, m친 du vite "hvorfor" du b칮r ta denne ruten, og "n친r" du skal starte prosessen med tilpasning. Begynn med 친 stille deg selv disse sp칮rsm친lene:

- **Brukstilfelle**: Hva er ditt _brukstilfelle_ for tilpasning? Hvilken del av den n친v칝rende forh친ndstrente modellen vil du forbedre?
- **Alternativer**: Har du pr칮vd _andre teknikker_ for 친 oppn친 de 칮nskede resultatene? Bruk dem til 친 lage en baseline for sammenligning.
  - Prompt engineering: Pr칮v teknikker som f친-shot prompting med eksempler p친 relevante prompt-svar. Evaluer kvaliteten p친 svarene.
  - Retrieval Augmented Generation: Pr칮v 친 utvide prompts med sp칮rringsresultater hentet ved 친 s칮ke i dataene dine. Evaluer kvaliteten p친 svarene.
- **Kostnader**: Har du identifisert kostnadene for tilpasning?
  - Tilpasningsmulighet - er den forh친ndstrente modellen tilgjengelig for tilpasning?
  - Innsats - for 친 forberede treningsdata, evaluere og forbedre modellen.
  - Beregning - for 친 kj칮re tilpasningsjobber, og distribuere tilpasset modell
  - Data - tilgang til tilstrekkelige kvalitets eksempler for tilpasningsp친virkning
- **Fordeler**: Har du bekreftet fordelene ved tilpasning?
  - Kvalitet - overgikk den tilpassede modellen baseline?
  - Kostnad - reduserer det tokenbruk ved 친 forenkle prompts?
  - Utvidbarhet - kan du gjenbruke basismodellen for nye domener?

Ved 친 svare p친 disse sp칮rsm친lene, b칮r du kunne avgj칮re om tilpasning er den rette tiln칝rmingen for ditt brukstilfelle. Ideelt sett er tiln칝rmingen gyldig bare hvis fordelene oppveier kostnadene. N친r du bestemmer deg for 친 fortsette, er det p친 tide 친 tenke p친 _hvordan_ du kan tilpasse den forh친ndstrente modellen.

Vil du f친 mer innsikt i beslutningsprosessen? Se [칀 tilpasse eller ikke tilpasse](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Hvordan kan vi tilpasse en forh친ndstrent modell?

For 친 tilpasse en forh친ndstrent modell, trenger du:

- en forh친ndstrent modell 친 tilpasse
- et datasett 친 bruke til tilpasning
- et treningsmilj칮 for 친 kj칮re tilpasningsjobben
- et vertsmilj칮 for 친 distribuere tilpasset modell

## Tilpasning i praksis

F칮lgende ressurser gir trinnvise veiledninger for 친 g친 gjennom et reelt eksempel ved bruk av en valgt modell med et kuratert datasett. For 친 arbeide gjennom disse veiledningene, trenger du en konto hos den spesifikke leverand칮ren, sammen med tilgang til den relevante modellen og datasettene.

| Leverand칮r   | Veiledning                                                                                                                                                                     | Beskrivelse                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [Hvordan tilpasse chat-modeller](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)               | L칝r 친 tilpasse en `gpt-35-turbo` for et spesifikt domene ("oppskrift assistent") ved 친 forberede treningsdata, kj칮re tilpasningsjobben, og bruke den tilpassede modellen til inferens.                                                                                                                                                                                                                                             |
| Azure OpenAI | [GPT 3.5 Turbo tilpasningsveiledning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | L칝r 친 tilpasse en `gpt-35-turbo-0613` modell **p친 Azure** ved 친 ta steg for 친 lage og laste opp treningsdata, kj칮re tilpasningsjobben. Distribuer og bruk den nye modellen.                                                                                                                                                                                                                                                          |
| Hugging Face | [Tilpasse LLMs med Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                                   | Denne bloggposten guider deg gjennom tilpasning av en _친pen LLM_ (eks: `CodeLlama 7B`) ved bruk av [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteket & [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) med 친pne [datasets](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) p친 Hugging Face. |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 游뱅 AutoTrain | [Tilpasse LLMs med AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                             | AutoTrain (eller AutoTrain Advanced) er et python-bibliotek utviklet av Hugging Face som tillater tilpasning for mange forskjellige oppgaver inkludert LLM tilpasning. AutoTrain er en no-code l칮sning og tilpasning kan gj칮res i din egen sky, p친 Hugging Face Spaces eller lokalt. Det st칮tter b친de en web-basert GUI, CLI og trening via yaml-konfigurasjonsfiler.                                                              |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Oppgave

Velg en av veiledningene ovenfor og g친 gjennom dem. _Vi kan replikere en versjon av disse veiledningene i Jupyter Notebooks i dette repoet kun for referanse. Vennligst bruk de opprinnelige kildene direkte for 친 f친 de nyeste versjonene_.

## Flott arbeid! Fortsett din l칝ring.

Etter 친 ha fullf칮rt denne leksjonen, sjekk ut v친r [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) for 친 fortsette 친 heve din Generative AI kunnskap!

Gratulerer!! Du har fullf칮rt den siste leksjonen fra v2-serien for dette kurset! Ikke stopp 친 l칝re og bygge. \*\*Sjekk ut [RESOURCES](RESOURCES.md?WT.mc_id=academic-105485-koreyst) siden for en liste over ekstra forslag for akkurat dette emnet.

V친r v1-serie av leksjoner har ogs친 blitt oppdatert med flere oppgaver og konsepter. S친 ta et minutt for 친 friske opp kunnskapen din - og vennligst [del dine sp칮rsm친l og tilbakemeldinger](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) for 친 hjelpe oss med 친 forbedre disse leksjonene for fellesskapet.

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber n칮yaktighet, v칝r oppmerksom p친 at automatiserte oversettelser kan inneholde feil eller un칮yaktigheter. Det originale dokumentet p친 sitt opprinnelige spr친k b칮r betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst친elser eller feiltolkninger som oppst친r ved bruk av denne oversettelsen.