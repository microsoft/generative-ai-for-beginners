{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduksjon\n",
    "\n",
    "Denne leksjonen vil ta for seg:\n",
    "- Hva funksjonskall er og når det brukes\n",
    "- Hvordan lage et funksjonskall med Azure OpenAI\n",
    "- Hvordan integrere et funksjonskall i en applikasjon\n",
    "\n",
    "## Læringsmål\n",
    "\n",
    "Etter å ha fullført denne leksjonen vil du vite og forstå:\n",
    "\n",
    "- Hensikten med å bruke funksjonskall\n",
    "- Sette opp funksjonskall med Azure Open AI Service\n",
    "- Designe effektive funksjonskall tilpasset din applikasjons behov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forstå funksjonskall\n",
    "\n",
    "I denne leksjonen skal vi lage en funksjon for vår utdanningsstartup som lar brukere bruke en chatbot for å finne tekniske kurs. Vi vil anbefale kurs som passer til deres ferdighetsnivå, nåværende rolle og teknologi de er interessert i.\n",
    "\n",
    "For å få til dette vil vi bruke en kombinasjon av:\n",
    " - `Azure Open AI` for å lage en chatopplevelse for brukeren\n",
    " - `Microsoft Learn Catalog API` for å hjelpe brukere å finne kurs basert på deres forespørsel\n",
    " - `Function Calling` for å ta brukerens spørsmål og sende det til en funksjon som gjør API-forespørselen.\n",
    "\n",
    "La oss først se på hvorfor vi ønsker å bruke funksjonskall i det hele tatt:\n",
    "\n",
    "print(\"Meldinger i neste forespørsel:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # få et nytt svar fra GPT hvor den kan se funksjonens respons\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hvorfor bruke Function Calling\n",
    "\n",
    "Hvis du har fullført noen av de andre leksjonene i dette kurset, forstår du sikkert hvor kraftige Large Language Models (LLMs) kan være. Forhåpentligvis har du også lagt merke til noen av begrensningene deres.\n",
    "\n",
    "Function Calling er en funksjon i Azure Open AI Service som hjelper til med å overvinne følgende begrensninger:\n",
    "1) Konsistent responsformat\n",
    "2) Mulighet til å bruke data fra andre kilder i en applikasjon i en chatkontekst\n",
    "\n",
    "Før function calling var svarene fra en LLM ustrukturerte og inkonsistente. Utviklere måtte skrive komplisert valideringskode for å kunne håndtere alle variasjoner i svarene.\n",
    "\n",
    "Brukere kunne ikke få svar som \"Hva er været i Stockholm akkurat nå?\". Dette skyldes at modellene var begrenset til tidspunktet dataene ble trent på.\n",
    "\n",
    "La oss se på eksempelet under som illustrerer dette problemet:\n",
    "\n",
    "La oss si at vi ønsker å lage en database med studentdata slik at vi kan foreslå riktig kurs til dem. Under har vi to beskrivelser av studenter som inneholder veldig like data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ønsker å sende dette til en LLM for å analysere dataene. Dette kan senere brukes i applikasjonen vår for å sende det til et API eller lagre det i en database.\n",
    "\n",
    "La oss lage to identiske forespørsler der vi instruerer LLM om hvilken informasjon vi er interessert i:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ønsker å sende dette til en LLM for å analysere delene som er viktige for vårt produkt. Slik kan vi lage to identiske instruksjoner for å veilede LLM-en:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etter å ha laget disse to promptene, sender vi dem til LLM ved å bruke `openai.ChatCompletion`. Vi lagrer prompten i variabelen `messages` og tildeler rollen til `user`. Dette er for å etterligne en melding fra en bruker som skrives til en chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selv om promptene er de samme og beskrivelsene ligner, kan vi få ulike formater på `Grades`-egenskapen.\n",
    "\n",
    "Hvis du kjører cellen over flere ganger, kan formatet være `3.7` eller `3.7 GPA`.\n",
    "\n",
    "Dette skjer fordi LLM-en tar inn ustrukturert data i form av den skrevne prompten og returnerer også ustrukturert data. Vi trenger et strukturert format slik at vi vet hva vi kan forvente når vi skal lagre eller bruke denne dataen.\n",
    "\n",
    "Ved å bruke funksjonskall kan vi sørge for at vi får tilbake strukturert data. Når vi bruker funksjonskall, kjører eller utfører ikke LLM-en faktisk noen funksjoner. I stedet lager vi en struktur som LLM-en skal følge i svarene sine. Vi bruker så disse strukturerte svarene for å vite hvilken funksjon vi skal kjøre i applikasjonene våre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruksområder for å bruke funksjonskall\n",
    "\n",
    "**Kalle eksterne verktøy**  \n",
    "Chatboter er flinke til å gi svar på spørsmål fra brukere. Ved å bruke funksjonskall kan chatbotene bruke meldinger fra brukerne til å utføre bestemte oppgaver. For eksempel kan en student be chatboten om å \"Sende e-post til læreren min og si at jeg trenger mer hjelp med dette emnet\". Dette kan gjøre et funksjonskall til `send_email(to: string, body: string)`\n",
    "\n",
    "**Lage API- eller databaseforespørsler**  \n",
    "Brukere kan finne informasjon ved å bruke naturlig språk som blir omgjort til en formatert spørring eller API-forespørsel. Et eksempel på dette kan være en lærer som spør \"Hvem er studentene som fullførte siste oppgave\", som kan kalle en funksjon som heter `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Lage strukturert data**  \n",
    "Brukere kan ta en tekstblokk eller CSV og bruke LLM til å hente ut viktig informasjon fra den. For eksempel kan en student gjøre om en Wikipedia-artikkel om fredsavtaler for å lage AI-flashkort. Dette kan gjøres ved å bruke en funksjon som heter `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lage ditt første funksjonskall\n",
    "\n",
    "Prosessen med å lage et funksjonskall består av tre hovedsteg:\n",
    "1. Kall Chat Completions API med en liste over funksjonene dine og en brukermelding\n",
    "2. Les modellens svar for å utføre en handling, for eksempel kjøre en funksjon eller et API-kall\n",
    "3. Gjør et nytt kall til Chat Completions API med svaret fra funksjonen din for å bruke denne informasjonen til å lage et svar til brukeren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementer i et funksjonskall\n",
    "\n",
    "#### Brukerens inndata\n",
    "\n",
    "Første steg er å lage en brukermelding. Dette kan settes dynamisk ved å hente verdien fra et tekstfelt, eller du kan sette en verdi her. Hvis dette er første gang du jobber med Chat Completions API, må vi definere `role` og `content` for meldingen.\n",
    "\n",
    "`role` kan være enten `system` (lager regler), `assistant` (modellen) eller `user` (sluttbrukeren). For funksjonskall setter vi dette til `user` og gir et eksempelspørsmål.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lage funksjoner.\n",
    "\n",
    "Nå skal vi definere en funksjon og parameterne til den funksjonen. Vi bruker bare én funksjon her som heter `search_courses`, men du kan lage flere funksjoner.\n",
    "\n",
    "**Viktig**: Funksjoner blir lagt til i systemmeldingen til LLM-en og vil telle med i antall tilgjengelige tokens du har.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definisjoner**\n",
    "\n",
    "`name` - Navnet på funksjonen som vi ønsker skal bli kalt.\n",
    "\n",
    "`description` - Dette er en beskrivelse av hvordan funksjonen fungerer. Her er det viktig å være spesifikk og tydelig.\n",
    "\n",
    "`parameters` - En liste over verdier og formatet du ønsker at modellen skal bruke i sitt svar.\n",
    "\n",
    "`type` - Datatypen som egenskapene vil bli lagret i.\n",
    "\n",
    "`properties` - Liste over de spesifikke verdiene som modellen vil bruke i sitt svar.\n",
    "\n",
    "`name` - Navnet på egenskapen som modellen vil bruke i sitt formaterte svar.\n",
    "\n",
    "`type` - Datatypen til denne egenskapen.\n",
    "\n",
    "`description` - Beskrivelse av den spesifikke egenskapen.\n",
    "\n",
    "**Valgfritt**\n",
    "\n",
    "`required` - Påkrevd egenskap for at funksjonskallet skal kunne fullføres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lage funksjonskallet\n",
    "Etter at vi har definert en funksjon, må vi nå inkludere den i kallet til Chat Completion API. Dette gjør vi ved å legge til `functions` i forespørselen. I dette tilfellet `functions=functions`.\n",
    "\n",
    "Det finnes også et alternativ for å sette `function_call` til `auto`. Dette betyr at vi lar LLM-en bestemme hvilken funksjon som skal kalles basert på brukermeldingen, i stedet for å tildele det selv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå kan vi se på svaret og hvordan det er formatert:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Du kan se at navnet på funksjonen blir kalt, og fra brukermeldingen klarte LLM å finne dataene som passer til argumentene for funksjonen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrere funksjonskall i en applikasjon.\n",
    "\n",
    "Etter at vi har testet det formaterte svaret fra LLM, kan vi nå integrere dette i en applikasjon.\n",
    "\n",
    "### Håndtere flyten\n",
    "\n",
    "For å integrere dette i applikasjonen vår, la oss følge disse stegene:\n",
    "\n",
    "Først gjør vi et kall til Open AI-tjenestene og lagrer meldingen i en variabel som heter `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå skal vi definere funksjonen som vil kalle Microsoft Learn API for å hente en liste over kurs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som en god praksis vil vi deretter se om modellen ønsker å kalle en funksjon. Etter det vil vi opprette en av de tilgjengelige funksjonene og matche den til funksjonen som blir kalt.\n",
    "Deretter tar vi argumentene til funksjonen og kobler dem til argumentene fra LLM.\n",
    "\n",
    "Til slutt legger vi til meldingen om funksjonskallet og verdiene som ble returnert av `search_courses`-meldingen. Dette gir LLM all informasjonen den trenger for å\n",
    "svare brukeren med naturlig språk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kodeutfordring\n",
    "\n",
    "Godt jobbet! For å fortsette læringen om Azure Open AI Function Calling kan du bygge: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Flere parametere for funksjonen som kan hjelpe brukere med å finne flere kurs. Du finner tilgjengelige API-parametere her:\n",
    " - Lag en ny funksjonskall som tar imot mer informasjon fra brukeren, som for eksempel morsmål\n",
    " - Lag feilhåndtering dersom funksjonskallet og/eller API-kallet ikke returnerer noen relevante kurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber nøyaktighet, vær oppmerksom på at automatiske oversettelser kan inneholde feil eller unøyaktigheter. Det opprinnelige dokumentet på sitt originale språk skal anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell, menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-08-25T20:18:59+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "no"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}