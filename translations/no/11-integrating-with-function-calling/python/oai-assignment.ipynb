{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduksjon \n",
    "\n",
    "Denne leksjonen vil dekke: \n",
    "- Hva funksjonskalling er og bruksområdene \n",
    "- Hvordan lage et funksjonskall ved bruk av OpenAI \n",
    "- Hvordan integrere et funksjonskall i en applikasjon \n",
    "\n",
    "## Læringsmål \n",
    "\n",
    "Etter å ha fullført denne leksjonen vil du vite hvordan og forstå: \n",
    "\n",
    "-  Formålet med å bruke funksjonskalling \n",
    "- Sette opp funksjonskall ved bruk av OpenAI-tjenesten \n",
    "- Designe effektive funksjonskall for applikasjonens bruksområde \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forstå funksjonskall\n",
    "\n",
    "For denne leksjonen ønsker vi å bygge en funksjon for vår utdanningsstartup som lar brukere bruke en chatbot for å finne tekniske kurs. Vi vil anbefale kurs som passer deres ferdighetsnivå, nåværende rolle og teknologi av interesse.\n",
    "\n",
    "For å fullføre dette vil vi bruke en kombinasjon av:\n",
    " - `OpenAI` for å lage en chatteopplevelse for brukeren\n",
    " - `Microsoft Learn Catalog API` for å hjelpe brukere med å finne kurs basert på brukerens forespørsel\n",
    " - `Function Calling` for å ta brukerens spørsmål og sende det til en funksjon for å gjøre API-forespørselen.\n",
    "\n",
    "For å komme i gang, la oss se på hvorfor vi ønsker å bruke funksjonskall i utgangspunktet:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # få et nytt svar fra GPT hvor det kan se funksjonssvaret\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hvorfor Funksjonskalling\n",
    "\n",
    "Hvis du har fullført noen annen leksjon i dette kurset, forstår du sannsynligvis kraften i å bruke store språkmodeller (LLMs). Forhåpentligvis kan du også se noen av begrensningene deres.\n",
    "\n",
    "Funksjonskalling er en funksjon i OpenAI-tjenesten designet for å løse følgende utfordringer:\n",
    "\n",
    "Uensartet responsformat:\n",
    "- Før funksjonskalling var svar fra en stor språkmodell ustrukturert og inkonsekvent. Utviklere måtte skrive kompleks valideringskode for å håndtere hver variasjon i utdataene.\n",
    "\n",
    "Begrenset integrasjon med ekstern data:\n",
    "- Før denne funksjonen var det vanskelig å innlemme data fra andre deler av en applikasjon i en chat-kontekst.\n",
    "\n",
    "Ved å standardisere responsformater og muliggjøre sømløs integrasjon med ekstern data, forenkler funksjonskalling utviklingen og reduserer behovet for ekstra valideringslogikk.\n",
    "\n",
    "Brukere kunne ikke få svar som \"Hva er været i Stockholm akkurat nå?\". Dette skyldes at modellene var begrenset til tidspunktet dataene ble trent på.\n",
    "\n",
    "La oss se på eksempelet nedenfor som illustrerer dette problemet:\n",
    "\n",
    "La oss si at vi ønsker å lage en database med studentdata slik at vi kan foreslå riktig kurs til dem. Nedenfor har vi to beskrivelser av studenter som er veldig like i dataene de inneholder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ønsker å sende dette til en LLM for å analysere dataene. Dette kan senere brukes i applikasjonen vår for å sende dette til en API eller lagre det i en database.\n",
    "\n",
    "La oss lage to identiske prompts der vi instruerer LLM om hvilken informasjon vi er interessert i:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ønsker å sende dette til en LLM for å analysere de delene som er viktige for produktet vårt. Slik at vi kan lage to identiske prompt for å instruere LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etter å ha laget disse to promptene, sender vi dem til LLM ved å bruke `openai.ChatCompletion`. Vi lagrer prompten i variabelen `messages` og tildeler rollen til `user`. Dette er for å etterligne en melding fra en bruker som skrives til en chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå kan vi sende begge forespørslene til LLM og undersøke svaret vi mottar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selv om promptene er de samme og beskrivelsene ligner, kan vi få forskjellige formater på `Grades`-egenskapen.\n",
    "\n",
    "Hvis du kjører cellen ovenfor flere ganger, kan formatet være `3.7` eller `3.7 GPA`.\n",
    "\n",
    "Dette er fordi LLM tar ustrukturert data i form av den skrevne prompten og returnerer også ustrukturert data. Vi trenger å ha et strukturert format slik at vi vet hva vi kan forvente når vi lagrer eller bruker disse dataene.\n",
    "\n",
    "Ved å bruke funksjonskall kan vi sørge for at vi mottar strukturerte data tilbake. Når vi bruker funksjonskall, kaller eller kjører ikke LLM faktisk noen funksjoner. I stedet lager vi en struktur for at LLM skal følge i sine svar. Vi bruker deretter disse strukturerte svarene for å vite hvilken funksjon vi skal kjøre i våre applikasjoner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Funksjonskall Flytdiagram](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.no.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan deretter ta det som returneres fra funksjonen og sende dette tilbake til LLM. LLM vil deretter svare med naturlig språk for å besvare brukerens spørsmål.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruksområder for bruk av funksjonskall\n",
    "\n",
    "**Kalle eksterne verktøy**  \n",
    "Chatboter er gode til å gi svar på spørsmål fra brukere. Ved å bruke funksjonskall kan chatbotene bruke meldinger fra brukere til å utføre visse oppgaver. For eksempel kan en student be chatboten om å \"Sende e-post til min instruktør og si at jeg trenger mer hjelp med dette emnet\". Dette kan gjøre et funksjonskall til `send_email(to: string, body: string)`\n",
    "\n",
    "**Opprette API- eller databaseforespørsler**  \n",
    "Brukere kan finne informasjon ved å bruke naturlig språk som blir konvertert til en formatert forespørsel eller API-forespørsel. Et eksempel på dette kan være en lærer som spør \"Hvem er studentene som fullførte den siste oppgaven\" som kan kalle en funksjon kalt `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Opprette strukturert data**  \n",
    "Brukere kan ta et tekstavsnitt eller CSV og bruke LLM til å hente ut viktig informasjon fra det. For eksempel kan en student konvertere en Wikipedia-artikkel om fredsavtaler for å lage AI-flashkort. Dette kan gjøres ved å bruke en funksjon kalt `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lage ditt første funksjonskall\n",
    "\n",
    "Prosessen med å lage et funksjonskall inkluderer 3 hovedtrinn:\n",
    "1. Kalle Chat Completions API med en liste over funksjonene dine og en brukermelding\n",
    "2. Lese modellens svar for å utføre en handling, dvs. kjøre en funksjon eller API-kall\n",
    "3. Gjøre et nytt kall til Chat Completions API med svaret fra funksjonen din for å bruke den informasjonen til å lage et svar til brukeren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flyt av et Funksjonskall](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.no.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementer i et funksjonskall \n",
    "\n",
    "#### Brukerens inndata \n",
    "\n",
    "Det første steget er å lage en brukermelding. Denne kan tildeles dynamisk ved å ta verdien fra et tekstfelt, eller du kan tildele en verdi her. Hvis dette er første gang du jobber med Chat Completions API, må vi definere `role` og `content` i meldingen. \n",
    "\n",
    "`role` kan være enten `system` (opprette regler), `assistant` (modellen) eller `user` (sluttbrukeren). For funksjonskall vil vi tildele dette som `user` og et eksempelspørsmål. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lage funksjoner.\n",
    "\n",
    "Neste skal vi definere en funksjon og parameterne til den funksjonen. Vi vil bruke bare én funksjon her kalt `search_courses`, men du kan lage flere funksjoner.\n",
    "\n",
    "**Viktig**: Funksjoner er inkludert i systemmeldingen til LLM og vil bli inkludert i mengden tilgjengelige tokens du har tilgjengelig.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definisjoner** \n",
    "\n",
    "Funksjonsdefinisjonsstrukturen har flere nivåer, hver med sine egne egenskaper. Her er en oversikt over den nestede strukturen:\n",
    "\n",
    "**Funksjonsegenskaper på øverste nivå:**\n",
    "\n",
    "`name` - Navnet på funksjonen vi ønsker å få kalt. \n",
    "\n",
    "`description` - Dette er beskrivelsen av hvordan funksjonen fungerer. Her er det viktig å være spesifikk og tydelig \n",
    "\n",
    "`parameters` - En liste over verdier og format som du ønsker at modellen skal produsere i sitt svar \n",
    "\n",
    "**Egenskaper for parameterobjektet:**\n",
    "\n",
    "`type` - Datatypen til parameterobjektet (vanligvis \"object\")\n",
    "\n",
    "`properties` - Liste over de spesifikke verdiene som modellen vil bruke i sitt svar \n",
    "\n",
    "**Egenskaper for individuelle parametere:**\n",
    "\n",
    "`name` - Implisitt definert av egenskapsnøkkelen (f.eks. \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Datatypen til denne spesifikke parameteren (f.eks. \"string\", \"number\", \"boolean\") \n",
    "\n",
    "`description` - Beskrivelse av den spesifikke parameteren \n",
    "\n",
    "**Valgfrie egenskaper:**\n",
    "\n",
    "`required` - En liste som angir hvilke parametere som er nødvendige for at funksjonskallet skal fullføres \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lage funksjonskallet  \n",
    "Etter å ha definert en funksjon, må vi nå inkludere den i kallet til Chat Completion API. Vi gjør dette ved å legge til `functions` i forespørselen. I dette tilfellet `functions=functions`.  \n",
    "\n",
    "Det finnes også et alternativ for å sette `function_call` til `auto`. Dette betyr at vi lar LLM bestemme hvilken funksjon som skal kalles basert på brukermeldingen i stedet for å tildele det selv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La oss nå se på svaret og hvordan det er formatert:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Du kan se at navnet på funksjonen blir kalt, og ut fra brukerens melding klarte LLM å finne dataene som passer til argumentene til funksjonen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Integrering av funksjonskall i en applikasjon. \n",
    "\n",
    "\n",
    "Etter at vi har testet det formaterte svaret fra LLM, kan vi nå integrere dette i en applikasjon. \n",
    "\n",
    "### Håndtering av flyten \n",
    "\n",
    "For å integrere dette i applikasjonen vår, la oss ta følgende steg: \n",
    "\n",
    "Først, la oss gjøre kallet til OpenAI-tjenestene og lagre meldingen i en variabel kalt `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå skal vi definere funksjonen som vil kalle Microsoft Learn API for å hente en liste over kurs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som en beste praksis vil vi deretter se om modellen ønsker å kalle en funksjon. Etter det vil vi opprette en av de tilgjengelige funksjonene og matche den med funksjonen som blir kalt.  \n",
    "Vi vil deretter ta argumentene til funksjonen og kartlegge dem til argumenter fra LLM.\n",
    "\n",
    "Til slutt vil vi legge til funksjonskallmeldingen og verdiene som ble returnert av `search_courses`-meldingen. Dette gir LLM all informasjonen den trenger for å svare brukeren med naturlig språk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå vil vi sende den oppdaterte meldingen til LLM slik at vi kan motta et svar på naturlig språk i stedet for et API JSON-formatert svar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kodeutfordring \n",
    "\n",
    "Flott arbeid! For å fortsette læringen din om OpenAI Function Calling kan du bygge: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - Flere parametere for funksjonen som kan hjelpe lærere med å finne flere kurs. Du kan finne tilgjengelige API-parametere her: \n",
    " - Lag et annet funksjonskall som tar mer informasjon fra læreren, som deres morsmål \n",
    " - Lag feilhåndtering når funksjonskallet og/eller API-kallet ikke returnerer noen passende kurs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Ansvarsfraskrivelse**:\nDette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vennligst vær oppmerksom på at automatiske oversettelser kan inneholde feil eller unøyaktigheter. Det opprinnelige dokumentet på originalspråket skal anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T10:38:52+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "no"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}