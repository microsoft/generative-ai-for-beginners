{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduksjon\n",
    "\n",
    "Denne leksjonen vil ta for seg:\n",
    "- Hva funksjonskall er og når det brukes\n",
    "- Hvordan lage et funksjonskall med OpenAI\n",
    "- Hvordan integrere et funksjonskall i en applikasjon\n",
    "\n",
    "## Læringsmål\n",
    "\n",
    "Etter å ha fullført denne leksjonen vil du vite hvordan og forstå:\n",
    "\n",
    "- Hensikten med å bruke funksjonskall\n",
    "- Sette opp funksjonskall med OpenAI-tjenesten\n",
    "- Lage effektive funksjonskall tilpasset din applikasjons brukstilfelle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forstå funksjonskall\n",
    "\n",
    "I denne leksjonen skal vi lage en funksjon for vår utdanningsstartup som lar brukere bruke en chatbot for å finne tekniske kurs. Vi vil anbefale kurs som passer til deres ferdighetsnivå, nåværende rolle og teknologi de er interessert i.\n",
    "\n",
    "For å få til dette vil vi bruke en kombinasjon av:\n",
    " - `OpenAI` for å lage en chatopplevelse for brukeren\n",
    " - `Microsoft Learn Catalog API` for å hjelpe brukere å finne kurs basert på deres forespørsel\n",
    " - `Function Calling` for å ta brukerens spørsmål og sende det til en funksjon som gjør API-forespørselen.\n",
    "\n",
    "La oss først se på hvorfor vi ønsker å bruke funksjonskall i det hele tatt:\n",
    "\n",
    "print(\"Meldinger i neste forespørsel:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # få et nytt svar fra GPT hvor den kan se funksjonens respons\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hvorfor bruke Function Calling\n",
    "\n",
    "Hvis du har fullført noen av de andre leksjonene i dette kurset, har du sannsynligvis fått med deg hvor kraftige store språkmodeller (LLMs) kan være. Forhåpentligvis har du også lagt merke til noen av begrensningene deres.\n",
    "\n",
    "Function Calling er en funksjon i OpenAI-tjenesten som er laget for å løse følgende utfordringer:\n",
    "\n",
    "Ujevn responsformatering:\n",
    "- Før function calling var svarene fra en stor språkmodell ustrukturerte og varierende. Utviklere måtte skrive komplisert valideringskode for å håndtere alle mulige variasjoner i utdataene.\n",
    "\n",
    "Begrenset integrasjon med eksterne data:\n",
    "- Før denne funksjonen var det vanskelig å hente inn data fra andre deler av en applikasjon inn i en chatkontekst.\n",
    "\n",
    "Ved å standardisere responsformatet og gjøre det enkelt å integrere med eksterne data, gjør function calling utviklingen enklere og reduserer behovet for ekstra valideringslogikk.\n",
    "\n",
    "Brukere kunne ikke få svar som \"Hva er været i Stockholm akkurat nå?\". Dette skyldes at modellene kun hadde tilgang til data de var trent på.\n",
    "\n",
    "La oss se på eksempelet under som illustrerer dette problemet:\n",
    "\n",
    "La oss si at vi ønsker å lage en database med studentdata slik at vi kan foreslå riktig kurs til dem. Under har vi to beskrivelser av studenter som inneholder nesten de samme opplysningene.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ønsker å sende dette til en LLM for å analysere dataene. Dette kan senere brukes i applikasjonen vår for å sende til et API eller lagre i en database.\n",
    "\n",
    "La oss lage to identiske forespørsler der vi instruerer LLM-en om hvilken informasjon vi er interessert i:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ønsker å sende dette til en LLM for å analysere delene som er viktige for vårt produkt. Slik kan vi lage to identiske instruksjoner for å veilede LLM-en:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etter å ha laget disse to promptene, sender vi dem til LLM ved å bruke `openai.ChatCompletion`. Vi lagrer prompten i variabelen `messages` og tildeler rollen til `user`. Dette er for å etterligne en melding fra en bruker som skrives til en chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selv om promptene er de samme og beskrivelsene ligner, kan vi få ulike formater på `Grades`-egenskapen.\n",
    "\n",
    "Hvis du kjører cellen over flere ganger, kan formatet være `3.7` eller `3.7 GPA`.\n",
    "\n",
    "Dette skjer fordi LLM-en tar inn ustrukturert data i form av den skrevne prompten og returnerer også ustrukturert data. Vi må ha et strukturert format slik at vi vet hva vi kan forvente når vi lagrer eller bruker disse dataene.\n",
    "\n",
    "Ved å bruke funksjonskall kan vi sørge for at vi får tilbake strukturerte data. Når vi bruker funksjonskall, kjører eller utfører ikke LLM-en faktisk noen funksjoner. I stedet lager vi en struktur som LLM-en skal følge for sine svar. Vi bruker så disse strukturerte svarene for å vite hvilken funksjon vi skal kjøre i applikasjonene våre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruksområder for å bruke funksjonskall\n",
    "\n",
    "**Kalle eksterne verktøy**  \n",
    "Chatboter er flinke til å gi svar på spørsmål fra brukere. Ved å bruke funksjonskall kan chatbotene bruke meldinger fra brukerne til å utføre bestemte oppgaver. For eksempel kan en student be chatboten om å \"Sende e-post til læreren min og si at jeg trenger mer hjelp med dette emnet\". Dette kan gjøre et funksjonskall til `send_email(to: string, body: string)`\n",
    "\n",
    "**Lage API- eller databaseforespørsler**  \n",
    "Brukere kan finne informasjon ved å bruke naturlig språk som blir gjort om til en formatert spørring eller API-forespørsel. Et eksempel på dette kan være en lærer som spør \"Hvem er studentene som fullførte siste oppgave\", som kan kalle en funksjon som heter `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Lage strukturert data**  \n",
    "Brukere kan ta en tekstblokk eller CSV og bruke LLM til å hente ut viktig informasjon fra den. For eksempel kan en student gjøre om en Wikipedia-artikkel om fredsavtaler for å lage AI-flashkort. Dette kan gjøres ved å bruke en funksjon som heter `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lage ditt første funksjonskall\n",
    "\n",
    "Prosessen med å lage et funksjonskall består av tre hovedsteg:\n",
    "1. Kall Chat Completions API med en liste over funksjonene dine og en brukermelding\n",
    "2. Les modellens svar for å utføre en handling, for eksempel kjøre en funksjon eller et API-kall\n",
    "3. Gjør et nytt kall til Chat Completions API med svaret fra funksjonen din for å bruke denne informasjonen til å lage et svar til brukeren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementer i et funksjonskall\n",
    "\n",
    "#### Brukerens inndata\n",
    "\n",
    "Første steg er å lage en brukermelding. Dette kan settes dynamisk ved å hente verdien fra et tekstfelt, eller du kan sette en verdi her. Hvis dette er første gang du jobber med Chat Completions API, må vi definere `role` og `content` for meldingen.\n",
    "\n",
    "`role` kan være enten `system` (lager regler), `assistant` (modellen) eller `user` (sluttbrukeren). For funksjonskall setter vi dette til `user` og gir et eksempelspørsmål.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lage funksjoner.\n",
    "\n",
    "Nå skal vi definere en funksjon og parameterne til den funksjonen. Vi bruker bare én funksjon her som heter `search_courses`, men du kan lage flere funksjoner.\n",
    "\n",
    "**Viktig** : Funksjoner blir lagt til i systemmeldingen til LLM og vil telle med i antallet tilgjengelige tokens du har.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definisjoner**\n",
    "\n",
    "Strukturen for funksjonsdefinisjon har flere nivåer, hvor hvert nivå har sine egne egenskaper. Her er en oversikt over den nestede strukturen:\n",
    "\n",
    "**Egenskaper for funksjon på øverste nivå:**\n",
    "\n",
    "`name` - Navnet på funksjonen vi ønsker å kalle.\n",
    "\n",
    "`description` - Dette er en beskrivelse av hvordan funksjonen fungerer. Her er det viktig å være tydelig og konkret.\n",
    "\n",
    "`parameters` - En liste over verdier og format du ønsker at modellen skal bruke i sitt svar.\n",
    "\n",
    "**Egenskaper for parameterobjekt:**\n",
    "\n",
    "`type` - Datatypen til parameterobjektet (vanligvis \"object\")\n",
    "\n",
    "`properties` - Liste over de spesifikke verdiene modellen vil bruke i sitt svar\n",
    "\n",
    "**Egenskaper for individuelle parametere:**\n",
    "\n",
    "`name` - Implisitt definert av nøkkelen til egenskapen (f.eks. \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Datatypen til denne spesifikke parameteren (f.eks. \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - Beskrivelse av den spesifikke parameteren\n",
    "\n",
    "**Valgfrie egenskaper:**\n",
    "\n",
    "`required` - En liste som viser hvilke parametere som må være med for at funksjonskallet skal kunne fullføres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lage funksjonskallet\n",
    "Etter at vi har definert en funksjon, må vi nå inkludere den i kallet til Chat Completion API. Dette gjør vi ved å legge til `functions` i forespørselen. I dette tilfellet `functions=functions`.\n",
    "\n",
    "Det finnes også et alternativ for å sette `function_call` til `auto`. Dette betyr at vi lar LLM-en bestemme hvilken funksjon som skal kalles basert på brukermeldingen, i stedet for å tildele det selv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå kan vi se på svaret og hvordan det er formatert:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Du kan se at navnet på funksjonen blir kalt, og fra brukermeldingen klarte LLM å finne dataene som passer til argumentene for funksjonen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrere funksjonskall i en applikasjon.\n",
    "\n",
    "Etter at vi har testet det formaterte svaret fra LLM, kan vi nå integrere dette i en applikasjon.\n",
    "\n",
    "### Håndtere flyten\n",
    "\n",
    "For å integrere dette i applikasjonen vår, la oss følge disse stegene:\n",
    "\n",
    "Først gjør vi et kall til OpenAI-tjenestene og lagrer meldingen i en variabel som heter `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå skal vi definere funksjonen som vil kalle Microsoft Learn API for å hente en liste over kurs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som en beste praksis vil vi deretter se om modellen ønsker å kalle en funksjon. Etter det vil vi opprette en av de tilgjengelige funksjonene og matche den til funksjonen som blir kalt.  \n",
    "Vi tar så argumentene til funksjonen og kobler dem til argumentene fra LLM.\n",
    "\n",
    "Til slutt legger vi til funksjonskall-meldingen og verdiene som ble returnert av `search_courses`-meldingen. Dette gir LLM all informasjonen den trenger for å\n",
    "svare brukeren med naturlig språk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kodeutfordring\n",
    "\n",
    "Godt jobbet! For å fortsette læringen om OpenAI Function Calling kan du bygge: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Flere parametere for funksjonen som kan hjelpe brukere å finne flere kurs. Du finner tilgjengelige API-parametere her:\n",
    " - Lag et nytt funksjonskall som tar imot mer informasjon fra brukeren, som for eksempel morsmål\n",
    " - Lag feilhåndtering dersom funksjonskallet og/eller API-kallet ikke returnerer noen relevante kurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber nøyaktighet, vær oppmerksom på at automatiske oversettelser kan inneholde feil eller unøyaktigheter. Det opprinnelige dokumentet på sitt originale språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell, menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T21:07:54+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "no"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}