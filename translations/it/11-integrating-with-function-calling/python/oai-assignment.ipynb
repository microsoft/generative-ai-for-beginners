{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduzione \n",
    "\n",
    "Questa lezione coprirà: \n",
    "- Cos'è la chiamata di funzione e i suoi casi d'uso \n",
    "- Come creare una chiamata di funzione usando OpenAI \n",
    "- Come integrare una chiamata di funzione in un'applicazione \n",
    "\n",
    "## Obiettivi di apprendimento \n",
    "\n",
    "Dopo aver completato questa lezione saprai come e comprenderai: \n",
    "\n",
    "- Lo scopo dell'uso della chiamata di funzione \n",
    "- Configurare la chiamata di funzione utilizzando il servizio OpenAI \n",
    "- Progettare chiamate di funzione efficaci per il caso d'uso della tua applicazione\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendere le Chiamate di Funzione\n",
    "\n",
    "Per questa lezione, vogliamo costruire una funzionalità per la nostra startup educativa che permetta agli utenti di utilizzare un chatbot per trovare corsi tecnici. Consiglieremo corsi che si adattano al loro livello di competenza, ruolo attuale e tecnologia di interesse.\n",
    "\n",
    "Per completare questo utilizzeremo una combinazione di:\n",
    " - `OpenAI` per creare un'esperienza di chat per l'utente\n",
    " - `Microsoft Learn Catalog API` per aiutare gli utenti a trovare corsi basati sulla richiesta dell'utente\n",
    " - `Function Calling` per prendere la query dell'utente e inviarla a una funzione per effettuare la richiesta API.\n",
    "\n",
    "Per iniziare, vediamo perché vorremmo usare la chiamata di funzione in primo luogo:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # ottenere una nuova risposta da GPT dove può vedere la risposta della funzione\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perché la Chiamata di Funzioni\n",
    "\n",
    "Se hai completato qualche altra lezione in questo corso, probabilmente comprendi il potere dell'uso dei Modelli di Linguaggio di Grandi Dimensioni (LLM). Speriamo che tu possa anche vedere alcune delle loro limitazioni.\n",
    "\n",
    "La Chiamata di Funzioni è una funzionalità del Servizio OpenAI progettata per affrontare le seguenti sfide:\n",
    "\n",
    "Formattazione della Risposta Incoerente:\n",
    "- Prima della chiamata di funzioni, le risposte di un modello di linguaggio di grandi dimensioni erano non strutturate e incoerenti. Gli sviluppatori dovevano scrivere codice di validazione complesso per gestire ogni variazione nell'output.\n",
    "\n",
    "Integrazione Limitata con Dati Esterni:\n",
    "- Prima di questa funzionalità, era difficile incorporare dati da altre parti di un'applicazione in un contesto di chat.\n",
    "\n",
    "Standardizzando i formati di risposta e abilitando un'integrazione fluida con dati esterni, la chiamata di funzioni semplifica lo sviluppo e riduce la necessità di logiche di validazione aggiuntive.\n",
    "\n",
    "Gli utenti non potevano ottenere risposte come \"Qual è il meteo attuale a Stoccolma?\". Questo perché i modelli erano limitati al tempo in cui i dati erano stati addestrati.\n",
    "\n",
    "Guardiamo l'esempio qui sotto che illustra questo problema:\n",
    "\n",
    "Supponiamo di voler creare un database di dati degli studenti così da poter suggerire loro il corso giusto. Qui sotto abbiamo due descrizioni di studenti che sono molto simili nei dati che contengono.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo inviare questo a un LLM per analizzare i dati. Questo potrà poi essere utilizzato nella nostra applicazione per inviarlo a un'API o memorizzarlo in un database.\n",
    "\n",
    "Creiamo due prompt identici in cui istruiamo l'LLM su quali informazioni ci interessano:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo inviare questo a un LLM per analizzare le parti importanti per il nostro prodotto. Così possiamo creare due prompt identici per istruire l'LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopo aver creato questi due prompt, li invieremo al LLM utilizzando `openai.ChatCompletion`. Memorizziamo il prompt nella variabile `messages` e assegniamo il ruolo a `user`. Questo serve a simulare un messaggio scritto da un utente a un chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora possiamo inviare entrambe le richieste al LLM ed esaminare la risposta che riceviamo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche se i prompt sono gli stessi e le descrizioni sono simili, possiamo ottenere formati diversi della proprietà `Grades`.\n",
    "\n",
    "Se esegui la cella sopra più volte, il formato può essere `3.7` o `3.7 GPA`.\n",
    "\n",
    "Questo perché il LLM prende dati non strutturati sotto forma di prompt scritto e restituisce anche dati non strutturati. Abbiamo bisogno di un formato strutturato in modo da sapere cosa aspettarci quando memorizziamo o utilizziamo questi dati.\n",
    "\n",
    "Utilizzando il functional calling, possiamo assicurarci di ricevere dati strutturati in risposta. Quando si usa il functional calling, il LLM in realtà non chiama o esegue alcuna funzione. Invece, creiamo una struttura che il LLM deve seguire per le sue risposte. Usiamo quindi quelle risposte strutturate per sapere quale funzione eseguire nelle nostre applicazioni.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagramma del flusso di chiamata della funzione](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.it.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo quindi prendere ciò che viene restituito dalla funzione e inviarlo indietro al LLM. Il LLM risponderà quindi utilizzando il linguaggio naturale per rispondere alla domanda dell'utente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casi d'uso per l'utilizzo delle chiamate di funzione\n",
    "\n",
    "**Chiamare Strumenti Esterni**  \n",
    "I chatbot sono ottimi nel fornire risposte alle domande degli utenti. Utilizzando le chiamate di funzione, i chatbot possono usare i messaggi degli utenti per completare determinate attività. Ad esempio, uno studente può chiedere al chatbot di \"Inviare un'email al mio insegnante dicendo che ho bisogno di più assistenza su questo argomento\". Questo può fare una chiamata di funzione a `send_email(to: string, body: string)`\n",
    "\n",
    "\n",
    "**Creare Query API o Database**  \n",
    "Gli utenti possono trovare informazioni usando il linguaggio naturale che viene convertito in una query formattata o richiesta API. Un esempio potrebbe essere un insegnante che chiede \"Chi sono gli studenti che hanno completato l'ultimo compito\" che potrebbe chiamare una funzione chiamata `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "\n",
    "**Creare Dati Strutturati**  \n",
    "Gli utenti possono prendere un blocco di testo o CSV e usare l'LLM per estrarre informazioni importanti da esso. Ad esempio, uno studente può convertire un articolo di Wikipedia sugli accordi di pace per creare flashcard AI. Questo può essere fatto usando una funzione chiamata `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creare la tua prima chiamata di funzione\n",
    "\n",
    "Il processo di creazione di una chiamata di funzione include 3 passaggi principali:  \n",
    "1. Chiamare l'API Chat Completions con un elenco delle tue funzioni e un messaggio dell'utente  \n",
    "2. Leggere la risposta del modello per eseguire un'azione, cioè eseguire una funzione o una chiamata API  \n",
    "3. Effettuare un'altra chiamata all'API Chat Completions con la risposta della tua funzione per utilizzare tali informazioni per creare una risposta per l'utente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flusso di una Chiamata di Funzione](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.it.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementi di una chiamata di funzione \n",
    "\n",
    "#### Input degli utenti \n",
    "\n",
    "Il primo passo è creare un messaggio utente. Questo può essere assegnato dinamicamente prendendo il valore di un input di testo oppure puoi assegnare un valore qui. Se è la prima volta che lavori con l'API Chat Completions, dobbiamo definire il `role` e il `content` del messaggio. \n",
    "\n",
    "Il `role` può essere `system` (creazione di regole), `assistant` (il modello) o `user` (l'utente finale). Per la chiamata di funzione, assegneremo questo come `user` e una domanda di esempio. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione di funzioni.\n",
    "\n",
    "Successivamente definiremo una funzione e i parametri di quella funzione. Useremo una sola funzione qui chiamata `search_courses` ma puoi creare più funzioni.\n",
    "\n",
    "**Importante** : Le funzioni sono incluse nel messaggio di sistema per il LLM e saranno conteggiate nel numero di token disponibili.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definizioni** \n",
    "\n",
    "La struttura della definizione della funzione ha più livelli, ciascuno con le proprie proprietà. Ecco una suddivisione della struttura annidata:\n",
    "\n",
    "**Proprietà della funzione di livello superiore:**\n",
    "\n",
    "`name` - Il nome della funzione che vogliamo venga chiamata.\n",
    "\n",
    "`description` - Questa è la descrizione di come funziona la funzione. Qui è importante essere specifici e chiari.\n",
    "\n",
    "`parameters` - Un elenco di valori e formato che si desidera che il modello produca nella sua risposta.\n",
    "\n",
    "**Proprietà dell'oggetto Parameters:**\n",
    "\n",
    "`type` - Il tipo di dato dell'oggetto parameters (di solito \"object\").\n",
    "\n",
    "`properties` - Elenco dei valori specifici che il modello utilizzerà per la sua risposta.\n",
    "\n",
    "**Proprietà dei singoli parametri:**\n",
    "\n",
    "`name` - Implicitamente definito dalla chiave della proprietà (es. \"role\", \"product\", \"level\").\n",
    "\n",
    "`type` - Il tipo di dato di questo parametro specifico (es. \"string\", \"number\", \"boolean\").\n",
    "\n",
    "`description` - Descrizione del parametro specifico.\n",
    "\n",
    "**Proprietà opzionali:**\n",
    "\n",
    "`required` - Un array che elenca quali parametri sono necessari per completare la chiamata della funzione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effettuare la chiamata alla funzione  \n",
    "Dopo aver definito una funzione, ora dobbiamo includerla nella chiamata all'API di completamento chat. Lo facciamo aggiungendo `functions` alla richiesta. In questo caso `functions=functions`.  \n",
    "\n",
    "C'è anche un'opzione per impostare `function_call` su `auto`. Questo significa che lasceremo che sia il LLM a decidere quale funzione deve essere chiamata in base al messaggio dell'utente invece di assegnarla noi stessi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora diamo un'occhiata alla risposta e vediamo come è formattata:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Puoi vedere che il nome della funzione viene chiamato e dal messaggio dell'utente, il LLM è stato in grado di trovare i dati per adattarsi agli argomenti della funzione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Integrare le chiamate di funzione in un'applicazione. \n",
    "\n",
    "\n",
    "Dopo aver testato la risposta formattata dal LLM, ora possiamo integrarla in un'applicazione. \n",
    "\n",
    "### Gestire il flusso \n",
    "\n",
    "Per integrare questo nella nostra applicazione, seguiamo i seguenti passaggi: \n",
    "\n",
    "Per prima cosa, facciamo la chiamata ai servizi OpenAI e memorizziamo il messaggio in una variabile chiamata `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora definiremo la funzione che chiamerà l'API di Microsoft Learn per ottenere un elenco di corsi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come buona pratica, vedremo quindi se il modello vuole chiamare una funzione. Successivamente, creeremo una delle funzioni disponibili e la abbineremo alla funzione che viene chiamata.  \n",
    "Prenderemo quindi gli argomenti della funzione e li mapperemo agli argomenti provenienti dal LLM.\n",
    "\n",
    "Infine, aggiungeremo il messaggio di chiamata della funzione e i valori restituiti dal messaggio `search_courses`. Questo fornisce al LLM tutte le informazioni di cui ha bisogno per rispondere all'utente usando il linguaggio naturale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora invieremo il messaggio aggiornato al LLM in modo da poter ricevere una risposta in linguaggio naturale invece di una risposta formattata in JSON API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sfida di Codice \n",
    "\n",
    "Ottimo lavoro! Per continuare il tuo apprendimento su OpenAI Function Calling puoi costruire: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - Più parametri della funzione che potrebbero aiutare gli studenti a trovare più corsi. Puoi trovare i parametri API disponibili qui: \n",
    " - Crea un'altra chiamata di funzione che prenda più informazioni dallo studente come la loro lingua madre \n",
    " - Crea una gestione degli errori quando la chiamata di funzione e/o la chiamata API non restituiscono corsi adatti \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:  \nQuesto documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Pur impegnandoci per garantire l’accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa deve essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un umano. Non ci assumiamo alcuna responsabilità per eventuali malintesi o interpretazioni errate derivanti dall’uso di questa traduzione.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T10:07:46+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "it"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}