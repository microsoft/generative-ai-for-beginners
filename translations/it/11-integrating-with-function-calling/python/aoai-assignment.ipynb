{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduzione\n",
    "\n",
    "Questa lezione tratterà:\n",
    "- Che cos'è la chiamata di funzione e i suoi casi d'uso\n",
    "- Come creare una chiamata di funzione utilizzando Azure OpenAI\n",
    "- Come integrare una chiamata di funzione in un'applicazione\n",
    "\n",
    "## Obiettivi di apprendimento\n",
    "\n",
    "Al termine di questa lezione saprai come e capirai:\n",
    "\n",
    "- Lo scopo dell'utilizzo della chiamata di funzione\n",
    "- Configurare la chiamata di funzione utilizzando il servizio Azure OpenAI\n",
    "- Progettare chiamate di funzione efficaci per il caso d'uso della tua applicazione\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendere le chiamate di funzione\n",
    "\n",
    "In questa lezione, vogliamo sviluppare una funzionalità per la nostra startup educativa che permetta agli utenti di utilizzare un chatbot per trovare corsi tecnici. Consiglieremo corsi adatti al loro livello di competenza, ruolo attuale e tecnologia di interesse.\n",
    "\n",
    "Per realizzare questo obiettivo useremo una combinazione di:\n",
    " - `Azure Open AI` per creare un'esperienza di chat per l'utente\n",
    " - `Microsoft Learn Catalog API` per aiutare gli utenti a trovare corsi in base alla loro richiesta\n",
    " - `Function Calling` per prendere la richiesta dell'utente e inviarla a una funzione che effettuerà la chiamata API.\n",
    "\n",
    "Per iniziare, vediamo perché potremmo voler utilizzare le chiamate di funzione:\n",
    "\n",
    "print(\"Messaggi nella prossima richiesta:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # ottieni una nuova risposta da GPT che può vedere la risposta della funzione\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perché usare Function Calling\n",
    "\n",
    "Se hai già seguito altre lezioni di questo corso, probabilmente conosci la potenza dei Large Language Models (LLM). Speriamo che tu abbia anche notato alcune delle loro limitazioni.\n",
    "\n",
    "Function Calling è una funzionalità dell’Azure Open AI Service pensata per superare queste limitazioni:\n",
    "1) Formato di risposta coerente\n",
    "2) Possibilità di utilizzare dati provenienti da altre fonti di un’applicazione all’interno di una chat\n",
    "\n",
    "Prima di function calling, le risposte di un LLM erano spesso non strutturate e incoerenti. Gli sviluppatori dovevano scrivere codice di validazione complesso per gestire tutte le possibili variazioni delle risposte.\n",
    "\n",
    "Gli utenti non potevano ricevere risposte come “Qual è il meteo attuale a Stoccolma?”. Questo perché i modelli erano limitati ai dati disponibili al momento dell’addestramento.\n",
    "\n",
    "Vediamo l’esempio qui sotto che illustra questo problema:\n",
    "\n",
    "Supponiamo di voler creare un database di dati degli studenti per suggerire loro il corso più adatto. Qui sotto ci sono due descrizioni di studenti che sono molto simili nei dati che contengono.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo inviare questi dati a un LLM per analizzarli. In seguito, potremo usarli nella nostra applicazione per inviarli a un'API o salvarli in un database.\n",
    "\n",
    "Creiamo due prompt identici con cui indichiamo all'LLM quali informazioni ci interessano:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo inviare questo a un LLM per analizzare le parti che sono importanti per il nostro prodotto. Così possiamo creare due prompt identici per istruire l'LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopo aver creato questi due prompt, li invieremo al LLM utilizzando `openai.ChatCompletion`. Conserviamo il prompt nella variabile `messages` e assegniamo il ruolo a `user`. Questo serve a simulare un messaggio di un utente scritto a un chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche se i prompt sono gli stessi e le descrizioni sono simili, possiamo ottenere formati diversi per la proprietà `Grades`.\n",
    "\n",
    "Se esegui la cella sopra più volte, il formato può essere `3.7` oppure `3.7 GPA`.\n",
    "\n",
    "Questo succede perché il LLM riceve dati non strutturati sotto forma di prompt scritto e restituisce anch'esso dati non strutturati. Abbiamo bisogno di un formato strutturato per sapere cosa aspettarci quando memorizziamo o utilizziamo questi dati.\n",
    "\n",
    "Utilizzando la chiamata funzionale, possiamo assicurarci di ricevere dati strutturati. Quando si usa la chiamata di funzione, il LLM in realtà non chiama né esegue alcuna funzione. Invece, creiamo una struttura che il LLM deve seguire nelle sue risposte. Poi usiamo queste risposte strutturate per sapere quale funzione eseguire nelle nostre applicazioni.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagramma di flusso della chiamata di funzione](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.it.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casi d’uso per l’utilizzo delle chiamate di funzione\n",
    "\n",
    "**Chiamare strumenti esterni**  \n",
    "I chatbot sono ottimi nel fornire risposte alle domande degli utenti. Utilizzando le chiamate di funzione, i chatbot possono usare i messaggi degli utenti per svolgere determinati compiti. Ad esempio, uno studente può chiedere al chatbot di \"Inviare un’email al mio docente dicendo che ho bisogno di ulteriore assistenza su questa materia\". Questo può generare una chiamata alla funzione `send_email(to: string, body: string)`\n",
    "\n",
    "**Creare richieste API o query al database**  \n",
    "Gli utenti possono trovare informazioni usando il linguaggio naturale che viene poi convertito in una query formattata o in una richiesta API. Un esempio potrebbe essere un insegnante che chiede \"Chi sono gli studenti che hanno completato l’ultimo compito\", che potrebbe chiamare una funzione chiamata `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Creazione di dati strutturati**  \n",
    "Gli utenti possono prendere un blocco di testo o un file CSV e usare il LLM per estrarre le informazioni più importanti. Ad esempio, uno studente può convertire un articolo di Wikipedia sugli accordi di pace per creare flash card AI. Questo può essere fatto utilizzando una funzione chiamata `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creare la tua prima chiamata di funzione\n",
    "\n",
    "Il processo per creare una chiamata di funzione include 3 passaggi principali:\n",
    "1. Chiamare l’API Chat Completions con un elenco delle tue funzioni e un messaggio dell’utente\n",
    "2. Leggere la risposta del modello per eseguire un’azione, ad esempio eseguire una funzione o una chiamata API\n",
    "3. Effettuare un’altra chiamata all’API Chat Completions con la risposta della tua funzione per utilizzare queste informazioni e creare una risposta per l’utente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flusso di una chiamata di funzione](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.it.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementi di una chiamata di funzione\n",
    "\n",
    "#### Input dell’utente\n",
    "\n",
    "Il primo passo è creare un messaggio utente. Questo può essere assegnato dinamicamente prendendo il valore da un campo di testo, oppure puoi assegnare un valore direttamente qui. Se è la prima volta che lavori con l’API Chat Completions, dobbiamo definire il `role` e il `content` del messaggio.\n",
    "\n",
    "Il `role` può essere `system` (per creare regole), `assistant` (il modello) oppure `user` (l’utente finale). Per la chiamata di funzione, lo imposteremo su `user` e forniremo una domanda di esempio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione delle funzioni.\n",
    "\n",
    "Ora definiremo una funzione e i suoi parametri. Useremo solo una funzione qui chiamata `search_courses`, ma puoi crearne diverse.\n",
    "\n",
    "**Importante** : Le funzioni vengono incluse nel messaggio di sistema inviato al LLM e verranno conteggiate nel numero di token disponibili.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definizioni**\n",
    "\n",
    "`name` - Il nome della funzione che vogliamo venga chiamata.\n",
    "\n",
    "`description` - Questa è la descrizione di come funziona la funzione. Qui è importante essere specifici e chiari.\n",
    "\n",
    "`parameters` - Un elenco di valori e il formato che si desidera che il modello produca nella sua risposta.\n",
    "\n",
    "`type` - Il tipo di dato in cui verranno memorizzate le proprietà.\n",
    "\n",
    "`properties` - Elenco dei valori specifici che il modello utilizzerà per la sua risposta.\n",
    "\n",
    "`name` - Il nome della proprietà che il modello userà nella sua risposta formattata.\n",
    "\n",
    "`type` - Il tipo di dato di questa proprietà.\n",
    "\n",
    "`description` - Descrizione della proprietà specifica.\n",
    "\n",
    "**Opzionale**\n",
    "\n",
    "`required` - Proprietà richiesta affinché la chiamata della funzione venga completata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effettuare la chiamata alla funzione\n",
    "Dopo aver definito una funzione, ora dobbiamo includerla nella chiamata all’API Chat Completion. Lo facciamo aggiungendo `functions` alla richiesta. In questo caso `functions=functions`.\n",
    "\n",
    "C’è anche la possibilità di impostare `function_call` su `auto`. Questo significa che lasciamo decidere al LLM quale funzione chiamare in base al messaggio dell’utente, invece di assegnarla noi direttamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora diamo un’occhiata alla risposta e vediamo come è formattata:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Puoi vedere che viene chiamato il nome della funzione e, dal messaggio dell’utente, il LLM è stato in grado di trovare i dati da inserire negli argomenti della funzione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrare le chiamate di funzione in un'applicazione.\n",
    "\n",
    "Dopo aver testato la risposta formattata dal LLM, ora possiamo integrarla in un'applicazione.\n",
    "\n",
    "### Gestire il flusso\n",
    "\n",
    "Per integrare questo nel nostro applicativo, seguiamo questi passaggi:\n",
    "\n",
    "Per prima cosa, effettuiamo la chiamata ai servizi Open AI e salviamo il messaggio in una variabile chiamata `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora definiremo la funzione che chiamerà l'API di Microsoft Learn per ottenere un elenco di corsi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come buona pratica, vedremo poi se il modello desidera chiamare una funzione. Successivamente, creeremo una delle funzioni disponibili e la abbineremo alla funzione che viene chiamata.\n",
    "Poi prenderemo gli argomenti della funzione e li mapperemo agli argomenti provenienti dal LLM.\n",
    "\n",
    "Infine, aggiungeremo il messaggio di chiamata della funzione e i valori che sono stati restituiti dal messaggio `search_courses`. Questo fornisce al LLM tutte le informazioni necessarie per\n",
    "rispondere all’utente utilizzando il linguaggio naturale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sfida di Codice\n",
    "\n",
    "Ottimo lavoro! Per continuare ad approfondire l’uso di Azure Open AI Function Calling puoi realizzare: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Altri parametri della funzione che potrebbero aiutare chi sta imparando a trovare più corsi. Puoi trovare i parametri disponibili dell’API qui:\n",
    " - Crea un’altra chiamata di funzione che raccolga più informazioni dall’utente, come la sua lingua madre\n",
    " - Gestisci gli errori quando la chiamata di funzione e/o la chiamata API non restituisce corsi adatti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nQuesto documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Pur impegnandoci per garantire l’accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa deve essere considerato la fonte autorevole. Per informazioni di carattere critico, si raccomanda una traduzione professionale effettuata da un essere umano. Non siamo responsabili per eventuali malintesi o interpretazioni errate derivanti dall’uso di questa traduzione.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-08-25T20:11:53+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "it"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}