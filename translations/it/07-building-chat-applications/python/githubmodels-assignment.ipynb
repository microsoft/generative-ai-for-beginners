{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Capitolo 7: Creare applicazioni di chat\n",
    "## Guida rapida all’API Github Models\n",
    "\n",
    "Questo notebook è stato adattato dal [Repository di esempi Azure OpenAI](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst), che include notebook per accedere ai servizi [Azure OpenAI](notebook-azure-openai.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Panoramica  \n",
    "\"I grandi modelli linguistici sono funzioni che associano testo a testo. Dato un input di testo, un grande modello linguistico cerca di prevedere quale testo verrà dopo\"(1). Questo notebook \"quickstart\" introdurrà gli utenti ai concetti principali degli LLM, ai requisiti fondamentali dei pacchetti per iniziare con AML, a una breve introduzione alla progettazione dei prompt e a diversi brevi esempi di casi d’uso differenti.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Indice  \n",
    "\n",
    "[Panoramica](../../../../07-building-chat-applications/python)  \n",
    "[Come utilizzare il servizio OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[1. Creare il tuo servizio OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[2. Installazione](../../../../07-building-chat-applications/python)    \n",
    "[3. Credenziali](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Casi d’uso](../../../../07-building-chat-applications/python)    \n",
    "[1. Riassumere un testo](../../../../07-building-chat-applications/python)  \n",
    "[2. Classificare un testo](../../../../07-building-chat-applications/python)  \n",
    "[3. Generare nuovi nomi di prodotto](../../../../07-building-chat-applications/python)  \n",
    "[4. Ottimizzare un classificatore](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Riferimenti](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Crea il tuo primo prompt  \n",
    "Questo breve esercizio ti darà un'introduzione di base su come inviare prompt a un modello in Github Models per un compito semplice come la \"sintesi\".\n",
    "\n",
    "\n",
    "**Passaggi**:  \n",
    "1. Installa la libreria `azure-ai-inference` nel tuo ambiente python, se non l'hai già fatto.  \n",
    "2. Carica le librerie di supporto standard e configura le credenziali per Github Models.  \n",
    "3. Scegli un modello per il tuo compito  \n",
    "4. Crea un prompt semplice per il modello  \n",
    "5. Invia la tua richiesta all'API del modello!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Installa `azure-ai-inference`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Trovare il modello giusto  \n",
    "I modelli GPT-3.5-turbo o GPT-4 sono in grado di comprendere e generare linguaggio naturale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Progettazione dei prompt  \n",
    "\n",
    "\"La magia dei modelli linguistici di grandi dimensioni sta nel fatto che, essendo addestrati a minimizzare l’errore di previsione su enormi quantità di testo, finiscono per apprendere concetti utili per queste previsioni. Ad esempio, imparano concetti come\"(1):\n",
    "\n",
    "* come si scrive correttamente\n",
    "* come funziona la grammatica\n",
    "* come parafrasare\n",
    "* come rispondere alle domande\n",
    "* come sostenere una conversazione\n",
    "* come scrivere in molte lingue\n",
    "* come programmare\n",
    "* ecc.\n",
    "\n",
    "#### Come controllare un modello linguistico di grandi dimensioni  \n",
    "\"Tra tutti gli input di un modello linguistico di grandi dimensioni, il più influente è di gran lunga il prompt testuale(1).\n",
    "\n",
    "I modelli linguistici di grandi dimensioni possono essere guidati a produrre output in diversi modi:\n",
    "\n",
    "Istruzione: Dì al modello cosa vuoi\n",
    "Completamento: Induci il modello a completare l’inizio di ciò che desideri\n",
    "Dimostrazione: Mostra al modello cosa vuoi, con:\n",
    "Alcuni esempi nel prompt\n",
    "Centinaia o migliaia di esempi in un dataset di addestramento per il fine-tuning\"\n",
    "\n",
    "\n",
    "\n",
    "#### Ci sono tre linee guida fondamentali per creare prompt:\n",
    "\n",
    "**Mostra e spiega**. Fai capire chiaramente cosa vuoi, sia tramite istruzioni, esempi, o una combinazione di entrambi. Se vuoi che il modello ordini una lista di elementi in ordine alfabetico o che classifichi un paragrafo in base al sentimento, mostragli chiaramente cosa ti aspetti.\n",
    "\n",
    "**Fornisci dati di qualità**. Se stai cercando di costruire un classificatore o di far seguire al modello uno schema, assicurati di fornire abbastanza esempi. Ricontrolla sempre i tuoi esempi — il modello di solito è abbastanza intelligente da ignorare errori di ortografia basilari e darti comunque una risposta, ma potrebbe anche pensare che siano intenzionali e questo può influenzare il risultato.\n",
    "\n",
    "**Controlla le impostazioni.** I parametri temperature e top_p determinano quanto il modello sia deterministico nel generare una risposta. Se chiedi una risposta dove esiste una sola soluzione corretta, conviene impostarli su valori bassi. Se invece vuoi risposte più varie, puoi alzarli. L’errore più comune con queste impostazioni è pensare che siano controlli di \"intelligenza\" o \"creatività\".\n",
    "\n",
    "\n",
    "Fonte: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Riassumere un testo  \n",
    "#### Sfida  \n",
    "Riassumi un testo aggiungendo un 'tl;dr:' alla fine di un passaggio. Nota come il modello sia in grado di svolgere diversi compiti senza istruzioni aggiuntive. Puoi sperimentare con prompt più descrittivi rispetto a tl;dr per modificare il comportamento del modello e personalizzare il riassunto che ricevi(3).  \n",
    "\n",
    "Studi recenti hanno dimostrato notevoli progressi in molti compiti e benchmark di NLP grazie al pre-addestramento su un ampio corpus di testi, seguito da una fase di fine-tuning su un compito specifico. Sebbene l’architettura sia generalmente indipendente dal compito, questo metodo richiede comunque dataset di fine-tuning specifici composti da migliaia o decine di migliaia di esempi. Al contrario, gli esseri umani sono in grado di affrontare un nuovo compito linguistico con pochi esempi o semplici istruzioni, cosa che i sistemi NLP attuali faticano ancora a fare. Qui mostriamo che aumentare la scala dei modelli linguistici migliora notevolmente le prestazioni few-shot indipendenti dal compito, arrivando talvolta a competere con i precedenti approcci di fine-tuning all’avanguardia. \n",
    "\n",
    "\n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Esercizi per diversi casi d'uso  \n",
    "1. Riassumi il testo  \n",
    "2. Classifica il testo  \n",
    "3. Genera nuovi nomi di prodotti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Classifica il testo  \n",
    "#### Sfida  \n",
    "Classifica gli elementi nelle categorie fornite al momento dell'inferenza. Nell'esempio seguente, forniamo sia le categorie che il testo da classificare nel prompt (*playground_reference).\n",
    "\n",
    "Richiesta del cliente: Salve, uno dei tasti della tastiera del mio portatile si è rotto di recente e avrò bisogno di una sostituzione:\n",
    "\n",
    "Categoria classificata:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Genera Nuovi Nomi di Prodotti\n",
    "#### Sfida\n",
    "Crea nomi di prodotti partendo da parole di esempio. Includiamo nel prompt informazioni sul prodotto per cui vogliamo generare i nomi. Forniamo anche un esempio simile per mostrare il modello che desideriamo ricevere. Abbiamo inoltre impostato un valore di temperatura alto per aumentare la casualità e ottenere risposte più innovative.\n",
    "\n",
    "Descrizione del prodotto: Un frullatore per frappè da casa\n",
    "Parole chiave: veloce, salutare, compatto.\n",
    "Nomi di prodotti: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Descrizione del prodotto: Un paio di scarpe che si adattano a qualsiasi misura di piede.\n",
    "Parole chiave: adattabile, calzata, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Riferimenti  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Esempi di OpenAI Studio](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Best practice per il fine-tuning di GPT-3 per la classificazione del testo](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Per ulteriore assistenza  \n",
    "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Collaboratori\n",
    "* [Chew-Yean Yam](https://www.linkedin.com/in/cyyam/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nQuesto documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Pur impegnandoci per garantire l’accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa deve essere considerato la fonte autorevole. Per informazioni di carattere critico, si raccomanda una traduzione professionale umana. Non siamo responsabili per eventuali malintesi o interpretazioni errate derivanti dall’uso di questa traduzione.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "3eeb8e5cad61b52a8366f6259a53ed49",
   "translation_date": "2025-08-25T17:35:10+00:00",
   "source_file": "07-building-chat-applications/python/githubmodels-assignment.ipynb",
   "language_code": "it"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}