{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costruire con i modelli della famiglia Meta\n",
    "\n",
    "## Introduzione\n",
    "\n",
    "Questa lezione coprirà:\n",
    "\n",
    "- Esplorare i due principali modelli della famiglia Meta - Llama 3.1 e Llama 3.2\n",
    "- Comprendere i casi d’uso e gli scenari per ciascun modello\n",
    "- Esempio di codice per mostrare le caratteristiche uniche di ogni modello\n",
    "\n",
    "## La famiglia di modelli Meta\n",
    "\n",
    "In questa lezione esploreremo 2 modelli della famiglia Meta, o “Llama Herd” - Llama 3.1 e Llama 3.2\n",
    "\n",
    "Questi modelli sono disponibili in diverse varianti e si trovano sul marketplace dei modelli di Github. Qui trovi maggiori dettagli su come usare i modelli Github per [prototipare con modelli AI](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Varianti dei modelli:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Nota: Llama 3 è disponibile anche su Github Models ma non verrà trattato in questa lezione*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Con 405 miliardi di parametri, Llama 3.1 rientra nella categoria degli LLM open source.\n",
    "\n",
    "Il modello è un aggiornamento rispetto alla versione precedente Llama 3 offrendo:\n",
    "\n",
    "- Finestra di contesto più ampia - 128k token contro 8k token\n",
    "- Maggior numero massimo di token in output - 4096 contro 2048\n",
    "- Miglior supporto multilingue - grazie all’aumento dei token di addestramento\n",
    "\n",
    "Queste caratteristiche permettono a Llama 3.1 di gestire casi d’uso più complessi nella creazione di applicazioni GenAI, tra cui:\n",
    "- Native Function Calling - la possibilità di richiamare strumenti e funzioni esterne al flusso di lavoro dell’LLM\n",
    "- Migliori prestazioni RAG - grazie alla finestra di contesto più ampia\n",
    "- Generazione di dati sintetici - la capacità di creare dati efficaci per attività come il fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chiamata Nativa alle Funzioni\n",
    "\n",
    "Llama 3.1 è stato ottimizzato per essere più efficace nell’effettuare chiamate a funzioni o strumenti. Dispone anche di due strumenti integrati che il modello può riconoscere come necessari in base alla richiesta dell’utente. Questi strumenti sono:\n",
    "\n",
    "- **Brave Search** - Può essere utilizzato per ottenere informazioni aggiornate come il meteo effettuando una ricerca sul web\n",
    "- **Wolfram Alpha** - Può essere utilizzato per calcoli matematici più complessi, così non è necessario scrivere le proprie funzioni.\n",
    "\n",
    "Puoi anche creare i tuoi strumenti personalizzati che l’LLM può richiamare.\n",
    "\n",
    "Nell’esempio di codice qui sotto:\n",
    "\n",
    "- Definiamo gli strumenti disponibili (brave_search, wolfram_alpha) nel prompt di sistema.\n",
    "- Inviamo un prompt utente che chiede informazioni sul meteo in una certa città.\n",
    "- L’LLM risponderà con una chiamata allo strumento Brave Search che apparirà così `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Nota: Questo esempio effettua solo la chiamata allo strumento; se desideri ottenere i risultati, dovrai creare un account gratuito sulla pagina delle API di Brave e definire la funzione stessa*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Nonostante sia un LLM, una delle limitazioni di Llama 3.1 è la multimodalità. Ovvero, la capacità di utilizzare diversi tipi di input come immagini come prompt e fornire risposte. Questa funzionalità è una delle principali novità di Llama 3.2. Queste caratteristiche includono anche:\n",
    "\n",
    "- Multimodalità - ha la capacità di valutare sia prompt testuali che immagini\n",
    "- Varianti di dimensioni piccole e medie (11B e 90B) - questo offre opzioni di implementazione flessibili,\n",
    "- Varianti solo testo (1B e 3B) - questo permette al modello di essere utilizzato su dispositivi edge / mobili e garantisce bassa latenza\n",
    "\n",
    "Il supporto multimodale rappresenta un grande passo avanti nel mondo dei modelli open source. L’esempio di codice qui sotto accetta sia un’immagine che un prompt testuale per ottenere un’analisi dell’immagine da Llama 3.2 90B.\n",
    "\n",
    "### Supporto multimodale con Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'apprendimento non si ferma qui, continua il tuo percorso\n",
    "\n",
    "Dopo aver completato questa lezione, dai un'occhiata alla nostra [collezione di apprendimento sull'IA generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) per continuare a migliorare le tue conoscenze sull'IA generativa!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nQuesto documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Pur impegnandoci per garantire l’accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa deve essere considerato la fonte autorevole. Per informazioni di carattere critico, si raccomanda una traduzione professionale eseguita da un essere umano. Non siamo responsabili per eventuali malintesi o interpretazioni errate derivanti dall’uso di questa traduzione.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:42:15+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "it"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}