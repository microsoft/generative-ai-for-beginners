<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:45:40+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "it"
}
-->
# Introduzione alle Reti Neurali. Perceptrone Multistrato

Nella sezione precedente, hai imparato il modello di rete neurale piÃ¹ semplice: il perceptrone a singolo strato, un modello lineare per la classificazione binaria.

In questa sezione estenderemo questo modello in un framework piÃ¹ flessibile, che ci permetterÃ  di:

* eseguire la **classificazione multiclasse** oltre alla classificazione binaria
* risolvere problemi di **regressione** oltre alla classificazione
* separare classi che non sono linearmente separabili

Svilupperemo inoltre un nostro framework modulare in Python che ci consentirÃ  di costruire diverse architetture di reti neurali.

## Formalizzazione del Machine Learning

Iniziamo formalizzando il problema del Machine Learning. Supponiamo di avere un dataset di addestramento **X** con etichette **Y**, e di dover costruire un modello *f* che faccia previsioni il piÃ¹ accurate possibile. La qualitÃ  delle previsioni Ã¨ misurata dalla **funzione di perdita** â„’. Le seguenti funzioni di perdita sono spesso utilizzate:

* Per problemi di regressione, quando dobbiamo prevedere un numero, possiamo usare lâ€™**errore assoluto** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, oppure lâ€™**errore quadratico** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Per la classificazione, usiamo la **perdita 0-1** (che Ã¨ sostanzialmente la stessa cosa della **accuratezza** del modello), oppure la **perdita logistica**.

Per il perceptrone a un livello, la funzione *f* era definita come una funzione lineare *f(x)=wx+b* (qui *w* Ã¨ la matrice dei pesi, *x* Ã¨ il vettore delle caratteristiche di input, e *b* Ã¨ il vettore di bias). Per diverse architetture di reti neurali, questa funzione puÃ² assumere forme piÃ¹ complesse.

> Nel caso della classificazione, spesso Ã¨ desiderabile ottenere come output della rete le probabilitÃ  delle classi corrispondenti. Per convertire numeri arbitrari in probabilitÃ  (ad esempio per normalizzare lâ€™output), usiamo spesso la funzione **softmax** Ïƒ, e la funzione *f* diventa *f(x)=Ïƒ(wx+b)*

Nella definizione di *f* sopra, *w* e *b* sono chiamati **parametri** Î¸=âŸ¨*w,b*âŸ©. Dato il dataset âŸ¨**X**,**Y**âŸ©, possiamo calcolare lâ€™errore complessivo su tutto il dataset come funzione dei parametri Î¸.

> âœ… **Lâ€™obiettivo dellâ€™addestramento della rete neurale Ã¨ minimizzare lâ€™errore variando i parametri Î¸**

## Ottimizzazione con Discesa del Gradiente

Esiste un metodo ben noto per lâ€™ottimizzazione di funzioni chiamato **discesa del gradiente**. Lâ€™idea Ã¨ che possiamo calcolare la derivata (nel caso multidimensionale chiamata **gradiente**) della funzione di perdita rispetto ai parametri, e modificare i parametri in modo che lâ€™errore diminuisca. Questo puÃ² essere formalizzato come segue:

* Inizializza i parametri con valori casuali w<sup>(0)</sup>, b<sup>(0)</sup>
* Ripeti piÃ¹ volte il seguente passo:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Durante lâ€™addestramento, i passi di ottimizzazione dovrebbero essere calcolati considerando lâ€™intero dataset (ricorda che la perdita Ã¨ calcolata come somma su tutti i campioni di addestramento). Tuttavia, nella pratica si prendono piccole porzioni del dataset chiamate **minibatch**, e si calcolano i gradienti su un sottoinsieme di dati. PoichÃ© il sottoinsieme viene scelto casualmente ogni volta, questo metodo Ã¨ chiamato **discesa del gradiente stocastica** (SGD).

## Perceptroni Multistrato e Backpropagation

La rete a un solo strato, come abbiamo visto sopra, Ã¨ in grado di classificare classi linearmente separabili. Per costruire un modello piÃ¹ ricco, possiamo combinare piÃ¹ strati della rete. Matematicamente questo significa che la funzione *f* avrÃ  una forma piÃ¹ complessa, e sarÃ  calcolata in piÃ¹ passaggi:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Qui, Î± Ã¨ una **funzione di attivazione non lineare**, Ïƒ Ã¨ la funzione softmax, e i parametri Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Lâ€™algoritmo di discesa del gradiente rimane lo stesso, ma il calcolo dei gradienti diventa piÃ¹ complesso. Applicando la regola della derivazione a catena, possiamo calcolare le derivate come:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… La regola della derivazione a catena viene usata per calcolare le derivate della funzione di perdita rispetto ai parametri.

Nota che la parte piÃ¹ a sinistra di tutte queste espressioni Ã¨ la stessa, quindi possiamo calcolare efficacemente le derivate partendo dalla funzione di perdita e procedendo "a ritroso" attraverso il grafo computazionale. Per questo motivo il metodo di addestramento di un perceptrone multistrato si chiama **backpropagation**, o semplicemente 'backprop'.



> TODO: citazione immagine

> âœ… Approfondiremo il backprop in modo molto piÃ¹ dettagliato nel nostro esempio nel notebook.

## Conclusione

In questa lezione abbiamo costruito la nostra libreria per reti neurali e lâ€™abbiamo utilizzata per un semplice compito di classificazione bidimensionale.

## ðŸš€ Sfida

Nel notebook allegato, implementerai il tuo framework per costruire e addestrare perceptroni multistrato. Potrai vedere nel dettaglio come funzionano le reti neurali moderne.

Procedi al notebook OwnFramework e segui le istruzioni.



## Revisione e Autoapprendimento

La backpropagation Ã¨ un algoritmo comune usato in AI e ML, vale la pena studiarlo piÃ¹ a fondo.

## Compito

In questo laboratorio ti viene chiesto di usare il framework costruito in questa lezione per risolvere la classificazione delle cifre scritte a mano del dataset MNIST.

* Istruzioni
* Notebook

**Disclaimer**:  
Questo documento Ã¨ stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Pur impegnandoci per garantire accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa deve essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un umano. Non ci assumiamo alcuna responsabilitÃ  per eventuali malintesi o interpretazioni errate derivanti dallâ€™uso di questa traduzione.