<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ef74ad58fc01f7ad80788f79505f9816",
  "translation_date": "2025-08-26T16:36:24+00:00",
  "source_file": "09-building-image-applications/README.md",
  "language_code": "it"
}
-->
# Costruire Applicazioni di Generazione di Immagini

[![Building Image Generation Applications](../../../translated_images/09-lesson-banner.906e408c741f44112ff5da17492a30d3872abb52b8530d6506c2631e86e704d0.it.png)](https://aka.ms/gen-ai-lesson9-gh?WT.mc_id=academic-105485-koreyst)

Le LLM non si limitano solo alla generazione di testo. È anche possibile generare immagini a partire da descrizioni testuali. Avere le immagini come modalità può essere estremamente utile in diversi ambiti, dalla tecnologia medica, all’architettura, al turismo, allo sviluppo di videogiochi e molto altro. In questo capitolo, vedremo i due modelli di generazione di immagini più popolari: DALL-E e Midjourney.

## Introduzione

In questa lezione vedremo:

- La generazione di immagini e perché è utile.
- DALL-E e Midjourney: cosa sono e come funzionano.
- Come costruire un’applicazione per la generazione di immagini.

## Obiettivi di apprendimento

Al termine di questa lezione sarai in grado di:

- Creare un’applicazione di generazione di immagini.
- Definire i limiti della tua applicazione con meta prompt.
- Lavorare con DALL-E e Midjourney.

## Perché costruire un’applicazione di generazione di immagini?

Le applicazioni di generazione di immagini sono un ottimo modo per esplorare le potenzialità dell’Intelligenza Artificiale Generativa. Possono essere utilizzate, ad esempio, per:

- **Modifica e sintesi di immagini**. Puoi generare immagini per diversi casi d’uso, come l’editing o la sintesi di immagini.

- **Applicazioni in diversi settori**. Possono essere usate per generare immagini in vari settori come Medtech, Turismo, Sviluppo di videogiochi e altro ancora.

## Scenario: Edu4All

In questa lezione continueremo a lavorare con la nostra startup, Edu4All. Gli studenti creeranno immagini per le loro valutazioni; quali immagini realizzare dipende da loro, ma potrebbero essere illustrazioni per una loro fiaba, la creazione di un nuovo personaggio per la loro storia o per aiutarli a visualizzare idee e concetti.

Ecco un esempio di cosa potrebbero generare gli studenti di Edu4All se stanno lavorando in classe sui monumenti:

![Edu4All startup, class on monuments, Eiffel Tower](../../../translated_images/startup.94d6b79cc4bb3f5afbf6e2ddfcf309aa5d1e256b5f30cc41d252024eaa9cc5dc.it.png)

utilizzando un prompt come

> "Cane accanto alla Torre Eiffel nella luce del primo mattino"

## Che cosa sono DALL-E e Midjourney?

[DALL-E](https://openai.com/dall-e-2?WT.mc_id=academic-105485-koreyst) e [Midjourney](https://www.midjourney.com/?WT.mc_id=academic-105485-koreyst) sono due dei modelli di generazione di immagini più popolari e permettono di utilizzare prompt per generare immagini.

### DALL-E

Iniziamo con DALL-E, un modello di Intelligenza Artificiale Generativa che crea immagini a partire da descrizioni testuali.

> [DALL-E è una combinazione di due modelli, CLIP e diffused attention](https://towardsdatascience.com/openais-dall-e-and-clip-101-a-brief-introduction-3a4367280d4e?WT.mc_id=academic-105485-koreyst).

- **CLIP** è un modello che genera embedding, cioè rappresentazioni numeriche dei dati, sia da immagini che da testo.

- **Diffused attention** è un modello che genera immagini a partire dagli embedding. DALL-E è addestrato su un dataset di immagini e testo e può essere usato per generare immagini da descrizioni testuali. Ad esempio, DALL-E può essere usato per generare immagini di un gatto con un cappello o di un cane con la cresta.

### Midjourney

Midjourney funziona in modo simile a DALL-E: genera immagini a partire da prompt testuali. Anche Midjourney può essere usato per generare immagini con prompt come “un gatto con un cappello” o “un cane con la cresta”.

![Image generated by Midjourney, mechanical pigeon](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Rupert_Breheny_mechanical_dove_eca144e7-476d-4976-821d-a49c408e4f36.png/440px-Rupert_Breheny_mechanical_dove_eca144e7-476d-4976-821d-a49c408e4f36.png?WT.mc_id=academic-105485-koreyst)
_Crediti immagine Wikipedia, immagine generata da Midjourney_

## Come funzionano DALL-E e Midjourney

Prima di tutto, [DALL-E](https://arxiv.org/pdf/2102.12092.pdf?WT.mc_id=academic-105485-koreyst). DALL-E è un modello di AI Generativa basato sull’architettura transformer con un _autoregressive transformer_.

Un _autoregressive transformer_ definisce come un modello genera immagini da descrizioni testuali: genera un pixel alla volta, poi usa i pixel già generati per creare il successivo, passando attraverso diversi strati di una rete neurale fino a completare l’immagine.

Con questo processo, DALL-E controlla attributi, oggetti, caratteristiche e altro ancora nell’immagine generata. Tuttavia, DALL-E 2 e 3 offrono un controllo ancora maggiore sull’immagine prodotta.

## Costruire la tua prima applicazione di generazione di immagini

Cosa serve per costruire un’applicazione di generazione di immagini? Ti servono le seguenti librerie:

- **python-dotenv**, è fortemente consigliato usare questa libreria per mantenere le tue credenziali in un file _.env_ separato dal codice.
- **openai**, questa libreria serve per interagire con l’API di OpenAI.
- **pillow**, per lavorare con le immagini in Python.
- **requests**, per effettuare richieste HTTP.

## Creare e distribuire un modello Azure OpenAI

Se non l’hai già fatto, segui le istruzioni sulla pagina [Microsoft Learn](https://learn.microsoft.com/azure/ai-foundry/openai/how-to/create-resource?pivots=web-portal)
per creare una risorsa e un modello Azure OpenAI. Seleziona DALL-E 3 come modello.  

## Crea l’app

1. Crea un file _.env_ con il seguente contenuto:

   ```text
   AZURE_OPENAI_ENDPOINT=<your endpoint>
   AZURE_OPENAI_API_KEY=<your key>
   AZURE_OPENAI_DEPLOYMENT="dall-e-3"
   ```

   Trova queste informazioni nel portale Azure OpenAI Foundry per la tua risorsa nella sezione "Deployments".

1. Raccogli le librerie sopra in un file chiamato _requirements.txt_ come segue:

   ```text
   python-dotenv
   openai
   pillow
   requests
   ```

1. Successivamente, crea un ambiente virtuale e installa le librerie:

   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

   Su Windows, usa i seguenti comandi per creare e attivare l’ambiente virtuale:

   ```bash
   python3 -m venv venv
   venv\Scripts\activate.bat
   ```

1. Aggiungi il seguente codice in un file chiamato _app.py_:

    ```python
    import openai
    import os
    import requests
    from PIL import Image
    import dotenv
    from openai import OpenAI, AzureOpenAI
    
    # import dotenv
    dotenv.load_dotenv()
    
    # configure Azure OpenAI service client 
    client = AzureOpenAI(
      azure_endpoint = os.environ["AZURE_OPENAI_ENDPOINT"],
      api_key=os.environ['AZURE_OPENAI_API_KEY'],
      api_version = "2024-02-01"
      )
    try:
        # Create an image by using the image generation API
        generation_response = client.images.generate(
                                prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',
                                size='1024x1024', n=1,
                                model=os.environ['AZURE_OPENAI_DEPLOYMENT']
                              )

        # Set the directory for the stored image
        image_dir = os.path.join(os.curdir, 'images')

        # If the directory doesn't exist, create it
        if not os.path.isdir(image_dir):
            os.mkdir(image_dir)

        # Initialize the image path (note the filetype should be png)
        image_path = os.path.join(image_dir, 'generated-image.png')

        # Retrieve the generated image
        image_url = generation_response.data[0].url  # extract image URL from response
        generated_image = requests.get(image_url).content  # download the image
        with open(image_path, "wb") as image_file:
            image_file.write(generated_image)

        # Display the image in the default image viewer
        image = Image.open(image_path)
        image.show()

    # catch exceptions
    except openai.InvalidRequestError as err:
        print(err)
   ```

Spieghiamo questo codice:

- Per prima cosa importiamo le librerie necessarie, tra cui la libreria OpenAI, dotenv, requests e Pillow.

  ```python
  import openai
  import os
  import requests
  from PIL import Image
  import dotenv
  ```

- Poi carichiamo le variabili d’ambiente dal file _.env_.

  ```python
  # import dotenv
  dotenv.load_dotenv()
  ```

- Successivamente, configuriamo il client del servizio Azure OpenAI

  ```python
  # Get endpoint and key from environment variables
  client = AzureOpenAI(
      azure_endpoint = os.environ["AZURE_OPENAI_ENDPOINT"],
      api_key=os.environ['AZURE_OPENAI_API_KEY'],
      api_version = "2024-02-01"
      )
  ```

- Poi generiamo l’immagine:

  ```python
  # Create an image by using the image generation API
  generation_response = client.images.generate(
                        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',
                        size='1024x1024', n=1,
                        model=os.environ['AZURE_OPENAI_DEPLOYMENT']
                      )
  ```

  Il codice sopra restituisce un oggetto JSON che contiene l’URL dell’immagine generata. Possiamo usare l’URL per scaricare l’immagine e salvarla su un file.

- Infine, apriamo l’immagine e la visualizziamo con il visualizzatore di immagini standard:

  ```python
  image = Image.open(image_path)
  image.show()
  ```

### Dettagli sulla generazione dell’immagine

Vediamo più nel dettaglio il codice che genera l’immagine:

    ```python
      generation_response = client.images.generate(
                                prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',
                                size='1024x1024', n=1,
                                model=os.environ['AZURE_OPENAI_DEPLOYMENT']
                            )
    ```

- **prompt** è il testo che viene usato per generare l’immagine. In questo caso, usiamo il prompt "Coniglio a cavallo, con un lecca-lecca, in un prato nebbioso dove crescono narcisi".
- **size** è la dimensione dell’immagine generata. In questo caso, generiamo un’immagine di 1024x1024 pixel.
- **n** è il numero di immagini generate. In questo caso, ne generiamo due.
- **temperature** è un parametro che controlla la casualità dell’output di un modello di AI Generativa. Il valore va da 0 a 1, dove 0 significa che l’output è deterministico e 1 che è casuale. Il valore predefinito è 0.7.

Ci sono altre cose che puoi fare con le immagini e che vedremo nella prossima sezione.

## Altre funzionalità della generazione di immagini

Finora hai visto come sia possibile generare un’immagine con poche righe di Python. Tuttavia, ci sono altre possibilità.

Puoi anche:

- **Effettuare modifiche**. Fornendo un’immagine esistente, una maschera e un prompt, puoi modificare un’immagine. Ad esempio, puoi aggiungere qualcosa in una parte dell’immagine. Immagina la nostra immagine del coniglio: puoi aggiungere un cappello al coniglio. Per farlo, fornisci l’immagine, una maschera (che identifica la parte da modificare) e un prompt testuale che descrive cosa fare.
> Nota: questa funzione non è supportata in DALL-E 3.
 
Ecco un esempio usando GPT Image:

    ```python
    response = client.images.edit(
        model="gpt-image-1",
        image=open("sunlit_lounge.png", "rb"),
        mask=open("mask.png", "rb"),
        prompt="A sunlit indoor lounge area with a pool containing a flamingo"
    )
    image_url = response.data[0].url
    ```

  L’immagine di base contiene solo il salotto con piscina, ma l’immagine finale avrà anche un fenicottero:

<div style="display: flex; justify-content: space-between; align-items: center; margin: 20px 0;">
  <img src="./images/sunlit_lounge.png" style="width: 30%; max-width: 200px; height: auto;">
  <img src="./images/mask.png" style="width: 30%; max-width: 200px; height: auto;">
  <img src="./images/sunlit_lounge_result.png" style="width: 30%; max-width: 200px; height: auto;">
</div>


- **Creare variazioni**. L’idea è prendere un’immagine esistente e chiedere di crearne delle varianti. Per creare una variazione, fornisci un’immagine, un prompt testuale e un codice come questo:

  ```python
  response = openai.Image.create_variation(
    image=open("bunny-lollipop.png", "rb"),
    n=1,
    size="1024x1024"
  )
  image_url = response['data'][0]['url']
  ```

  > Nota: questa funzione è supportata solo su OpenAI

## Temperature

La temperature è un parametro che controlla la casualità dell’output di un modello di AI Generativa. Il valore va da 0 a 1, dove 0 significa che l’output è deterministico e 1 che è casuale. Il valore predefinito è 0.7.

Vediamo un esempio di come funziona la temperature, eseguendo questo prompt due volte:

> Prompt: "Coniglio a cavallo, con un lecca-lecca, in un prato nebbioso dove crescono narcisi"

![Bunny on a horse holding a lollipop, version 1](../../../translated_images/v1-generated-image.a295cfcffa3c13c2432eb1e41de7e49a78c814000fb1b462234be24b6e0db7ea.it.png)

Ora eseguiamo lo stesso prompt per vedere che non otterremo due volte la stessa immagine:

![Generated image of bunny on horse](../../../translated_images/v2-generated-image.33f55a3714efe61dc19622c869ba6cd7d6e6de562e26e95b5810486187aace39.it.png)

Come puoi vedere, le immagini sono simili ma non identiche. Proviamo ora a cambiare il valore della temperature a 0.1 e vediamo cosa succede:

```python
 generation_response = client.images.create(
        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here
        size='1024x1024',
        n=2
    )
```

### Cambiare la temperature

Proviamo a rendere la risposta più deterministica. Abbiamo visto che nelle due immagini generate, nella prima c’è un coniglio e nella seconda un cavallo, quindi le immagini variano molto.

Modifichiamo quindi il nostro codice e impostiamo la temperature a 0, così:

```python
generation_response = client.images.create(
        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here
        size='1024x1024',
        n=2,
        temperature=0
    )
```

Ora, eseguendo questo codice, ottieni queste due immagini:

- ![Temperature 0, v1](../../../translated_images/v1-temp-generated-image.a4346e1d2360a056d855ee3dfcedcce91211747967cb882e7d2eff2076f90e4a.it.png)
- ![Temperature 0 , v2](../../../translated_images/v2-temp-generated-image.871d0c920dbfb0f1cb5d9d80bffd52da9b41f83b386320d9a9998635630ec83d.it.png)

Qui puoi vedere chiaramente che le immagini si somigliano molto di più.

## Come definire i limiti della tua applicazione con i metaprompt

Con la nostra demo, possiamo già generare immagini per i nostri clienti. Tuttavia, dobbiamo creare dei limiti per la nostra applicazione.

Ad esempio, non vogliamo generare immagini non adatte all’ambiente di lavoro o non appropriate per i bambini.

Possiamo farlo con i _metaprompt_. I metaprompt sono prompt testuali che servono a controllare l’output di un modello di AI Generativa. Ad esempio, possiamo usare i metaprompt per controllare l’output e assicurarci che le immagini generate siano adatte all’ambiente di lavoro o ai bambini.

### Come funziona?

Come funzionano i metaprompt?

I metaprompt sono prompt testuali che vengono usati per controllare l’output di un modello di AI Generativa. Vengono posizionati prima del prompt testuale e servono a controllare l’output del modello, integrandoli nelle applicazioni per guidare il risultato. Si incapsula l’input del prompt e quello del metaprompt in un unico prompt testuale.

Un esempio di metaprompt potrebbe essere il seguente:

```text
You are an assistant designer that creates images for children.

The image needs to be safe for work and appropriate for children.

The image needs to be in color.

The image needs to be in landscape orientation.

The image needs to be in a 16:9 aspect ratio.

Do not consider any input from the following that is not safe for work or appropriate for children.

(Input)

```

Vediamo ora come possiamo usare i metaprompt nella nostra demo.

```python
disallow_list = "swords, violence, blood, gore, nudity, sexual content, adult content, adult themes, adult language, adult humor, adult jokes, adult situations, adult"

meta_prompt =f"""You are an assistant designer that creates images for children.

The image needs to be safe for work and appropriate for children.

The image needs to be in color.

The image needs to be in landscape orientation.

The image needs to be in a 16:9 aspect ratio.

Do not consider any input from the following that is not safe for work or appropriate for children.
{disallow_list}
"""

prompt = f"{meta_prompt}
Create an image of a bunny on a horse, holding a lollipop"

# TODO add request to generate image
```

Dal prompt sopra, puoi vedere come tutte le immagini create tengano conto del metaprompt.

## Esercizio - diamo spazio agli studenti

Abbiamo introdotto Edu4All all’inizio di questa lezione. Ora è il momento di permettere agli studenti di generare immagini per le loro valutazioni.

Gli studenti creeranno immagini per le loro valutazioni che contengano monumenti; quali monumenti scegliere dipende da loro. Gli studenti sono invitati a usare la loro creatività per collocare questi monumenti in contesti diversi.

## Soluzione

Ecco una possibile soluzione:

```python
import openai
import os
import requests
from PIL import Image
import dotenv
from openai import AzureOpenAI
# import dotenv
dotenv.load_dotenv()

# Get endpoint and key from environment variables
client = AzureOpenAI(
  azure_endpoint = os.environ["AZURE_OPENAI_ENDPOINT"],
  api_key=os.environ['AZURE_OPENAI_API_KEY'],
  api_version = "2024-02-01"
  )


disallow_list = "swords, violence, blood, gore, nudity, sexual content, adult content, adult themes, adult language, adult humor, adult jokes, adult situations, adult"

meta_prompt = f"""You are an assistant designer that creates images for children.

The image needs to be safe for work and appropriate for children.

The image needs to be in color.

The image needs to be in landscape orientation.

The image needs to be in a 16:9 aspect ratio.

Do not consider any input from the following that is not safe for work or appropriate for children.
{disallow_list}
"""

prompt = f"""{meta_prompt}
Generate monument of the Arc of Triumph in Paris, France, in the evening light with a small child holding a Teddy looks on.
""""

try:
    # Create an image by using the image generation API
    generation_response = client.images.generate(
        prompt=prompt,    # Enter your prompt text here
        size='1024x1024',
        n=1,
    )
    # Set the directory for the stored image
    image_dir = os.path.join(os.curdir, 'images')

    # If the directory doesn't exist, create it
    if not os.path.isdir(image_dir):
        os.mkdir(image_dir)

    # Initialize the image path (note the filetype should be png)
    image_path = os.path.join(image_dir, 'generated-image.png')

    # Retrieve the generated image
    image_url = generation_response.data[0].url  # extract image URL from response
    generated_image = requests.get(image_url).content  # download the image
    with open(image_path, "wb") as image_file:
        image_file.write(generated_image)

    # Display the image in the default image viewer
    image = Image.open(image_path)
    image.show()

# catch exceptions
except openai.BadRequestError as err:
    print(err)
```

## Ottimo lavoro! Continua a imparare
Dopo aver completato questa lezione, dai un’occhiata alla nostra [collezione di apprendimento sull’IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) per continuare a migliorare le tue conoscenze sull’IA Generativa!

Passa alla Lezione 10 dove vedremo come [creare applicazioni AI con poco codice](../10-building-low-code-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

---

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione AI [Co-op Translator](https://github.com/Azure/co-op-translator). Pur impegnandoci per garantire l’accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa deve essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall’uso di questa traduzione.