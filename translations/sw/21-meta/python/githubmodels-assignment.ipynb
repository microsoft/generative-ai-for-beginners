{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kujenga Kwa Kutumia Mifano ya Familia ya Meta\n",
    "\n",
    "## Utangulizi\n",
    "\n",
    "Somo hili litashughulikia:\n",
    "\n",
    "- Kuchunguza mifano miwili kuu ya familia ya Meta - Llama 3.1 na Llama 3.2\n",
    "- Kuelewa matumizi na mazingira yanayofaa kwa kila mfano\n",
    "- Mfano wa msimbo unaoonyesha sifa za kipekee za kila mfano\n",
    "\n",
    "## Familia ya Mifano ya Meta\n",
    "\n",
    "Katika somo hili, tutachunguza mifano 2 kutoka familia ya Meta au \"Llama Herd\" - Llama 3.1 na Llama 3.2\n",
    "\n",
    "Mifano hii inapatikana katika matoleo tofauti na inapatikana kwenye soko la Github Model. Hapa kuna maelezo zaidi kuhusu kutumia Github Models kwa [kutengeneza mfano wa haraka na mifano ya AI](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Toleo la Mifano:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Angalizo: Llama 3 pia inapatikana kwenye Github Models lakini haitajadiliwa kwenye somo hili*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Ikiwa na Vigezo Bilioni 405, Llama 3.1 inaingia katika kundi la LLM za chanzo huria.\n",
    "\n",
    "Toleo hili ni uboreshaji wa Llama 3 ya awali kwa kutoa:\n",
    "\n",
    "- Dirisha kubwa la muktadha - tokeni 128k dhidi ya tokeni 8k\n",
    "- Idadi kubwa ya Tokeni za Utoaji - 4096 dhidi ya 2048\n",
    "- Msaada Bora wa Lugha Nyingi - kutokana na ongezeko la tokeni za mafunzo\n",
    "\n",
    "Haya yote yanaiwezesha Llama 3.1 kushughulikia matumizi magumu zaidi wakati wa kujenga programu za GenAI ikiwa ni pamoja na:\n",
    "- Uwezo wa Kuitisha Kazi Asili - uwezo wa kuita zana na kazi za nje nje ya mtiririko wa LLM\n",
    "- Utendaji Bora wa RAG - kutokana na dirisha kubwa la muktadha\n",
    "- Uzalishaji wa Data Bandia - uwezo wa kuunda data bora kwa kazi kama vile uboreshaji wa kina\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kuitisha Kazi Asili\n",
    "\n",
    "Llama 3.1 imeboreshwa zaidi ili kuwa bora katika kuitisha kazi au kutumia zana. Pia ina zana mbili zilizojengwa ndani ambazo mfano unaweza kutambua zinahitajika kutumika kulingana na ombi la mtumiaji. Zana hizi ni:\n",
    "\n",
    "- **Brave Search** - Inaweza kutumika kupata taarifa za hivi karibuni kama hali ya hewa kwa kufanya utafutaji mtandaoni\n",
    "- **Wolfram Alpha** - Inaweza kutumika kwa mahesabu magumu zaidi ya kihisabati hivyo hauhitaji kuandika kazi zako mwenyewe.\n",
    "\n",
    "Unaweza pia kutengeneza zana zako maalum ambazo LLM inaweza kuitisha.\n",
    "\n",
    "Katika mfano wa msimbo hapa chini:\n",
    "\n",
    "- Tunabainisha zana zinazopatikana (brave_search, wolfram_alpha) kwenye amri ya mfumo.\n",
    "- Tunatuma ombi la mtumiaji linalouliza kuhusu hali ya hewa katika jiji fulani.\n",
    "- LLM itajibu kwa kuitisha zana ya Brave Search ambayo itaonekana kama hivi `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Angalizo: Mfano huu unafanya tu kuitisha zana, kama ungependa kupata matokeo, utahitaji kufungua akaunti bila malipo kwenye ukurasa wa Brave API na kubainisha kazi yenyewe*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Ingawa ni LLM, moja ya mipaka ambayo Llama 3.1 ina ni uwezo wa kutumia aina mbalimbali za data. Hii inamaanisha kuwa na uwezo wa kutumia aina tofauti za maingizo kama vile picha kama vichocheo na kutoa majibu. Uwezo huu ni moja ya sifa kuu za Llama 3.2. Sifa hizi pia zinajumuisha:\n",
    "\n",
    "- Uwezo wa kutumia aina mbalimbali za data - ina uwezo wa kuchakata vichocheo vya maandishi na picha\n",
    "- Toleo la ukubwa mdogo hadi wa kati (11B na 90B) - hili linatoa chaguo rahisi za usambazaji,\n",
    "- Toleo la maandishi pekee (1B na 3B) - hili linaiwezesha modeli kusambazwa kwenye vifaa vya pembeni / simu na kutoa majibu ya haraka\n",
    "\n",
    "Uwezo wa kutumia aina mbalimbali za data ni hatua kubwa katika ulimwengu wa modeli za chanzo huria. Mfano wa msimbo hapa chini unachukua picha na kichocheo cha maandishi ili kupata uchambuzi wa picha kutoka Llama 3.2 90B.\n",
    "\n",
    "### Uwezo wa Multimodal na Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kujifunza hakuishii hapa, endelea na Safari\n",
    "\n",
    "Baada ya kumaliza somo hili, angalia [mkusanyiko wetu wa Kujifunza kuhusu AI Inayotengeneza](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ili kuendelea kuongeza ujuzi wako wa AI Inayotengeneza!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Kanusho**:  \nHati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kwa usahihi, tafadhali fahamu kwamba tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati asili katika lugha yake ya asili inapaswa kuchukuliwa kama chanzo cha mamlaka. Kwa taarifa muhimu, inashauriwa kutumia mtafsiri wa kibinadamu mtaalamu. Hatuwajibiki kwa kutoelewana au tafsiri potofu zitokanazo na matumizi ya tafsiri hii.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:47:17+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "sw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}