{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kurekebisha kwa Ukamilifu Mifano ya Open AI\n",
    "\n",
    "Daftari hili linatokana na mwongozo wa sasa uliotolewa katika nyaraka za [Fine Tuning](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst) kutoka Open AI.\n",
    "\n",
    "Kurekebisha kwa ukamilifu huongeza utendaji wa mifano ya msingi kwa matumizi yako kwa kuirekebisha tena kwa data na muktadha wa ziada unaohusiana na matumizi au hali hiyo maalum. Kumbuka kuwa mbinu za uhandisi wa prompt kama _few shot learning_ na _retrieval augmented generation_ hukuwezesha kuboresha prompt ya msingi kwa data inayofaa ili kuboresha ubora. Hata hivyo, mbinu hizi zina kikomo cha ukubwa wa dirisha la tokeni la mfano wa msingi unaolengwa.\n",
    "\n",
    "Kwa kurekebisha kwa ukamilifu, tunarekebisha tena mfano wenyewe kwa data inayohitajika (kuturuhusu kutumia mifano mingi zaidi kuliko inavyoweza kufaa katika dirisha la tokeni la juu kabisa) - na kupeleka toleo _la kawaida_ la mfano ambalo halihitaji tena mifano kutolewa wakati wa utambuzi. Hii si tu huongeza ufanisi wa muundo wa prompt yetu (tunakuwa na uhuru zaidi kutumia dirisha la tokeni kwa mambo mengine) bali pia huenda ikaboresha gharama zetu (kwa kupunguza idadi ya tokeni tunazohitaji kutuma kwa mfano wakati wa utambuzi).\n",
    "\n",
    "Kurekebisha kwa ukamilifu kuna hatua 4:\n",
    "1. Andaa data ya mafunzo na uipakishe.\n",
    "1. Endesha kazi ya mafunzo kupata mfano uliorekebishwa kwa ukamilifu.\n",
    "1. Tathmini mfano uliorekebishwa na rudia kwa ubora.\n",
    "1. Tumia mfano uliorekebishwa kwa utambuzi wakati umefurahishwa.\n",
    "\n",
    "Kumbuka kuwa si mifano yote ya msingi inaunga mkono kurekebisha kwa ukamilifu - [angalia nyaraka za OpenAI](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned?WT.mc_id=academic-105485-koreyst) kwa taarifa za hivi karibuni. Unaweza pia kurekebisha mfano uliorekebishwa awali. Katika mafunzo haya, tutatumia `gpt-35-turbo` kama mfano wetu wa msingi wa lengo kwa kurekebisha kwa ukamilifu. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hatua 1.1: Andaa Seti Yako ya Data\n",
    "\n",
    "Tujenge chatbot inayokusaidia kuelewa jedwali la vipengele kwa kujibu maswali kuhusu kipengele kwa limerick. Katika mafunzo haya rahisi, tutaunda tu seti ya data ya kufundisha mfano kwa mifano michache ya majibu inayonyesha muundo unaotarajiwa wa data. Katika matumizi halisi, utahitaji kuunda seti ya data yenye mifano mingi zaidi. Pia unaweza kutumia seti ya data ya wazi (kwa eneo lako la matumizi) ikiwa ipo, na kuibadilisha muundo kwa ajili ya kufundisha kwa usahihi.\n",
    "\n",
    "Kwa kuwa tunazingatia `gpt-35-turbo` na kutafuta jibu la mzunguko mmoja (ukamilishaji wa mazungumzo) tunaweza kuunda mifano tukitumia [muundo huu uliopendekezwa](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?WT.mc_id=academic-105485-koreyst) unaoendana na mahitaji ya ukamilishaji wa mazungumzo ya OpenAI. Ikiwa unatarajia mazungumzo ya mizunguko mingi, utatumia [muundo wa mfano wa mizunguko mingi](https://platform.openai.com/docs/guides/fine-tuning/multi-turn-chat-examples?WT.mc_id=academic-105485-koreyst) unaojumuisha kipengele cha `weight` kuashiria ni ujumbe gani unapaswa kutumika (au la) katika mchakato wa kufundisha kwa usahihi.\n",
    "\n",
    "Tutatumia muundo rahisi wa mzunguko mmoja kwa mafunzo yetu hapa. Data iko katika [muundo wa jsonl](https://jsonlines.org/?WT.mc_id=academic-105485-koreyst) na rekodi 1 kwa kila mstari, kila moja ikiwakilishwa kama kitu kilichoandikwa kwa muundo wa JSON. Kipande kilicho chini kinaonyesha rekodi 2 kama sampuli - angalia [training-data.jsonl](../../../../../18-fine-tuning/python/openai/training-data.jsonl) kwa seti kamili ya sampuli (mifano 10) tutakayotumia kwa mafunzo haya ya kufundisha kwa usahihi. **Kumbuka:** Kila rekodi _inapaswa_ kufafanuliwa katika mstari mmoja (si kugawanywa kwenye mistari kama kawaida katika faili ya JSON iliyopangwa)\n",
    "\n",
    "```json\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Gallium\"}, {\"role\": \"assistant\", \"content\": \"Gallium, oh gallium, so light - Melts in your hand, oh what a sight - At 86 degrees - Its liquid with ease - And in semiconductors, it's out of sight\"}]}\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Hydrogen\"}, {\"role\": \"assistant\", \"content\": \"Hydrogen, the first in the line - The lightest of all, so divine - It's in water, you see - And in stars, it's the key - The universe's most common sign\"}]}\n",
    "```\n",
    "\n",
    "Katika matumizi halisi utahitaji seti kubwa zaidi ya mifano kwa matokeo mazuri - mabadiliko yatakuwa kati ya ubora wa majibu na muda/gharama za kufundisha kwa usahihi. Tunatumia seti ndogo ili tuweze kumaliza kufundisha kwa usahihi haraka kuonyesha mchakato. Angalia [mfano huu wa OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst) kwa mafunzo ya kufundisha kwa usahihi yenye ugumu zaidi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Hatua 1.2 Pandisha Dataset Yako\n",
    "\n",
    "Pandisha data kwa kutumia Files API [kama ilivyoelezwa hapa](https://platform.openai.com/docs/guides/fine-tuning/upload-a-training-file). Kumbuka kwamba ili kuendesha msimbo huu, lazima uwe umefanya hatua zifuatazo kwanza:\n",
    " - Umeweka kifurushi cha Python cha `openai` (hakikisha unatumia toleo >=0.28.0 kwa vipengele vya hivi karibuni)\n",
    " - Umeweka variable ya mazingira `OPENAI_API_KEY` kwa ufunguo wako wa API wa OpenAI\n",
    "Ili kujifunza zaidi, angalia [Mwongozo wa Usanidi](./../../../00-course-setup/02-setup-local.md?WT.mc_id=academic-105485-koreyst) uliotolewa kwa kozi.\n",
    "\n",
    "Sasa, endesha msimbo ili kuunda faili la kupandisha kutoka kwa faili lako la JSONL la ndani.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-JdAJcagdOTG6ACNlFWzuzmyV', bytes=4021, created_at=1715566183, filename='training-data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
      "Training File ID: file-JdAJcagdOTG6ACNlFWzuzmyV\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_file = client.files.create(\n",
    "  file=open(\"./training-data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(ft_file)\n",
    "print(\"Training File ID: \" + ft_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Hatua 2.1: Unda kazi ya Urekebishaji kwa kutumia SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', created_at=1715566184, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-EZ6ag0n0S6Zm8eV9BSWKmE6l', result_files=[], seed=830529052, status='validating_files', trained_tokens=None, training_file='file-JdAJcagdOTG6ACNlFWzuzmyV', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)\n",
      "Fine-tuning Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_filejob = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file.id, \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "print(ft_filejob)\n",
    "print(\"Fine-tuning Job ID: \" + ft_filejob.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Hatua 2.2: Angalia Hali ya Kazi\n",
    "\n",
    "Hapa kuna mambo machache unayoweza kufanya na API ya `client.fine_tuning.jobs`:\n",
    "- `client.fine_tuning.jobs.list(limit=<n>)` - Orodhesha kazi za fine-tuning za mwisho n\n",
    "- `client.fine_tuning.jobs.retrieve(<job_id>)` - Pata maelezo ya kazi maalum ya fine-tuning\n",
    "- `client.fine_tuning.jobs.cancel(<job_id>)` - Ghairi kazi ya fine-tuning\n",
    "- `client.fine_tuning.jobs.list_events(fine_tuning_job_id=<job_id>, limit=<b>)` - Orodhesha hadi matukio n kutoka kwa kazi\n",
    "- `client.fine_tuning.jobs.create(model=\"gpt-35-turbo\", training_file=\"your-training-file.jsonl\", ...)`\n",
    "\n",
    "Hatua ya kwanza ya mchakato ni _kuthibitisha faili la mafunzo_ kuhakikisha data iko katika muundo sahihi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-GkWiDgZmOsuv4q5cSTEGscY6', created_at=1715566184, level='info', message='Validating training file: file-JdAJcagdOTG6ACNlFWzuzmyV', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-3899xdVTO3LN7Q7LkKLMJUnb', created_at=1715566184, level='info', message='Created fine-tuning job: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_filejob.id, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n",
      "Status: running\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "# Once the training data is validated\n",
    "# Track the job status to see if it is running and when it is complete\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(\"Trained Tokens:\", response.trained_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Hatua 2.3: Fuatilia matukio kufuatilia maendeleo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 85/100: training loss=0.14\n",
      "Step 86/100: training loss=0.00\n",
      "Step 87/100: training loss=0.00\n",
      "Step 88/100: training loss=0.07\n",
      "Step 89/100: training loss=0.00\n",
      "Step 90/100: training loss=0.00\n",
      "Step 91/100: training loss=0.00\n",
      "Step 92/100: training loss=0.00\n",
      "Step 93/100: training loss=0.00\n",
      "Step 94/100: training loss=0.00\n",
      "Step 95/100: training loss=0.08\n",
      "Step 96/100: training loss=0.05\n",
      "Step 97/100: training loss=0.00\n",
      "Step 98/100: training loss=0.00\n",
      "Step 99/100: training loss=0.00\n",
      "Step 100/100: training loss=0.00\n",
      "Checkpoint created at step 80 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyyF2:ckpt-step-80\n",
      "Checkpoint created at step 90 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyzhK:ckpt-step-90\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "# You can also track progress in a more granular way by checking for events\n",
    "# Refresh this code till you get the `The job has successfully completed` message\n",
    "response = client.fine_tuning.jobs.list_events(ft_filejob.id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hatua 2.4: Tazama hali kwenye Dashibodi ya OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unaweza pia kuona hali kwa kutembelea tovuti ya OpenAI na kuchunguza sehemu ya _Fine-tuning_ ya jukwaa. Hii itakuonyesha hali ya kazi ya sasa, na pia itakuwezesha kufuatilia historia ya utekelezaji wa kazi za awali. Katika picha hii ya skrini, unaweza kuona kwamba utekelezaji wa awali ulishindwa, na utekelezaji wa pili ulifanikiwa. Kwa muktadha, hili lilitokea wakati utekelezaji wa kwanza ulitumia faili la JSON lenye rekodi zilizopangwa vibaya - mara tu liliporekebishwa, utekelezaji wa pili ulikamilika kwa mafanikio na kufanya mfano upatikane kwa matumizi.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/sw/fine-tuned-model-status.563271727bf7bfba.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unaweza pia kuona ujumbe wa hali na vipimo kwa kusogeza chini zaidi kwenye dashibodi ya kuona kama inavyoonyeshwa:\n",
    "\n",
    "| Ujumbe | Vipimo |\n",
    "|:---|:---|\n",
    "| ![Messages](../../../../../translated_images/sw/fine-tuned-messages-panel.4ed0c2da5ea1313b.png) |  ![Metrics](../../../../../translated_images/sw/fine-tuned-metrics-panel.700d7e4995a65229.png)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Hatua 3.1: Pata Kitambulisho & Jaribu Mfano Uliyorekebishwa kwa Uangalifu katika Msimbo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the identity of the fine-tuned model once ready\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "fine_tuned_model_id = response.fine_tuned_model\n",
    "print(\"Fine-tuned Model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Strontium, a metal so bright - It's in fireworks, a dazzling sight - It's in bones, you see - And in tea, it's the key - It's the fortieth, so pure, that's the right\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# You can then use that model to generate completions from the SDK as shown\n",
    "# Or you can load that model into the OpenAI Playground (in the UI) to validate it from there.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Elle, a factual chatbot that answers questions about elements in the periodic table with a limerick\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Strontium\"},\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Hatua 3.2: Pakia & Jaribu Mfano Uliyoboreshwa katika Playground\n",
    "\n",
    "Sasa unaweza kujaribu mfano ulioboreshwa kwa njia mbili. Kwanza, unaweza kutembelea Playground na kutumia menyu ya Models kuchagua mfano wako mpya ulioboreshwa kutoka kwa chaguzi zilizoorodheshwa. Chaguo jingine ni kutumia chaguo la \"Playground\" linaloonyeshwa kwenye paneli ya Fine-tuning (angalia picha hapo juu) ambalo linaanzisha mtazamo wa _kulinganisha_ unaoonyesha toleo la msingi na toleo la mfano ulioboreshwa kando kwa kando kwa tathmini ya haraka.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/sw/fine-tuned-playground-compare.56e06f0ad8922016.png)\n",
    "\n",
    "Jaza tu muktadha wa mfumo ulio tumika katika data yako ya mafunzo na toa swali lako la majaribio. Utaona kwamba pande zote mbili zinasasishwa na muktadha na swali sawa. Endesha kulinganisha na utaona tofauti katika matokeo kati yao. _Kumbuka jinsi mfano ulioboreshwa unavyotoa jibu kwa muundo ulioutoa katika mifano yako wakati mfano wa msingi unafuata tu maelekezo ya mfumo_.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/sw/fine-tuned-playground-launch.5a26495c983c6350.png)\n",
    "\n",
    "Utaona kwamba kulinganisha pia kunatoa idadi ya tokeni kwa kila mfano, na muda uliotumika kwa utambuzi. **Mfano huu maalum ni rahisi na unakusudia kuonyesha mchakato lakini hauwakilishi data halisi au hali halisi ya dunia**. Unaweza kuona kwamba sampuli zote mbili zinaonyesha idadi sawa ya tokeni (muktadha wa mfumo na maelekezo ya mtumiaji ni sawa) huku mfano ulioboreshwa ukichukua muda mrefu zaidi kwa utambuzi (mfano maalum).\n",
    "\n",
    "Katika hali halisi za dunia, hutatumia mfano wa toy kama huu, bali utafanya fine-tuning dhidi ya data halisi (mfano, katalogi ya bidhaa kwa huduma kwa wateja) ambapo ubora wa jibu utakuwa dhahiri zaidi. Katika _muktadha huo_, kupata ubora sawa wa jibu na mfano wa msingi kutahitaji uhandisi zaidi wa maelekezo maalum ambayo yataongeza matumizi ya tokeni na pengine muda wa usindikaji kwa utambuzi. _Ili kujaribu hili, angalia mifano ya fine-tuning katika OpenAI Cookbook kuanza._\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Kiarifa cha Kukataa**:\nHati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kwa usahihi, tafadhali fahamu kwamba tafsiri za kiotomatiki zinaweza kuwa na makosa au upungufu wa usahihi. Hati ya asili katika lugha yake ya asili inapaswa kuchukuliwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatubebei dhamana kwa kutoelewana au tafsiri potofu zinazotokana na matumizi ya tafsiri hii.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "1725725c956564056baf895e6ca92aa5",
   "translation_date": "2025-12-19T11:14:29+00:00",
   "source_file": "18-fine-tuning/python/openai/oai-assignment.ipynb",
   "language_code": "sw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}