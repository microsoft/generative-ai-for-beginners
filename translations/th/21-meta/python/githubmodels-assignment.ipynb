{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# การสร้างด้วยโมเดลตระกูล Meta\n",
    "\n",
    "## บทนำ\n",
    "\n",
    "บทเรียนนี้จะพูดถึง:\n",
    "\n",
    "- การสำรวจโมเดลหลักสองตัวในตระกูล Meta - Llama 3.1 และ Llama 3.2\n",
    "- ทำความเข้าใจกรณีการใช้งานและสถานการณ์ที่เหมาะสมของแต่ละโมเดล\n",
    "- ตัวอย่างโค้ดเพื่อแสดงจุดเด่นของแต่ละโมเดล\n",
    "\n",
    "## ตระกูลโมเดล Meta\n",
    "\n",
    "ในบทเรียนนี้ เราจะสำรวจ 2 โมเดลจากตระกูล Meta หรือที่เรียกว่า \"Llama Herd\" ได้แก่ Llama 3.1 และ Llama 3.2\n",
    "\n",
    "โมเดลเหล่านี้มีหลายเวอร์ชัน และสามารถใช้งานได้ในตลาดโมเดลของ Github ดูรายละเอียดเพิ่มเติมเกี่ยวกับการใช้ Github Models เพื่อ [สร้างต้นแบบด้วยโมเดล AI](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst)\n",
    "\n",
    "เวอร์ชันของโมเดล:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*หมายเหตุ: Llama 3 ก็มีให้ใช้งานบน Github Models เช่นกัน แต่จะไม่กล่าวถึงในบทเรียนนี้*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "ด้วยจำนวนพารามิเตอร์ 405 พันล้าน Llama 3.1 จัดอยู่ในหมวดหมู่ LLM แบบโอเพ่นซอร์ส\n",
    "\n",
    "เวอร์ชันนี้เป็นการอัปเกรดจาก Llama 3 รุ่นก่อนหน้า โดยมีจุดเด่นดังนี้:\n",
    "\n",
    "- หน้าต่างบริบทที่ใหญ่ขึ้น - 128k โทเคน เทียบกับ 8k โทเคน\n",
    "- จำนวนโทเคนเอาต์พุตสูงสุดที่มากขึ้น - 4096 เทียบกับ 2048\n",
    "- รองรับหลายภาษาดีขึ้น - เนื่องจากจำนวนโทเคนที่ใช้ฝึกสอนเพิ่มขึ้น\n",
    "\n",
    "สิ่งเหล่านี้ทำให้ Llama 3.1 สามารถรองรับกรณีการใช้งานที่ซับซ้อนมากขึ้นในการสร้างแอปพลิเคชัน GenAI เช่น:\n",
    "- การเรียกใช้ฟังก์ชันโดยตรง - สามารถเรียกใช้เครื่องมือหรือฟังก์ชันภายนอกนอกเหนือจากเวิร์กโฟลว์ของ LLM ได้\n",
    "- ประสิทธิภาพ RAG ที่ดีขึ้น - เนื่องจากหน้าต่างบริบทที่กว้างขึ้น\n",
    "- การสร้างข้อมูลสังเคราะห์ - สามารถสร้างข้อมูลที่มีประสิทธิภาพสำหรับงานอย่างการปรับแต่งโมเดล\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การเรียกใช้ฟังก์ชันโดยตรง\n",
    "\n",
    "Llama 3.1 ได้รับการปรับแต่งให้มีประสิทธิภาพมากขึ้นในการเรียกใช้ฟังก์ชันหรือเครื่องมือต่าง ๆ นอกจากนี้ยังมีเครื่องมือในตัวสองอย่างที่โมเดลสามารถระบุได้ว่าควรใช้ตามคำสั่งที่ผู้ใช้ป้อนเข้ามา เครื่องมือเหล่านี้ได้แก่\n",
    "\n",
    "- **Brave Search** - ใช้ค้นหาข้อมูลล่าสุด เช่น สภาพอากาศ โดยการค้นหาผ่านเว็บ\n",
    "- **Wolfram Alpha** - ใช้สำหรับคำนวณทางคณิตศาสตร์ที่ซับซ้อนมากขึ้น จึงไม่จำเป็นต้องเขียนฟังก์ชันเอง\n",
    "\n",
    "คุณยังสามารถสร้างเครื่องมือแบบกำหนดเองที่ LLM สามารถเรียกใช้งานได้ด้วย\n",
    "\n",
    "ในตัวอย่างโค้ดด้านล่างนี้:\n",
    "\n",
    "- เรากำหนดเครื่องมือที่มีให้ใช้ (brave_search, wolfram_alpha) ใน system prompt\n",
    "- ส่งคำถามจากผู้ใช้ที่สอบถามเกี่ยวกับสภาพอากาศในเมืองหนึ่ง\n",
    "- LLM จะตอบกลับด้วยการเรียกใช้เครื่องมือ Brave Search ซึ่งจะมีลักษณะดังนี้ `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*หมายเหตุ: ตัวอย่างนี้เป็นเพียงการเรียกใช้เครื่องมือเท่านั้น หากต้องการผลลัพธ์ คุณจะต้องสมัครบัญชีฟรีที่หน้า Brave API และกำหนดฟังก์ชันเอง*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "แม้ว่า Llama 3.1 จะเป็น LLM แต่ก็ยังมีข้อจำกัดในเรื่องมัลติโมดัล นั่นคือ ความสามารถในการใช้ข้อมูลเข้าแบบต่าง ๆ เช่น รูปภาพเป็นพรอมต์และให้คำตอบกลับ ความสามารถนี้เป็นหนึ่งในฟีเจอร์หลักของ Llama 3.2 โดยฟีเจอร์เหล่านี้ยังรวมถึง:\n",
    "\n",
    "- มัลติโมดัล - สามารถประเมินทั้งพรอมต์ข้อความและรูปภาพได้\n",
    "- ขนาดเล็กถึงกลาง (11B และ 90B) - ช่วยให้สามารถปรับใช้งานได้อย่างยืดหยุ่น\n",
    "- เวอร์ชันเฉพาะข้อความ (1B และ 3B) - ช่วยให้โมเดลสามารถนำไปใช้กับอุปกรณ์ edge หรือมือถือได้ และตอบสนองได้รวดเร็ว\n",
    "\n",
    "การรองรับมัลติโมดัลถือเป็นก้าวสำคัญในโลกของโมเดลโอเพ่นซอร์ส ตัวอย่างโค้ดด้านล่างนี้จะรับทั้งรูปภาพและพรอมต์ข้อความเพื่อวิเคราะห์รูปภาพด้วย Llama 3.2 90B\n",
    "\n",
    "### การรองรับมัลติโมดัลด้วย Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การเรียนรู้ไม่ได้หยุดแค่นี้ เดินหน้าต่อไป\n",
    "\n",
    "หลังจากจบบทเรียนนี้แล้ว ลองเข้าไปดู [ชุดการเรียนรู้ Generative AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ของเรา เพื่อพัฒนาความรู้ด้าน Generative AI ให้มากยิ่งขึ้น!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ข้อจำกัดความรับผิดชอบ**:  \nเอกสารฉบับนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามอย่างเต็มที่เพื่อความถูกต้อง แต่โปรดทราบว่าการแปลโดยระบบอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาต้นทางควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่มีความสำคัญ แนะนำให้ใช้บริการแปลโดยนักแปลมืออาชีพ ทางเราจะไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่คลาดเคลื่อนซึ่งเกิดจากการใช้การแปลนี้\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:43:37+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "th"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}