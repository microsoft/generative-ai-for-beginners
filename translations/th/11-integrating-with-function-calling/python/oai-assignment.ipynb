{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## บทนำ\n",
    "\n",
    "บทเรียนนี้จะครอบคลุมถึง:\n",
    "- ฟังก์ชันคอลคืออะไร และใช้งานในกรณีใดบ้าง\n",
    "- วิธีสร้างฟังก์ชันคอลโดยใช้ OpenAI\n",
    "- วิธีนำฟังก์ชันคอลไปใช้ในแอปพลิเคชัน\n",
    "\n",
    "## เป้าหมายการเรียนรู้\n",
    "\n",
    "หลังจากเรียนจบบทเรียนนี้ คุณจะสามารถและเข้าใจว่า:\n",
    "\n",
    "- จุดประสงค์ของการใช้ฟังก์ชันคอลคืออะไร\n",
    "- การตั้งค่าฟังก์ชันคอลโดยใช้ OpenAI Service\n",
    "- การออกแบบฟังก์ชันคอลให้เหมาะสมกับกรณีการใช้งานของแอปพลิเคชันของคุณ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ทำความเข้าใจเกี่ยวกับการเรียกใช้งานฟังก์ชัน\n",
    "\n",
    "สำหรับบทเรียนนี้ เราต้องการสร้างฟีเจอร์ให้กับสตาร์ทอัพด้านการศึกษาของเราที่ช่วยให้ผู้ใช้สามารถใช้แชทบอทเพื่อค้นหาคอร์สเทคนิคต่าง ๆ ได้ เราจะช่วยแนะนำคอร์สที่เหมาะกับระดับทักษะ ตำแหน่งงานปัจจุบัน และเทคโนโลยีที่ผู้ใช้สนใจ\n",
    "\n",
    "เพื่อให้ทำสิ่งนี้ได้ เราจะใช้เครื่องมือต่อไปนี้ร่วมกัน:\n",
    " - `OpenAI` เพื่อสร้างประสบการณ์แชทสำหรับผู้ใช้\n",
    " - `Microsoft Learn Catalog API` เพื่อช่วยให้ผู้ใช้ค้นหาคอร์สตามที่ร้องขอ\n",
    " - `Function Calling` เพื่อรับคำถามของผู้ใช้และส่งไปยังฟังก์ชันเพื่อเรียกใช้งาน API\n",
    "\n",
    "ก่อนเริ่มต้น มาดูกันว่าทำไมเราถึงควรใช้การเรียกใช้งานฟังก์ชันตั้งแต่แรก:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # รับคำตอบใหม่จาก GPT ที่สามารถเห็นผลลัพธ์ของฟังก์ชันได้\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ทำไมต้องใช้ Function Calling\n",
    "\n",
    "ถ้าคุณเคยเรียนบทเรียนอื่นในคอร์สนี้มาก่อน คุณน่าจะเข้าใจถึงพลังของการใช้ Large Language Models (LLMs) แล้ว และก็น่าจะเห็นข้อจำกัดบางอย่างของมันด้วย\n",
    "\n",
    "Function Calling เป็นฟีเจอร์ของ OpenAI Service ที่ถูกออกแบบมาเพื่อแก้ไขปัญหาต่อไปนี้:\n",
    "\n",
    "รูปแบบการตอบกลับที่ไม่สม่ำเสมอ:\n",
    "- ก่อนจะมี function calling การตอบกลับจาก LLM มักไม่มีโครงสร้างที่แน่นอนและไม่สม่ำเสมอ นักพัฒนาต้องเขียนโค้ดตรวจสอบความถูกต้องที่ซับซ้อนเพื่อรองรับความหลากหลายของผลลัพธ์\n",
    "\n",
    "การเชื่อมต่อกับข้อมูลภายนอกที่จำกัด:\n",
    "- ก่อนจะมีฟีเจอร์นี้ การนำข้อมูลจากส่วนอื่นของแอปพลิเคชันมาใช้ในบริบทของแชทเป็นเรื่องยาก\n",
    "\n",
    "ด้วยการกำหนดรูปแบบการตอบกลับให้เป็นมาตรฐาน และเปิดทางให้เชื่อมต่อกับข้อมูลภายนอกได้อย่างราบรื่น function calling จึงช่วยให้งานพัฒนาง่ายขึ้น และลดความจำเป็นในการเขียนโค้ดตรวจสอบเพิ่มเติม\n",
    "\n",
    "ผู้ใช้ไม่สามารถถามคำถามอย่างเช่น \"ตอนนี้อากาศที่สตอกโฮล์มเป็นอย่างไร?\" ได้ นั่นเป็นเพราะโมเดลมีข้อจำกัดอยู่แค่ข้อมูลที่ใช้ฝึกเท่านั้น\n",
    "\n",
    "ลองดูตัวอย่างด้านล่างนี้ที่แสดงให้เห็นถึงปัญหานี้:\n",
    "\n",
    "สมมติว่าเราต้องการสร้างฐานข้อมูลนักเรียนเพื่อแนะนำคอร์สที่เหมาะสมให้กับแต่ละคน ด้านล่างนี้คือลักษณะของนักเรียนสองคนที่มีข้อมูลคล้ายกันมาก\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราต้องการส่งข้อมูลนี้ไปยัง LLM เพื่อให้มันแยกวิเคราะห์ข้อมูล หลังจากนั้นเราสามารถนำข้อมูลนี้ไปใช้ในแอปพลิเคชันของเราเพื่อส่งไปยัง API หรือเก็บไว้ในฐานข้อมูล\n",
    "\n",
    "มาสร้าง prompt สองชุดที่เหมือนกัน เพื่อสั่งให้ LLM ทราบว่าข้อมูลแบบไหนที่เราต้องการ:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราต้องการส่งข้อความนี้ไปยัง LLM เพื่อแยกส่วนที่สำคัญต่อผลิตภัณฑ์ของเรา ดังนั้นเราจึงสามารถสร้างพรอมต์ที่เหมือนกันสองชุดเพื่อสั่งงาน LLM ได้\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "หลังจากสร้างพรอมต์ทั้งสองนี้แล้ว เราจะส่งไปยัง LLM โดยใช้ `openai.ChatCompletion` เราเก็บพรอมต์ไว้ในตัวแปร `messages` และกำหนดบทบาทเป็น `user` ซึ่งเป็นการจำลองข้อความจากผู้ใช้ที่เขียนถึงแชทบอท\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ถึงแม้ว่าคำสั่งจะเหมือนกันและคำอธิบายจะคล้ายกัน แต่เราสามารถได้รูปแบบของ property `Grades` ที่แตกต่างกัน\n",
    "\n",
    "ถ้าคุณรันเซลล์ข้างบนหลายครั้ง รูปแบบที่ได้อาจเป็น `3.7` หรือ `3.7 GPA`\n",
    "\n",
    "สาเหตุก็เพราะ LLM รับข้อมูลที่ไม่มีโครงสร้างในรูปแบบของ prompt ที่เขียนขึ้น และก็ส่งคืนข้อมูลที่ไม่มีโครงสร้างเช่นกัน เราจำเป็นต้องมีรูปแบบข้อมูลที่มีโครงสร้างเพื่อที่เราจะได้รู้ว่าจะคาดหวังอะไรเมื่อจัดเก็บหรือใช้งานข้อมูลนี้\n",
    "\n",
    "โดยการใช้ functional calling เราสามารถมั่นใจได้ว่าเราจะได้รับข้อมูลที่มีโครงสร้างกลับมา เวลาที่ใช้ function calling, LLM จะไม่ได้เรียกหรือรันฟังก์ชันจริง ๆ แต่เราจะสร้างโครงสร้างให้ LLM ทำตามสำหรับการตอบกลับ จากนั้นเราจะใช้ข้อมูลที่มีโครงสร้างเหล่านั้นเพื่อรู้ว่าจะต้องรันฟังก์ชันไหนในแอปพลิเคชันของเรา\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### กรณีการใช้งานสำหรับการเรียกใช้ฟังก์ชัน\n",
    "\n",
    "**การเรียกใช้เครื่องมือภายนอก**\n",
    "แชทบอทสามารถให้คำตอบกับคำถามของผู้ใช้ได้ดีมาก ด้วยการใช้การเรียกฟังก์ชัน แชทบอทสามารถนำข้อความจากผู้ใช้ไปดำเนินงานบางอย่างได้ เช่น นักเรียนอาจขอให้แชทบอท \"ส่งอีเมลถึงอาจารย์ของฉันว่าฉันต้องการความช่วยเหลือเพิ่มเติมในวิชานี้\" ซึ่งจะสามารถเรียกใช้ฟังก์ชัน `send_email(to: string, body: string)`\n",
    "\n",
    "**สร้างคำสั่ง API หรือฐานข้อมูล**\n",
    "ผู้ใช้สามารถค้นหาข้อมูลด้วยภาษาธรรมชาติที่ถูกแปลงเป็นคำสั่งหรือคำขอ API ที่มีรูปแบบ เช่น ครูอาจถามว่า \"นักเรียนคนไหนที่ทำงานที่แล้วเสร็จบ้าง\" ซึ่งสามารถเรียกใช้ฟังก์ชันชื่อ `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**สร้างข้อมูลแบบมีโครงสร้าง**\n",
    "ผู้ใช้สามารถนำข้อความหรือไฟล์ CSV มาให้ LLM เพื่อดึงข้อมูลสำคัญออกมา เช่น นักเรียนสามารถแปลงบทความวิกิพีเดียเกี่ยวกับข้อตกลงสันติภาพเพื่อสร้างแฟลชการ์ด AI ได้ โดยสามารถใช้ฟังก์ชัน `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. การสร้างการเรียกใช้ฟังก์ชันครั้งแรกของคุณ\n",
    "\n",
    "ขั้นตอนการสร้างการเรียกใช้ฟังก์ชันมี 3 ขั้นตอนหลัก:\n",
    "1. เรียกใช้ Chat Completions API พร้อมกับรายการฟังก์ชันของคุณและข้อความจากผู้ใช้\n",
    "2. อ่านคำตอบของโมเดลเพื่อดำเนินการ เช่น เรียกใช้ฟังก์ชันหรือ API\n",
    "3. เรียกใช้ Chat Completions API อีกครั้ง พร้อมกับผลลัพธ์ที่ได้จากฟังก์ชันของคุณ เพื่อนำข้อมูลนั้นมาใช้สร้างคำตอบให้กับผู้ใช้\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### องค์ประกอบของการเรียกใช้ฟังก์ชัน\n",
    "\n",
    "#### ข้อมูลที่ผู้ใช้ป้อน\n",
    "\n",
    "ขั้นตอนแรกคือการสร้างข้อความจากผู้ใช้ ซึ่งสามารถกำหนดค่าได้แบบไดนามิกโดยรับค่าจากกล่องข้อความ หรือจะกำหนดค่าตรงนี้เลยก็ได้ หากนี่เป็นครั้งแรกที่คุณใช้งาน Chat Completions API เราต้องกำหนด `role` และ `content` ของข้อความ\n",
    "\n",
    "`role` สามารถเป็นได้ทั้ง `system` (กำหนดกติกา), `assistant` (โมเดล), หรือ `user` (ผู้ใช้งานจริง) สำหรับการเรียกใช้ฟังก์ชัน เราจะกำหนดเป็น `user` พร้อมตัวอย่างคำถาม\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การสร้างฟังก์ชัน\n",
    "\n",
    "ต่อไปเราจะกำหนดฟังก์ชันและพารามิเตอร์ของฟังก์ชันนั้น ที่นี่เราจะใช้แค่ฟังก์ชันเดียวชื่อว่า `search_courses` แต่คุณสามารถสร้างฟังก์ชันได้หลายอัน\n",
    "\n",
    "**สำคัญ** : ฟังก์ชันจะถูกรวมอยู่ใน system message ที่ส่งไปยัง LLM และจะถูกนับรวมในจำนวนโทเคนที่คุณสามารถใช้ได้\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**คำนิยาม**\n",
    "\n",
    "โครงสร้างการกำหนดฟังก์ชันมีหลายระดับ โดยแต่ละระดับจะมีคุณสมบัติของตัวเอง ด้านล่างนี้คือรายละเอียดของโครงสร้างที่ซ้อนกัน:\n",
    "\n",
    "**คุณสมบัติของฟังก์ชันระดับบนสุด:**\n",
    "\n",
    "`name` - ชื่อของฟังก์ชันที่เราต้องการให้ถูกเรียกใช้งาน\n",
    "\n",
    "`description` - คำอธิบายเกี่ยวกับวิธีการทำงานของฟังก์ชัน ตรงนี้ควรอธิบายให้ชัดเจนและเจาะจง\n",
    "\n",
    "`parameters` - รายการของค่าและรูปแบบที่ต้องการให้โมเดลสร้างขึ้นมาในคำตอบ\n",
    "\n",
    "**คุณสมบัติของวัตถุ Parameters:**\n",
    "\n",
    "`type` - ประเภทข้อมูลของวัตถุ parameters (โดยปกติจะเป็น \"object\")\n",
    "\n",
    "`properties` - รายการของค่าที่เฉพาะเจาะจงที่โมเดลจะใช้ในการตอบกลับ\n",
    "\n",
    "**คุณสมบัติของแต่ละ Parameter:**\n",
    "\n",
    "`name` - ถูกกำหนดโดยอัตโนมัติจากคีย์ของ property (เช่น \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - ประเภทข้อมูลของ parameter นั้น ๆ (เช่น \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - คำอธิบายของ parameter นั้น ๆ\n",
    "\n",
    "**คุณสมบัติเสริม:**\n",
    "\n",
    "`required` - อาร์เรย์ที่ระบุว่า parameter ใดบ้างที่จำเป็นต้องมีเพื่อให้การเรียกฟังก์ชันสมบูรณ์\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การเรียกใช้งานฟังก์ชัน\n",
    "หลังจากที่เราได้กำหนดฟังก์ชันแล้ว ขั้นตอนถัดไปคือการใส่ฟังก์ชันนั้นเข้าไปในคำขอที่ส่งไปยัง Chat Completion API โดยเราจะเพิ่ม `functions` ลงในคำขอ ในที่นี้คือ `functions=functions`\n",
    "\n",
    "นอกจากนี้ เรายังสามารถตั้งค่า `function_call` ให้เป็น `auto` ได้ด้วย ซึ่งหมายความว่าเราจะให้ LLM เป็นผู้ตัดสินใจเองว่าควรเรียกใช้ฟังก์ชันไหนตามข้อความที่ผู้ใช้ส่งมา แทนที่เราจะกำหนดเอง\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตอนนี้เรามาดูที่การตอบกลับและดูว่ามีรูปแบบอย่างไร:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "คุณจะเห็นว่าชื่อของฟังก์ชันถูกเรียกใช้ และจากข้อความของผู้ใช้ LLM สามารถหาข้อมูลมาใส่ใน arguments ของฟังก์ชันได้\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.การผสานการเรียกใช้งานฟังก์ชันเข้ากับแอปพลิเคชัน\n",
    "\n",
    "หลังจากที่เราได้ทดสอบรูปแบบการตอบกลับจาก LLM แล้ว ตอนนี้เราสามารถนำสิ่งนี้ไปผสานเข้ากับแอปพลิเคชันของเราได้\n",
    "\n",
    "### การจัดการลำดับขั้นตอน\n",
    "\n",
    "เพื่อผสานสิ่งนี้เข้ากับแอปพลิเคชันของเรา ให้ทำตามขั้นตอนดังนี้:\n",
    "\n",
    "ก่อนอื่น ให้เรียกใช้งานบริการของ OpenAI และเก็บข้อความที่ได้ไว้ในตัวแปรชื่อ `response_message`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตอนนี้เราจะกำหนดฟังก์ชันที่จะเรียก Microsoft Learn API เพื่อดึงรายการคอร์ส:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เป็นแนวทางปฏิบัติที่ดี เราจะตรวจสอบก่อนว่าระบบต้องการเรียกฟังก์ชันหรือไม่ หลังจากนั้น เราจะสร้างหนึ่งในฟังก์ชันที่มีอยู่และจับคู่กับฟังก์ชันที่ถูกเรียก  \n",
    "จากนั้น เราจะนำอาร์กิวเมนต์ของฟังก์ชันนั้นมาแมปกับอาร์กิวเมนต์ที่ได้จาก LLM\n",
    "\n",
    "สุดท้าย เราจะเพิ่มข้อความเรียกฟังก์ชันและค่าที่ได้จากข้อความ `search_courses` ซึ่งจะทำให้ LLM มีข้อมูลครบถ้วน  \n",
    "เพื่อใช้ตอบกลับผู้ใช้ด้วยภาษาธรรมชาติ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## โจทย์ท้าทายด้านโค้ด\n",
    "\n",
    "เยี่ยมมาก! หากต้องการเรียนรู้เพิ่มเติมเกี่ยวกับ OpenAI Function Calling คุณสามารถลองสร้าง: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - เพิ่มพารามิเตอร์ของฟังก์ชันให้มากขึ้น เพื่อช่วยให้ผู้เรียนค้นหาคอร์สได้ตรงใจยิ่งขึ้น คุณสามารถดูพารามิเตอร์ของ API ที่มีได้ที่นี่:\n",
    " - สร้างฟังก์ชันอีกตัวที่รับข้อมูลเพิ่มเติมจากผู้เรียน เช่น ภาษาหลักที่ใช้\n",
    " - สร้างการจัดการข้อผิดพลาดในกรณีที่การเรียกฟังก์ชันหรือ API ไม่สามารถหาคอร์สที่เหมาะสมให้ได้\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ข้อจำกัดความรับผิดชอบ**:  \nเอกสารฉบับนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามอย่างเต็มที่เพื่อความถูกต้อง แต่โปรดทราบว่าการแปลโดยระบบอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาต้นทางควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่มีความสำคัญ แนะนำให้ใช้บริการแปลโดยนักแปลมืออาชีพ ทางเราจะไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่คลาดเคลื่อนซึ่งเกิดจากการใช้การแปลนี้\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T21:05:06+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "th"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}