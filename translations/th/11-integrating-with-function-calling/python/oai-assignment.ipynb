{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "บทเรียนนี้จะครอบคลุม: \n",
    "- การเรียกใช้ฟังก์ชันคืออะไรและกรณีการใช้งานของมัน \n",
    "- วิธีสร้างการเรียกใช้ฟังก์ชันโดยใช้ OpenAI \n",
    "- วิธีการผสานการเรียกใช้ฟังก์ชันเข้ากับแอปพลิเคชัน \n",
    "\n",
    "## Learning Goals \n",
    "\n",
    "หลังจากทำบทเรียนนี้เสร็จสิ้น คุณจะรู้วิธีและเข้าใจ: \n",
    "\n",
    "- วัตถุประสงค์ของการใช้การเรียกใช้ฟังก์ชัน \n",
    "- การตั้งค่าการเรียกใช้ฟังก์ชันโดยใช้บริการ OpenAI \n",
    "- การออกแบบการเรียกใช้ฟังก์ชันที่มีประสิทธิภาพสำหรับกรณีการใช้งานของแอปพลิเคชันของคุณ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การเข้าใจการเรียกใช้ฟังก์ชัน\n",
    "\n",
    "สำหรับบทเรียนนี้ เราต้องการสร้างฟีเจอร์สำหรับสตาร์ทอัพด้านการศึกษาของเราที่อนุญาตให้ผู้ใช้ใช้แชทบอทเพื่อค้นหาหลักสูตรทางเทคนิค เราจะแนะนำหลักสูตรที่เหมาะสมกับระดับทักษะ บทบาทปัจจุบัน และเทคโนโลยีที่สนใจ\n",
    "\n",
    "เพื่อให้เสร็จสมบูรณ์ เราจะใช้การผสมผสานของ:\n",
    " - `OpenAI` เพื่อสร้างประสบการณ์แชทสำหรับผู้ใช้\n",
    " - `Microsoft Learn Catalog API` เพื่อช่วยผู้ใช้ค้นหาหลักสูตรตามคำขอของผู้ใช้\n",
    " - `Function Calling` เพื่อรับคำถามของผู้ใช้และส่งไปยังฟังก์ชันเพื่อทำคำขอ API\n",
    "\n",
    "เพื่อเริ่มต้น มาดูว่าทำไมเราถึงต้องการใช้การเรียกใช้ฟังก์ชันตั้งแต่แรก:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # รับคำตอบใหม่จาก GPT ที่สามารถเห็นการตอบกลับของฟังก์ชัน\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ทำไมต้องเรียกใช้ฟังก์ชัน\n",
    "\n",
    "ถ้าคุณได้เรียนบทเรียนอื่น ๆ ในคอร์สนี้แล้ว คุณน่าจะเข้าใจพลังของการใช้โมเดลภาษาขนาดใหญ่ (LLMs) แล้ว หวังว่าคุณจะเห็นข้อจำกัดบางอย่างของพวกมันด้วยเช่นกัน\n",
    "\n",
    "การเรียกใช้ฟังก์ชันเป็นคุณสมบัติของบริการ OpenAI ที่ออกแบบมาเพื่อแก้ไขปัญหาดังต่อไปนี้:\n",
    "\n",
    "รูปแบบการตอบกลับที่ไม่สม่ำเสมอ:\n",
    "- ก่อนการเรียกใช้ฟังก์ชัน การตอบกลับจากโมเดลภาษาขนาดใหญ่จะไม่มีโครงสร้างและไม่สม่ำเสมอ นักพัฒนาต้องเขียนโค้ดยืนยันความถูกต้องที่ซับซ้อนเพื่อจัดการกับความแตกต่างแต่ละแบบในผลลัพธ์\n",
    "\n",
    "การรวมข้อมูลภายนอกที่จำกัด:\n",
    "- ก่อนคุณสมบัตินี้ การนำข้อมูลจากส่วนอื่นของแอปพลิเคชันมาใช้ในบริบทของแชทเป็นเรื่องยาก\n",
    "\n",
    "ด้วยการทำให้รูปแบบการตอบกลับเป็นมาตรฐานและเปิดใช้งานการรวมข้อมูลภายนอกได้อย่างราบรื่น การเรียกใช้ฟังก์ชันช่วยให้ง่ายต่อการพัฒนาและลดความจำเป็นในการเขียนตรรกะการยืนยันเพิ่มเติม\n",
    "\n",
    "ผู้ใช้ไม่สามารถรับคำตอบเช่น \"สภาพอากาศปัจจุบันในสตอกโฮล์มเป็นอย่างไร?\" ได้ เนื่องจากโมเดลถูกจำกัดด้วยเวลาที่ข้อมูลถูกฝึกมา\n",
    "\n",
    "มาดูตัวอย่างด้านล่างที่แสดงปัญหานี้:\n",
    "\n",
    "สมมติว่าเราต้องการสร้างฐานข้อมูลข้อมูลนักเรียนเพื่อที่เราจะได้แนะนำหลักสูตรที่เหมาะสมให้กับพวกเขา ด้านล่างนี้เรามีคำอธิบายของนักเรียนสองคนที่มีข้อมูลคล้ายกันมาก\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราต้องการส่งสิ่งนี้ไปยัง LLM เพื่อวิเคราะห์ข้อมูล ซึ่งต่อไปสามารถนำไปใช้ในแอปพลิเคชันของเราเพื่อส่งไปยัง API หรือเก็บไว้ในฐานข้อมูล\n",
    "\n",
    "มาสร้างพรอมต์สองชุดที่เหมือนกันซึ่งเราจะสั่งให้ LLM ทราบว่าข้อมูลใดที่เราสนใจ:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราต้องการส่งสิ่งนี้ไปยัง LLM เพื่อวิเคราะห์ส่วนที่สำคัญต่อผลิตภัณฑ์ของเรา ดังนั้นเราสามารถสร้างพรอมต์สองชุดที่เหมือนกันเพื่อสั่งงาน LLM ได้:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "หลังจากสร้างพรอมต์ทั้งสองนี้แล้ว เราจะส่งพวกมันไปยัง LLM โดยใช้ `openai.ChatCompletion` เราจะเก็บพรอมต์ไว้ในตัวแปร `messages` และกำหนดบทบาทเป็น `user` ซึ่งเป็นการเลียนแบบข้อความจากผู้ใช้ที่เขียนถึงแชทบอท\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตอนนี้เราสามารถส่งคำขอทั้งสองไปยัง LLM และตรวจสอบการตอบกลับที่เราได้รับได้แล้ว\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "แม้ว่าคำสั่งจะเหมือนกันและคำอธิบายจะคล้ายกัน แต่เราสามารถได้รูปแบบที่แตกต่างกันของคุณสมบัติ `Grades`\n",
    "\n",
    "หากคุณรันเซลล์ข้างต้นหลายครั้ง รูปแบบอาจเป็น `3.7` หรือ `3.7 GPA`\n",
    "\n",
    "นี่เป็นเพราะ LLM รับข้อมูลที่ไม่มีโครงสร้างในรูปแบบของคำสั่งที่เขียนและส่งคืนข้อมูลที่ไม่มีโครงสร้างเช่นกัน เราจำเป็นต้องมีรูปแบบที่มีโครงสร้างเพื่อที่เราจะได้รู้ว่าจะคาดหวังอะไรเมื่อจัดเก็บหรือใช้ข้อมูลนี้\n",
    "\n",
    "โดยการใช้การเรียกฟังก์ชัน เราสามารถมั่นใจได้ว่าเราจะได้รับข้อมูลที่มีโครงสร้างกลับมา เมื่อใช้การเรียกฟังก์ชัน LLM จะไม่เรียกหรือรันฟังก์ชันใด ๆ จริง ๆ แต่เราจะสร้างโครงสร้างให้ LLM ปฏิบัติตามสำหรับการตอบกลับ จากนั้นเราจะใช้การตอบกลับที่มีโครงสร้างเหล่านั้นเพื่อรู้ว่าจะรันฟังก์ชันใดในแอปพลิเคชันของเรา\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![แผนภาพการไหลของการเรียกฟังก์ชัน](../../../../translated_images/Function-Flow.083875364af4f4bb.th.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราสามารถนำสิ่งที่ฟังก์ชันส่งกลับมาแล้วส่งกลับไปยัง LLM ได้ จากนั้น LLM จะตอบกลับโดยใช้ภาษาธรรมชาติเพื่อตอบคำถามของผู้ใช้\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### กรณีการใช้งานสำหรับการเรียกฟังก์ชัน\n",
    "\n",
    "**การเรียกใช้เครื่องมือภายนอก**  \n",
    "แชทบอทมีความสามารถยอดเยี่ยมในการให้คำตอบต่อคำถามจากผู้ใช้ โดยการใช้การเรียกฟังก์ชัน แชทบอทสามารถใช้ข้อความจากผู้ใช้เพื่อทำงานบางอย่างให้เสร็จสมบูรณ์ ตัวอย่างเช่น นักเรียนสามารถขอให้แชทบอท \"ส่งอีเมลถึงอาจารย์ของฉันบอกว่าฉันต้องการความช่วยเหลือเพิ่มเติมในวิชานี้\" ซึ่งสามารถทำการเรียกฟังก์ชัน `send_email(to: string, body: string)`\n",
    "\n",
    "**สร้างคำสั่ง API หรือฐานข้อมูล**  \n",
    "ผู้ใช้สามารถค้นหาข้อมูลโดยใช้ภาษาธรรมชาติที่ถูกแปลงเป็นคำสั่งค้นหาหรือคำขอ API ที่มีรูปแบบ ตัวอย่างเช่น ครูที่ขอว่า \"ใครคือผู้ที่ทำการบ้านชิ้นสุดท้ายเสร็จแล้ว\" ซึ่งอาจเรียกฟังก์ชันชื่อ `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**การสร้างข้อมูลที่มีโครงสร้าง**  \n",
    "ผู้ใช้สามารถนำข้อความหรือไฟล์ CSV มาใช้ LLM เพื่อสกัดข้อมูลสำคัญออกมา ตัวอย่างเช่น นักเรียนสามารถแปลงบทความวิกิพีเดียเกี่ยวกับข้อตกลงสันติภาพเพื่อสร้างบัตรแฟลช AI ได้ ซึ่งสามารถทำได้โดยใช้ฟังก์ชันที่ชื่อ `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. การสร้างการเรียกฟังก์ชันแรกของคุณ\n",
    "\n",
    "กระบวนการสร้างการเรียกฟังก์ชันประกอบด้วย 3 ขั้นตอนหลัก:\n",
    "1. เรียกใช้ Chat Completions API พร้อมกับรายการฟังก์ชันของคุณและข้อความจากผู้ใช้\n",
    "2. อ่านการตอบกลับของโมเดลเพื่อดำเนินการ เช่น การเรียกใช้ฟังก์ชันหรือ API\n",
    "3. ทำการเรียกอีกครั้งไปยัง Chat Completions API พร้อมกับผลลัพธ์จากฟังก์ชันของคุณเพื่อใช้ข้อมูลนั้นในการสร้างการตอบกลับให้กับผู้ใช้\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![การไหลของการเรียกฟังก์ชัน](../../../../translated_images/LLM-Flow.3285ed8caf4796d7.th.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### องค์ประกอบของการเรียกฟังก์ชัน\n",
    "\n",
    "#### การป้อนข้อมูลของผู้ใช้\n",
    "\n",
    "ขั้นตอนแรกคือการสร้างข้อความของผู้ใช้ ซึ่งสามารถกำหนดค่าได้แบบไดนามิกโดยการรับค่าจากการป้อนข้อความ หรือคุณสามารถกำหนดค่าที่นี่ได้ หากนี่เป็นครั้งแรกที่คุณทำงานกับ Chat Completions API เราจำเป็นต้องกำหนด `role` และ `content` ของข้อความ\n",
    "\n",
    "`role` สามารถเป็นได้ทั้ง `system` (การสร้างกฎ), `assistant` (โมเดล) หรือ `user` (ผู้ใช้ปลายทาง) สำหรับการเรียกฟังก์ชัน เราจะกำหนดเป็น `user` และตัวอย่างคำถามหนึ่งข้อ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การสร้างฟังก์ชัน\n",
    "\n",
    "ต่อไปเราจะกำหนดฟังก์ชันและพารามิเตอร์ของฟังก์ชันนั้น เราจะใช้เพียงฟังก์ชันเดียวที่ชื่อว่า `search_courses` แต่คุณสามารถสร้างฟังก์ชันหลายฟังก์ชันได้\n",
    "\n",
    "**สำคัญ** : ฟังก์ชันจะถูกรวมอยู่ในข้อความระบบสำหรับ LLM และจะถูกนับรวมในจำนวนโทเค็นที่คุณมีอยู่ด้วย\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**คำนิยาม**\n",
    "\n",
    "โครงสร้างการกำหนดฟังก์ชันมีหลายระดับ โดยแต่ละระดับมีคุณสมบัติของตัวเอง นี่คือการแยกโครงสร้างแบบซ้อน:\n",
    "\n",
    "**คุณสมบัติฟังก์ชันระดับบนสุด:**\n",
    "\n",
    "`name` - ชื่อของฟังก์ชันที่เราต้องการให้เรียกใช้\n",
    "\n",
    "`description` - คำอธิบายเกี่ยวกับวิธีการทำงานของฟังก์ชัน ที่นี่สำคัญที่จะต้องชัดเจนและเฉพาะเจาะจง\n",
    "\n",
    "`parameters` - รายการของค่าต่าง ๆ และรูปแบบที่คุณต้องการให้โมเดลสร้างขึ้นในคำตอบ\n",
    "\n",
    "**คุณสมบัติวัตถุพารามิเตอร์:**\n",
    "\n",
    "`type` - ประเภทข้อมูลของวัตถุพารามิเตอร์ (โดยปกติจะเป็น \"object\")\n",
    "\n",
    "`properties` - รายการของค่าที่เฉพาะเจาะจงที่โมเดลจะใช้สำหรับคำตอบ\n",
    "\n",
    "**คุณสมบัติพารามิเตอร์แต่ละตัว:**\n",
    "\n",
    "`name` - กำหนดโดยคีย์ของคุณสมบัติอย่างชัดเจน (เช่น \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - ประเภทข้อมูลของพารามิเตอร์เฉพาะนี้ (เช่น \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - คำอธิบายของพารามิเตอร์เฉพาะนี้\n",
    "\n",
    "**คุณสมบัติเพิ่มเติม (ไม่บังคับ):**\n",
    "\n",
    "`required` - อาร์เรย์ที่ระบุว่าพารามิเตอร์ใดบ้างที่จำเป็นสำหรับการเรียกใช้ฟังก์ชันให้เสร็จสมบูรณ์\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การเรียกใช้ฟังก์ชัน  \n",
    "หลังจากกำหนดฟังก์ชันแล้ว ตอนนี้เราจำเป็นต้องรวมฟังก์ชันนั้นในการเรียกใช้ Chat Completion API เราทำได้โดยการเพิ่ม `functions` ลงในคำขอ ในกรณีนี้คือ `functions=functions`  \n",
    "\n",
    "นอกจากนี้ยังมีตัวเลือกในการตั้งค่า `function_call` เป็น `auto` ซึ่งหมายความว่าเราจะปล่อยให้ LLM ตัดสินใจว่าควรเรียกใช้ฟังก์ชันใดตามข้อความของผู้ใช้ แทนที่จะกำหนดเอง\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตอนนี้เรามาดูการตอบกลับและดูว่ามันถูกจัดรูปแบบอย่างไร:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "คุณจะเห็นว่าชื่อของฟังก์ชันถูกเรียกใช้และจากข้อความของผู้ใช้ LLM สามารถค้นหาข้อมูลให้ตรงกับอาร์กิวเมนต์ของฟังก์ชันได้\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.การรวมการเรียกฟังก์ชันเข้ากับแอปพลิเคชัน\n",
    "\n",
    "หลังจากที่เราได้ทดสอบการตอบกลับที่จัดรูปแบบจาก LLM แล้ว ตอนนี้เราสามารถรวมสิ่งนี้เข้ากับแอปพลิเคชันได้\n",
    "\n",
    "### การจัดการลำดับการทำงาน\n",
    "\n",
    "เพื่อรวมสิ่งนี้เข้ากับแอปพลิเคชันของเรา ให้ทำตามขั้นตอนดังต่อไปนี้:\n",
    "\n",
    "อันดับแรก ให้ทำการเรียกใช้บริการ OpenAI และเก็บข้อความไว้ในตัวแปรที่ชื่อว่า `response_message`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตอนนี้เราจะกำหนดฟังก์ชันที่จะเรียกใช้ Microsoft Learn API เพื่อรับรายการหลักสูตร:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในฐานะแนวทางปฏิบัติที่ดีที่สุด เราจะตรวจสอบว่ารุ่นต้องการเรียกใช้ฟังก์ชันหรือไม่ หลังจากนั้น เราจะสร้างหนึ่งในฟังก์ชันที่มีอยู่และจับคู่กับฟังก์ชันที่ถูกเรียกใช้  \n",
    "จากนั้นเราจะนำอาร์กิวเมนต์ของฟังก์ชันมาแมปกับอาร์กิวเมนต์จาก LLM  \n",
    "\n",
    "สุดท้าย เราจะต่อข้อความการเรียกฟังก์ชันและค่าที่ถูกส่งกลับโดยข้อความ `search_courses` ซึ่งจะให้ข้อมูลทั้งหมดที่ LLM ต้องการ  \n",
    "เพื่อตอบกลับผู้ใช้โดยใช้ภาษาธรรมชาติ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตอนนี้เราจะส่งข้อความที่อัปเดตไปยัง LLM เพื่อที่เราจะได้รับการตอบกลับเป็นภาษาธรรมชาติแทนการตอบกลับในรูปแบบ JSON ของ API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge \n",
    "\n",
    "เยี่ยมมาก! เพื่อสานต่อการเรียนรู้เกี่ยวกับ OpenAI Function Calling คุณสามารถสร้างได้ที่: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - พารามิเตอร์เพิ่มเติมของฟังก์ชันที่อาจช่วยให้ผู้เรียนค้นหาคอร์สเพิ่มเติมได้ คุณสามารถดูพารามิเตอร์ API ที่มีได้ที่นี่: \n",
    " - สร้างการเรียกฟังก์ชันอีกตัวที่รับข้อมูลเพิ่มเติมจากผู้เรียน เช่น ภาษาพื้นเมืองของพวกเขา \n",
    " - สร้างการจัดการข้อผิดพลาดเมื่อการเรียกฟังก์ชันและ/หรือการเรียก API ไม่ส่งคืนคอร์สที่เหมาะสมใดๆ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**ข้อจำกัดความรับผิดชอบ**:  \nเอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษาอัตโนมัติ [Co-op Translator](https://github.com/Azure/co-op-translator) แม้เราจะพยายามให้ความถูกต้องสูงสุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาต้นทางถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลโดยผู้เชี่ยวชาญมนุษย์ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดใด ๆ ที่เกิดจากการใช้การแปลนี้\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T10:25:48+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "th"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}