{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## บทนำ\n",
    "\n",
    "บทเรียนนี้จะครอบคลุมถึง:\n",
    "- ฟังก์ชันคอลคืออะไร และใช้งานในกรณีใดบ้าง\n",
    "- วิธีสร้างฟังก์ชันคอลโดยใช้ Azure OpenAI\n",
    "- วิธีผสานฟังก์ชันคอลเข้ากับแอปพลิเคชัน\n",
    "\n",
    "## เป้าหมายการเรียนรู้\n",
    "\n",
    "หลังจากจบบทเรียนนี้ คุณจะสามารถและเข้าใจว่า:\n",
    "\n",
    "- จุดประสงค์ของการใช้ฟังก์ชันคอล\n",
    "- การตั้งค่าฟังก์ชันคอลโดยใช้ Azure Open AI Service\n",
    "- ออกแบบฟังก์ชันคอลให้มีประสิทธิภาพสำหรับกรณีการใช้งานของแอปพลิเคชันของคุณ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ทำความเข้าใจเกี่ยวกับ Function Calls\n",
    "\n",
    "สำหรับบทเรียนนี้ เราต้องการสร้างฟีเจอร์ให้กับสตาร์ทอัพด้านการศึกษาของเราที่ช่วยให้ผู้ใช้สามารถใช้แชทบอทเพื่อค้นหาคอร์สเทคนิคต่าง ๆ ได้ เราจะช่วยแนะนำคอร์สที่เหมาะกับระดับทักษะ ตำแหน่งงานปัจจุบัน และเทคโนโลยีที่ผู้ใช้สนใจ\n",
    "\n",
    "เพื่อให้ทำสิ่งนี้ได้ เราจะใช้เครื่องมือผสมผสานกันดังนี้:\n",
    " - `Azure Open AI` เพื่อสร้างประสบการณ์แชทให้กับผู้ใช้\n",
    " - `Microsoft Learn Catalog API` เพื่อช่วยให้ผู้ใช้ค้นหาคอร์สตามที่ต้องการ\n",
    " - `Function Calling` เพื่อรับคำถามของผู้ใช้และส่งไปยังฟังก์ชันเพื่อเรียก API\n",
    "\n",
    "ก่อนจะเริ่ม มาดูกันว่าทำไมเราถึงควรใช้ function calling ตั้งแต่แรก:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # ขอรับคำตอบใหม่จาก GPT ที่สามารถเห็นผลลัพธ์จากฟังก์ชันได้\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ทำไมต้องใช้ Function Calling\n",
    "\n",
    "ถ้าคุณเคยเรียนบทเรียนอื่นในคอร์สนี้มาก่อน คุณน่าจะเข้าใจถึงพลังของการใช้ Large Language Models (LLMs) แล้ว และก็น่าจะเห็นข้อจำกัดบางอย่างของมันด้วย\n",
    "\n",
    "Function Calling เป็นฟีเจอร์ของ Azure Open AI Service ที่ถูกออกแบบมาเพื่อแก้ไขข้อจำกัดเหล่านี้:\n",
    "1) รูปแบบการตอบกลับที่สม่ำเสมอ\n",
    "2) ความสามารถในการนำข้อมูลจากแหล่งอื่น ๆ ของแอปพลิเคชันมาใช้ในบริบทของแชท\n",
    "\n",
    "ก่อนที่จะมี function calling การตอบกลับจาก LLM มักจะไม่มีโครงสร้างและไม่สม่ำเสมอ นักพัฒนาต้องเขียนโค้ดตรวจสอบความถูกต้องที่ซับซ้อนเพื่อให้สามารถจัดการกับรูปแบบการตอบกลับที่หลากหลายได้\n",
    "\n",
    "ผู้ใช้ไม่สามารถถามคำถามอย่างเช่น \"ตอนนี้อากาศที่สตอกโฮล์มเป็นอย่างไร?\" ได้ เพราะโมเดลถูกจำกัดอยู่แค่ข้อมูลที่ใช้ฝึกเท่านั้น\n",
    "\n",
    "ลองดูตัวอย่างด้านล่างนี้ที่แสดงให้เห็นถึงปัญหานี้:\n",
    "\n",
    "สมมติว่าเราต้องการสร้างฐานข้อมูลนักเรียนเพื่อแนะนำคอร์สที่เหมาะสมให้กับพวกเขา ด้านล่างนี้คือลักษณะการบรรยายของนักเรียนสองคนที่มีข้อมูลคล้ายกันมาก\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราต้องการส่งข้อมูลนี้ไปยัง LLM เพื่อให้มันแยกวิเคราะห์ข้อมูล หลังจากนั้นเราสามารถนำข้อมูลนี้ไปใช้ในแอปพลิเคชันของเราเพื่อส่งไปยัง API หรือเก็บไว้ในฐานข้อมูล\n",
    "\n",
    "มาสร้าง prompt สองชุดที่เหมือนกัน เพื่อสั่งให้ LLM ทราบว่าข้อมูลแบบไหนที่เราต้องการ:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราต้องการส่งข้อความนี้ไปยัง LLM เพื่อแยกส่วนที่สำคัญต่อผลิตภัณฑ์ของเรา ดังนั้นเราจึงสามารถสร้างพรอมต์ที่เหมือนกันสองชุดเพื่อสั่งงาน LLM ได้\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "หลังจากสร้างพรอมต์ทั้งสองนี้แล้ว เราจะส่งไปยัง LLM โดยใช้ `openai.ChatCompletion` เราเก็บพรอมต์ไว้ในตัวแปร `messages` และกำหนดบทบาทเป็น `user` ซึ่งเป็นการจำลองข้อความจากผู้ใช้ที่เขียนถึงแชทบอท\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ถึงแม้ว่าคำสั่งจะเหมือนกันและคำอธิบายจะคล้ายกัน แต่เราก็อาจได้รูปแบบของ property `Grades` ที่ต่างกัน\n",
    "\n",
    "ถ้าคุณรันเซลล์ข้างบนหลายครั้ง รูปแบบที่ได้อาจเป็น `3.7` หรือ `3.7 GPA`\n",
    "\n",
    "สาเหตุก็เพราะ LLM รับข้อมูลที่ไม่มีโครงสร้างในรูปแบบของ prompt ที่เขียนขึ้น และก็คืนค่าข้อมูลที่ไม่มีโครงสร้างเช่นกัน เราจำเป็นต้องมีรูปแบบข้อมูลที่มีโครงสร้าง เพื่อที่เราจะได้รู้ว่าควรคาดหวังข้อมูลแบบไหนเมื่อจะจัดเก็บหรือใช้งานข้อมูลนี้\n",
    "\n",
    "โดยการใช้ functional calling เราสามารถมั่นใจได้ว่าเราจะได้รับข้อมูลที่มีโครงสร้างกลับมา เวลาที่ใช้ function calling จริง ๆ แล้ว LLM จะไม่ได้เรียกหรือรันฟังก์ชันใด ๆ จริง ๆ แต่เราจะสร้างโครงสร้างให้ LLM ทำตามสำหรับการตอบกลับ จากนั้นเราก็ใช้ข้อมูลที่มีโครงสร้างนี้เพื่อรู้ว่าควรจะรันฟังก์ชันไหนในแอปพลิเคชันของเรา\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### กรณีการใช้งานสำหรับการเรียกใช้ฟังก์ชัน\n",
    "\n",
    "**การเรียกใช้เครื่องมือภายนอก**  \n",
    "แชทบอทเหมาะกับการให้คำตอบกับคำถามของผู้ใช้มาก เมื่อใช้การเรียกฟังก์ชัน แชทบอทจะสามารถนำข้อความจากผู้ใช้ไปทำงานบางอย่างได้ เช่น นักเรียนอาจขอให้แชทบอท \"ส่งอีเมลถึงอาจารย์ของฉัน บอกว่าฉันต้องการความช่วยเหลือเพิ่มเติมในวิชานี้\" ซึ่งจะสามารถเรียกใช้ฟังก์ชัน `send_email(to: string, body: string)`\n",
    "\n",
    "**สร้าง API หรือคำสั่งค้นหาฐานข้อมูล**  \n",
    "ผู้ใช้สามารถค้นหาข้อมูลด้วยภาษาธรรมชาติที่ถูกแปลงเป็นคำสั่งค้นหาหรือ API ที่มีรูปแบบชัดเจน ตัวอย่างเช่น ครูอาจถามว่า \"นักเรียนคนไหนที่ทำงานล่าสุดเสร็จแล้วบ้าง\" ซึ่งอาจเรียกใช้ฟังก์ชันชื่อ `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**สร้างข้อมูลที่มีโครงสร้าง**  \n",
    "ผู้ใช้สามารถนำข้อความหรือไฟล์ CSV มาให้ LLM ช่วยดึงข้อมูลสำคัญออกมาได้ เช่น นักเรียนอาจแปลงบทความวิกิพีเดียเกี่ยวกับข้อตกลงสันติภาพเพื่อสร้างแฟลชการ์ด AI วิธีนี้สามารถทำได้โดยใช้ฟังก์ชันชื่อ `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. การสร้างการเรียกใช้ฟังก์ชันครั้งแรกของคุณ\n",
    "\n",
    "ขั้นตอนการสร้างการเรียกใช้ฟังก์ชันประกอบด้วย 3 ขั้นตอนหลัก:\n",
    "1. เรียกใช้ Chat Completions API พร้อมกับรายการฟังก์ชันของคุณและข้อความจากผู้ใช้\n",
    "2. อ่านคำตอบของโมเดลเพื่อดำเนินการบางอย่าง เช่น เรียกใช้ฟังก์ชันหรือ API\n",
    "3. เรียกใช้ Chat Completions API อีกครั้ง พร้อมกับผลลัพธ์ที่ได้จากฟังก์ชันของคุณ เพื่อนำข้อมูลนั้นมาใช้สร้างคำตอบให้กับผู้ใช้\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### องค์ประกอบของการเรียกใช้ฟังก์ชัน\n",
    "\n",
    "#### ข้อมูลที่ผู้ใช้ป้อน\n",
    "\n",
    "ขั้นตอนแรกคือการสร้างข้อความจากผู้ใช้ ซึ่งสามารถกำหนดค่าได้แบบไดนามิกโดยรับค่าจากกล่องข้อความ หรือจะกำหนดค่าตรงนี้เลยก็ได้ หากนี่เป็นครั้งแรกที่คุณใช้งาน Chat Completions API เราต้องกำหนด `role` และ `content` ของข้อความ\n",
    "\n",
    "`role` สามารถเป็นได้ทั้ง `system` (สร้างกฎ), `assistant` (โมเดล), หรือ `user` (ผู้ใช้งาน) สำหรับการเรียกใช้ฟังก์ชัน เราจะกำหนดเป็น `user` พร้อมตัวอย่างคำถาม\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การสร้างฟังก์ชัน\n",
    "\n",
    "ต่อไปเราจะกำหนดฟังก์ชันและพารามิเตอร์ของฟังก์ชันนั้น ที่นี่เราจะใช้เพียงฟังก์ชันเดียวชื่อว่า `search_courses` แต่คุณสามารถสร้างฟังก์ชันได้หลายอัน\n",
    "\n",
    "**สำคัญ** : ฟังก์ชันจะถูกรวมอยู่ใน system message ที่ส่งไปยัง LLM และจะถูกนับรวมในจำนวนโทเคนที่คุณมีอยู่ด้วย\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**คำนิยาม**\n",
    "\n",
    "`name` - ชื่อของฟังก์ชันที่เราต้องการให้ถูกเรียกใช้งาน\n",
    "\n",
    "`description` - คำอธิบายเกี่ยวกับวิธีการทำงานของฟังก์ชันนี้ ตรงนี้ควรอธิบายให้ชัดเจนและเฉพาะเจาะจง\n",
    "\n",
    "`parameters` - รายการของค่าและรูปแบบที่คุณต้องการให้โมเดลสร้างขึ้นในคำตอบ\n",
    "\n",
    "`type` - ประเภทข้อมูลของ properties ที่จะถูกจัดเก็บไว้\n",
    "\n",
    "`properties` - รายการของค่าที่เฉพาะเจาะจงที่โมเดลจะใช้สำหรับคำตอบ\n",
    "\n",
    "`name` - ชื่อของ property ที่โมเดลจะใช้ในคำตอบที่จัดรูปแบบแล้ว\n",
    "\n",
    "`type` - ประเภทข้อมูลของ property นี้\n",
    "\n",
    "`description` - คำอธิบายของ property เฉพาะเจาะจงนี้\n",
    "\n",
    "**ตัวเลือกเพิ่มเติม**\n",
    "\n",
    "`required` - property ที่จำเป็นสำหรับการเรียกใช้งานฟังก์ชันให้สมบูรณ์\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การเรียกใช้งานฟังก์ชัน\n",
    "หลังจากที่เราได้กำหนดฟังก์ชันแล้ว ขั้นตอนถัดไปคือการนำฟังก์ชันนั้นไปใช้ในการเรียก Chat Completion API โดยเราจะเพิ่ม `functions` เข้าไปในคำขอ ในที่นี้คือ `functions=functions`\n",
    "\n",
    "นอกจากนี้ เรายังสามารถตั้งค่า `function_call` ให้เป็น `auto` ได้ด้วย ซึ่งหมายความว่าเราจะให้ LLM เป็นผู้ตัดสินใจเองว่าควรเรียกใช้ฟังก์ชันใดตามข้อความที่ผู้ใช้ส่งมา แทนที่เราจะกำหนดเอง\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตอนนี้เรามาดูที่การตอบกลับและดูว่ามีรูปแบบอย่างไร:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "คุณจะเห็นว่าชื่อของฟังก์ชันถูกเรียกใช้ และจากข้อความของผู้ใช้ LLM สามารถหาข้อมูลมาใส่ใน arguments ของฟังก์ชันได้\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.การผสานการเรียกใช้งานฟังก์ชันเข้ากับแอปพลิเคชัน\n",
    "\n",
    "หลังจากที่เราได้ทดสอบรูปแบบการตอบกลับจาก LLM แล้ว ตอนนี้เราสามารถนำสิ่งนี้ไปผสานเข้ากับแอปพลิเคชันของเราได้\n",
    "\n",
    "### การจัดการลำดับขั้นตอน\n",
    "\n",
    "เพื่อผสานสิ่งนี้เข้ากับแอปพลิเคชันของเรา ให้ทำตามขั้นตอนดังนี้:\n",
    "\n",
    "ขั้นแรก ให้เราเรียกใช้งานบริการของ Open AI และเก็บข้อความที่ได้ไว้ในตัวแปรชื่อ `response_message`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตอนนี้เราจะกำหนดฟังก์ชันที่จะเรียก Microsoft Learn API เพื่อดึงรายการคอร์ส:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เป็นแนวทางปฏิบัติที่ดี เราจะตรวจสอบก่อนว่าระบบต้องการเรียกฟังก์ชันหรือไม่ หลังจากนั้น เราจะสร้างหนึ่งในฟังก์ชันที่มีอยู่และจับคู่กับฟังก์ชันที่ถูกเรียก\n",
    "\n",
    "จากนั้น เราจะนำอาร์กิวเมนต์ของฟังก์ชันนั้นมาแมปกับอาร์กิวเมนต์ที่ได้จาก LLM\n",
    "\n",
    "สุดท้าย เราจะเพิ่มข้อความเรียกฟังก์ชันและค่าที่ได้จากข้อความ `search_courses` เข้าไปด้วย วิธีนี้จะทำให้ LLM มีข้อมูลครบถ้วนสำหรับตอบกลับผู้ใช้เป็นภาษาธรรมชาติ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## โจทย์ท้าทายโค้ด\n",
    "\n",
    "เยี่ยมมาก! ถ้าอยากเรียนรู้เพิ่มเติมเกี่ยวกับ Azure Open AI Function Calling คุณสามารถลองสร้างตามนี้: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - เพิ่มพารามิเตอร์ของฟังก์ชันให้มากขึ้น เพื่อช่วยให้ผู้เรียนค้นหาคอร์สได้ง่ายขึ้น คุณสามารถดูพารามิเตอร์ของ API ที่มีได้ที่นี่:\n",
    " - สร้างฟังก์ชันอีกตัวที่รับข้อมูลเพิ่มเติมจากผู้เรียน เช่น ภาษาแม่ของผู้เรียน\n",
    " - สร้างการจัดการข้อผิดพลาด เมื่อการเรียกฟังก์ชันหรือ API ไม่สามารถคืนคอร์สที่เหมาะสมได้\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ข้อจำกัดความรับผิดชอบ**:  \nเอกสารฉบับนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามอย่างเต็มที่เพื่อความถูกต้อง แต่โปรดทราบว่าการแปลโดยระบบอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาต้นทางควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่มีความสำคัญ แนะนำให้ใช้บริการแปลโดยนักแปลมืออาชีพ ทางเราจะไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่คลาดเคลื่อนซึ่งเกิดจากการใช้การแปลนี้\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-08-25T20:16:09+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "th"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}