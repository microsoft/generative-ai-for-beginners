{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เพื่อรันโน้ตบุ๊กต่อไปนี้ หากคุณยังไม่ได้ทำ คุณต้องปรับใช้โมเดลที่ใช้ `text-embedding-ada-002` เป็นโมเดลฐาน และตั้งชื่อการปรับใช้ในไฟล์ .env เป็น `AZURE_OPENAI_EMBEDDINGS_ENDPOINT`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-05-15\"\n",
    "  )\n",
    "\n",
    "model = os.environ['AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT']\n",
    "\n",
    "SIMILARITIES_RESULTS_THRESHOLD = 0.75\n",
    "DATASET_NAME = \"../embedding_index_3m.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ต่อไป เราจะโหลดดัชนีฝังตัว (Embedding Index) ลงใน Pandas Dataframe ดัชนีฝังตัวถูกเก็บไว้ในไฟล์ JSON ที่ชื่อว่า `embedding_index_3m.json` ดัชนีฝังตัวนี้ประกอบด้วยการฝังตัวสำหรับแต่ละคำบรรยายของ YouTube จนถึงปลายเดือนตุลาคม 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(source: str) -> pd.core.frame.DataFrame:\n",
    "    # Load the video session index\n",
    "    pd_vectors = pd.read_json(source)\n",
    "    return pd_vectors.drop(columns=[\"text\"], errors=\"ignore\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ต่อไป เราจะสร้างฟังก์ชันชื่อ `get_videos` ที่จะค้นหาใน Embedding Index สำหรับคำค้นหา ฟังก์ชันนี้จะคืนค่าวิดีโอ 5 อันดับแรกที่มีความคล้ายคลึงกับคำค้นหามากที่สุด ฟังก์ชันทำงานดังนี้:\n",
    "\n",
    "1. ก่อนอื่น จะสร้างสำเนาของ Embedding Index\n",
    "2. ต่อไป จะคำนวณ Embedding สำหรับคำค้นหาโดยใช้ OpenAI Embedding API\n",
    "3. จากนั้น จะสร้างคอลัมน์ใหม่ใน Embedding Index ชื่อ `similarity` คอลัมน์ `similarity` จะเก็บค่าความคล้ายคลึงแบบโคไซน์ระหว่าง Embedding ของคำค้นหาและ Embedding ของแต่ละส่วนของวิดีโอ\n",
    "4. ต่อไป จะกรอง Embedding Index โดยใช้คอลัมน์ `similarity` โดยกรองให้เหลือเฉพาะวิดีโอที่มีความคล้ายคลึงแบบโคไซน์มากกว่าหรือเท่ากับ 0.75\n",
    "5. สุดท้าย จะเรียงลำดับ Embedding Index ตามคอลัมน์ `similarity` และคืนค่าวิดีโอ 5 อันดับแรก\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    if len(a) > len(b):\n",
    "        b = np.pad(b, (0, len(a) - len(b)), 'constant')\n",
    "    elif len(b) > len(a):\n",
    "        a = np.pad(a, (0, len(b) - len(a)), 'constant')\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_videos(\n",
    "    query: str, dataset: pd.core.frame.DataFrame, rows: int\n",
    ") -> pd.core.frame.DataFrame:\n",
    "    # create a copy of the dataset\n",
    "    video_vectors = dataset.copy()\n",
    "\n",
    "    # get the embeddings for the query    \n",
    "    query_embeddings = client.embeddings.create(input=query, model=model).data[0].embedding\n",
    "\n",
    "    # create a new column with the calculated similarity for each row\n",
    "    video_vectors[\"similarity\"] = video_vectors[\"ada_v2\"].apply(\n",
    "        lambda x: cosine_similarity(np.array(query_embeddings), np.array(x))\n",
    "    )\n",
    "\n",
    "    # filter the videos by similarity\n",
    "    mask = video_vectors[\"similarity\"] >= SIMILARITIES_RESULTS_THRESHOLD\n",
    "    video_vectors = video_vectors[mask].copy()\n",
    "\n",
    "    # sort the videos by similarity\n",
    "    video_vectors = video_vectors.sort_values(by=\"similarity\", ascending=False).head(\n",
    "        rows\n",
    "    )\n",
    "\n",
    "    # return the top rows\n",
    "    return video_vectors.head(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ฟังก์ชันนี้ง่ายมาก มันเพียงแค่พิมพ์ผลลัพธ์ของการค้นหาออกมาเท่านั้น\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(videos: pd.core.frame.DataFrame, query: str):\n",
    "    def _gen_yt_url(video_id: str, seconds: int) -> str:\n",
    "        \"\"\"convert time in format 00:00:00 to seconds\"\"\"\n",
    "        return f\"https://youtu.be/{video_id}?t={seconds}\"\n",
    "\n",
    "    print(f\"\\nVideos similar to '{query}':\")\n",
    "    for _, row in videos.iterrows():\n",
    "        youtube_url = _gen_yt_url(row[\"videoId\"], row[\"seconds\"])\n",
    "        print(f\" - {row['title']}\")\n",
    "        print(f\"   Summary: {' '.join(row['summary'].split()[:15])}...\")\n",
    "        print(f\"   YouTube: {youtube_url}\")\n",
    "        print(f\"   Similarity: {row['similarity']}\")\n",
    "        print(f\"   Speakers: {row['speaker']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ขั้นแรก จะโหลด Embedding Index ลงใน Pandas Dataframe  \n",
    "2. ต่อมา ผู้ใช้จะถูกขอให้ป้อนคำค้นหา  \n",
    "3. จากนั้นจะเรียกใช้ฟังก์ชัน `get_videos` เพื่อค้นหา Embedding Index สำหรับคำค้นหา  \n",
    "4. สุดท้ายจะเรียกใช้ฟังก์ชัน `display_results` เพื่อแสดงผลลัพธ์ให้ผู้ใช้ดู  \n",
    "5. จากนั้นผู้ใช้จะถูกขอให้ป้อนคำค้นหาอีกครั้ง กระบวนการนี้จะดำเนินต่อไปจนกว่าผู้ใช้จะป้อน `exit`  \n",
    "\n",
    "![](../../../../translated_images/notebook-search.1e320b9c7fcbb0bc.th.png)\n",
    "\n",
    "คุณจะถูกขอให้ป้อนคำค้นหา ป้อนคำค้นหาแล้วกด enter แอปพลิเคชันจะแสดงรายการวิดีโอที่เกี่ยวข้องกับคำค้นหา แอปพลิเคชันยังจะแสดงลิงก์ไปยังตำแหน่งในวิดีโอที่คำตอบของคำถามนั้นอยู่ด้วย  \n",
    "\n",
    "นี่คือตัวอย่างคำค้นหาที่คุณสามารถลองได้:  \n",
    "\n",
    "- Azure Machine Learning คืออะไร?  \n",
    "- เครือข่ายประสาทเทียมแบบ convolutional ทำงานอย่างไร?  \n",
    "- เครือข่ายประสาทเทียมคืออะไร?  \n",
    "- ฉันสามารถใช้ Jupyter Notebooks กับ Azure Machine Learning ได้หรือไม่?  \n",
    "- ONNX คืออะไร?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_vectors = load_dataset(DATASET_NAME)\n",
    "\n",
    "# get user query from input\n",
    "while True:\n",
    "    query = input(\"Enter a query: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    videos = get_videos(query, pd_vectors, 5)\n",
    "    display_results(videos, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**ข้อจำกัดความรับผิดชอบ**:  \nเอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษาอัตโนมัติ [Co-op Translator](https://github.com/Azure/co-op-translator) แม้เราจะพยายามให้ความถูกต้องสูงสุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาต้นทางถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลโดยผู้เชี่ยวชาญมนุษย์ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดใด ๆ ที่เกิดจากการใช้การแปลนี้\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "ff5415e24294df268ec7e7a2b12af509",
   "translation_date": "2025-12-19T10:23:22+00:00",
   "source_file": "08-building-search-applications/python/aoai-solution.ipynb",
   "language_code": "th"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}