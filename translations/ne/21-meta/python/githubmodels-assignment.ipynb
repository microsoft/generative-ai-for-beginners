{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# मेटा परिवारका मोडेलहरूसँग निर्माण गर्दै\n",
    "\n",
    "## परिचय\n",
    "\n",
    "यो पाठमा समेटिने विषयहरू:\n",
    "\n",
    "- दुई मुख्य मेटा परिवारका मोडेलहरू - Llama 3.1 र Llama 3.2 को अन्वेषण\n",
    "- प्रत्येक मोडेलका प्रयोगका क्षेत्र र परिदृश्य बुझ्ने\n",
    "- प्रत्येक मोडेलका विशेषता देखाउने कोड उदाहरण\n",
    "\n",
    "## मेटा परिवारका मोडेलहरू\n",
    "\n",
    "यस पाठमा, हामी मेटा परिवार वा \"Llama Herd\" का २ मोडेलहरू - Llama 3.1 र Llama 3.2 को अन्वेषण गर्नेछौं।\n",
    "\n",
    "यी मोडेलहरू विभिन्न भेरियन्टमा उपलब्ध छन् र Github Model marketplace मा पाइन्छन्। यहाँ Github Models प्रयोग गरेर [एआई मोडेलहरूसँग प्रोटोटाइप बनाउने](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) बारे थप जानकारी छ।\n",
    "\n",
    "मोडेल भेरियन्टहरू:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*नोट: Llama 3 Github Models मा पनि उपलब्ध छ तर यो पाठमा समेटिने छैन*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "४०५ अर्ब प्यारामिटरहरूसँग, Llama 3.1 खुला स्रोत LLM श्रेणीमा पर्छ।\n",
    "\n",
    "यो मोडेल पहिलेको Llama 3 को अपग्रेड हो, जसले निम्न सुधारहरू ल्याएको छ:\n",
    "\n",
    "- ठूलो सन्दर्भ विन्डो - १२८k टोकन बनाम ८k टोकन\n",
    "- बढी अधिकतम आउटपुट टोकन - ४०९६ बनाम २०४८\n",
    "- अझ राम्रो बहुभाषिक समर्थन - तालिम टोकनको वृद्धि भएकोले\n",
    "\n",
    "यी सुधारहरूले Llama 3.1 लाई GenAI एप्लिकेसनहरू बनाउँदा अझ जटिल प्रयोगका केसहरू सम्हाल्न सक्षम बनाउँछ, जस्तै:\n",
    "- नेटिभ फंक्शन कलिंग - LLM वर्कफ्लो बाहिरका बाह्य टुल्स र फंक्शनहरू कल गर्न सक्ने क्षमता\n",
    "- अझ राम्रो RAG प्रदर्शन - ठूलो सन्दर्भ विन्डोको कारण\n",
    "- कृत्रिम डेटा उत्पादन - फाइन-ट्युनिङ जस्ता कार्यहरूको लागि प्रभावकारी डेटा सिर्जना गर्न सक्ने क्षमता\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### नेटिभ फंक्शन कलिङ\n",
    "\n",
    "Llama 3.1 लाई फंक्शन वा टुल कल गर्न अझ प्रभावकारी बनाउन फाइन-ट्युन गरिएको छ। यसमा दुईवटा बिल्ट-इन टुलहरू छन् जसलाई मोडेलले प्रयोगकर्ताको प्रॉम्प्ट अनुसार प्रयोग गर्न आवश्यक छ भनेर चिन्न सक्छ। यी टुलहरू हुन्:\n",
    "\n",
    "- **Brave Search** - वेब सर्च गरेर मौसम जस्ता ताजा जानकारी प्राप्त गर्न प्रयोग गर्न सकिन्छ\n",
    "- **Wolfram Alpha** - जटिल गणितीय गणनाका लागि प्रयोग गर्न सकिन्छ, जसले गर्दा आफ्नै फंक्शन लेख्न आवश्यक पर्दैन।\n",
    "\n",
    "तपाईंले आफ्नै कस्टम टुलहरू पनि बनाउन सक्नुहुन्छ जसलाई LLM ले कल गर्न सक्छ।\n",
    "\n",
    "तलको कोड उदाहरणमा:\n",
    "\n",
    "- हामीले उपलब्ध टुलहरू (brave_search, wolfram_alpha) लाई सिस्टम प्रॉम्प्टमा परिभाषित गर्छौं।\n",
    "- प्रयोगकर्ताको प्रॉम्प्ट पठाउँछौं जसले कुनै सहरको मौसमको बारेमा सोध्छ।\n",
    "- LLM ले Brave Search टुलमा टुल कल गरेर प्रतिक्रिया दिनेछ, जुन यसरी देखिन्छ `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*नोट: यो उदाहरणले केवल टुल कल गर्छ, यदि तपाईंलाई नतिजा प्राप्त गर्न चाहिन्छ भने, तपाईंले Brave API पेजमा निःशुल्क खाता बनाउनुपर्नेछ र फंक्शन आफैं परिभाषित गर्नुपर्नेछ*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "एलएलएम भए तापनि, Llama 3.1 को एउटा सीमितता भनेको मल्टिमोडालिटी हो। अर्थात्, विभिन्न प्रकारका इनपुटहरू जस्तै तस्बिरहरूलाई प्रॉम्प्टको रूपमा प्रयोग गर्न र प्रतिक्रिया दिन सक्ने क्षमता। यो क्षमता Llama 3.2 का मुख्य विशेषताहरू मध्ये एक हो। यी विशेषताहरूमा समावेश छन्:\n",
    "\n",
    "- मल्टिमोडालिटी - पाठ र तस्बिर दुवै प्रॉम्प्टको मूल्याङ्कन गर्न सक्ने क्षमता\n",
    "- सानोदेखि मध्यम आकारका भेरिएसनहरू (11B र 90B) - यसले लचिलो डिप्लोयमेन्ट विकल्पहरू प्रदान गर्छ,\n",
    "- केवल-पाठ भेरिएसनहरू (1B र 3B) - यसले मोडेललाई एज / मोबाइल डिभाइसहरूमा डिप्लोय गर्न र कम लेटेन्सी प्रदान गर्न सम्भव बनाउँछ\n",
    "\n",
    "मल्टिमोडल समर्थनले खुला स्रोत मोडेलहरूको दुनियाँमा ठूलो फड्को प्रस्तुत गर्छ। तलको कोड उदाहरणले तस्बिर र पाठ दुवै प्रॉम्प्ट लिएर Llama 3.2 90B बाट तस्बिरको विश्लेषण प्राप्त गर्छ।\n",
    "\n",
    "### Llama 3.2 सँग मल्टिमोडल समर्थन\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## सिकाइ यहाँ रोकिँदैन, यात्रा जारी राख्नुहोस्\n",
    "\n",
    "यो पाठ समाप्त गरेपछि, हाम्रो [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) हेर्नुहोस् र आफ्नो Generative AI ज्ञान अझ बढाउनुहोस्!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nयो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी शुद्धताको लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धि हुन सक्छ। मूल भाषा भएको मूल दस्तावेज़लाई नै आधिकारिक स्रोत मान्नुपर्छ। महत्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुने छैनौं।\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:40:57+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "ne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}