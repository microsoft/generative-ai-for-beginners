<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-05-20T01:40:20+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "cs"
}
-->
# Retrieval Augmented Generation (RAG) a vektorov칠 datab치ze

[![Retrieval Augmented Generation (RAG) a vektorov칠 datab치ze](../../../translated_images/15-lesson-banner.799d0cd2229970edb365f6667a4c7b3a0f526eb8698baa7d2e05c3bd49a5d83f.cs.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

V lekci o vyhled치vac칤ch aplikac칤ch jsme se stru캜n캩 nau캜ili, jak integrovat vlastn칤 data do velk칳ch jazykov칳ch model콢 (LLM). V t칠to lekci se podrobn캩ji pod칤v치me na koncepty zakotven칤 va코ich dat v aplikaci LLM, mechanismy procesu a metody ukl치d치n칤 dat, v캜etn캩 embedding콢 a textu.

> **Video brzy dostupn칠**

## 칔vod

V t칠to lekci se budeme zab칳vat n치sleduj칤c칤mi t칠maty:

- 칔vod do RAG, co to je a pro캜 se pou쮂셨치 v um캩l칠 inteligenci (AI).

- Pochopen칤, co jsou vektorov칠 datab치ze, a vytvo콏en칤 jedn칠 pro na코i aplikaci.

- Praktick칳 p콏칤klad, jak integrovat RAG do aplikace.

## C칤le u캜en칤

Po dokon캜en칤 t칠to lekce budete schopni:

- Vysv캩tlit v칳znam RAG p콏i z칤sk치v치n칤 a zpracov치n칤 dat.

- Nastavit aplikaci RAG a zakotvit sv치 data do LLM.

- Efektivn칤 integrace RAG a vektorov칳ch datab치z칤 v aplikac칤ch LLM.

## Na코e sc칠n치콏: vylep코en칤 na코ich LLM vlastn칤mi daty

V t칠to lekci chceme p콏idat vlastn칤 pozn치mky do vzd캩l치vac칤ho startupu, co umo쬹칤 chatbotovi z칤skat v칤ce informac칤 o r콢zn칳ch p콏edm캩tech. Pomoc칤 pozn치mek, kter칠 m치me, se studenti budou moci l칠pe u캜it a porozum캩t r콢zn칳m t칠mat콢m, co usnadn칤 p콏칤pravu na zkou코ky. K vytvo콏en칤 na코eho sc칠n치콏e pou쬴jeme:

- `Azure OpenAI:` LLM, kter칠 pou쬴jeme k vytvo콏en칤 na코eho chatbota

- `AI for beginners' lesson on Neural Networks`: to budou data, na kter칳ch zakotv칤me na코e LLM

- `Azure AI Search` a `Azure Cosmos DB:` vektorov치 datab치ze pro ulo쬰n칤 na코ich dat a vytvo콏en칤 vyhled치vac칤ho indexu

U쬴vatel칠 budou moci vytv치콏et cvi캜n칠 kv칤zy z jejich pozn치mek, revizn칤 karti캜ky a shrnout je do stru캜n칳ch p콏ehled콢. Abychom za캜ali, pod칤vejme se, co je RAG a jak funguje:

## Retrieval Augmented Generation (RAG)

Chatbot poh치n캩n칳 LLM zpracov치v치 u쬴vatelsk칠 podn캩ty k generov치n칤 odpov캩d칤. Je navr쬰n tak, aby byl interaktivn칤 a zapojoval se do 코irok칠 코k치ly t칠mat. Jeho odpov캩di jsou v코ak omezeny na kontext, kter칳 mu je poskytnut, a jeho z치kladn칤 tr칠ninkov치 data. Nap콏칤klad, GPT-4 m치 znalostn칤 limit do z치콏칤 2021, co znamen치, 쬰 postr치d치 znalosti o ud치lostech, kter칠 nastaly po tomto obdob칤. Nav칤c data pou쬴t치 k tr칠ninku LLM vylu캜uj칤 d콢v캩rn칠 informace, jako jsou osobn칤 pozn치mky nebo manu치l produktu spole캜nosti.

### Jak funguj칤 RAGs (Retrieval Augmented Generation)

![kresba ukazuj칤c칤, jak funguj칤 RAGs](../../../translated_images/how-rag-works.d87a7ed9c30f43126bb9e8e259be5d66e16cd1fef65374e6914746ba9bfb0b2f.cs.png)

P콏edstavte si, 쬰 chcete nasadit chatbota, kter칳 vytv치콏칤 kv칤zy z va코ich pozn치mek, budete pot콏ebovat spojen칤 s datab치z칤 znalost칤. Zde p콏ich치z칤 RAG na pomoc. RAGs funguj칤 n치sledovn캩:

- **Datab치ze znalost칤:** P콏ed z칤sk치v치n칤m mus칤 b칳t tyto dokumenty ingestov치ny a p콏edzpracov치ny, obvykle rozd캩len칤m velk칳ch dokument콢 na men코칤 캜치sti, transformac칤 na textov칠 embeddingy a ulo쬰n칤m do datab치ze.

- **Dotaz u쬴vatele:** u쬴vatel polo쮂 ot치zku

- **Z칤sk치v치n칤:** Kdy u쬴vatel polo쮂 ot치zku, embedding model z칤sk치 relevantn칤 informace z na코칤 datab치ze znalost칤, aby poskytl v칤ce kontextu, kter칳 bude za캜len캩n do podn캩tu.

- **Augmentovan치 generace:** LLM vylep코uje svou odpov캩캞 na z치klad캩 z칤skan칳ch dat. To umo쮄갓je, aby odpov캩캞 byla zalo쬰na nejen na p콏edtr칠novan칳ch datech, ale tak칠 na relevantn칤ch informac칤ch z p콏idan칠ho kontextu. Z칤skan치 data jsou pou쬴ta k augmentaci odpov캩d칤 LLM. LLM pak vr치t칤 odpov캩캞 na ot치zku u쬴vatele.

![kresba ukazuj칤c칤 architekturu RAGs](../../../translated_images/encoder-decode.75eebc7093ccefec17568eebc80d3d0b831ecf2ea204566377a04c77a5a57ebb.cs.png)

Architektura pro RAGs je implementov치na pomoc칤 transform치tor콢, kter칠 se skl치daj칤 ze dvou 캜치st칤: enkod칠ru a dekod칠ru. Nap콏칤klad, kdy u쬴vatel polo쮂 ot치zku, vstupn칤 text je 'zak칩dov치n' do vektor콢 zachycuj칤c칤ch v칳znam slov a vektory jsou 'dek칩dov치ny' do na코eho indexu dokument콢 a generuj칤 nov칳 text na z치klad캩 dotazu u쬴vatele. LLM pou쮂셨치 model enkod칠r-dekod칠r k vygenerov치n칤 v칳stupu.

Dva p콏칤stupy p콏i implementaci RAG podle navrhovan칠ho dokumentu: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) jsou:

- **_RAG-Sequence_** pou쮂셨aj칤c칤 z칤skan칠 dokumenty k predikci nejlep코칤 mo쬹칠 odpov캩di na dotaz u쬴vatele

- **RAG-Token** pou쮂셨aj칤c칤 dokumenty k vygenerov치n칤 dal코칤ho tokenu, pot칠 je z칤sk치 k odpov캩di na dotaz u쬴vatele

### Pro캜 byste pou쬴li RAGs?

- **Bohatost informac칤:** zaji코콘uje, 쬰 textov칠 odpov캩di jsou aktu치ln칤 a modern칤. Proto zlep코uje v칳kon p콏i 칰kolech specifick칳ch pro danou dom칠nu p콏칤stupem k intern칤 datab치zi znalost칤.

- Sni쬿je fabulaci vyu쬴t칤m **ov캩콏iteln칳ch dat** v datab치zi znalost칤 k poskytnut칤 kontextu u쬴vatelsk칳m dotaz콢m.

- Je **n치kladov캩 efektivn칤**, proto쬰 je ekonomi캜t캩j코칤 ne dolad캩n칤 LLM.

## Vytv치콏en칤 datab치ze znalost칤

Na코e aplikace je zalo쬰na na na코ich osobn칤ch datech, tj. lekci o neuronov칳ch s칤t칤ch v kurikulu AI pro za캜치te캜n칤ky.

### Vektorov칠 datab치ze

Vektorov치 datab치ze, na rozd칤l od tradi캜n칤ch datab치z칤, je specializovan치 datab치ze ur캜en치 k ukl치d치n칤, spr치v캩 a vyhled치v치n칤 vlo쬰n칳ch vektor콢. Ukl치d치 캜칤seln칠 reprezentace dokument콢. Rozlo쬰n칤 dat na 캜칤seln칠 embeddingy usnad켿uje na코emu AI syst칠mu porozum캩n칤 a zpracov치n칤 dat.

Ukl치d치me na코e embeddingy do vektorov칳ch datab치z칤, proto쬰 LLM maj칤 limit po캜tu token콢, kter칠 p콏ij칤maj칤 jako vstup. Proto쬰 nem콢쬰te p콏edat cel칠 embeddingy do LLM, budeme je muset rozd캩lit na 캜치sti a kdy u쬴vatel polo쮂 ot치zku, embeddingy nejv칤ce podobn칠 ot치zce budou vr치ceny spolu s podn캩tem. Rozd캩len칤 tak칠 sni쬿je n치klady na po캜et token콢 pro코l칳ch LLM.

N캩kter칠 popul치rn칤 vektorov칠 datab치ze zahrnuj칤 Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant a DeepLake. M콢쬰te vytvo콏it model Azure Cosmos DB pomoc칤 Azure CLI s n치sleduj칤c칤m p콏칤kazem:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od textu k embedding콢m

P콏ed ulo쬰n칤m na코ich dat je budeme muset p콏ev칠st na vektorov칠 embeddingy, ne budou ulo쬰ny v datab치zi. Pokud pracujete s velk칳mi dokumenty nebo dlouh칳mi texty, m콢쬰te je rozd캩lit na z치klad캩 dotaz콢, kter칠 o캜ek치v치te. Rozd캩len칤 m콢쬰 b칳t provedeno na 칰rovni v캩t nebo na 칰rovni odstavc콢. Proto쬰 rozd캩len칤 odvozuje v칳znamy z okoln칤ch slov, m콢쬰te p콏idat n캩jak칳 dal코칤 kontext k 캜치sti, nap콏칤klad p콏id치n칤m n치zvu dokumentu nebo zahrnut칤m n캩jak칠ho textu p콏ed nebo po 캜치sti. M콢쬰te data rozd캩lit n치sledovn캩:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Jakmile jsou rozd캩lena, m콢쬰me na코e texty vlo쬴t pomoc칤 r콢zn칳ch embeddingov칳ch model콢. N캩kter칠 modely, kter칠 m콢쬰te pou쮂셦, zahrnuj칤: word2vec, ada-002 od OpenAI, Azure Computer Vision a mnoho dal코칤ch. V칳b캩r modelu, kter칳 pou쬴jete, bude z치viset na jazyc칤ch, kter칠 pou쮂셨치te, typu obsahu k칩dovan칠ho (text/obr치zky/audio), velikosti vstupu, kter칳 m콢쬰 k칩dovat, a d칠lce v칳stupu embeddingu.

P콏칤klad vlo쬰n칠ho textu pomoc칤 modelu `text-embedding-ada-002` od OpenAI je:
![vlo쬰n칳 text slova ko캜ka](../../../translated_images/cat.3db013cbca4fd5d90438ea7b312ad0364f7686cf79931ab15cd5922151aea53e.cs.png)

## Z칤sk치v치n칤 a vektorov칠 vyhled치v치n칤

Kdy u쬴vatel polo쮂 ot치zku, retriever ji transformuje na vektor pomoc칤 enkod칠ru dotazu, pot칠 prohled치v치 n치코 vyhled치vac칤 index dokument콢 pro relevantn칤 vektory v dokumentu, kter칠 jsou spojeny se vstupem. Jakmile je hotovo, p콏ev치d칤 jak vstupn칤 vektor, tak vektory dokumentu na text a p콏ed치v치 jej p콏es LLM.

### Z칤sk치v치n칤

Z칤sk치v치n칤 se d캩je, kdy se syst칠m sna쮂 rychle naj칤t dokumenty z indexu, kter칠 spl켿uj칤 krit칠ria vyhled치v치n칤. C칤lem retrieveru je z칤skat dokumenty, kter칠 budou pou쬴ty k poskytnut칤 kontextu a zakotven칤 LLM na va코ich datech.

Existuje n캩kolik zp콢sob콢, jak prov치d캩t vyhled치v치n칤 v na코칤 datab치zi, nap콏칤klad:

- **Vyhled치v치n칤 kl칤캜ov칳ch slov** - pou쮂셨치 se pro textov칠 vyhled치v치n칤

- **S칠mantick칠 vyhled치v치n칤** - pou쮂셨치 s칠mantick칳 v칳znam slov

- **Vektorov칠 vyhled치v치n칤** - p콏ev치d칤 dokumenty z textu na vektorov칠 reprezentace pomoc칤 embeddingov칳ch model콢. Z칤sk치v치n칤 bude provedeno dotazov치n칤m dokument콢, jejich vektorov칠 reprezentace jsou nejbl칤쬰 ot치zce u쬴vatele.

- **Hybridn칤** - kombinace jak vyhled치v치n칤 kl칤캜ov칳ch slov, tak vektorov칠ho vyhled치v치n칤.

V칳zva p콏i z칤sk치v치n칤 p콏ich치z칤, kdy v datab치zi nen칤 podobn치 odpov캩캞 na dotaz, syst칠m pak vr치t칤 nejlep코칤 informace, kter칠 m콢쬰 z칤skat, nicm칠n캩 m콢쬰te pou쮂셦 taktiky jako nastaven칤 maxim치ln칤 vzd치lenosti pro relevantnost nebo pou쬴t칤 hybridn칤ho vyhled치v치n칤, kter칠 kombinuje jak kl칤캜ov치 slova, tak vektorov칠 vyhled치v치n칤. V t칠to lekci pou쬴jeme hybridn칤 vyhled치v치n칤, kombinaci jak vektorov칠ho, tak kl칤캜ov칠ho vyhled치v치n칤. Ulo쮂셠e na코e data do datov칠ho r치mce se sloupci obsahuj칤c칤mi 캜치sti i embeddingy.

### Vektorov치 podobnost

Retriever prohled치 datab치zi znalost칤 pro embeddingy, kter칠 jsou bl칤zko sebe, nejbli쮄뫆 soused, proto쬰 jsou to texty, kter칠 jsou podobn칠. V p콏칤pad캩, 쬰 u쬴vatel polo쮂 dotaz, je nejprve vlo쬰n a pot칠 sp치rov치n s podobn칳mi embeddingy. B캩쬹칠 m캩콏en칤, kter칠 se pou쮂셨치 k zji코t캩n칤, jak podobn칠 jsou r콢zn칠 vektory, je kosinov치 podobnost, kter치 je zalo쬰na na 칰hlu mezi dv캩ma vektory.

M콢쬰me m캩콏it podobnost pomoc칤 jin칳ch alternativ, kter칠 m콢쬰me pou쮂셦, jako je Euklidovsk치 vzd치lenost, kter치 je p콏칤mkou mezi koncov칳mi body vektor콢, a skal치rn칤 sou캜in, kter칳 m캩콏칤 sou캜et sou캜in콢 odpov칤daj칤c칤ch prvk콢 dvou vektor콢.

### Vyhled치vac칤 index

P콏i prov치d캩n칤 z칤sk치v치n칤 budeme pot콏ebovat vytvo콏it vyhled치vac칤 index pro na코i datab치zi znalost칤 p콏ed proveden칤m vyhled치v치n칤. Index ulo쮂 na코e embeddingy a m콢쬰 rychle z칤skat nejpodobn캩j코칤 캜치sti i ve velk칠 datab치zi. M콢쬰me vytvo콏it n치코 index lok치ln캩 pomoc칤:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### P콏erovn치n칤

Jakmile dotazujete datab치zi, mo쬹치 budete pot콏ebovat se콏adit v칳sledky od nejrelevantn캩j코칤ch. P콏erovn치vac칤 LLM vyu쮂셨치 strojov칠 u캜en칤 ke zlep코en칤 relevance v칳sledk콢 vyhled치v치n칤 jejich uspo콏치d치n칤m od nejrelevantn캩j코칤ch. Pomoc칤 Azure AI Search je p콏erovn치n칤 provedeno automaticky pomoc칤 s칠mantick칠ho p콏erovn치va캜e. P콏칤klad, jak p콏erovn치n칤 funguje pomoc칤 nejbli쮄뫆셖h soused콢:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Spojen칤 v코eho dohromady

Posledn칤m krokem je p콏id치n칤 na코eho LLM do sm캩si, abychom mohli z칤skat odpov캩di, kter칠 jsou zakotven칠 v na코ich datech. M콢쬰me to implementovat n치sledovn캩:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Hodnocen칤 na코칤 aplikace

### Hodnot칤c칤 metriky

- Kvalita poskytovan칳ch odpov캩d칤 zaji코콘uj칤c칤, 쬰 zn칤 p콏irozen캩, plynule a lidsky

- Zakotvenost dat: hodnocen칤, zda odpov캩캞 poch치z칤 z poskytnut칳ch dokument콢

- Relevance: hodnocen칤, zda odpov캩캞 odpov칤d치 a souvis칤 s polo쬰nou ot치zkou

- Plynulost - zda odpov캩캞 d치v치 smysl gramaticky

## Pou쬴t칤 RAG (Retrieval Augmented Generation) a vektorov칳ch datab치z칤

Existuje mnoho r콢zn칳ch p콏칤pad콢 pou쬴t칤, kde funk캜n칤 vol치n칤 m콢쬰 zlep코it va코i aplikaci, nap콏칤klad:

- Ot치zky a odpov캩di: zakotven칤 dat va코칤 spole캜nosti do chatu, kter칳 mohou zam캩stnanci pou쮂셨at k pokl치d치n칤 ot치zek.

- Doporu캜ovac칤 syst칠my: kde m콢쬰te vytvo콏it syst칠m, kter칳 p치ruje nejpodobn캩j코칤 hodnoty, nap콏. filmy, restaurace a mnoho dal코칤ch.

- Slu쬭y chatbot콢: m콢쬰te ukl치dat historii chatu a personalizovat konverzaci na z치klad캩 u쬴vatelsk칳ch dat.

- Vyhled치v치n칤 obr치zk콢 na z치klad캩 vektorov칳ch embedding콢, u쬴te캜n칠 p콏i rozpozn치v치n칤 obr치zk콢 a detekci anom치li칤.

## Shrnut칤

Pokryli jsme z치kladn칤 oblasti RAG od p콏id치n칤 na코ich dat do aplikace, u쬴vatelsk칠ho dotazu a v칳stupu. K zjednodu코en칤 tvorby RAG m콢쬰te pou쮂셦 r치mce jako Semanti Kernel, Langchain nebo Autogen.

## Zad치n칤

Pro pokra캜ov치n칤 ve studiu Retrieval Augmented Generation (RAG) m콢쬰te vytvo콏it:

- Vytvo콏te front-end pro aplikaci pomoc칤 v치mi zvolen칠ho r치mce

- Vyu쬴jte r치mec, bu캞 LangChain nebo Semantic Kernel, a znovu vytvo콏te svou aplikaci.

Gratulujeme k dokon캜en칤 lekce 游녪.

## U캜en칤 zde nekon캜칤, pokra캜ujte v cest캩

Po dokon캜en칤 t칠to lekce se pod칤vejte na na코i [Generativn칤 AI Learning kolekci](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) a pokra캜ujte v roz코i콏ov치n칤 sv칳ch znalost칤 o generativn칤 AI!

**Prohl치코en칤**:  
Tento dokument byl p콏elo쬰n pomoc칤 AI p콏ekladov칠 slu쬭y [Co-op Translator](https://github.com/Azure/co-op-translator). A캜koli se sna쮂셠e o p콏esnost, m캩jte pros칤m na pam캩ti, 쬰 automatizovan칠 p콏eklady mohou obsahovat chyby nebo nep콏esnosti. P콢vodn칤 dokument v jeho rodn칠m jazyce by m캩l b칳t pova쬺v치n za autoritativn칤 zdroj. Pro d콢le쬴t칠 informace se doporu캜uje profesion치ln칤 lidsk칳 p콏eklad. Nejsme zodpov캩dn칤 za jak칠koli nedorozum캩n칤 nebo nespr치vn칠 interpretace vypl칳vaj칤c칤 z pou쬴t칤 tohoto p콏ekladu.