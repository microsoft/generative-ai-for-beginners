# Retrieval Augmented Generation (RAG) a vektorovÃ© databÃ¡ze

[![Retrieval Augmented Generation (RAG) a vektorovÃ© databÃ¡ze](../../../translated_images/cs/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

V lekci o vyhledÃ¡vacÃ­ch aplikacÃ­ch jsme se struÄnÄ› nauÄili, jak integrovat vlastnÃ­ data do VelkÃ½ch jazykovÃ½ch modelÅ¯ (LLM). V tÃ©to lekci se podÃ­vÃ¡me podrobnÄ›ji na koncepty zaklÃ¡dÃ¡nÃ­ vaÅ¡ich dat v aplikaci LLM, mechaniku tohoto procesu a metody uklÃ¡dÃ¡nÃ­ dat, vÄetnÄ› embeddingÅ¯ i textu.

> **Video brzy k dispozici**

## Ãšvod

V tÃ©to lekci pokryjeme nÃ¡sledujÃ­cÃ­ tÃ©mata:

- Ãšvod do RAG, co to je a proÄ se pouÅ¾Ã­vÃ¡ v AI (umÄ›lÃ© inteligenci).

- PochopenÃ­, co jsou vektorovÃ© databÃ¡ze a vytvoÅ™enÃ­ jednÃ© pro naÅ¡i aplikaci.

- PraktickÃ½ pÅ™Ã­klad, jak integrovat RAG do aplikace.

## VÃ½ukovÃ© cÃ­le

Po dokonÄenÃ­ tÃ©to lekce budete schopni:

- VysvÄ›tlit vÃ½znam RAG pÅ™i zÃ­skÃ¡vÃ¡nÃ­ a zpracovÃ¡nÃ­ dat.

- Nastavit aplikaci RAG a zakotvit svÃ¡ data do LLM.

- EfektivnÄ› integrovat RAG a vektorovÃ© databÃ¡ze v aplikacÃ­ch zaloÅ¾enÃ½ch na LLM.

## NÃ¡Å¡ scÃ©nÃ¡Å™: vylepÅ¡enÃ­ naÅ¡ich LLM pomocÃ­ vlastnÃ­ch dat

Pro tuto lekci chceme pÅ™idat naÅ¡e vlastnÃ­ poznÃ¡mky do vzdÄ›lÃ¡vacÃ­ startupovÃ© aplikace, coÅ¾ umoÅ¾nÃ­ chatbotu zÃ­skat vÃ­ce informacÃ­ o rÅ¯znÃ½ch tÃ©matech. PouÅ¾itÃ­m poznÃ¡mek, kterÃ© mÃ¡me, budou studenti schopni se lÃ©pe uÄit a porozumÄ›t rÅ¯znÃ½m tÃ©matÅ¯m, coÅ¾ usnadnÃ­ opakovÃ¡nÃ­ pro jejich zkouÅ¡ky. Pro vytvoÅ™enÃ­ scÃ©nÃ¡Å™e pouÅ¾ijeme:

- `Azure OpenAI:` LLM, kterÃ© pouÅ¾ijeme k vytvoÅ™enÃ­ naÅ¡eho chatbota

- `Lekce AI pro zaÄÃ¡teÄnÃ­ky o neuronovÃ½ch sÃ­tÃ­ch:` na tÄ›chto datech zaloÅ¾Ã­me nÃ¡Å¡ LLM

- `Azure AI Search` a `Azure Cosmos DB:` vektorovÃ¡ databÃ¡ze pro uklÃ¡dÃ¡nÃ­ naÅ¡ich dat a vytvoÅ™enÃ­ vyhledÃ¡vacÃ­ho indexu

UÅ¾ivatelÃ© budou moci vytvÃ¡Å™et cviÄnÃ© kvÃ­zy z jejich poznÃ¡mek, kartiÄky pro opakovÃ¡nÃ­ a shrnutÃ­ do konciznÃ­ch pÅ™ehledÅ¯. Pro zaÄÃ¡tek si pojÄme ukÃ¡zat, co je RAG a jak funguje:

## Retrieval Augmented Generation (RAG)

Chatbot zaloÅ¾enÃ½ na LLM zpracovÃ¡vÃ¡ uÅ¾ivatelskÃ© dotazy k vytvoÅ™enÃ­ odpovÄ›dÃ­. Je navrÅ¾en tak, aby byl interaktivnÃ­ a komunikoval s uÅ¾ivateli na Å¡irokÃ© spektrum tÃ©mat. Jeho odpovÄ›di jsou vÅ¡ak omezeny kontextem, kterÃ½ byl poskytnut, a zÃ¡kladnÃ­mi trÃ©ninkovÃ½mi daty. NapÅ™Ã­klad znalostnÃ­ mez GPT-4 je zÃ¡Å™Ã­ 2021, coÅ¾ znamenÃ¡, Å¾e postrÃ¡dÃ¡ informace o udÃ¡lostech po tomto obdobÃ­. NavÃ­c data pouÅ¾itÃ¡ pro trÃ©nink LLM nezahrnujÃ­ dÅ¯vÄ›rnÃ© informace, jako jsou osobnÃ­ poznÃ¡mky nebo uÅ¾ivatelskÃ½ manuÃ¡l spoleÄnosti.

### Jak fungujÃ­ RAG (Retrieval Augmented Generation)

![drawing showing how RAGs work](../../../translated_images/cs/how-rag-works.f5d0ff63942bd3a6.webp)

PÅ™edpoklÃ¡dejme, Å¾e chcete nasadit chatbota, kterÃ½ vytvÃ¡Å™Ã­ kvÃ­zy z vaÅ¡ich poznÃ¡mek; budete potÅ™ebovat pÅ™ipojenÃ­ k znalostnÃ­ bÃ¡zi. Zde pÅ™ichÃ¡zÃ­ na pomoc RAG. RAG fungujÃ­ takto:

- **ZnalostnÃ­ bÃ¡ze:** PÅ™ed vyhledÃ¡vÃ¡nÃ­m je tÅ™eba dokumenty importovat a pÅ™edzpracovat, obvykle rozdÄ›lenÃ­m velkÃ½ch dokumentÅ¯ na menÅ¡Ã­ ÄÃ¡sti, pÅ™evedenÃ­m na textovÃ© embeddingy a uloÅ¾enÃ­ do databÃ¡ze.

- **UÅ¾ivatelskÃ½ dotaz:** uÅ¾ivatel poloÅ¾Ã­ otÃ¡zku.

- **VyhledÃ¡vÃ¡nÃ­:** KdyÅ¾ uÅ¾ivatel poloÅ¾Ã­ otÃ¡zku, embeddingovÃ½ model vyhledÃ¡ relevantnÃ­ informace ze znalostnÃ­ bÃ¡ze, aby poskytl vÃ­ce kontextu, kterÃ½ bude zahrnut do promptu.

- **RozÅ¡Ã­Å™enÃ¡ generace:** LLM vylepÅ¡uje svou odpovÄ›Ä na zÃ¡kladÄ› zÃ­skanÃ½ch dat. UmoÅ¾Åˆuje generovat odpovÄ›di nejen na zÃ¡kladÄ› pÅ™edem natrÃ©novanÃ½ch dat, ale i relevantnÃ­ch informacÃ­ z dodanÃ©ho kontextu. ZÃ­skanÃ¡ data slouÅ¾Ã­ k rozÅ¡Ã­Å™enÃ­ odpovÄ›dÃ­ LLM. LLM pak vracÃ­ odpovÄ›Ä na dotaz uÅ¾ivatele.

![drawing showing how RAGs architecture](../../../translated_images/cs/encoder-decode.f2658c25d0eadee2.webp)

Architektura RAG je implementovÃ¡na pomocÃ­ transformÃ¡torÅ¯ sklÃ¡dajÃ­cÃ­ch se ze dvou ÄÃ¡stÃ­: enkodÃ©ru a dekodÃ©ru. NapÅ™Ã­klad kdyÅ¾ uÅ¾ivatel poloÅ¾Ã­ otÃ¡zku, vstupnÃ­ text je â€zakÃ³dovÃ¡nâ€œ do vektorÅ¯ zachycujÃ­cÃ­ch vÃ½znam slov, a tyto vektory jsou â€dekÃ³dovÃ¡nyâ€œ na nÃ¡Å¡ index dokumentÅ¯ a generujÃ­ novÃ½ text na zÃ¡kladÄ› uÅ¾ivatelskÃ©ho dotazu. LLM pouÅ¾Ã­vÃ¡ model enkodÃ©r-dekodÃ©r k vytvoÅ™enÃ­ vÃ½stupu.

Dva pÅ™Ã­stupy k implementaci RAG podle navrhovanÃ©ho ÄlÃ¡nku: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) jsou:

- **_RAG-Sequence_** pouÅ¾Ã­vajÃ­cÃ­ zÃ­skanÃ© dokumenty k pÅ™edpovÄ›di nejlepÅ¡Ã­ moÅ¾nÃ© odpovÄ›di na uÅ¾ivatelskÃ½ dotaz

- **RAG-Token** pouÅ¾Ã­vajÃ­cÃ­ dokumenty k generovÃ¡nÃ­ dalÅ¡Ã­ho tokenu, a potÃ© je znovu zÃ­skat k odpovÄ›di na dotaz uÅ¾ivatele

### ProÄ pouÅ¾Ã­vat RAG?

- **Bohatost informacÃ­:** zajiÅ¡Å¥uje, Å¾e textovÃ© odpovÄ›di jsou aktuÃ¡lnÃ­ a aktuÃ¡lnÃ­. ZvÃ½razÅˆuje vÃ½kon na specifickÃ½ch domÃ©novÃ½ch ÃºlohÃ¡ch pÅ™Ã­stupem do internÃ­ znalostnÃ­ bÃ¡ze.

- SniÅ¾uje vÃ½robu nesprÃ¡vnÃ½ch informacÃ­ vyuÅ¾itÃ­m **ovÄ›Å™itelnÃ½ch dat** v znalostnÃ­ bÃ¡zi k poskytnutÃ­ kontextu uÅ¾ivatelskÃ½m dotazÅ¯m.

- Je **nÃ¡kladovÄ› efektivnÃ­**, protoÅ¾e je ekonomiÄtÄ›jÅ¡Ã­ neÅ¾ doladÄ›nÃ­ (fine-tuning) LLM.

## VytvoÅ™enÃ­ znalostnÃ­ bÃ¡ze

NaÅ¡e aplikace je zaloÅ¾ena na naÅ¡ich osobnÃ­ch datech, tj. lekci o neuronovÃ½ch sÃ­tÃ­ch z kurikula AI pro zaÄÃ¡teÄnÃ­ky.

### VektorovÃ© databÃ¡ze

VektorovÃ¡ databÃ¡ze, na rozdÃ­l od tradiÄnÃ­ch databÃ¡zÃ­, je specializovanÃ¡ databÃ¡ze navrÅ¾enÃ¡ k uklÃ¡dÃ¡nÃ­, sprÃ¡vÄ› a vyhledÃ¡vÃ¡nÃ­ vloÅ¾enÃ½ch vektorÅ¯. UklÃ¡dÃ¡ ÄÃ­selnÃ© reprezentace dokumentÅ¯. RozloÅ¾enÃ­ dat na ÄÃ­selnÃ© embeddingy usnadÅˆuje naÅ¡emu AI systÃ©mu porozumÄ›nÃ­ a zpracovÃ¡nÃ­ dat.

UklÃ¡dÃ¡me naÅ¡e embeddingy ve vektorovÃ½ch databÃ¡zÃ­ch, protoÅ¾e LLM majÃ­ omezenÃ½ poÄet tokenÅ¯, kterÃ© pÅ™ijÃ­majÃ­ jako vstup. JelikoÅ¾ nelze poslat celÃ© embeddingy do LLM, musÃ­me je rozdÄ›lit na ÄÃ¡sti a kdyÅ¾ uÅ¾ivatel poloÅ¾Ã­ otÃ¡zku, vrÃ¡tÃ­ se nejpravdÄ›podobnÄ›jÅ¡Ã­ embeddingy spolu s promptem. RozdÄ›lenÃ­ takÃ© sniÅ¾uje nÃ¡klady na poÄet tokenÅ¯ odesÃ­lanÃ½ch do LLM.

Mezi oblÃ­benÃ© vektorovÃ© databÃ¡ze patÅ™Ã­ Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant a DeepLake. Azure Cosmos DB model mÅ¯Å¾ete vytvoÅ™it pomocÃ­ Azure CLI pomocÃ­ nÃ¡sledujÃ­cÃ­ho pÅ™Ã­kazu:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od textu k embeddingÅ¯m

PÅ™ed uloÅ¾enÃ­m dat je tÅ™eba je pÅ™evÃ©st na vektorovÃ© embeddingy. Pokud pracujete s velkÃ½mi dokumenty nebo dlouhÃ½mi texty, mÅ¯Å¾ete je rozdÄ›lit na ÄÃ¡sti podle oÄekÃ¡vanÃ½ch dotazÅ¯. RozdÄ›lenÃ­ mÅ¯Å¾e bÃ½t na Ãºrovni vÄ›t nebo odstavcÅ¯. ProtoÅ¾e rozdÄ›lenÃ­ odvozuje vÃ½znamy z okolnÃ­ch slov, mÅ¯Å¾ete k ÄÃ¡sti pÅ™idat i jinÃ½ kontext, napÅ™Ã­klad nÃ¡zev dokumentu nebo nÄ›jakÃ½ text pÅ™ed nebo za ÄÃ¡stÃ­. Data mÅ¯Å¾ete rozdÄ›lit takto:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # Pokud poslednÃ­ kus nedosÃ¡hl minimÃ¡lnÃ­ dÃ©lky, pÅ™idejte ho pÅ™esto
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Jakmile mÃ¡me data rozdÄ›lenÃ¡ na ÄÃ¡sti, mÅ¯Å¾eme je vloÅ¾it pomocÃ­ rÅ¯znÃ½ch embedding modelÅ¯. NÄ›kterÃ© modely, kterÃ© mÅ¯Å¾ete pouÅ¾Ã­t, zahrnujÃ­: word2vec, ada-002 od OpenAI, Azure Computer Vision a mnoho dalÅ¡Ã­ch. VÃ½bÄ›r modelu zÃ¡visÃ­ na jazycÃ­ch, kterÃ© pouÅ¾Ã­vÃ¡te, na typu kÃ³dovanÃ©ho obsahu (text/obrÃ¡zky/audio), velikosti vstupu, kterÃ½ mÅ¯Å¾e kÃ³dovat, a dÃ©lce vÃ½stupnÃ­ho embeddingu.

PÅ™Ã­klad vloÅ¾enÃ©ho textu pomocÃ­ modelu OpenAI `text-embedding-ada-002` je:
![an embedding of the word cat](../../../translated_images/cs/cat.74cbd7946bc9ca38.webp)

## VyhledÃ¡vÃ¡nÃ­ a vektorovÃ© dotazy

KdyÅ¾ uÅ¾ivatel poloÅ¾Ã­ otÃ¡zku, vyhledÃ¡vaÄ ji pÅ™evede na vektor pomocÃ­ dotazovÃ©ho enkodÃ©ru a potÃ© prohledÃ¡vÃ¡ nÃ¡Å¡ vyhledÃ¡vacÃ­ index dokumentÅ¯, hledaje relevantnÃ­ vektory v dokumentu vztahujÃ­cÃ­ se k vstupu. Jakmile je vyhledÃ¡vÃ¡nÃ­ hotovo, pÅ™evede vstupnÃ­ vektor i vektory dokumentÅ¯ zpÄ›t na text a pÅ™edÃ¡ je LLM.

### VyhledÃ¡vÃ¡nÃ­

VyhledÃ¡vÃ¡nÃ­ nastÃ¡vÃ¡, kdyÅ¾ se systÃ©m snaÅ¾Ã­ rychle najÃ­t dokumenty v indexu, kterÃ© splÅˆujÃ­ kritÃ©ria vyhledÃ¡vÃ¡nÃ­. CÃ­lem vyhledÃ¡vaÄe je zÃ­skat dokumenty, kterÃ© budou pouÅ¾ity k poskytnutÃ­ kontextu a zakotvenÃ­ LLM ve vaÅ¡ich datech.

Existuje nÄ›kolik zpÅ¯sobÅ¯, jak vyhledÃ¡vat v databÃ¡zi, napÅ™Ã­klad:

- **KlÃ­ÄovÃ© slovo** - pouÅ¾Ã­vÃ¡ se pro textovÃ© vyhledÃ¡vÃ¡nÃ­

- **VektorovÃ© vyhledÃ¡vÃ¡nÃ­** - pÅ™evÃ¡dÃ­ dokumenty z textu do vektorovÃ© reprezentace pomocÃ­ embedding modelÅ¯, coÅ¾ umoÅ¾Åˆuje **sÃ©mantickÃ© vyhledÃ¡vÃ¡nÃ­** podle vÃ½znamu slov. VyhledÃ¡vÃ¡nÃ­ probÃ­hÃ¡ dotazem na dokumenty, jejichÅ¾ vektorovÃ© reprezentace jsou nejbliÅ¾Å¡Ã­ dotazu uÅ¾ivatele.

- **HybridnÃ­** - kombinace klÃ­ÄovÃ©ho slova a vektorovÃ©ho vyhledÃ¡vÃ¡nÃ­.

VÃ½zvou pÅ™i vyhledÃ¡vÃ¡nÃ­ je situace, kdy v databÃ¡zi nenÃ­ Å¾Ã¡dnÃ¡ podobnÃ¡ odpovÄ›Ä na dotaz, systÃ©m potom vrÃ¡tÃ­ nejlepÅ¡Ã­ dostupnÃ© informace. MÅ¯Å¾ete vÅ¡ak pouÅ¾Ã­t taktiky jako nastavenÃ­ maximÃ¡lnÃ­ vzdÃ¡lenosti pro relevanci nebo hybridnÃ­ vyhledÃ¡vÃ¡nÃ­, kterÃ© kombinuje klÃ­ÄovÃ¡ slova s vektorovÃ½m vyhledÃ¡vÃ¡nÃ­m. V tÃ©to lekci pouÅ¾ijeme hybridnÃ­ vyhledÃ¡vÃ¡nÃ­, tedy kombinaci vektorovÃ©ho a klÃ­ÄovÃ©ho vyhledÃ¡vÃ¡nÃ­. Data uloÅ¾Ã­me do dataframe s sloupci obsahujÃ­cÃ­mi ÄÃ¡sti dat i embeddingy.

### VektorovÃ¡ podobnost

VyhledÃ¡vaÄ prohledÃ¡ znalostnÃ­ databÃ¡zi na embeddingy, kterÃ© jsou blÃ­zkÃ©, neboli nejbliÅ¾Å¡Ã­ sousedÃ©, protoÅ¾e jsou texty podobnÃ©. V situaci, kdy uÅ¾ivatel poloÅ¾Ã­ dotaz, je nejprve vloÅ¾en a potÃ© porovnÃ¡n s podobnÃ½mi embeddingy. BÄ›Å¾nÃ½m mÄ›Å™Ã­tkem podobnosti je kosinovÃ¡ podobnost, zaloÅ¾enÃ¡ na Ãºhlu mezi dvÄ›ma vektory.

Podobnost lze mÄ›Å™it i dalÅ¡Ã­mi alternativami, napÅ™Ã­klad Euklidovskou vzdÃ¡lenostÃ­, coÅ¾ je pÅ™Ã­mÃ¡ vzdÃ¡lenost mezi koncovÃ½mi body vektorÅ¯, nebo skalÃ¡rnÃ­m souÄinem, kterÃ½ mÄ›Å™Ã­ souÄet souÄinÅ¯ odpovÃ­dajÃ­cÃ­ch prvkÅ¯ dvou vektorÅ¯.

### VyhledÃ¡vacÃ­ index

PÅ™i vyhledÃ¡vÃ¡nÃ­ je nutnÃ© pÅ™ed samotnÃ½m hledÃ¡nÃ­m vytvoÅ™it vyhledÃ¡vacÃ­ index pro naÅ¡i znalostnÃ­ bÃ¡zi. Index uloÅ¾Ã­ naÅ¡e embeddingy a mÅ¯Å¾e rychle najÃ­t nejpodobnÄ›jÅ¡Ã­ ÄÃ¡sti i v rozsÃ¡hlÃ© databÃ¡zi. Index mÅ¯Å¾eme vytvoÅ™it lokÃ¡lnÄ› pomocÃ­:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# VytvoÅ™it vyhledÃ¡vacÃ­ index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# K dotazu na index mÅ¯Å¾ete pouÅ¾Ã­t metodu kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```

### PÅ™erovnÃ¡vÃ¡nÃ­ vÃ½sledkÅ¯ (Re-ranking)

Po zÃ­skÃ¡nÃ­ vÃ½sledkÅ¯ z databÃ¡ze je Äasto potÅ™eba je seÅ™adit dle relevance. PÅ™erovnÃ¡vacÃ­ LLM vyuÅ¾Ã­vÃ¡ strojovÃ© uÄenÃ­ ke zlepÅ¡enÃ­ relevance vyhledÃ¡vacÃ­ch vÃ½sledkÅ¯ pomocÃ­ jejich seÅ™azenÃ­ od nejrelevantnÄ›jÅ¡Ã­ch. PomocÃ­ Azure AI Search je pÅ™erovnÃ¡nÃ­ provedeno automaticky dÃ­ky sÃ©mantickÃ©mu pÅ™erovnavaÄi. PÅ™Ã­klad pÅ™erovnÃ¡nÃ­ pomocÃ­ nejbliÅ¾Å¡Ã­ch sousedÅ¯:

```python
# NajdÄ›te nejpodobnÄ›jÅ¡Ã­ dokumenty
distances, indices = nbrs.kneighbors([query_vector])

index = []
# VytisknÄ›te nejpodobnÄ›jÅ¡Ã­ dokumenty
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## VÅ¡e spojeno dohromady

PoslednÃ­m krokem je zaÄlenÄ›nÃ­ naÅ¡eho LLM do procesu, abychom mohli zÃ­skat odpovÄ›di zaloÅ¾enÃ© na naÅ¡ich datech. MÅ¯Å¾eme to implementovat nÃ¡sledovnÄ›:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # PÅ™eveÄte otÃ¡zku na vektor dotazu
    query_vector = create_embeddings(user_input)

    # NajdÄ›te nejpodobnÄ›jÅ¡Ã­ dokumenty
    distances, indices = nbrs.kneighbors([query_vector])

    # pÅ™idejte dokumenty k dotazu pro poskytnutÃ­ kontextu
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # spojte historii a uÅ¾ivatelskÃ½ vstup
    history.append(user_input)

    # vytvoÅ™te objekt zprÃ¡vy
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # pouÅ¾ijte dokonÄenÃ­ chatu pro generovÃ¡nÃ­ odpovÄ›di
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## HodnocenÃ­ naÅ¡Ã­ aplikace

### EvaluaÄnÃ­ metriky

- Kvalita odpovÄ›dÃ­, aby znÄ›ly pÅ™irozenÄ›, plynule a lidsky

- ZakotvenÃ­ dat: hodnocenÃ­, zda odpovÄ›Ä vychÃ¡zÃ­ ze poskytnutÃ½ch dokumentÅ¯

- Relevance: hodnocenÃ­, zda odpovÄ›Ä odpovÃ­dÃ¡ a souvisÃ­ s poloÅ¾enou otÃ¡zkou

- Plynulost â€“ zda odpovÄ›Ä dÃ¡vÃ¡ gramatickÃ½ smysl

## PÅ™Ã­padovÃ© pouÅ¾itÃ­ RAG (Retrieval Augmented Generation) a vektorovÃ½ch databÃ¡zÃ­

Existuje mnoho rÅ¯znÃ½ch pÅ™Ã­padÅ¯ pouÅ¾itÃ­, kde mohou funkÄnÃ­ volÃ¡nÃ­ zlepÅ¡it vaÅ¡i aplikaci, napÅ™Ã­klad:

- OtÃ¡zky a odpovÄ›di: zakotvenÃ­ firemnÃ­ch dat do chatu, kterÃ½ mohou zamÄ›stnanci pouÅ¾Ã­t k dotazovÃ¡nÃ­.

- DoporuÄovacÃ­ systÃ©my: kde mÅ¯Å¾ete vytvoÅ™it systÃ©m, kterÃ½ pÅ™iÅ™adÃ­ nejpodobnÄ›jÅ¡Ã­ hodnoty, napÅ™. filmy, restaurace a mnoho dalÅ¡Ã­ch.

- Chatbot sluÅ¾by: lze uklÃ¡dat historii chatu a personalizovat konverzaci na zÃ¡kladÄ› uÅ¾ivatelskÃ½ch dat.

- VyhledÃ¡vÃ¡nÃ­ obrÃ¡zkÅ¯ na zÃ¡kladÄ› vektorovÃ½ch embeddingÅ¯, uÅ¾iteÄnÃ© pro rozpoznÃ¡vÃ¡nÃ­ obrÃ¡zkÅ¯ a detekci anomÃ¡liÃ­.

## ShrnutÃ­

Probrali jsme zÃ¡kladnÃ­ oblasti RAG od pÅ™idÃ¡nÃ­ naÅ¡ich dat do aplikace, uÅ¾ivatelskÃ©ho dotazu aÅ¾ po vÃ½stup. Pro zjednoduÅ¡enÃ­ tvorby RAG mÅ¯Å¾ete pouÅ¾Ã­t frameworky jako Semanti Kernel, Langchain nebo Autogen.

## ZadÃ¡nÃ­

Pro pokraÄovÃ¡nÃ­ ve studiu Retrieval Augmented Generation (RAG) mÅ¯Å¾ete vytvoÅ™it:

- Front-end pro aplikaci pomocÃ­ vÃ¡mi zvolenÃ©ho frameworku

- VyuÅ¾Ã­t framework LangChain nebo Semantic Kernel a znovu vytvoÅ™it svou aplikaci.

Gratulujeme k dokonÄenÃ­ lekce ğŸ‘.

## UÄenÃ­ zde nekonÄÃ­, pokraÄujte v cestÄ›

Po dokonÄenÃ­ tÃ©to lekce si prohlÃ©dnÄ›te naÅ¡i [sbÃ­rku Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), abyste pokraÄovali ve zvyÅ¡ovÃ¡nÃ­ svÃ½ch znalostÃ­ GenerativnÃ­ AI!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ProhlÃ¡Å¡enÃ­ o omezenÃ­ odpovÄ›dnosti**:
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ AI pÅ™ekladatelskÃ© sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). PÅ™estoÅ¾e usilujeme o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatizovanÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho mateÅ™skÃ©m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za zÃ¡vaznÃ½ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. Nejsme odpovÄ›dnÃ­ za jakÃ©koliv nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© vÃ½klady vzniklÃ© pouÅ¾itÃ­m tohoto pÅ™ekladu.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->