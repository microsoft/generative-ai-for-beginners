<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-07-09T16:19:06+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "cs"
}
-->
# Retrieval Augmented Generation (RAG) a vektorovÃ© databÃ¡ze

[![Retrieval Augmented Generation (RAG) a vektorovÃ© databÃ¡ze](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.cs.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

V lekci o vyhledÃ¡vacÃ­ch aplikacÃ­ch jsme si struÄnÄ› ukÃ¡zali, jak integrovat vlastnÃ­ data do velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM). V tÃ©to lekci se podrobnÄ›ji podÃ­vÃ¡me na koncept zakotvenÃ­ vaÅ¡ich dat v aplikaci LLM, na mechaniku tohoto procesu a metody uklÃ¡dÃ¡nÃ­ dat, vÄetnÄ› embeddingÅ¯ i textu.

> **Video brzy k dispozici**

## Ãšvod

V tÃ©to lekci se budeme vÄ›novat nÃ¡sledujÃ­cÃ­m tÃ©matÅ¯m:

- Ãšvod do RAG, co to je a proÄ se pouÅ¾Ã­vÃ¡ v AI (umÄ›lÃ© inteligenci).

- PochopenÃ­, co jsou vektorovÃ© databÃ¡ze a vytvoÅ™enÃ­ jednÃ© pro naÅ¡i aplikaci.

- PraktickÃ½ pÅ™Ã­klad, jak integrovat RAG do aplikace.

## CÃ­le uÄenÃ­

Po dokonÄenÃ­ tÃ©to lekce budete schopni:

- VysvÄ›tlit vÃ½znam RAG pÅ™i vyhledÃ¡vÃ¡nÃ­ a zpracovÃ¡nÃ­ dat.

- Nastavit RAG aplikaci a zakotvit svÃ¡ data v LLM.

- EfektivnÄ› integrovat RAG a vektorovÃ© databÃ¡ze v aplikacÃ­ch LLM.

## NÃ¡Å¡ scÃ©nÃ¡Å™: vylepÅ¡enÃ­ naÅ¡ich LLM pomocÃ­ vlastnÃ­ch dat

Pro tuto lekci chceme pÅ™idat vlastnÃ­ poznÃ¡mky do vzdÄ›lÃ¡vacÃ­ho startupu, coÅ¾ umoÅ¾nÃ­ chatbotovi zÃ­skat vÃ­ce informacÃ­ o rÅ¯znÃ½ch pÅ™edmÄ›tech. DÃ­ky poznÃ¡mkÃ¡m, kterÃ© mÃ¡me, budou studenti schopni lÃ©pe studovat a porozumÄ›t rÅ¯znÃ½m tÃ©matÅ¯m, coÅ¾ jim usnadnÃ­ pÅ™Ã­pravu na zkouÅ¡ky. Pro vytvoÅ™enÃ­ naÅ¡eho scÃ©nÃ¡Å™e pouÅ¾ijeme:

- `Azure OpenAI:` LLM, kterÃ© pouÅ¾ijeme k vytvoÅ™enÃ­ naÅ¡eho chatbota

- `Lekci AI pro zaÄÃ¡teÄnÃ­ky o neuronovÃ½ch sÃ­tÃ­ch:` tato data pouÅ¾ijeme jako zÃ¡klad pro naÅ¡e LLM

- `Azure AI Search` a `Azure Cosmos DB:` vektorovÃ¡ databÃ¡ze pro uklÃ¡dÃ¡nÃ­ dat a vytvoÅ™enÃ­ vyhledÃ¡vacÃ­ho indexu

UÅ¾ivatelÃ© budou moci vytvÃ¡Å™et cviÄnÃ© kvÃ­zy ze svÃ½ch poznÃ¡mek, opakovacÃ­ flash karty a shrnutÃ­ do pÅ™ehlednÃ½ch pÅ™ehledÅ¯. Pro zaÄÃ¡tek si pojÄme vysvÄ›tlit, co je RAG a jak funguje:

## Retrieval Augmented Generation (RAG)

Chatbot pohÃ¡nÄ›nÃ½ LLM zpracovÃ¡vÃ¡ uÅ¾ivatelskÃ© dotazy a generuje odpovÄ›di. Je navrÅ¾en tak, aby byl interaktivnÃ­ a komunikoval s uÅ¾ivateli na Å¡irokÃ© Å¡kÃ¡le tÃ©mat. Jeho odpovÄ›di jsou vÅ¡ak omezeny kontextem, kterÃ½ mÃ¡ k dispozici, a zÃ¡kladnÃ­mi trÃ©ninkovÃ½mi daty. NapÅ™Ã­klad znalostnÃ­ hranice GPT-4 je zÃ¡Å™Ã­ 2021, coÅ¾ znamenÃ¡, Å¾e neznÃ¡ udÃ¡losti, kterÃ© nastaly po tomto datu. NavÃ­c data pouÅ¾itÃ¡ k trÃ©ninku LLM neobsahujÃ­ dÅ¯vÄ›rnÃ© informace, jako jsou osobnÃ­ poznÃ¡mky nebo manuÃ¡ly produktÅ¯ firmy.

### Jak fungujÃ­ RAG (Retrieval Augmented Generation)

![obrÃ¡zek ukazujÃ­cÃ­, jak RAG funguje](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.cs.png)

PÅ™edstavte si, Å¾e chcete nasadit chatbota, kterÃ½ vytvÃ¡Å™Ã­ kvÃ­zy z vaÅ¡ich poznÃ¡mek, budete potÅ™ebovat pÅ™ipojenÃ­ k databÃ¡zi znalostÃ­. PrÃ¡vÄ› zde pÅ™ichÃ¡zÃ­ na scÃ©nu RAG. RAG funguje takto:

- **DatabÃ¡ze znalostÃ­:** PÅ™ed vyhledÃ¡vÃ¡nÃ­m je potÅ™eba dokumenty naÄÃ­st a pÅ™edzpracovat, obvykle rozdÄ›lenÃ­m velkÃ½ch dokumentÅ¯ na menÅ¡Ã­ ÄÃ¡sti, pÅ™evedenÃ­m na textovÃ© embeddingy a uloÅ¾enÃ­m do databÃ¡ze.

- **UÅ¾ivatelskÃ½ dotaz:** uÅ¾ivatel poloÅ¾Ã­ otÃ¡zku

- **VyhledÃ¡vÃ¡nÃ­:** KdyÅ¾ uÅ¾ivatel poloÅ¾Ã­ otÃ¡zku, embeddingovÃ½ model vyhledÃ¡ relevantnÃ­ informace v databÃ¡zi znalostÃ­, aby poskytl vÃ­ce kontextu, kterÃ½ bude zaÄlenÄ›n do promptu.

- **RozÅ¡Ã­Å™enÃ¡ generace:** LLM vylepÅ¡Ã­ svou odpovÄ›Ä na zÃ¡kladÄ› zÃ­skanÃ½ch dat. To umoÅ¾Åˆuje, aby odpovÄ›Ä nebyla zaloÅ¾ena pouze na pÅ™edtrÃ©novanÃ½ch datech, ale takÃ© na relevantnÃ­ch informacÃ­ch z pÅ™idanÃ©ho kontextu. ZÃ­skanÃ¡ data se pouÅ¾Ã­vajÃ­ k rozÅ¡Ã­Å™enÃ­ odpovÄ›dÃ­ LLM. LLM pak vrÃ¡tÃ­ odpovÄ›Ä na uÅ¾ivatelskou otÃ¡zku.

![obrÃ¡zek ukazujÃ­cÃ­ architekturu RAG](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.cs.png)

Architektura RAG je implementovÃ¡na pomocÃ­ transformerÅ¯, kterÃ© se sklÃ¡dajÃ­ ze dvou ÄÃ¡stÃ­: enkodÃ©ru a dekodÃ©ru. NapÅ™Ã­klad kdyÅ¾ uÅ¾ivatel poloÅ¾Ã­ otÃ¡zku, vstupnÃ­ text je â€zakÃ³dovÃ¡nâ€œ do vektorÅ¯, kterÃ© zachycujÃ­ vÃ½znam slov, a tyto vektory jsou â€dekÃ³dovÃ¡nyâ€œ do naÅ¡eho dokumentovÃ©ho indexu a generujÃ­ novÃ½ text na zÃ¡kladÄ› uÅ¾ivatelskÃ©ho dotazu. LLM pouÅ¾Ã­vÃ¡ model enkodÃ©r-dekodÃ©r k vytvoÅ™enÃ­ vÃ½stupu.

Dva pÅ™Ã­stupy k implementaci RAG podle navrÅ¾enÃ©ho ÄlÃ¡nku: [Retrieval-Augmented Generation for Knowledge intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) jsou:

- **_RAG-Sequence_** pouÅ¾Ã­vÃ¡ zÃ­skanÃ© dokumenty k predikci nejlepÅ¡Ã­ moÅ¾nÃ© odpovÄ›di na uÅ¾ivatelskÃ½ dotaz

- **RAG-Token** pouÅ¾Ã­vÃ¡ dokumenty k generovÃ¡nÃ­ dalÅ¡Ã­ho tokenu a potÃ© je znovu zÃ­skÃ¡vÃ¡ k odpovÄ›di na dotaz uÅ¾ivatele

### ProÄ pouÅ¾Ã­vat RAG?

- **Bohatost informacÃ­:** zajiÅ¡Å¥uje, Å¾e textovÃ© odpovÄ›di jsou aktuÃ¡lnÃ­ a relevantnÃ­. TÃ­m zlepÅ¡uje vÃ½kon v ÃºlohÃ¡ch specifickÃ½ch pro danou oblast dÃ­ky pÅ™Ã­stupu k internÃ­ databÃ¡zi znalostÃ­.

- SniÅ¾uje vymyÅ¡lenÃ© informace tÃ­m, Å¾e vyuÅ¾Ã­vÃ¡ **ovÄ›Å™itelnÃ¡ data** v databÃ¡zi znalostÃ­ k poskytnutÃ­ kontextu uÅ¾ivatelskÃ½m dotazÅ¯m.

- Je **nÃ¡kladovÄ› efektivnÃ­**, protoÅ¾e je levnÄ›jÅ¡Ã­ neÅ¾ doladÄ›nÃ­ LLM.

## VytvoÅ™enÃ­ databÃ¡ze znalostÃ­

NaÅ¡e aplikace je zaloÅ¾ena na naÅ¡ich osobnÃ­ch datech, tj. lekci o neuronovÃ½ch sÃ­tÃ­ch z kurikula AI pro zaÄÃ¡teÄnÃ­ky.

### VektorovÃ© databÃ¡ze

VektorovÃ¡ databÃ¡ze, na rozdÃ­l od tradiÄnÃ­ch databÃ¡zÃ­, je specializovanÃ¡ databÃ¡ze navrÅ¾enÃ¡ pro uklÃ¡dÃ¡nÃ­, sprÃ¡vu a vyhledÃ¡vÃ¡nÃ­ vektorovÃ½ch embeddingÅ¯. UklÃ¡dÃ¡ ÄÃ­selnÃ© reprezentace dokumentÅ¯. RozloÅ¾enÃ­ dat na ÄÃ­selnÃ© embeddingy usnadÅˆuje naÅ¡emu AI systÃ©mu porozumÄ›t a zpracovat data.

Embeddingy uklÃ¡dÃ¡me do vektorovÃ½ch databÃ¡zÃ­, protoÅ¾e LLM majÃ­ omezenÃ½ poÄet tokenÅ¯, kterÃ© mohou pÅ™ijmout jako vstup. JelikoÅ¾ nelze pÅ™edat celÃ© embeddingy najednou, musÃ­me je rozdÄ›lit na ÄÃ¡sti a kdyÅ¾ uÅ¾ivatel poloÅ¾Ã­ otÃ¡zku, vrÃ¡tÃ­ se embeddingy nejvÃ­ce odpovÃ­dajÃ­cÃ­ dotazu spolu s promptem. RozdÄ›lenÃ­ takÃ© sniÅ¾uje nÃ¡klady na poÄet tokenÅ¯ pÅ™edÃ¡vanÃ½ch LLM.

Mezi populÃ¡rnÃ­ vektorovÃ© databÃ¡ze patÅ™Ã­ Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant a DeepLake. Azure Cosmos DB model mÅ¯Å¾ete vytvoÅ™it pomocÃ­ Azure CLI pÅ™Ã­kazem:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od textu k embeddingÅ¯m

NeÅ¾ data uloÅ¾Ã­me, musÃ­me je pÅ™evÃ©st na vektorovÃ© embeddingy. Pokud pracujete s velkÃ½mi dokumenty nebo dlouhÃ½mi texty, mÅ¯Å¾ete je rozdÄ›lit podle oÄekÃ¡vanÃ½ch dotazÅ¯. RozdÄ›lenÃ­ lze provÃ©st na Ãºrovni vÄ›t nebo odstavcÅ¯. ProtoÅ¾e rozdÄ›lenÃ­ vychÃ¡zÃ­ z vÃ½znamu slov v okolÃ­, mÅ¯Å¾ete k ÄÃ¡sti pÅ™idat dalÅ¡Ã­ kontext, napÅ™Ã­klad nÃ¡zev dokumentu nebo text pÅ™ed Äi za ÄÃ¡stÃ­. Data mÅ¯Å¾ete rozdÄ›lit takto:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Po rozdÄ›lenÃ­ mÅ¯Å¾eme text vloÅ¾it do embeddingÅ¯ pomocÃ­ rÅ¯znÃ½ch embeddingovÃ½ch modelÅ¯. NÄ›kterÃ© modely, kterÃ© mÅ¯Å¾ete pouÅ¾Ã­t, jsou: word2vec, ada-002 od OpenAI, Azure Computer Vision a dalÅ¡Ã­. VÃ½bÄ›r modelu zÃ¡visÃ­ na jazycÃ­ch, kterÃ© pouÅ¾Ã­vÃ¡te, typu obsahu (text/obrÃ¡zky/audio), velikosti vstupu, kterÃ½ mÅ¯Å¾e model zpracovat, a dÃ©lce vÃ½stupu embeddingu.

PÅ™Ã­klad embeddingu slova â€catâ€œ pomocÃ­ modelu OpenAI `text-embedding-ada-002` je:
![embedding slova cat](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.cs.png)

## VyhledÃ¡vÃ¡nÃ­ a vektorovÃ© hledÃ¡nÃ­

KdyÅ¾ uÅ¾ivatel poloÅ¾Ã­ otÃ¡zku, retriever ji pÅ™evede na vektor pomocÃ­ query enkodÃ©ru, potÃ© prohledÃ¡ nÃ¡Å¡ dokumentovÃ½ index a hledÃ¡ relevantnÃ­ vektory v dokumentech, kterÃ© souvisejÃ­ se vstupem. Po dokonÄenÃ­ pÅ™evede vstupnÃ­ i dokumentovÃ© vektory zpÄ›t na text a pÅ™edÃ¡ je LLM.

### VyhledÃ¡vÃ¡nÃ­

VyhledÃ¡vÃ¡nÃ­ probÃ­hÃ¡, kdyÅ¾ systÃ©m rychle hledÃ¡ dokumenty v indexu, kterÃ© splÅˆujÃ­ kritÃ©ria vyhledÃ¡vÃ¡nÃ­. CÃ­lem retrieveru je zÃ­skat dokumenty, kterÃ© poskytnou kontext a zakotvÃ­ LLM ve vaÅ¡ich datech.

Existuje nÄ›kolik zpÅ¯sobÅ¯, jak vyhledÃ¡vat v databÃ¡zi, napÅ™Ã­klad:

- **VyhledÃ¡vÃ¡nÃ­ podle klÃ­ÄovÃ½ch slov** â€“ pouÅ¾Ã­vÃ¡ se pro textovÃ© vyhledÃ¡vÃ¡nÃ­

- **SÃ©mantickÃ© vyhledÃ¡vÃ¡nÃ­** â€“ vyuÅ¾Ã­vÃ¡ sÃ©mantickÃ½ vÃ½znam slov

- **VektorovÃ© vyhledÃ¡vÃ¡nÃ­** â€“ pÅ™evÃ¡dÃ­ dokumenty z textu na vektorovÃ© reprezentace pomocÃ­ embeddingovÃ½ch modelÅ¯. VyhledÃ¡vÃ¡nÃ­ probÃ­hÃ¡ dotazovÃ¡nÃ­m dokumentÅ¯, jejichÅ¾ vektorovÃ© reprezentace jsou nejblÃ­Å¾e uÅ¾ivatelskÃ© otÃ¡zce.

- **HybridnÃ­** â€“ kombinace vyhledÃ¡vÃ¡nÃ­ podle klÃ­ÄovÃ½ch slov a vektorovÃ©ho vyhledÃ¡vÃ¡nÃ­.

ProblÃ©m nastÃ¡vÃ¡, kdyÅ¾ v databÃ¡zi nenÃ­ Å¾Ã¡dnÃ¡ podobnÃ¡ odpovÄ›Ä na dotaz, systÃ©m pak vrÃ¡tÃ­ nejlepÅ¡Ã­ dostupnÃ© informace. MÅ¯Å¾ete vÅ¡ak pouÅ¾Ã­t taktiky jako nastavenÃ­ maximÃ¡lnÃ­ vzdÃ¡lenosti pro relevanci nebo pouÅ¾Ã­t hybridnÃ­ vyhledÃ¡vÃ¡nÃ­, kterÃ© kombinuje klÃ­ÄovÃ¡ slova a vektorovÃ© vyhledÃ¡vÃ¡nÃ­. V tÃ©to lekci pouÅ¾ijeme hybridnÃ­ vyhledÃ¡vÃ¡nÃ­, tedy kombinaci vektorovÃ©ho a klÃ­ÄovÃ©ho vyhledÃ¡vÃ¡nÃ­. Data uloÅ¾Ã­me do dataframe s sloupci obsahujÃ­cÃ­mi ÄÃ¡sti textu i embeddingy.

### VektorovÃ¡ podobnost

Retriever bude hledat v databÃ¡zi znalostÃ­ embeddingy, kterÃ© jsou blÃ­zko sebe, tedy nejbliÅ¾Å¡Ã­ sousedy, protoÅ¾e se jednÃ¡ o podobnÃ© texty. V naÅ¡em scÃ©nÃ¡Å™i, kdyÅ¾ uÅ¾ivatel poloÅ¾Ã­ dotaz, je nejprve pÅ™eveden na embedding a potÃ© porovnÃ¡n s podobnÃ½mi embeddingy. BÄ›Å¾nou metrikou pro mÄ›Å™enÃ­ podobnosti vektorÅ¯ je kosinovÃ¡ podobnost, kterÃ¡ vychÃ¡zÃ­ z Ãºhlu mezi dvÄ›ma vektory.

Podobnost mÅ¯Å¾eme mÄ›Å™it i jinÃ½mi zpÅ¯soby, napÅ™Ã­klad eukleidovskou vzdÃ¡lenostÃ­, coÅ¾ je pÅ™Ã­mÃ¡ vzdÃ¡lenost mezi koncovÃ½mi body vektorÅ¯, nebo skalÃ¡rnÃ­m souÄinem, kterÃ½ mÄ›Å™Ã­ souÄet souÄinÅ¯ odpovÃ­dajÃ­cÃ­ch prvkÅ¯ dvou vektorÅ¯.

### VyhledÃ¡vacÃ­ index

PÅ™ed vyhledÃ¡vÃ¡nÃ­m je potÅ™eba vytvoÅ™it vyhledÃ¡vacÃ­ index pro naÅ¡i databÃ¡zi znalostÃ­. Index uloÅ¾Ã­ naÅ¡e embeddingy a umoÅ¾nÃ­ rychle najÃ­t nejpodobnÄ›jÅ¡Ã­ ÄÃ¡sti i ve velkÃ© databÃ¡zi. Index mÅ¯Å¾eme vytvoÅ™it lokÃ¡lnÄ› pomocÃ­:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### PÅ™erovnÃ¡nÃ­ vÃ½sledkÅ¯ (re-ranking)

Po dotazu do databÃ¡ze mÅ¯Å¾e bÃ½t potÅ™eba vÃ½sledky seÅ™adit podle relevance. PÅ™erovnÃ¡vacÃ­ LLM vyuÅ¾Ã­vÃ¡ strojovÃ© uÄenÃ­ ke zlepÅ¡enÃ­ relevance vÃ½sledkÅ¯ tÃ­m, Å¾e je seÅ™adÃ­ od nejrelevantnÄ›jÅ¡Ã­ch. PomocÃ­ Azure AI Search je pÅ™erovnÃ¡nÃ­ automaticky provÃ¡dÄ›no pomocÃ­ sÃ©mantickÃ©ho pÅ™erovnavaÄe. PÅ™Ã­klad, jak funguje pÅ™erovnÃ¡nÃ­ pomocÃ­ nejbliÅ¾Å¡Ã­ch sousedÅ¯:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## VÅ¡e dohromady

PoslednÃ­m krokem je pÅ™idÃ¡nÃ­ naÅ¡eho LLM, aby bylo moÅ¾nÃ© zÃ­skat odpovÄ›di zakotvenÃ© v naÅ¡ich datech. Implementaci provedeme takto:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## HodnocenÃ­ naÅ¡Ã­ aplikace

### Metriky hodnocenÃ­

- Kvalita odpovÄ›dÃ­ â€“ zajistit, aby odpovÄ›di znÄ›ly pÅ™irozenÄ›, plynule a lidsky

- ZakotvenÃ­ dat â€“ ovÄ›Å™it, zda odpovÄ›Ä vychÃ¡zÃ­ z poskytnutÃ½ch dokumentÅ¯

- Relevance â€“ zhodnotit, zda odpovÄ›Ä odpovÃ­dÃ¡ a souvisÃ­ s poloÅ¾enou otÃ¡zkou

- Plynulost â€“ zda odpovÄ›Ä dÃ¡vÃ¡ gramatickÃ½ smysl

## PÅ™Ã­pady pouÅ¾itÃ­ RAG a vektorovÃ½ch databÃ¡zÃ­

Existuje mnoho rÅ¯znÃ½ch pÅ™Ã­padÅ¯, kde mohou funkÄnÃ­ volÃ¡nÃ­ zlepÅ¡it vaÅ¡i aplikaci, napÅ™Ã­klad:

- OtÃ¡zky a odpovÄ›di: zakotvenÃ­ firemnÃ­ch dat do chatu, kterÃ½ mohou zamÄ›stnanci pouÅ¾Ã­vat k dotazÅ¯m.

- DoporuÄovacÃ­ systÃ©my: kde mÅ¯Å¾ete vytvoÅ™it systÃ©m, kterÃ½ najde nejpodobnÄ›jÅ¡Ã­ hodnoty, napÅ™. filmy, restaurace a dalÅ¡Ã­.

- Chatbot sluÅ¾by: mÅ¯Å¾ete uklÃ¡dat historii chatu a personalizovat konverzaci na zÃ¡kladÄ› uÅ¾ivatelskÃ½ch dat.

- VyhledÃ¡vÃ¡nÃ­ obrÃ¡zkÅ¯ na zÃ¡kladÄ› vektorovÃ½ch embeddingÅ¯, uÅ¾iteÄnÃ© pÅ™i rozpoznÃ¡vÃ¡nÃ­ obrÃ¡zkÅ¯ a detekci anomÃ¡liÃ­.

## ShrnutÃ­

Probrali jsme zÃ¡kladnÃ­ oblasti RAG od pÅ™idÃ¡nÃ­ dat do aplikace, pÅ™es uÅ¾ivatelskÃ½ dotaz aÅ¾ po vÃ½stup. Pro zjednoduÅ¡enÃ­ tvorby RAG mÅ¯Å¾ete pouÅ¾Ã­t frameworky jako Semantic Kernel, Langchain nebo Autogen.

## ZadÃ¡nÃ­

Pro pokraÄovÃ¡nÃ­ ve studiu Retrieval Augmented Generation (RAG) mÅ¯Å¾ete:

- VytvoÅ™it front-end aplikace pomocÃ­ frameworku dle vaÅ¡eho vÃ½bÄ›ru

- VyuÅ¾Ã­t framework LangChain nebo Semantic Kernel a znovu vytvoÅ™it svou aplikaci.

Gratulujeme k dokonÄenÃ­ lekce ğŸ‘.

## UÄenÃ­ zde nekonÄÃ­, pokraÄujte v cestÄ›

Po dokonÄenÃ­ tÃ©to lekce si prohlÃ©dnÄ›te naÅ¡i [kolekci Generative AI Learning](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) a pokraÄujte ve zvyÅ¡ovÃ¡nÃ­ svÃ½ch znalostÃ­ v oblasti generativnÃ­ AI!

**ProhlÃ¡Å¡enÃ­ o vylouÄenÃ­ odpovÄ›dnosti**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ AI pÅ™ekladatelskÃ© sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). I kdyÅ¾ usilujeme o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatizovanÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho mateÅ™skÃ©m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. Nejsme odpovÄ›dnÃ­ za jakÃ©koliv nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© vÃ½klady vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.