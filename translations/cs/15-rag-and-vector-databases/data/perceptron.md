<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-05-20T06:42:56+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "cs"
}
-->
# Ãšvod do neuronovÃ½ch sÃ­tÃ­: Perceptron

JednÃ­m z prvnÃ­ch pokusÅ¯ o implementaci nÄ›Äeho podobnÃ©ho modernÃ­ neuronovÃ© sÃ­ti byl v roce 1957 Frank Rosenblatt z Cornell Aeronautical Laboratory. Byla to hardwarovÃ¡ implementace nazvanÃ¡ "Mark-1", navrÅ¾enÃ¡ k rozpoznÃ¡vÃ¡nÃ­ primitivnÃ­ch geometrickÃ½ch tvarÅ¯, jako jsou trojÃºhelnÃ­ky, Ätverce a kruhy.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> ObrÃ¡zky z Wikipedie

VstupnÃ­ obraz byl reprezentovÃ¡n maticÃ­ fotobunÄ›k o rozmÄ›ru 20x20, takÅ¾e neuronovÃ¡ sÃ­Å¥ mÄ›la 400 vstupÅ¯ a jeden binÃ¡rnÃ­ vÃ½stup. JednoduchÃ¡ sÃ­Å¥ obsahovala jeden neuron, takÃ© nazÃ½vanÃ½ **jednotka prahovÃ© logiky**. VÃ¡hy neuronovÃ© sÃ­tÄ› fungovaly jako potenciometry, kterÃ© vyÅ¾adovaly ruÄnÃ­ nastavenÃ­ bÄ›hem trÃ©ninkovÃ© fÃ¡ze.

> âœ… Potenciometr je zaÅ™Ã­zenÃ­, kterÃ© umoÅ¾Åˆuje uÅ¾ivateli nastavit odpor v obvodu.

> The New York Times tehdy o perceptronu napsal: *embryo elektronickÃ©ho poÄÃ­taÄe, kterÃ½ [nÃ¡moÅ™nictvo] oÄekÃ¡vÃ¡, Å¾e bude schopen chodit, mluvit, vidÄ›t, psÃ¡t, reprodukovat se a bÃ½t si vÄ›dom svÃ© existence.*

## Model perceptronu

PÅ™edpoklÃ¡dejme, Å¾e mÃ¡me v naÅ¡em modelu N znakÅ¯, v takovÃ©m pÅ™Ã­padÄ› by vstupnÃ­ vektor byl vektor o velikosti N. Perceptron je model pro **binÃ¡rnÃ­ klasifikaci**, tj. dokÃ¡Å¾e rozliÅ¡ovat mezi dvÄ›ma tÅ™Ã­dami vstupnÃ­ch dat. PÅ™edpoklÃ¡dÃ¡me, Å¾e pro kaÅ¾dÃ½ vstupnÃ­ vektor x by vÃ½stup naÅ¡eho perceptronu byl buÄ +1, nebo -1, v zÃ¡vislosti na tÅ™Ã­dÄ›. VÃ½stup bude vypoÄÃ­tÃ¡n pomocÃ­ vzorce:

y(x) = f(w<sup>T</sup>x)

kde f je krokovÃ¡ aktivaÄnÃ­ funkce

## TrÃ©novÃ¡nÃ­ perceptronu

Abychom perceptron natrÃ©novali, potÅ™ebujeme najÃ­t vektor vah w, kterÃ½ sprÃ¡vnÄ› klasifikuje vÄ›tÅ¡inu hodnot, tj. vede k nejmenÅ¡Ã­ **chybÄ›**. Tato chyba je definovÃ¡na **perceptronovÃ½m kritÃ©riem** nÃ¡sledujÃ­cÃ­m zpÅ¯sobem:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

kde:

* souÄet se bere pÅ™es ty trÃ©ninkovÃ© datovÃ© body i, kterÃ© vedou k nesprÃ¡vnÃ© klasifikaci
* x<sub>i</sub> je vstupnÃ­ data a t<sub>i</sub> je buÄ -1 nebo +1 pro negativnÃ­ a pozitivnÃ­ pÅ™Ã­klady.

Toto kritÃ©rium je povaÅ¾ovÃ¡no za funkci vah w, a my jej potÅ™ebujeme minimalizovat. ÄŒasto se pouÅ¾Ã­vÃ¡ metoda nazÃ½vanÃ¡ **gradientnÃ­ sestup**, pÅ™i kterÃ© zaÄÃ­nÃ¡me s nÄ›jakÃ½mi poÄÃ¡teÄnÃ­mi vÃ¡hami w<sup>(0)</sup> a potÃ© v kaÅ¾dÃ©m kroku aktualizujeme vÃ¡hy podle vzorce:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

Zde Î· je tzv. **rychlost uÄenÃ­** a âˆ‡E(w) oznaÄuje **gradient** E. Po vÃ½poÄtu gradientu skonÄÃ­me s

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Algoritmus v Pythonu vypadÃ¡ takto:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## ZÃ¡vÄ›r

V tÃ©to lekci jste se nauÄili o perceptronu, coÅ¾ je model pro binÃ¡rnÃ­ klasifikaci, a jak ho trÃ©novat pomocÃ­ vektoru vah.

## ğŸš€ VÃ½zva

Pokud byste si chtÄ›li zkusit vytvoÅ™it vlastnÃ­ perceptron, zkuste tento lab na Microsoft Learn, kterÃ½ vyuÅ¾Ã­vÃ¡ Azure ML designer.

## PÅ™ehled a samostudium

Abyste vidÄ›li, jak mÅ¯Å¾eme pouÅ¾Ã­t perceptron k Å™eÅ¡enÃ­ hraÄkovÃ©ho problÃ©mu i reÃ¡lnÃ½ch problÃ©mÅ¯, a pokraÄovali v uÄenÃ­, podÃ­vejte se na notebook o perceptronu.

Zde je takÃ© zajÃ­mavÃ½ ÄlÃ¡nek o perceptronech.

## ZadÃ¡nÃ­

V tÃ©to lekci jsme implementovali perceptron pro Ãºlohu binÃ¡rnÃ­ klasifikace a pouÅ¾ili jsme ho k rozliÅ¡enÃ­ mezi dvÄ›ma ruÄnÄ› psanÃ½mi ÄÃ­slicemi. V tomto labu jste poÅ¾Ã¡dÃ¡ni, abyste zcela vyÅ™eÅ¡ili problÃ©m klasifikace ÄÃ­slic, tj. urÄete, kterÃ¡ ÄÃ­slice nejpravdÄ›podobnÄ›ji odpovÃ­dÃ¡ danÃ©mu obrÃ¡zku.

* Pokyny
* Notebook

**UpozornÄ›nÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatizovanÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho rodnÃ©m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro kritickÃ© informace je doporuÄen profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. Nejsme odpovÄ›dnÃ­ za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.