<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:36:28+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "cs"
}
-->
# Frameworky neuronovÃ½ch sÃ­tÃ­

Jak jsme se jiÅ¾ nauÄili, abychom mohli efektivnÄ› trÃ©novat neuronovÃ© sÃ­tÄ›, musÃ­me udÄ›lat dvÄ› vÄ›ci:

* Pracovat s tensory, napÅ™. nÃ¡sobit, sÄÃ­tat a poÄÃ­tat nÄ›kterÃ© funkce jako sigmoid nebo softmax
* VypoÄÃ­tat gradienty vÅ¡ech vÃ½razÅ¯, abychom mohli provÃ©st optimalizaci pomocÃ­ gradientnÃ­ho sestupu

ZatÃ­mco knihovna `numpy` zvlÃ¡dne prvnÃ­ ÄÃ¡st, potÅ™ebujeme nÄ›jakÃ½ mechanismus pro vÃ½poÄet gradientÅ¯. V naÅ¡em frameworku, kterÃ½ jsme vyvinuli v pÅ™edchozÃ­ ÄÃ¡sti, jsme museli ruÄnÄ› naprogramovat vÅ¡echny derivace uvnitÅ™ metody `backward`, kterÃ¡ provÃ¡dÃ­ zpÄ›tnou propagaci. IdeÃ¡lnÄ› by framework mÄ›l umoÅ¾nit vÃ½poÄet gradientÅ¯ *libovolnÃ©ho vÃ½razu*, kterÃ½ definujeme.

DalÅ¡Ã­ dÅ¯leÅ¾itou vÄ›cÃ­ je moÅ¾nost provÃ¡dÄ›t vÃ½poÄty na GPU nebo jinÃ½ch specializovanÃ½ch vÃ½poÄetnÃ­ch jednotkÃ¡ch, jako je TPU. TrÃ©novÃ¡nÃ­ hlubokÃ½ch neuronovÃ½ch sÃ­tÃ­ vyÅ¾aduje *mnoho* vÃ½poÄtÅ¯, a proto je velmi dÅ¯leÅ¾itÃ© tyto vÃ½poÄty paralelizovat na GPU.

> âœ… TermÃ­n 'paralelizovat' znamenÃ¡ rozdÄ›lit vÃ½poÄty mezi vÃ­ce zaÅ™Ã­zenÃ­.

V souÄasnosti jsou dva nejpopulÃ¡rnÄ›jÅ¡Ã­ frameworky pro neuronovÃ© sÃ­tÄ›: TensorFlow a PyTorch. Oba poskytujÃ­ nÃ­zkoÃºrovÅˆovÃ© API pro prÃ¡ci s tensory jak na CPU, tak na GPU. Nad tÃ­mto nÃ­zkoÃºrovÅˆovÃ½m API existuje takÃ© vysokoÃºrovÅˆovÃ© API, nazÃ½vanÃ© Keras a PyTorch Lightning.

NÃ­zkourovÅˆovÃ© API | TensorFlow | PyTorch
-----------------|------------|---------
VysokoÃºrovÅˆovÃ© API | Keras | PyTorch Lightning

**NÃ­zkourovÅˆovÃ¡ API** v obou frameworcÃ­ch umoÅ¾ÅˆujÃ­ vytvÃ¡Å™et tzv. **vÃ½poÄetnÃ­ grafy**. Tento graf definuje, jak spoÄÃ­tat vÃ½stup (obvykle ztrÃ¡tovou funkci) pro danÃ© vstupnÃ­ parametry, a mÅ¯Å¾e bÃ½t spuÅ¡tÄ›n na GPU, pokud je k dispozici. ExistujÃ­ funkce pro diferenciaci tohoto vÃ½poÄetnÃ­ho grafu a vÃ½poÄet gradientÅ¯, kterÃ© lze nÃ¡slednÄ› pouÅ¾Ã­t k optimalizaci parametrÅ¯ modelu.

**VysokoÃºrovÅˆovÃ¡ API** povaÅ¾ujÃ­ neuronovÃ© sÃ­tÄ› za **sekvenci vrstev** a vÃ½raznÄ› usnadÅˆujÃ­ konstrukci vÄ›tÅ¡iny neuronovÃ½ch sÃ­tÃ­. TrÃ©novÃ¡nÃ­ modelu obvykle vyÅ¾aduje pÅ™Ã­pravu dat a nÃ¡slednÃ© zavolÃ¡nÃ­ funkce `fit`, kterÃ¡ celÃ½ proces provede.

VysokoÃºrovÅˆovÃ© API umoÅ¾Åˆuje rychle sestavit typickÃ© neuronovÃ© sÃ­tÄ›, aniÅ¾ byste se museli zabÃ½vat mnoha detaily. Na druhou stranu nÃ­zkoÃºrovÅˆovÃ© API nabÃ­zÃ­ mnohem vÄ›tÅ¡Ã­ kontrolu nad trÃ©novacÃ­m procesem, a proto se Äasto pouÅ¾Ã­vÃ¡ ve vÃ½zkumu, kdy pracujete s novÃ½mi architekturami neuronovÃ½ch sÃ­tÃ­.

Je takÃ© dÅ¯leÅ¾itÃ© pochopit, Å¾e mÅ¯Å¾ete pouÅ¾Ã­vat obÄ› API spoleÄnÄ›, napÅ™. mÅ¯Å¾ete vyvinout vlastnÃ­ architekturu vrstvy pomocÃ­ nÃ­zkoÃºrovÅˆovÃ©ho API a pak ji pouÅ¾Ã­t v rÃ¡mci vÄ›tÅ¡Ã­ sÃ­tÄ› vytvoÅ™enÃ© a trÃ©novanÃ© pomocÃ­ vysokoÃºrovÅˆovÃ©ho API. Nebo mÅ¯Å¾ete definovat sÃ­Å¥ pomocÃ­ vysokoÃºrovÅˆovÃ©ho API jako sekvenci vrstev a pak pouÅ¾Ã­t vlastnÃ­ nÃ­zkoÃºrovÅˆovou trÃ©novacÃ­ smyÄku pro optimalizaci. ObÄ› API pouÅ¾Ã­vajÃ­ stejnÃ© zÃ¡kladnÃ­ koncepty a jsou navrÅ¾ena tak, aby spolu dobÅ™e fungovala.

## UÄenÃ­

V tomto kurzu nabÃ­zÃ­me vÄ›tÅ¡inu obsahu jak pro PyTorch, tak pro TensorFlow. MÅ¯Å¾ete si vybrat preferovanÃ½ framework a projÃ­t si pouze odpovÃ­dajÃ­cÃ­ notebooky. Pokud si nejste jisti, kterÃ½ framework zvolit, pÅ™eÄtÄ›te si na internetu diskuse o **PyTorch vs. TensorFlow**. MÅ¯Å¾ete takÃ© vyzkouÅ¡et oba frameworky, abyste lÃ©pe porozumÄ›li.

Kde je to moÅ¾nÃ©, pouÅ¾ijeme pro jednoduchost vysokoÃºrovÅˆovÃ¡ API. NicmÃ©nÄ› vÄ›Å™Ã­me, Å¾e je dÅ¯leÅ¾itÃ© pochopit, jak neuronovÃ© sÃ­tÄ› fungujÃ­ od zÃ¡kladÅ¯, proto na zaÄÃ¡tku pracujeme s nÃ­zkoÃºrovÅˆovÃ½m API a tensory. Pokud ale chcete rychle zaÄÃ­t a nechcete trÃ¡vit Äas uÄenÃ­m tÄ›chto detailÅ¯, mÅ¯Å¾ete je pÅ™eskoÄit a rovnou pÅ™ejÃ­t k notebookÅ¯m s vysokoÃºrovÅˆovÃ½m API.

## âœï¸ CviÄenÃ­: Frameworky

PokraÄujte ve studiu v nÃ¡sledujÃ­cÃ­ch noteboocÃ­ch:

NÃ­zkourovÅˆovÃ© API | TensorFlow+Keras Notebook | PyTorch
-----------------|-----------------------------|---------
VysokoÃºrovÅˆovÃ© API | Keras | *PyTorch Lightning*

Po zvlÃ¡dnutÃ­ frameworkÅ¯ si shrneme pojem pÅ™euÄenÃ­.

# PÅ™euÄenÃ­ (Overfitting)

PÅ™euÄenÃ­ je extrÃ©mnÄ› dÅ¯leÅ¾itÃ½ pojem v oblasti strojovÃ©ho uÄenÃ­ a je velmi dÅ¯leÅ¾itÃ© mu sprÃ¡vnÄ› porozumÄ›t!

ZvaÅ¾me nÃ¡sledujÃ­cÃ­ problÃ©m aproximace 5 bodÅ¯ (na grafech nÃ­Å¾e oznaÄenÃ½ch `x`):

!linear | overfit
-------------------------|--------------------------
**LineÃ¡rnÃ­ model, 2 parametry** | **NelineÃ¡rnÃ­ model, 7 parametrÅ¯**
TrÃ©novacÃ­ chyba = 5.3 | TrÃ©novacÃ­ chyba = 0
ValidacnÃ­ chyba = 5.1 | ValidacnÃ­ chyba = 20

* Vlevo vidÃ­me dobrou pÅ™Ã­mkovou aproximaci. ProtoÅ¾e poÄet parametrÅ¯ je adekvÃ¡tnÃ­, model sprÃ¡vnÄ› zachycuje rozloÅ¾enÃ­ bodÅ¯.
* Vpravo je model pÅ™Ã­liÅ¡ sloÅ¾itÃ½. ProtoÅ¾e mÃ¡me jen 5 bodÅ¯ a model mÃ¡ 7 parametrÅ¯, mÅ¯Å¾e se pÅ™izpÅ¯sobit tak, Å¾e projde vÅ¡emi body, coÅ¾ vede k nulovÃ© trÃ©novacÃ­ chybÄ›. To vÅ¡ak brÃ¡nÃ­ modelu pochopit sprÃ¡vnÃ½ vzor za daty, a proto je validaÄnÃ­ chyba velmi vysokÃ¡.

Je velmi dÅ¯leÅ¾itÃ© najÃ­t sprÃ¡vnou rovnovÃ¡hu mezi sloÅ¾itostÃ­ modelu (poÄtem parametrÅ¯) a poÄtem trÃ©novacÃ­ch vzorkÅ¯.

## ProÄ dochÃ¡zÃ­ k pÅ™euÄenÃ­

  * Nedostatek trÃ©novacÃ­ch dat
  * PÅ™Ã­liÅ¡ sloÅ¾itÃ½ model
  * PÅ™Ã­liÅ¡ mnoho Å¡umu ve vstupnÃ­ch datech

## Jak odhalit pÅ™euÄenÃ­

Jak vidÃ­te z grafu vÃ½Å¡e, pÅ™euÄenÃ­ lze odhalit podle velmi nÃ­zkÃ© trÃ©novacÃ­ chyby a vysokÃ© validaÄnÃ­ chyby. BÄ›hem trÃ©ninku obvykle vidÃ­me, Å¾e jak trÃ©novacÃ­, tak validaÄnÃ­ chyba klesajÃ­, ale v urÄitÃ©m bodÄ› mÅ¯Å¾e validaÄnÃ­ chyba pÅ™estat klesat a zaÄÃ­t rÅ¯st. To je znÃ¡mka pÅ™euÄenÃ­ a signÃ¡l, Å¾e bychom mÄ›li pravdÄ›podobnÄ› trÃ©nink zastavit (nebo alespoÅˆ uloÅ¾it momentÃ¡lnÃ­ stav modelu).

overfitting

## Jak pÅ™euÄenÃ­ zabrÃ¡nit

Pokud zjistÃ­te, Å¾e dochÃ¡zÃ­ k pÅ™euÄenÃ­, mÅ¯Å¾ete udÄ›lat jednu z nÃ¡sledujÃ­cÃ­ch vÄ›cÃ­:

 * ZvÃ½Å¡it mnoÅ¾stvÃ­ trÃ©novacÃ­ch dat
 * SnÃ­Å¾it sloÅ¾itost modelu
 * PouÅ¾Ã­t nÄ›jakou regularizaÄnÃ­ techniku, napÅ™Ã­klad Dropout, kterou si pozdÄ›ji ukÃ¡Å¾eme.

## PÅ™euÄenÃ­ a kompromis mezi biasem a variancÃ­

PÅ™euÄenÃ­ je ve skuteÄnosti pÅ™Ã­pad obecnÄ›jÅ¡Ã­ho problÃ©mu ve statistice nazÃ½vanÃ©ho kompromis mezi biasem a variancÃ­. Pokud zvÃ¡Å¾Ã­me moÅ¾nÃ© zdroje chyb v naÅ¡em modelu, mÅ¯Å¾eme rozliÅ¡it dva typy chyb:

* **Bias (systÃ©movÃ¡ chyba)** je zpÅ¯sobena tÃ­m, Å¾e nÃ¡Å¡ algoritmus nedokÃ¡Å¾e sprÃ¡vnÄ› zachytit vztah mezi trÃ©novacÃ­mi daty. MÅ¯Å¾e to bÃ½t zpÅ¯sobeno tÃ­m, Å¾e model nenÃ­ dostateÄnÄ› vÃ½konnÃ½ (**podtrÃ©novÃ¡nÃ­**).
* **Variance (rozptyl)** je zpÅ¯sobena tÃ­m, Å¾e model aproximuje Å¡um ve vstupnÃ­ch datech mÃ­sto smysluplnÃ©ho vztahu (**pÅ™etrÃ©novÃ¡nÃ­**).

BÄ›hem trÃ©ninku bias klesÃ¡ (model se uÄÃ­ data aproximovat) a variance roste. Je dÅ¯leÅ¾itÃ© trÃ©nink zastavit â€“ buÄ ruÄnÄ› (kdyÅ¾ odhalÃ­me pÅ™euÄenÃ­), nebo automaticky (zavedenÃ­m regularizace) â€“ aby se pÅ™euÄenÃ­ zabrÃ¡nilo.

## ZÃ¡vÄ›r

V tÃ©to lekci jste se dozvÄ›dÄ›li o rozdÃ­lech mezi rÅ¯znÃ½mi API u dvou nejpopulÃ¡rnÄ›jÅ¡Ã­ch AI frameworkÅ¯, TensorFlow a PyTorch. NavÃ­c jste se seznÃ¡mili s velmi dÅ¯leÅ¾itÃ½m tÃ©matem, pÅ™euÄenÃ­m.

## ğŸš€ VÃ½zva

V pÅ™iloÅ¾enÃ½ch noteboocÃ­ch najdete na konci â€Ãºkolyâ€œ; projdÄ›te si notebooky a Ãºkoly splÅˆte.

## PÅ™ehled a samostudium

ProveÄte si vlastnÃ­ prÅ¯zkum na nÃ¡sledujÃ­cÃ­ tÃ©mata:

- TensorFlow
- PyTorch
- PÅ™euÄenÃ­ (Overfitting)

Zeptejte se sami sebe:

- JakÃ½ je rozdÃ­l mezi TensorFlow a PyTorch?
- JakÃ½ je rozdÃ­l mezi pÅ™euÄenÃ­m a podtrÃ©novÃ¡nÃ­m?

## ZadÃ¡nÃ­

V tomto laboratornÃ­m cviÄenÃ­ mÃ¡te za Ãºkol vyÅ™eÅ¡it dva klasifikaÄnÃ­ problÃ©my pomocÃ­ jednovrstvÃ½ch a vÃ­cevrstvÃ½ch plnÄ› propojenÃ½ch sÃ­tÃ­ s vyuÅ¾itÃ­m PyTorch nebo TensorFlow.

**ProhlÃ¡Å¡enÃ­ o vylouÄenÃ­ odpovÄ›dnosti**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ AI pÅ™ekladatelskÃ© sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). I kdyÅ¾ usilujeme o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho mateÅ™skÃ©m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. Nejsme odpovÄ›dnÃ­ za jakÃ©koliv nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© vÃ½klady vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.