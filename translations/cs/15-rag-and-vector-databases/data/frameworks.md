<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T02:06:17+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "cs"
}
-->
# RÃ¡mce pro neuronovÃ© sÃ­tÄ›

Jak jsme se jiÅ¾ nauÄili, abychom mohli efektivnÄ› trÃ©novat neuronovÃ© sÃ­tÄ›, musÃ­me udÄ›lat dvÄ› vÄ›ci:

* Pracovat s tensory, napÅ™. nÃ¡sobit, sÄÃ­tat a poÄÃ­tat nÄ›kterÃ© funkce jako sigmoid nebo softmax
* PoÄÃ­tat gradienty vÅ¡ech vÃ½razÅ¯, abychom mohli provÃ¡dÄ›t optimalizaci metodou gradientnÃ­ho sestupu

ZatÃ­mco knihovna `numpy` zvlÃ¡dne prvnÃ­ ÄÃ¡st, potÅ™ebujeme nÄ›jakÃ½ mechanismus pro vÃ½poÄet gradientÅ¯. V naÅ¡em rÃ¡mci, kterÃ½ jsme vyvinuli v pÅ™edchozÃ­ sekci, jsme museli ruÄnÄ› programovat vÅ¡echny derivÃ¡tovÃ© funkce uvnitÅ™ metody `backward`, kterÃ¡ provÃ¡dÃ­ zpÄ›tnou propagaci. IdeÃ¡lnÄ› by nÃ¡m rÃ¡mec mÄ›l umoÅ¾nit poÄÃ­tat gradienty *jakÃ©hokoli vÃ½razu*, kterÃ½ mÅ¯Å¾eme definovat.

DalÅ¡Ã­ dÅ¯leÅ¾itou vÄ›cÃ­ je bÃ½t schopen provÃ¡dÄ›t vÃ½poÄty na GPU nebo jinÃ½ch specializovanÃ½ch vÃ½poÄetnÃ­ch jednotkÃ¡ch, jako je TPU. TrÃ©novÃ¡nÃ­ hlubokÃ½ch neuronovÃ½ch sÃ­tÃ­ vyÅ¾aduje *hodnÄ›* vÃ½poÄtÅ¯ a je velmi dÅ¯leÅ¾itÃ© tyto vÃ½poÄty paralelizovat na GPU.

> âœ… TermÃ­n 'paralelizovat' znamenÃ¡ rozdÄ›lit vÃ½poÄty mezi vÃ­ce zaÅ™Ã­zenÃ­.

V souÄasnosti jsou dva nejpopulÃ¡rnÄ›jÅ¡Ã­ neuronovÃ© rÃ¡mce: TensorFlow a PyTorch. Oba poskytujÃ­ nÃ­zkoÃºrovÅˆovÃ© API pro prÃ¡ci s tensory jak na CPU, tak na GPU. Nad nÃ­zkoÃºrovÅˆovÃ½m API existuje takÃ© vyÅ¡Å¡Ã­ ÃºroveÅˆ API, nazÃ½vanÃ¡ Keras a PyTorch Lightning.

NÃ­zkoÃºrovÅˆovÃ© API | TensorFlow| PyTorch
-----------------|-------------------------------------|--------------------------------
VysokoÃºrovÅˆovÃ© API| Keras| Pytorch

**NÃ­zkoÃºrovÅˆovÃ© API** v obou rÃ¡mcÃ­ch vÃ¡m umoÅ¾Åˆuje stavÄ›t tzv. **vÃ½poÄetnÃ­ grafy**. Tento graf definuje, jak vypoÄÃ­tat vÃ½stup (obvykle ztrÃ¡tovou funkci) s danÃ½mi vstupnÃ­mi parametry a mÅ¯Å¾e bÃ½t odeslÃ¡n k vÃ½poÄtu na GPU, pokud je dostupnÃ½. ExistujÃ­ funkce pro diferenciaci tohoto vÃ½poÄetnÃ­ho grafu a vÃ½poÄet gradientÅ¯, kterÃ© pak mohou bÃ½t pouÅ¾ity pro optimalizaci parametrÅ¯ modelu.

**VysokoÃºrovÅˆovÃ© API** v podstatÄ› povaÅ¾ujÃ­ neuronovÃ© sÃ­tÄ› za **sekvenci vrstev** a usnadÅˆujÃ­ konstrukci vÄ›tÅ¡iny neuronovÃ½ch sÃ­tÃ­. TrÃ©novÃ¡nÃ­ modelu obvykle vyÅ¾aduje pÅ™Ã­pravu dat a nÃ¡slednÃ© volÃ¡nÃ­ funkce `fit`, aby se prÃ¡ce vykonala.

VysokoÃºrovÅˆovÃ© API vÃ¡m umoÅ¾Åˆuje velmi rychle sestavit typickÃ© neuronovÃ© sÃ­tÄ›, aniÅ¾ byste se museli starat o mnoho detailÅ¯. ZÃ¡roveÅˆ nÃ­zkoÃºrovÅˆovÃ© API nabÃ­zÃ­ mnohem vÄ›tÅ¡Ã­ kontrolu nad trÃ©novacÃ­m procesem, a proto se hodnÄ› pouÅ¾Ã­vajÃ­ ve vÃ½zkumu, kdyÅ¾ se zabÃ½vÃ¡te novÃ½mi architekturami neuronovÃ½ch sÃ­tÃ­.

Je takÃ© dÅ¯leÅ¾itÃ© pochopit, Å¾e mÅ¯Å¾ete pouÅ¾Ã­t obÄ› API spoleÄnÄ›, napÅ™. mÅ¯Å¾ete vyvinout vlastnÃ­ architekturu vrstvy sÃ­tÄ› pomocÃ­ nÃ­zkoÃºrovÅˆovÃ©ho API a potÃ© ji pouÅ¾Ã­t v rÃ¡mci vÄ›tÅ¡Ã­ sÃ­tÄ› sestavenÃ© a trÃ©novanÃ© pomocÃ­ vysokoÃºrovÅˆovÃ©ho API. Nebo mÅ¯Å¾ete definovat sÃ­Å¥ pomocÃ­ vysokoÃºrovÅˆovÃ©ho API jako sekvenci vrstev a potÃ© pouÅ¾Ã­t vlastnÃ­ nÃ­zkoÃºrovÅˆovou trÃ©novacÃ­ smyÄku k provedenÃ­ optimalizace. ObÄ› API pouÅ¾Ã­vajÃ­ stejnÃ© zÃ¡kladnÃ­ koncepty a jsou navrÅ¾eny tak, aby spolu dobÅ™e fungovaly.

## UÄenÃ­

V tomto kurzu nabÃ­zÃ­me vÄ›tÅ¡inu obsahu jak pro PyTorch, tak pro TensorFlow. MÅ¯Å¾ete si vybrat svÅ¯j preferovanÃ½ rÃ¡mec a projÃ­t pouze odpovÃ­dajÃ­cÃ­mi poznÃ¡mkovÃ½mi bloky. Pokud si nejste jisti, kterÃ½ rÃ¡mec zvolit, pÅ™eÄtÄ›te si nÄ›kterÃ© diskuse na internetu ohlednÄ› **PyTorch vs. TensorFlow**. MÅ¯Å¾ete se takÃ© podÃ­vat na oba rÃ¡mce, abyste zÃ­skali lepÅ¡Ã­ pÅ™edstavu.

Kde je to moÅ¾nÃ©, pouÅ¾ijeme vysokoÃºrovÅˆovÃ¡ API pro jednoduchost. NicmÃ©nÄ› vÄ›Å™Ã­me, Å¾e je dÅ¯leÅ¾itÃ© pochopit, jak neuronovÃ© sÃ­tÄ› fungujÃ­ od zÃ¡kladÅ¯, proto zaÄÃ­nÃ¡me pracÃ­ s nÃ­zkoÃºrovÅˆovÃ½m API a tensory. Pokud vÅ¡ak chcete zaÄÃ­t rychle a nechcete trÃ¡vit hodnÄ› Äasu uÄenÃ­m tÄ›chto detailÅ¯, mÅ¯Å¾ete je pÅ™eskoÄit a pÅ™ejÃ­t pÅ™Ã­mo k poznÃ¡mkovÃ½m blokÅ¯m vysokoÃºrovÅˆovÃ©ho API.

## âœï¸ CviÄenÃ­: RÃ¡mce

PokraÄujte ve svÃ©m uÄenÃ­ v nÃ¡sledujÃ­cÃ­ch poznÃ¡mkovÃ½ch blocÃ­ch:

NÃ­zkoÃºrovÅˆovÃ© API | TensorFlow+Keras Notebook | PyTorch
-----------------|-------------------------------------|--------------------------------
VysokoÃºrovÅˆovÃ© API| Keras | *PyTorch Lightning*

Po zvlÃ¡dnutÃ­ rÃ¡mcÅ¯ si zopakujme pojem pÅ™etrÃ©novÃ¡nÃ­.

# PÅ™etrÃ©novÃ¡nÃ­

PÅ™etrÃ©novÃ¡nÃ­ je velmi dÅ¯leÅ¾itÃ½ koncept v strojovÃ©m uÄenÃ­ a je velmi dÅ¯leÅ¾itÃ© ho pochopit sprÃ¡vnÄ›!

ZvaÅ¾te nÃ¡sledujÃ­cÃ­ problÃ©m aproximace 5 bodÅ¯ (reprezentovanÃ½ch `x` na grafech nÃ­Å¾e):

!linear | overfit
-------------------------|--------------------------
**LineÃ¡rnÃ­ model, 2 parametry** | **NelineÃ¡rnÃ­ model, 7 parametrÅ¯**
Chyba trÃ©novÃ¡nÃ­ = 5.3 | Chyba trÃ©novÃ¡nÃ­ = 0
Chyba validace = 5.1 | Chyba validace = 20

* Vlevo vidÃ­me dobrou aproximaci pÅ™Ã­mkou. ProtoÅ¾e poÄet parametrÅ¯ je adekvÃ¡tnÃ­, model sprÃ¡vnÄ› pochopÃ­ rozloÅ¾enÃ­ bodÅ¯.
* Vpravo je model pÅ™Ã­liÅ¡ vÃ½konnÃ½. ProtoÅ¾e mÃ¡me pouze 5 bodÅ¯ a model mÃ¡ 7 parametrÅ¯, mÅ¯Å¾e se nastavit tak, aby prochÃ¡zel vÅ¡emi body, coÅ¾ zpÅ¯sobÃ­, Å¾e chyba trÃ©novÃ¡nÃ­ bude 0. To vÅ¡ak brÃ¡nÃ­ modelu pochopit sprÃ¡vnÃ½ vzor v datech, takÅ¾e chyba validace je velmi vysokÃ¡.

Je velmi dÅ¯leÅ¾itÃ© najÃ­t sprÃ¡vnou rovnovÃ¡hu mezi bohatostÃ­ modelu (poÄtem parametrÅ¯) a poÄtem trÃ©novacÃ­ch vzorkÅ¯.

## ProÄ pÅ™etrÃ©novÃ¡nÃ­ nastÃ¡vÃ¡

  * Nedostatek trÃ©novacÃ­ch dat
  * PÅ™Ã­liÅ¡ vÃ½konnÃ½ model
  * PÅ™Ã­liÅ¡ mnoho Å¡umu ve vstupnÃ­ch datech

## Jak zjistit pÅ™etrÃ©novÃ¡nÃ­

Jak mÅ¯Å¾ete vidÄ›t z vÃ½Å¡e uvedenÃ©ho grafu, pÅ™etrÃ©novÃ¡nÃ­ lze zjistit velmi nÃ­zkou chybou trÃ©novÃ¡nÃ­ a vysokou chybou validace. BÄ›hem trÃ©novÃ¡nÃ­ obvykle vidÃ­me, Å¾e chyby trÃ©novÃ¡nÃ­ i validace zaÄÃ­najÃ­ klesat, a pak v urÄitÃ©m bodÄ› mÅ¯Å¾e chyba validace pÅ™estat klesat a zaÄÃ­t rÅ¯st. To bude znÃ¡mkou pÅ™etrÃ©novÃ¡nÃ­ a indikÃ¡torem, Å¾e bychom pravdÄ›podobnÄ› mÄ›li trÃ©novÃ¡nÃ­ v tomto bodÄ› zastavit (nebo alespoÅˆ udÄ›lat snÃ­mek modelu).

## Jak zabrÃ¡nit pÅ™etrÃ©novÃ¡nÃ­

Pokud vidÃ­te, Å¾e pÅ™etrÃ©novÃ¡nÃ­ nastÃ¡vÃ¡, mÅ¯Å¾ete udÄ›lat jedno z nÃ¡sledujÃ­cÃ­ch:

 * ZvÃ½Å¡it mnoÅ¾stvÃ­ trÃ©novacÃ­ch dat
 * SnÃ­Å¾it sloÅ¾itost modelu
 * PouÅ¾Ã­t nÄ›jakou regularizaÄnÃ­ techniku, jako je Dropout, kterou se budeme zabÃ½vat pozdÄ›ji.

## PÅ™etrÃ©novÃ¡nÃ­ a kompromis mezi zkreslenÃ­m a rozptylem

PÅ™etrÃ©novÃ¡nÃ­ je ve skuteÄnosti pÅ™Ã­pad obecnÄ›jÅ¡Ã­ho problÃ©mu ve statistice nazÃ½vanÃ©ho kompromis mezi zkreslenÃ­m a rozptylem. Pokud zvaÅ¾ujeme moÅ¾nÃ© zdroje chyby v naÅ¡em modelu, mÅ¯Å¾eme vidÄ›t dva typy chyb:

* **Chyby zkreslenÃ­** jsou zpÅ¯sobeny tÃ­m, Å¾e nÃ¡Å¡ algoritmus nedokÃ¡Å¾e sprÃ¡vnÄ› zachytit vztah mezi trÃ©novacÃ­mi daty. MÅ¯Å¾e to bÃ½t dÅ¯sledek toho, Å¾e nÃ¡Å¡ model nenÃ­ dostateÄnÄ› vÃ½konnÃ½ (**podtrÃ©novÃ¡nÃ­**).
* **Chyby rozptylu**, kterÃ© jsou zpÅ¯sobeny tÃ­m, Å¾e model aproximuje Å¡um ve vstupnÃ­ch datech mÃ­sto smysluplnÃ©ho vztahu (**pÅ™etrÃ©novÃ¡nÃ­**).

BÄ›hem trÃ©novÃ¡nÃ­ chyby zkreslenÃ­ klesajÃ­ (jak se nÃ¡Å¡ model uÄÃ­ aproximovat data) a chyby rozptylu rostou. Je dÅ¯leÅ¾itÃ© zastavit trÃ©novÃ¡nÃ­ - buÄ ruÄnÄ› (kdyÅ¾ zjistÃ­me pÅ™etrÃ©novÃ¡nÃ­) nebo automaticky (zavedenÃ­m regularizace) - abychom zabrÃ¡nili pÅ™etrÃ©novÃ¡nÃ­.

## ZÃ¡vÄ›r

V tÃ©to lekci jste se nauÄili o rozdÃ­lech mezi rÅ¯znÃ½mi API pro dva nejpopulÃ¡rnÄ›jÅ¡Ã­ AI rÃ¡mce, TensorFlow a PyTorch. KromÄ› toho jste se nauÄili o velmi dÅ¯leÅ¾itÃ©m tÃ©matu, pÅ™etrÃ©novÃ¡nÃ­.

## ğŸš€ VÃ½zva

V pÅ™iloÅ¾enÃ½ch poznÃ¡mkovÃ½ch blocÃ­ch najdete 'Ãºkoly' na konci; projdÄ›te si poznÃ¡mkovÃ© bloky a dokonÄete Ãºkoly.

## PÅ™ehled a samostudium

ProveÄte vÃ½zkum na nÃ¡sledujÃ­cÃ­ tÃ©mata:

- TensorFlow
- PyTorch
- PÅ™etrÃ©novÃ¡nÃ­

Zeptejte se sami sebe na nÃ¡sledujÃ­cÃ­ otÃ¡zky:

- JakÃ½ je rozdÃ­l mezi TensorFlow a PyTorch?
- JakÃ½ je rozdÃ­l mezi pÅ™etrÃ©novÃ¡nÃ­m a podtrÃ©novÃ¡nÃ­m?

## ZadÃ¡nÃ­

V tomto laboratornÃ­m cviÄenÃ­ jste poÅ¾Ã¡dÃ¡ni, abyste vyÅ™eÅ¡ili dva klasifikaÄnÃ­ problÃ©my pomocÃ­ jedno- a vÃ­cevrstvÃ½ch plnÄ› propojenÃ½ch sÃ­tÃ­ pomocÃ­ PyTorch nebo TensorFlow.

**ProhlÃ¡Å¡enÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by AI pro pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). I kdyÅ¾ se snaÅ¾Ã­me o pÅ™esnost, uvÄ›domte si, Å¾e automatizovanÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho rodnÃ©m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro kritickÃ© informace je doporuÄen profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za jakÃ©koli nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© vÃ½klady vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.