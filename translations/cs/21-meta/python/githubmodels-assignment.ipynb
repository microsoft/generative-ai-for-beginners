{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práce s modely rodiny Meta\n",
    "\n",
    "## Úvod\n",
    "\n",
    "Tato lekce se zaměří na:\n",
    "\n",
    "- Prozkoumání dvou hlavních modelů rodiny Meta – Llama 3.1 a Llama 3.2\n",
    "- Pochopení, k čemu se jednotlivé modely hodí a v jakých situacích je použít\n",
    "- Ukázka kódu, která předvede jedinečné vlastnosti každého modelu\n",
    "\n",
    "## Rodina modelů Meta\n",
    "\n",
    "V této lekci se podíváme na 2 modely z rodiny Meta, někdy označované jako „Llama Herd“ – Llama 3.1 a Llama 3.2\n",
    "\n",
    "Tyto modely existují v různých variantách a najdete je na tržišti Github Model. Další informace o využití Github Models k [prototypování s AI modely](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Varianty modelů:\n",
    "- Llama 3.1 – 70B Instruct\n",
    "- Llama 3.1 – 405B Instruct\n",
    "- Llama 3.2 – 11B Vision Instruct\n",
    "- Llama 3.2 – 90B Vision Instruct\n",
    "\n",
    "*Poznámka: Llama 3 je také dostupná na Github Models, ale v této lekci se jí věnovat nebudeme*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "S 405 miliardami parametrů spadá Llama 3.1 do kategorie open source LLM.\n",
    "\n",
    "Tento model je vylepšením předchozí verze Llama 3 a nabízí:\n",
    "\n",
    "- Větší kontextové okno – 128k tokenů oproti 8k tokenům\n",
    "- Vyšší maximální počet výstupních tokenů – 4096 oproti 2048\n",
    "- Lepší vícejazyčnou podporu – díky navýšení počtu trénovacích tokenů\n",
    "\n",
    "Díky tomu dokáže Llama 3.1 zvládat složitější případy použití při tvorbě GenAI aplikací, včetně:\n",
    "- Nativní volání funkcí – možnost volat externí nástroje a funkce mimo workflow LLM\n",
    "- Lepší výkon při RAG – díky většímu kontextovému oknu\n",
    "- Generování syntetických dat – možnost vytvářet efektivní data pro úlohy jako je doladění modelu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nativní volání funkcí\n",
    "\n",
    "Llama 3.1 byla doladěna tak, aby byla efektivnější při volání funkcí nebo nástrojů. Má také dva vestavěné nástroje, které dokáže rozpoznat jako vhodné k použití na základě uživatelského zadání. Tyto nástroje jsou:\n",
    "\n",
    "- **Brave Search** – Lze použít k získání aktuálních informací, například o počasí, pomocí webového vyhledávání\n",
    "- **Wolfram Alpha** – Lze využít pro složitější matematické výpočty, takže není nutné psát vlastní funkce.\n",
    "\n",
    "Můžete si také vytvořit vlastní nástroje, které může LLM volat.\n",
    "\n",
    "V níže uvedeném příkladu kódu:\n",
    "\n",
    "- Definujeme dostupné nástroje (brave_search, wolfram_alpha) v systémovém promptu.\n",
    "- Pošleme uživatelský dotaz, který se ptá na počasí v určitém městě.\n",
    "- LLM odpoví voláním nástroje Brave Search, které bude vypadat takto `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Poznámka: Tento příklad pouze provede volání nástroje. Pokud chcete získat výsledky, je potřeba si zdarma vytvořit účet na stránce Brave API a definovat samotnou funkci.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "I když je Llama 3.1 jazykový model, jednou z jeho omezení je multimodalita. To znamená, že neumí pracovat s různými typy vstupů, například s obrázky jako podněty a poskytovat na ně odpovědi. Právě tato schopnost je jednou z hlavních novinek Llama 3.2. Mezi další vlastnosti patří:\n",
    "\n",
    "- Multimodalita – dokáže zpracovávat jak textové, tak obrazové podněty\n",
    "- Varianty od malých po střední (11B a 90B) – umožňují flexibilní nasazení,\n",
    "- Pouze textové varianty (1B a 3B) – umožňují nasazení modelu na okrajových / mobilních zařízeních a zajišťují nízkou latenci\n",
    "\n",
    "Podpora multimodality představuje významný krok ve světě open source modelů. Následující ukázka kódu využívá jak obrázek, tak textový podnět, aby získala analýzu obrázku od Llama 3.2 90B.\n",
    "\n",
    "### Podpora multimodality s Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Učení zde nekončí, pokračujte na své cestě\n",
    "\n",
    "Po dokončení této lekce se podívejte na naši [sbírku o Generativní AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) a dále rozvíjejte své znalosti v oblasti generativní umělé inteligence!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Prohlášení**:  \nTento dokument byl přeložen pomocí AI překladatelské služby [Co-op Translator](https://github.com/Azure/co-op-translator). Přestože se snažíme o přesnost, mějte prosím na paměti, že automatizované překlady mohou obsahovat chyby nebo nepřesnosti. Za autoritativní zdroj by měl být považován původní dokument v jeho rodném jazyce. Pro kritické informace doporučujeme profesionální lidský překlad. Neodpovídáme za žádná nedorozumění nebo nesprávné výklady vzniklé v důsledku použití tohoto překladu.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:48:00+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "cs"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}