<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T21:37:33+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "cs"
}
-->
# ZabezpeÄenÃ­ vaÅ¡ich aplikacÃ­ generativnÃ­ AI

[![ZabezpeÄenÃ­ vaÅ¡ich aplikacÃ­ generativnÃ­ AI](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.cs.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Ãšvod

Tato lekce se zamÄ›Å™uje na:

- ZabezpeÄenÃ­ v kontextu AI systÃ©mÅ¯.
- BÄ›Å¾nÃ¡ rizika a hrozby pro AI systÃ©my.
- Metody a Ãºvahy pro zabezpeÄenÃ­ AI systÃ©mÅ¯.

## CÃ­le uÄenÃ­

Po dokonÄenÃ­ tÃ©to lekce budete rozumÄ›t:

- HrozbÃ¡m a rizikÅ¯m pro AI systÃ©my.
- BÄ›Å¾nÃ½m metodÃ¡m a postupÅ¯m pro zabezpeÄenÃ­ AI systÃ©mÅ¯.
- Jak implementace bezpeÄnostnÃ­ho testovÃ¡nÃ­ mÅ¯Å¾e zabrÃ¡nit neoÄekÃ¡vanÃ½m vÃ½sledkÅ¯m a ztrÃ¡tÄ› dÅ¯vÄ›ry uÅ¾ivatelÅ¯.

## Co znamenÃ¡ zabezpeÄenÃ­ v kontextu generativnÃ­ AI?

Jak technologie umÄ›lÃ© inteligence (AI) a strojovÃ©ho uÄenÃ­ (ML) stÃ¡le vÃ­ce ovlivÅˆujÃ­ naÅ¡e Å¾ivoty, je klÃ­ÄovÃ© chrÃ¡nit nejen data zÃ¡kaznÃ­kÅ¯, ale takÃ© samotnÃ© AI systÃ©my. AI/ML se stÃ¡le ÄastÄ›ji pouÅ¾Ã­vajÃ­ k podpoÅ™e rozhodovacÃ­ch procesÅ¯ s vysokou hodnotou v odvÄ›tvÃ­ch, kde Å¡patnÃ© rozhodnutÃ­ mÅ¯Å¾e mÃ­t vÃ¡Å¾nÃ© dÅ¯sledky.

Zde jsou klÃ­ÄovÃ© body, kterÃ© je tÅ™eba zvÃ¡Å¾it:

- **Dopad AI/ML**: AI/ML majÃ­ vÃ½znamnÃ½ vliv na kaÅ¾dodennÃ­ Å¾ivot, a proto je jejich ochrana nezbytnÃ¡.
- **VÃ½zvy v oblasti bezpeÄnosti**: Tento vliv AI/ML vyÅ¾aduje nÃ¡leÅ¾itou pozornost, aby bylo moÅ¾nÃ© zajistit ochranu produktÅ¯ zaloÅ¾enÃ½ch na AI pÅ™ed sofistikovanÃ½mi Ãºtoky, aÅ¥ uÅ¾ od trollÅ¯ nebo organizovanÃ½ch skupin.
- **StrategickÃ© problÃ©my**: TechnologickÃ½ prÅ¯mysl musÃ­ proaktivnÄ› Å™eÅ¡it strategickÃ© vÃ½zvy, aby zajistil dlouhodobou bezpeÄnost zÃ¡kaznÃ­kÅ¯ a ochranu dat.

KromÄ› toho modely strojovÃ©ho uÄenÃ­ vÄ›tÅ¡inou nedokÃ¡Å¾ou rozliÅ¡it mezi Å¡kodlivÃ½m vstupem a neÅ¡kodnÃ½mi anomÃ¡lnÃ­mi daty. VÃ½znamnÃ¡ ÄÃ¡st trÃ©ninkovÃ½ch dat pochÃ¡zÃ­ z nekurÃ¡torovanÃ½ch, nemoderovanÃ½ch veÅ™ejnÃ½ch datovÃ½ch sad, kterÃ© jsou otevÅ™enÃ© pÅ™Ã­spÄ›vkÅ¯m tÅ™etÃ­ch stran. ÃštoÄnÃ­ci nemusÃ­ kompromitovat datovÃ© sady, pokud do nich mohou volnÄ› pÅ™ispÃ­vat. Postupem Äasu se data s nÃ­zkou dÅ¯vÄ›ryhodnostÃ­ mohou stÃ¡t daty s vysokou dÅ¯vÄ›ryhodnostÃ­, pokud zÅ¯stane sprÃ¡vnÃ¡ struktura/formÃ¡t dat.

Proto je zÃ¡sadnÃ­ zajistit integritu a ochranu datovÃ½ch ÃºloÅ¾iÅ¡Å¥, kterÃ¡ vaÅ¡e modely pouÅ¾Ã­vajÃ­ k rozhodovÃ¡nÃ­.

## PorozumÄ›nÃ­ hrozbÃ¡m a rizikÅ¯m AI

V kontextu AI a souvisejÃ­cÃ­ch systÃ©mÅ¯ je dnes nejvÃ½znamnÄ›jÅ¡Ã­ bezpeÄnostnÃ­ hrozbou otrava dat. Otrava dat nastÃ¡vÃ¡, kdyÅ¾ nÄ›kdo ÃºmyslnÄ› zmÄ›nÃ­ informace pouÅ¾Ã­vanÃ© k trÃ©novÃ¡nÃ­ AI, coÅ¾ zpÅ¯sobÃ­, Å¾e AI zaÄne dÄ›lat chyby. To je zpÅ¯sobeno absencÃ­ standardizovanÃ½ch metod detekce a zmÃ­rnÄ›nÃ­, spolu s naÅ¡Ã­ zÃ¡vislostÃ­ na nedÅ¯vÄ›ryhodnÃ½ch nebo nekurÃ¡torovanÃ½ch veÅ™ejnÃ½ch datovÃ½ch sadÃ¡ch pro trÃ©nink. Aby byla zachovÃ¡na integrita dat a zabrÃ¡nilo se chybnÃ©mu trÃ©ninkovÃ©mu procesu, je klÃ­ÄovÃ© sledovat pÅ¯vod a rodokmen vaÅ¡ich dat. Jinak platÃ­ starÃ© pÅ™Ã­slovÃ­ â€odpad dovnitÅ™, odpad venâ€œ, coÅ¾ vede ke kompromitaci vÃ½konu modelu.

Zde jsou pÅ™Ã­klady, jak otrava dat mÅ¯Å¾e ovlivnit vaÅ¡e modely:

1. **PÅ™evrÃ¡cenÃ­ Å¡tÃ­tkÅ¯**: PÅ™i Ãºkolu binÃ¡rnÃ­ klasifikace ÃºtoÄnÃ­k ÃºmyslnÄ› pÅ™evrÃ¡tÃ­ Å¡tÃ­tky u malÃ© ÄÃ¡sti trÃ©ninkovÃ½ch dat. NapÅ™Ã­klad neÅ¡kodnÃ© vzorky jsou oznaÄeny jako Å¡kodlivÃ©, coÅ¾ vede k tomu, Å¾e se model nauÄÃ­ nesprÃ¡vnÃ© asociace.\
   **PÅ™Ã­klad**: SpamovÃ½ filtr chybnÄ› klasifikuje legitimnÃ­ e-maily jako spam kvÅ¯li manipulovanÃ½m Å¡tÃ­tkÅ¯m.
2. **Otrava vlastnostÃ­**: ÃštoÄnÃ­k jemnÄ› upravÃ­ vlastnosti v trÃ©ninkovÃ½ch datech, aby zavedl zkreslenÃ­ nebo zmÃ¡tl model.\
   **PÅ™Ã­klad**: PÅ™idÃ¡nÃ­ irelevantnÃ­ch klÃ­ÄovÃ½ch slov do popisÅ¯ produktÅ¯ za ÃºÄelem manipulace s doporuÄovacÃ­mi systÃ©my.
3. **Injekce dat**: VloÅ¾enÃ­ Å¡kodlivÃ½ch dat do trÃ©ninkovÃ© sady za ÃºÄelem ovlivnÄ›nÃ­ chovÃ¡nÃ­ modelu.\
   **PÅ™Ã­klad**: ZavedenÃ­ faleÅ¡nÃ½ch uÅ¾ivatelskÃ½ch recenzÃ­ k ovlivnÄ›nÃ­ vÃ½sledkÅ¯ analÃ½zy sentimentu.
4. **Ãštoky zadnÃ­mi vrÃ¡tky**: ÃštoÄnÃ­k vloÅ¾Ã­ skrytÃ½ vzor (zadnÃ­ vrÃ¡tka) do trÃ©ninkovÃ½ch dat. Model se nauÄÃ­ tento vzor rozpoznat a chovÃ¡ se Å¡kodlivÄ›, kdyÅ¾ je aktivovÃ¡n.\
   **PÅ™Ã­klad**: SystÃ©m rozpoznÃ¡vÃ¡nÃ­ obliÄeje trÃ©novanÃ½ na obrÃ¡zcÃ­ch se zadnÃ­mi vrÃ¡tky, kterÃ½ nesprÃ¡vnÄ› identifikuje konkrÃ©tnÃ­ osobu.

SpoleÄnost MITRE Corporation vytvoÅ™ila [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), znalostnÃ­ bÃ¡zi taktik a technik pouÅ¾Ã­vanÃ½ch ÃºtoÄnÃ­ky pÅ™i reÃ¡lnÃ½ch ÃºtocÃ­ch na AI systÃ©my.

> PoÄet zranitelnostÃ­ v systÃ©mech vyuÅ¾Ã­vajÃ­cÃ­ch AI roste, protoÅ¾e zaÄlenÄ›nÃ­ AI zvyÅ¡uje povrch Ãºtoku stÃ¡vajÃ­cÃ­ch systÃ©mÅ¯ nad rÃ¡mec tradiÄnÃ­ch kybernetickÃ½ch ÃºtokÅ¯. Vyvinuli jsme ATLAS, abychom zvÃ½Å¡ili povÄ›domÃ­ o tÄ›chto jedineÄnÃ½ch a vyvÃ­jejÃ­cÃ­ch se zranitelnostech, protoÅ¾e globÃ¡lnÃ­ komunita stÃ¡le vÃ­ce zaÄleÅˆuje AI do rÅ¯znÃ½ch systÃ©mÅ¯. ATLAS je modelovÃ¡n podle rÃ¡mce MITRE ATT&CKÂ® a jeho taktiky, techniky a postupy (TTPs) doplÅˆujÃ­ ty v ATT&CK.

PodobnÄ› jako rÃ¡mec MITRE ATT&CKÂ®, kterÃ½ se Å¡iroce pouÅ¾Ã­vÃ¡ v tradiÄnÃ­ kybernetickÃ© bezpeÄnosti pro plÃ¡novÃ¡nÃ­ pokroÄilÃ½ch scÃ©nÃ¡Å™Å¯ emulace hrozeb, poskytuje ATLAS snadno vyhledatelnou sadu TTPs, kterÃ© mohou pomoci lÃ©pe porozumÄ›t a pÅ™ipravit se na obranu proti novÄ› vznikajÃ­cÃ­m ÃºtokÅ¯m.

KromÄ› toho vytvoÅ™il projekt Open Web Application Security Project (OWASP) "[Top 10 seznam](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" nejkritiÄtÄ›jÅ¡Ã­ch zranitelnostÃ­ nalezenÃ½ch v aplikacÃ­ch vyuÅ¾Ã­vajÃ­cÃ­ch LLMs. Seznam zdÅ¯razÅˆuje rizika hrozeb, jako je vÃ½Å¡e zmÃ­nÄ›nÃ¡ otrava dat, spolu s dalÅ¡Ã­mi, jako napÅ™Ã­klad:

- **Injekce promptÅ¯**: technika, pÅ™i kterÃ© ÃºtoÄnÃ­ci manipulujÃ­ s velkÃ½m jazykovÃ½m modelem (LLM) prostÅ™ednictvÃ­m peÄlivÄ› vytvoÅ™enÃ½ch vstupÅ¯, coÅ¾ zpÅ¯sobÃ­, Å¾e se chovÃ¡ mimo svÅ¯j zamÃ½Å¡lenÃ½ ÃºÄel.
- **Zranitelnosti dodavatelskÃ©ho Å™etÄ›zce**: Komponenty a software, kterÃ© tvoÅ™Ã­ aplikace pouÅ¾Ã­vanÃ© LLM, jako jsou moduly Pythonu nebo externÃ­ datovÃ© sady, mohou bÃ½t samy o sobÄ› kompromitovÃ¡ny, coÅ¾ vede k neoÄekÃ¡vanÃ½m vÃ½sledkÅ¯m, zavedenÃ½m zkreslenÃ­m a dokonce k zranitelnostem v zÃ¡kladnÃ­ infrastruktuÅ™e.
- **PÅ™ehnanÃ¡ dÅ¯vÄ›ra**: LLMs jsou omylnÃ© a majÃ­ tendenci halucinovat, poskytovat nepÅ™esnÃ© nebo nebezpeÄnÃ© vÃ½sledky. V nÄ›kolika dokumentovanÃ½ch pÅ™Ã­padech lidÃ© vzali vÃ½sledky za bernou minci, coÅ¾ vedlo k nechtÄ›nÃ½m negativnÃ­m dÅ¯sledkÅ¯m v reÃ¡lnÃ©m svÄ›tÄ›.

Rod Trent, odbornÃ­k na cloudovÃ© technologie spoleÄnosti Microsoft, napsal bezplatnou elektronickou knihu [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), kterÃ¡ se podrobnÄ› zabÃ½vÃ¡ tÄ›mito a dalÅ¡Ã­mi vznikajÃ­cÃ­mi hrozbami AI a poskytuje rozsÃ¡hlÃ© pokyny, jak tyto scÃ©nÃ¡Å™e nejlÃ©pe Å™eÅ¡it.

## BezpeÄnostnÃ­ testovÃ¡nÃ­ AI systÃ©mÅ¯ a LLMs

UmÄ›lÃ¡ inteligence (AI) transformuje rÅ¯znÃ© oblasti a odvÄ›tvÃ­, nabÃ­zÃ­ novÃ© moÅ¾nosti a pÅ™Ã­nosy pro spoleÄnost. NicmÃ©nÄ› AI takÃ© pÅ™inÃ¡Å¡Ã­ vÃ½znamnÃ© vÃ½zvy a rizika, jako je ochrana dat, zkreslenÃ­, nedostatek vysvÄ›tlitelnosti a potenciÃ¡lnÃ­ zneuÅ¾itÃ­. Proto je zÃ¡sadnÃ­ zajistit, aby AI systÃ©my byly bezpeÄnÃ© a odpovÄ›dnÃ©, coÅ¾ znamenÃ¡, Å¾e dodrÅ¾ujÃ­ etickÃ© a prÃ¡vnÃ­ standardy a mohou bÃ½t dÅ¯vÄ›ryhodnÃ© pro uÅ¾ivatele a zainteresovanÃ© strany.

BezpeÄnostnÃ­ testovÃ¡nÃ­ je proces hodnocenÃ­ bezpeÄnosti AI systÃ©mu nebo LLM, pÅ™i kterÃ©m se identifikujÃ­ a vyuÅ¾Ã­vajÃ­ jejich zranitelnosti. To mÅ¯Å¾e provÃ¡dÄ›t vÃ½vojÃ¡Å™, uÅ¾ivatel nebo externÃ­ auditor, v zÃ¡vislosti na ÃºÄelu a rozsahu testovÃ¡nÃ­. NÄ›kterÃ© z nejbÄ›Å¾nÄ›jÅ¡Ã­ch metod bezpeÄnostnÃ­ho testovÃ¡nÃ­ pro AI systÃ©my a LLMs jsou:

- **Sanitace dat**: Proces odstraÅˆovÃ¡nÃ­ nebo anonymizace citlivÃ½ch nebo soukromÃ½ch informacÃ­ z trÃ©ninkovÃ½ch dat nebo vstupÅ¯ AI systÃ©mu nebo LLM. Sanitace dat mÅ¯Å¾e pomoci zabrÃ¡nit Ãºniku dat a Å¡kodlivÃ© manipulaci tÃ­m, Å¾e snÃ­Å¾Ã­ expozici dÅ¯vÄ›rnÃ½ch nebo osobnÃ­ch ÃºdajÅ¯.
- **AdversariÃ¡lnÃ­ testovÃ¡nÃ­**: Proces generovÃ¡nÃ­ a aplikace adversariÃ¡lnÃ­ch pÅ™Ã­kladÅ¯ na vstupy nebo vÃ½stupy AI systÃ©mu nebo LLM za ÃºÄelem hodnocenÃ­ jeho odolnosti vÅ¯Äi adversariÃ¡lnÃ­m ÃºtokÅ¯m. AdversariÃ¡lnÃ­ testovÃ¡nÃ­ mÅ¯Å¾e pomoci identifikovat a zmÃ­rnit zranitelnosti a slabiny AI systÃ©mu nebo LLM, kterÃ© by mohly bÃ½t zneuÅ¾ity ÃºtoÄnÃ­ky.
- **OvÄ›Å™enÃ­ modelu**: Proces ovÄ›Å™ovÃ¡nÃ­ sprÃ¡vnosti a Ãºplnosti parametrÅ¯ nebo architektury modelu AI systÃ©mu nebo LLM. OvÄ›Å™enÃ­ modelu mÅ¯Å¾e pomoci detekovat a zabrÃ¡nit krÃ¡deÅ¾i modelu tÃ­m, Å¾e zajistÃ­, Å¾e model je chrÃ¡nÄ›n a autentizovÃ¡n.
- **Validace vÃ½stupÅ¯**: Proces validace kvality a spolehlivosti vÃ½stupÅ¯ AI systÃ©mu nebo LLM. Validace vÃ½stupÅ¯ mÅ¯Å¾e pomoci detekovat a opravit Å¡kodlivou manipulaci tÃ­m, Å¾e zajistÃ­, Å¾e vÃ½stupy jsou konzistentnÃ­ a pÅ™esnÃ©.

OpenAI, lÃ­dr v oblasti AI systÃ©mÅ¯, zavedl sÃ©rii _hodnocenÃ­ bezpeÄnosti_ jako souÄÃ¡st iniciativy red teaming network, kterÃ¡ mÃ¡ za cÃ­l testovat vÃ½stupy AI systÃ©mÅ¯ a pÅ™ispÃ­vat k bezpeÄnosti AI.

> HodnocenÃ­ mÅ¯Å¾e zahrnovat jednoduchÃ© testy otÃ¡zek a odpovÄ›dÃ­ aÅ¾ po sloÅ¾itÄ›jÅ¡Ã­ simulace. Jako konkrÃ©tnÃ­ pÅ™Ã­klady zde jsou vzorovÃ¡ hodnocenÃ­ vyvinutÃ¡ OpenAI pro hodnocenÃ­ chovÃ¡nÃ­ AI z rÅ¯znÃ½ch ÃºhlÅ¯:

#### PÅ™esvÄ›dÄovÃ¡nÃ­

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m pÅ™imÄ›t jinÃ½ AI systÃ©m k tomu, aby Å™ekl tajnÃ© slovo?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m pÅ™esvÄ›dÄit jinÃ½ AI systÃ©m, aby daroval penÃ­ze?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m ovlivnit podporu politickÃ©ho nÃ¡vrhu jinÃ©ho AI systÃ©mu?

#### Steganografie (skrytÃ© zprÃ¡vy)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m pÅ™edat tajnÃ© zprÃ¡vy, aniÅ¾ by byl odhalen jinÃ½m AI systÃ©mem?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m komprimovat a dekomprimovat zprÃ¡vy, aby umoÅ¾nil skrytÃ­ tajnÃ½ch zprÃ¡v?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m koordinovat s jinÃ½m AI systÃ©mem bez pÅ™Ã­mÃ© komunikace?

### ZabezpeÄenÃ­ AI

Je nezbytnÃ© usilovat o ochranu AI systÃ©mÅ¯ pÅ™ed Å¡kodlivÃ½mi Ãºtoky, zneuÅ¾itÃ­m nebo nechtÄ›nÃ½mi dÅ¯sledky. To zahrnuje kroky k zajiÅ¡tÄ›nÃ­ bezpeÄnosti, spolehlivosti a dÅ¯vÄ›ryhodnosti AI systÃ©mÅ¯, jako napÅ™Ã­klad:

- ZabezpeÄenÃ­ dat a algoritmÅ¯ pouÅ¾Ã­vanÃ½ch k trÃ©novÃ¡nÃ­ a provozu AI modelÅ¯
- ZabrÃ¡nÄ›nÃ­ neoprÃ¡vnÄ›nÃ©mu pÅ™Ã­stupu, manipulaci nebo sabotÃ¡Å¾i AI systÃ©mÅ¯
- Detekce a zmÃ­rnÄ›nÃ­ zkreslenÃ­, diskriminace nebo etickÃ½ch problÃ©mÅ¯ v AI systÃ©mech
- ZajiÅ¡tÄ›nÃ­ odpovÄ›dnosti, transparentnosti a vysvÄ›tlitelnosti rozhodnutÃ­ a akcÃ­ AI
- SjednocenÃ­ cÃ­lÅ¯ a hodnot AI systÃ©mÅ¯ s cÃ­li a hodnotami lidÃ­ a spoleÄnosti

ZabezpeÄenÃ­ AI je dÅ¯leÅ¾itÃ© pro zajiÅ¡tÄ›nÃ­ integrity, dostupnosti a dÅ¯vÄ›rnosti AI systÃ©mÅ¯ a dat. NÄ›kterÃ© z vÃ½zev a pÅ™Ã­leÅ¾itostÃ­ v oblasti zabezpeÄenÃ­ AI jsou:

- **PÅ™Ã­leÅ¾itost**: ZaÄlenÄ›nÃ­ AI do strategiÃ­ kybernetickÃ© bezpeÄnosti, protoÅ¾e mÅ¯Å¾e hrÃ¡t klÃ­Äovou roli pÅ™i identifikaci hrozeb a zlepÅ¡ovÃ¡nÃ­ reakÄnÃ­ch ÄasÅ¯. AI mÅ¯Å¾e pomoci automatizovat a zlepÅ¡it detekci a zmÃ­rnÄ›nÃ­ kybernetickÃ½ch ÃºtokÅ¯, jako je phishing, malware nebo ransomware.
- **VÃ½zva**: AI mÅ¯Å¾e bÃ½t takÃ© vyuÅ¾Ã­vÃ¡na ÃºtoÄnÃ­ky k zahÃ¡jenÃ­ sofistikovanÃ½ch ÃºtokÅ¯, jako je generovÃ¡nÃ­ faleÅ¡nÃ©ho nebo zavÃ¡dÄ›jÃ­cÃ­ho obsahu, vydÃ¡vÃ¡nÃ­ se za uÅ¾ivatele nebo zneuÅ¾Ã­vÃ¡nÃ­ zranitelnostÃ­ v AI systÃ©mech. Proto majÃ­ vÃ½vojÃ¡Å™i AI jedineÄnou odpovÄ›dnost navrhovat systÃ©my, kterÃ© jsou robustnÃ­ a odolnÃ© vÅ¯Äi zneuÅ¾itÃ­.

### Ochrana dat

LLMs mohou pÅ™edstavovat rizika pro soukromÃ­ a bezpeÄnost dat, kterÃ¡ pouÅ¾Ã­vajÃ­. NapÅ™Ã­klad LLMs mohou potenciÃ¡lnÄ› zapamatovat a zveÅ™ejnit citlivÃ© informace ze svÃ½ch trÃ©ninkovÃ½ch dat, jako jsou osobnÃ­ jmÃ©na, adresy, hesla nebo ÄÃ­sla kreditnÃ­ch karet. Mohou bÃ½t takÃ© manipulovÃ¡ny nebo napadeny Å¡kodlivÃ½mi aktÃ©ry, kteÅ™Ã­ chtÄ›jÃ­ zneuÅ¾Ã­t jejich zranitelnosti nebo zkreslenÃ­. Proto je dÅ¯leÅ¾itÃ© bÃ½t si tÄ›chto rizik vÄ›dom a pÅ™ijmout vhodnÃ¡ opatÅ™enÃ­ k ochranÄ› dat pouÅ¾Ã­vanÃ½ch s LLMs. Existuje nÄ›kolik krokÅ¯, kterÃ© mÅ¯Å¾ete podniknout k ochranÄ› dat pouÅ¾Ã­vanÃ½ch s LLMs. Tyto kroky zahrnujÃ­:

- **OmezenÃ­ mnoÅ¾stvÃ­ a typu dat, kterÃ¡ sdÃ­lÃ­te s LLMs**: SdÃ­lejte pouze data, kterÃ¡ jsou nezbytnÃ¡ a relevantnÃ­ pro zamÃ½Å¡lenÃ© ÃºÄely, a vyhnÄ›te se sdÃ­lenÃ­ jakÃ½chkoli dat, kterÃ¡ jsou citlivÃ¡, dÅ¯vÄ›rnÃ¡ nebo osobnÃ­. UÅ¾ivatelÃ© by mÄ›li takÃ© anonymizovat nebo Å¡ifrovat data, kterÃ¡ sdÃ­lejÃ­ s LLMs, napÅ™Ã­klad odstranÄ›nÃ­m nebo maskovÃ¡nÃ­m jakÃ½chkoli identifikaÄnÃ­ch informacÃ­ nebo pouÅ¾itÃ­m bezpeÄnÃ½ch komunikaÄnÃ­ch kanÃ¡lÅ¯.
- **OvÄ›Å™enÃ­ dat, kterÃ¡ LLMs generujÃ­**: VÅ¾dy zkontrolujte pÅ™esnost a kvalitu vÃ½stupÅ¯ generovanÃ½ch LLMs, abyste zajistili, Å¾e neobsahujÃ­ Å¾Ã¡dnÃ© neÅ¾Ã¡doucÃ­ nebo nevhodnÃ© informace.
- **HlÃ¡Å¡enÃ­ a upozornÄ›nÃ­ na jakÃ©koli naruÅ¡enÃ­ dat nebo incidenty**: BuÄte ostraÅ¾itÃ­ vÅ¯Äi jakÃ½mkoli podezÅ™elÃ½m nebo abnormÃ¡lnÃ­m aktivitÃ¡m nebo chovÃ¡nÃ­ LLMs, napÅ™Ã­klad generovÃ¡nÃ­ textÅ¯, kterÃ© jsou irelevantnÃ­, nepÅ™esnÃ©, urÃ¡Å¾livÃ© nebo Å¡kodlivÃ©. To by mohlo naznaÄovat naruÅ¡enÃ­ dat nebo bezpeÄnostnÃ­ incident.

BezpeÄnost dat, sprÃ¡va a dodrÅ¾ovÃ¡nÃ­ pÅ™edpisÅ¯ jsou klÃ­ÄovÃ© pro kaÅ¾dou organizaci, kterÃ¡ chce vyuÅ¾Ã­t sÃ­lu dat a AI v prostÅ™edÃ­ s vÃ­ce cloudovÃ½mi sluÅ¾bami. ZabezpeÄenÃ­ a sprÃ¡va vÅ¡ech vaÅ¡ich dat je sloÅ¾itÃ½ a mnohostrannÃ½ Ãºkol. MusÃ­te zabezpeÄit a spravovat rÅ¯znÃ© typy dat (strukturovanÃ¡, nestrukturovanÃ¡ a data generovanÃ¡
NapodobovÃ¡nÃ­ reÃ¡lnÃ½ch hrozeb je nynÃ­ povaÅ¾ovÃ¡no za standardnÃ­ praxi pÅ™i budovÃ¡nÃ­ odolnÃ½ch AI systÃ©mÅ¯, a to prostÅ™ednictvÃ­m pouÅ¾itÃ­ podobnÃ½ch nÃ¡strojÅ¯, taktik a postupÅ¯ k identifikaci rizik pro systÃ©my a testovÃ¡nÃ­ reakce obrÃ¡ncÅ¯.

> Praxe AI red teamingu se vyvinula do Å¡irÅ¡Ã­ho vÃ½znamu: nezahrnuje pouze hledÃ¡nÃ­ bezpeÄnostnÃ­ch zranitelnostÃ­, ale takÃ© zkoumÃ¡nÃ­ dalÅ¡Ã­ch selhÃ¡nÃ­ systÃ©mu, jako je generovÃ¡nÃ­ potenciÃ¡lnÄ› Å¡kodlivÃ©ho obsahu. AI systÃ©my pÅ™inÃ¡Å¡ejÃ­ novÃ¡ rizika a red teaming je klÃ­ÄovÃ½ pro pochopenÃ­ tÄ›chto novÃ½ch rizik, jako je injekce promptÅ¯ nebo produkce neodÅ¯vodnÄ›nÃ©ho obsahu. - [Microsoft AI Red Team buduje budoucnost bezpeÄnÄ›jÅ¡Ã­ AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Pokyny a zdroje pro red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.cs.png)]()

NÃ­Å¾e jsou uvedeny klÃ­ÄovÃ© poznatky, kterÃ© formovaly program Microsoft AI Red Team.

1. **RozÅ¡Ã­Å™enÃ½ rozsah AI red teamingu:**
   AI red teaming nynÃ­ zahrnuje jak bezpeÄnostnÃ­, tak vÃ½sledky odpovÄ›dnÃ© AI (RAI). TradiÄnÄ› se red teaming zamÄ›Å™oval na bezpeÄnostnÃ­ aspekty, pÅ™iÄemÅ¾ model byl povaÅ¾ovÃ¡n za vektor (napÅ™. krÃ¡deÅ¾ zÃ¡kladnÃ­ho modelu). AI systÃ©my vÅ¡ak pÅ™inÃ¡Å¡ejÃ­ novÃ© bezpeÄnostnÃ­ zranitelnosti (napÅ™. injekce promptÅ¯, otrava daty), kterÃ© vyÅ¾adujÃ­ zvlÃ¡Å¡tnÃ­ pozornost. KromÄ› bezpeÄnosti se AI red teaming zamÄ›Å™uje takÃ© na otÃ¡zky spravedlnosti (napÅ™. stereotypizace) a Å¡kodlivÃ½ obsah (napÅ™. oslavovÃ¡nÃ­ nÃ¡silÃ­). VÄasnÃ¡ identifikace tÄ›chto problÃ©mÅ¯ umoÅ¾Åˆuje prioritizaci investic do obrany.
2. **ZlomyslnÃ¡ a neÅ¡kodnÃ¡ selhÃ¡nÃ­:**
   AI red teaming zohledÅˆuje selhÃ¡nÃ­ jak zlomyslnÃ¡, tak neÅ¡kodnÃ¡. NapÅ™Ã­klad pÅ™i red teamingu novÃ©ho Bingu zkoumÃ¡me nejen to, jak mohou zlomyslnÃ­ protivnÃ­ci systÃ©m naruÅ¡it, ale takÃ© jak bÄ›Å¾nÃ­ uÅ¾ivatelÃ© mohou narazit na problematickÃ½ nebo Å¡kodlivÃ½ obsah. Na rozdÃ­l od tradiÄnÃ­ho bezpeÄnostnÃ­ho red teamingu, kterÃ½ se zamÄ›Å™uje hlavnÄ› na zlomyslnÃ© aktÃ©ry, AI red teaming bere v Ãºvahu Å¡irÅ¡Ã­ Å¡kÃ¡lu osobnostÃ­ a potenciÃ¡lnÃ­ch selhÃ¡nÃ­.
3. **DynamickÃ¡ povaha AI systÃ©mÅ¯:**
   AI aplikace se neustÃ¡le vyvÃ­jejÃ­. U aplikacÃ­ s velkÃ½mi jazykovÃ½mi modely se vÃ½vojÃ¡Å™i pÅ™izpÅ¯sobujÃ­ mÄ›nÃ­cÃ­m se poÅ¾adavkÅ¯m. NepÅ™etrÅ¾itÃ½ red teaming zajiÅ¡Å¥uje trvalou ostraÅ¾itost a pÅ™izpÅ¯sobenÃ­ se vyvÃ­jejÃ­cÃ­m rizikÅ¯m.

AI red teaming nenÃ­ vÅ¡ezahrnujÃ­cÃ­ a mÄ›l by bÃ½t povaÅ¾ovÃ¡n za doplÅˆkovÃ½ krok k dalÅ¡Ã­m kontrolÃ¡m, jako je [role-based access control (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) a komplexnÃ­ Å™eÅ¡enÃ­ sprÃ¡vy dat. MÃ¡ doplÅˆovat bezpeÄnostnÃ­ strategii zamÄ›Å™enou na pouÅ¾Ã­vÃ¡nÃ­ bezpeÄnÃ½ch a odpovÄ›dnÃ½ch AI Å™eÅ¡enÃ­, kterÃ¡ zohledÅˆujÃ­ soukromÃ­ a bezpeÄnost, pÅ™iÄemÅ¾ se snaÅ¾Ã­ minimalizovat pÅ™edsudky, Å¡kodlivÃ½ obsah a dezinformace, kterÃ© mohou naruÅ¡it dÅ¯vÄ›ru uÅ¾ivatelÅ¯.

Zde je seznam dalÅ¡Ã­ch materiÃ¡lÅ¯, kterÃ© vÃ¡m mohou pomoci lÃ©pe pochopit, jak red teaming mÅ¯Å¾e pomoci identifikovat a zmÃ­rnit rizika ve vaÅ¡ich AI systÃ©mech:

- [PlÃ¡novÃ¡nÃ­ red teamingu pro velkÃ© jazykovÃ© modely (LLMs) a jejich aplikace](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Co je OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - KlÃ­ÄovÃ¡ praxe pro budovÃ¡nÃ­ bezpeÄnÄ›jÅ¡Ã­ch a odpovÄ›dnÄ›jÅ¡Ã­ch AI Å™eÅ¡enÃ­](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), databÃ¡ze znalostÃ­ o taktikÃ¡ch a technikÃ¡ch pouÅ¾Ã­vanÃ½ch protivnÃ­ky pÅ™i reÃ¡lnÃ½ch ÃºtocÃ­ch na AI systÃ©my.

## Kontrola znalostÃ­

JakÃ½ by mohl bÃ½t dobrÃ½ pÅ™Ã­stup k udrÅ¾enÃ­ integrity dat a prevenci jejich zneuÅ¾itÃ­?

1. MÃ­t silnÃ© role-based kontroly pro pÅ™Ã­stup k datÅ¯m a sprÃ¡vu dat
1. Implementovat a auditovat oznaÄovÃ¡nÃ­ dat, aby se zabrÃ¡nilo jejich nesprÃ¡vnÃ©mu zobrazenÃ­ nebo zneuÅ¾itÃ­
1. Zajistit, Å¾e vaÅ¡e AI infrastruktura podporuje filtrovÃ¡nÃ­ obsahu

A:1, I kdyÅ¾ vÅ¡echny tÅ™i jsou skvÄ›lÃ¡ doporuÄenÃ­, zajiÅ¡tÄ›nÃ­ sprÃ¡vnÃ©ho pÅ™idÄ›lenÃ­ pÅ™Ã­stupovÃ½ch prÃ¡v k datÅ¯m uÅ¾ivatelÅ¯m vÃ½raznÄ› pÅ™ispÄ›je k prevenci manipulace a nesprÃ¡vnÃ©ho zobrazenÃ­ dat pouÅ¾Ã­vanÃ½ch LLMs.

## ğŸš€ VÃ½zva

ZjistÄ›te vÃ­ce o tom, jak mÅ¯Å¾ete [spravovat a chrÃ¡nit citlivÃ© informace](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) v Ã©Å™e AI.

## SkvÄ›lÃ¡ prÃ¡ce, pokraÄujte ve svÃ©m vzdÄ›lÃ¡vÃ¡nÃ­

Po dokonÄenÃ­ tÃ©to lekce se podÃ­vejte na naÅ¡i [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), abyste dÃ¡le rozvÃ­jeli svÃ© znalosti o generativnÃ­ AI!

PÅ™ejdÄ›te na lekci 14, kde se podÃ­vÃ¡me na [Å¾ivotnÃ­ cyklus aplikacÃ­ generativnÃ­ AI](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**ProhlÃ¡Å¡enÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by AI pro pÅ™eklady [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatizovanÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho rodnÃ©m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.