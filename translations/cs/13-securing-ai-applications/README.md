<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:38:00+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "cs"
}
-->
# ZabezpeÄenÃ­ vaÅ¡ich generativnÃ­ch AI aplikacÃ­

[![ZabezpeÄenÃ­ vaÅ¡ich generativnÃ­ch AI aplikacÃ­](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.cs.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Ãšvod

Tato lekce pokryje:

- BezpeÄnost v kontextu AI systÃ©mÅ¯.
- BÄ›Å¾nÃ¡ rizika a hrozby pro AI systÃ©my.
- Metody a Ãºvahy pro zabezpeÄenÃ­ AI systÃ©mÅ¯.

## CÃ­le uÄenÃ­

Po dokonÄenÃ­ tÃ©to lekce budete rozumÄ›t:

- HrozbÃ¡m a rizikÅ¯m AI systÃ©mÅ¯.
- BÄ›Å¾nÃ½m metodÃ¡m a postupÅ¯m zabezpeÄenÃ­ AI systÃ©mÅ¯.
- Jak implementace bezpeÄnostnÃ­ho testovÃ¡nÃ­ mÅ¯Å¾e zabrÃ¡nit neoÄekÃ¡vanÃ½m vÃ½sledkÅ¯m a ztrÃ¡tÄ› dÅ¯vÄ›ry uÅ¾ivatelÅ¯.

## Co znamenÃ¡ bezpeÄnost v kontextu generativnÃ­ AI?

S tÃ­m, jak technologie umÄ›lÃ© inteligence (AI) a strojovÃ©ho uÄenÃ­ (ML) stÃ¡le vÃ­ce ovlivÅˆujÃ­ nÃ¡Å¡ Å¾ivot, je klÃ­ÄovÃ© chrÃ¡nit nejen data zÃ¡kaznÃ­kÅ¯, ale i samotnÃ© AI systÃ©my. AI/ML se stÃ¡le ÄastÄ›ji pouÅ¾Ã­vÃ¡ pÅ™i rozhodovÃ¡nÃ­ s vysokou hodnotou v odvÄ›tvÃ­ch, kde Å¡patnÃ© rozhodnutÃ­ mÅ¯Å¾e mÃ­t vÃ¡Å¾nÃ© nÃ¡sledky.

Zde jsou hlavnÃ­ body k zamyÅ¡lenÃ­:

- **Dopad AI/ML**: AI/ML majÃ­ vÃ½raznÃ½ vliv na kaÅ¾dodennÃ­ Å¾ivot, a proto je jejich ochrana nezbytnÃ¡.
- **VÃ½zvy v bezpeÄnosti**: Tento dopad vyÅ¾aduje nÃ¡leÅ¾itou pozornost, aby bylo moÅ¾nÃ© chrÃ¡nit AI produkty pÅ™ed sofistikovanÃ½mi Ãºtoky, aÅ¥ uÅ¾ od trollÅ¯ nebo organizovanÃ½ch skupin.
- **StrategickÃ© problÃ©my**: TechnologickÃ½ prÅ¯mysl musÃ­ aktivnÄ› Å™eÅ¡it strategickÃ© vÃ½zvy, aby zajistil dlouhodobou bezpeÄnost zÃ¡kaznÃ­kÅ¯ a ochranu dat.

NavÃ­c modely strojovÃ©ho uÄenÃ­ vÄ›tÅ¡inou nedokÃ¡Å¾ou rozliÅ¡it mezi Å¡kodlivÃ½mi vstupy a neÅ¡kodnÃ½mi anomÃ¡liemi. VÃ½znamnÃ¡ ÄÃ¡st trÃ©ninkovÃ½ch dat pochÃ¡zÃ­ z nekontrolovanÃ½ch, nemoderovanÃ½ch veÅ™ejnÃ½ch datasetÅ¯, kterÃ© jsou otevÅ™enÃ© pÅ™Ã­spÄ›vkÅ¯m tÅ™etÃ­ch stran. ÃštoÄnÃ­ci nemusÃ­ dataset kompromitovat, kdyÅ¾ do nÄ›j mohou volnÄ› pÅ™ispÃ­vat. Postupem Äasu se data s nÃ­zkou dÅ¯vÄ›rou, kterÃ¡ jsou Å¡kodlivÃ¡, mohou stÃ¡t daty s vysokou dÅ¯vÄ›rou, pokud struktura/formÃ¡t dat zÅ¯stane sprÃ¡vnÃ½.

Proto je zÃ¡sadnÃ­ zajistit integritu a ochranu datovÃ½ch ÃºloÅ¾iÅ¡Å¥, kterÃ¡ vaÅ¡e modely pouÅ¾Ã­vajÃ­ k rozhodovÃ¡nÃ­.

## PochopenÃ­ hrozeb a rizik AI

V oblasti AI a souvisejÃ­cÃ­ch systÃ©mÅ¯ je nejvÃ½znamnÄ›jÅ¡Ã­ bezpeÄnostnÃ­ hrozbou dnes otrava dat (data poisoning). Otrava dat nastÃ¡vÃ¡, kdyÅ¾ nÄ›kdo ÃºmyslnÄ› zmÄ›nÃ­ informace pouÅ¾Ã­vanÃ© k trÃ©ninku AI, coÅ¾ zpÅ¯sobÃ­, Å¾e AI dÄ›lÃ¡ chyby. To je zpÅ¯sobeno absencÃ­ standardizovanÃ½ch metod detekce a zmÃ­rnÄ›nÃ­, spolu s naÅ¡Ã­ zÃ¡vislostÃ­ na nedÅ¯vÄ›ryhodnÃ½ch nebo nekontrolovanÃ½ch veÅ™ejnÃ½ch datasetech pro trÃ©nink. Pro udrÅ¾enÃ­ integrity dat a zabrÃ¡nÄ›nÃ­ chybnÃ©ho trÃ©ninku je klÃ­ÄovÃ© sledovat pÅ¯vod a historii vaÅ¡ich dat. Jinak platÃ­ starÃ© pÅ™Ã­slovÃ­ â€Å¡patnÃ¡ data vedou k Å¡patnÃ½m vÃ½sledkÅ¯mâ€œ, coÅ¾ vede ke zhorÅ¡enÃ­ vÃ½konu modelu.

Zde jsou pÅ™Ã­klady, jak mÅ¯Å¾e otrava dat ovlivnit vaÅ¡e modely:

1. **PÅ™evrÃ¡cenÃ­ Å¡tÃ­tkÅ¯ (Label Flipping)**: V Ãºloze binÃ¡rnÃ­ klasifikace ÃºtoÄnÃ­k ÃºmyslnÄ› zmÄ›nÃ­ Å¡tÃ­tky malÃ© ÄÃ¡sti trÃ©ninkovÃ½ch dat. NapÅ™Ã­klad neÅ¡kodnÃ© vzorky jsou oznaÄeny jako Å¡kodlivÃ©, coÅ¾ vede k nesprÃ¡vnÃ½m asociacÃ­m modelu.\
   **PÅ™Ã­klad**: SpamovÃ½ filtr chybnÄ› oznaÄÃ­ legitimnÃ­ e-maily jako spam kvÅ¯li manipulovanÃ½m Å¡tÃ­tkÅ¯m.
2. **Otrava rysÅ¯ (Feature Poisoning)**: ÃštoÄnÃ­k jemnÄ› upravÃ­ rysy v trÃ©ninkovÃ½ch datech, aby zavodil zaujatost nebo zmÃ¡tl model.\
   **PÅ™Ã­klad**: PÅ™idÃ¡nÃ­ irelevantnÃ­ch klÃ­ÄovÃ½ch slov do popisÅ¯ produktÅ¯ za ÃºÄelem manipulace s doporuÄovacÃ­mi systÃ©my.
3. **VklÃ¡dÃ¡nÃ­ dat (Data Injection)**: VklÃ¡dÃ¡nÃ­ Å¡kodlivÃ½ch dat do trÃ©ninkovÃ© sady za ÃºÄelem ovlivnÄ›nÃ­ chovÃ¡nÃ­ modelu.\
   **PÅ™Ã­klad**: ZavedenÃ­ faleÅ¡nÃ½ch uÅ¾ivatelskÃ½ch recenzÃ­ k ovlivnÄ›nÃ­ vÃ½sledkÅ¯ analÃ½zy sentimentu.
4. **Ãštoky zadnÃ­mi vrÃ¡tky (Backdoor Attacks)**: ÃštoÄnÃ­k vloÅ¾Ã­ skrytÃ½ vzor (zadnÃ­ vrÃ¡tka) do trÃ©ninkovÃ½ch dat. Model se nauÄÃ­ tento vzor rozpoznat a pÅ™i jeho aktivaci se chovÃ¡ Å¡kodlivÄ›.\
   **PÅ™Ã­klad**: SystÃ©m rozpoznÃ¡vÃ¡nÃ­ obliÄejÅ¯ trÃ©novanÃ½ na obrÃ¡zcÃ­ch se zadnÃ­mi vrÃ¡tky, kterÃ½ nesprÃ¡vnÄ› identifikuje konkrÃ©tnÃ­ osobu.

SpoleÄnost MITRE vytvoÅ™ila [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), databÃ¡zi taktik a technik pouÅ¾Ã­vanÃ½ch ÃºtoÄnÃ­ky pÅ™i reÃ¡lnÃ½ch ÃºtocÃ­ch na AI systÃ©my.

> PoÄet zranitelnostÃ­ v AI systÃ©mech roste, protoÅ¾e zaÄlenÄ›nÃ­ AI rozÅ¡iÅ™uje povrch Ãºtoku stÃ¡vajÃ­cÃ­ch systÃ©mÅ¯ nad rÃ¡mec tradiÄnÃ­ch kybernetickÃ½ch ÃºtokÅ¯. Vyvinuli jsme ATLAS, abychom zvÃ½Å¡ili povÄ›domÃ­ o tÄ›chto jedineÄnÃ½ch a vyvÃ­jejÃ­cÃ­ch se zranitelnostech, protoÅ¾e globÃ¡lnÃ­ komunita stÃ¡le vÃ­ce integruje AI do rÅ¯znÃ½ch systÃ©mÅ¯. ATLAS je modelovÃ¡n podle rÃ¡mce MITRE ATT&CKÂ® a jeho taktiky, techniky a postupy (TTP) doplÅˆujÃ­ ty v ATT&CK.

PodobnÄ› jako rÃ¡mec MITRE ATT&CKÂ®, kterÃ½ se hojnÄ› vyuÅ¾Ã­vÃ¡ v tradiÄnÃ­ kybernetickÃ© bezpeÄnosti pro plÃ¡novÃ¡nÃ­ scÃ©nÃ¡Å™Å¯ pokroÄilÃ½ch hrozeb, ATLAS poskytuje snadno vyhledatelnou sadu TTP, kterÃ¡ pomÃ¡hÃ¡ lÃ©pe porozumÄ›t a pÅ™ipravit se na obranu proti novÄ› vznikajÃ­cÃ­m ÃºtokÅ¯m.

DÃ¡le Open Web Application Security Project (OWASP) vytvoÅ™il "[Top 10 seznam](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" nejkritiÄtÄ›jÅ¡Ã­ch zranitelnostÃ­ v aplikacÃ­ch vyuÅ¾Ã­vajÃ­cÃ­ch LLM. Seznam zdÅ¯razÅˆuje rizika hrozeb jako je zmÃ­nÄ›nÃ¡ otrava dat a dalÅ¡Ã­, napÅ™Ã­klad:

- **Prompt Injection**: technika, kdy ÃºtoÄnÃ­ci manipulujÃ­ velkÃ½ jazykovÃ½ model (LLM) pomocÃ­ peÄlivÄ› vytvoÅ™enÃ½ch vstupÅ¯, coÅ¾ zpÅ¯sobÃ­, Å¾e model se chovÃ¡ mimo svÅ¯j zamÃ½Å¡lenÃ½ rÃ¡mec.
- **Zranitelnosti dodavatelskÃ©ho Å™etÄ›zce**: Komponenty a software tvoÅ™Ã­cÃ­ aplikace pouÅ¾Ã­vanÃ© LLM, jako jsou Python moduly nebo externÃ­ datasety, mohou bÃ½t samy kompromitovÃ¡ny, coÅ¾ vede k neoÄekÃ¡vanÃ½m vÃ½sledkÅ¯m, zavedenÃ­ zaujatosti a dokonce zranitelnostem v zÃ¡kladnÃ­ infrastruktuÅ™e.
- **PÅ™Ã­liÅ¡nÃ¡ dÅ¯vÄ›ra**: LLM jsou omylnÃ© a majÃ­ tendenci halucinovat, poskytovat nepÅ™esnÃ© nebo nebezpeÄnÃ© vÃ½sledky. V nÄ›kolika zdokumentovanÃ½ch pÅ™Ã­padech lidÃ© brali vÃ½sledky doslova, coÅ¾ vedlo k nechtÄ›nÃ½m negativnÃ­m dÅ¯sledkÅ¯m v reÃ¡lnÃ©m svÄ›tÄ›.

Microsoft Cloud Advocate Rod Trent napsal bezplatnou elektronickou knihu, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), kterÃ¡ se podrobnÄ› vÄ›nuje tÄ›mto a dalÅ¡Ã­m novÄ› vznikajÃ­cÃ­m hrozbÃ¡m AI a poskytuje rozsÃ¡hlÃ© rady, jak tyto scÃ©nÃ¡Å™e nejlÃ©pe Å™eÅ¡it.

## BezpeÄnostnÃ­ testovÃ¡nÃ­ AI systÃ©mÅ¯ a LLM

UmÄ›lÃ¡ inteligence (AI) mÄ›nÃ­ rÅ¯znÃ¡ odvÄ›tvÃ­ a oblasti, nabÃ­zÃ­ novÃ© moÅ¾nosti a pÅ™Ã­nosy pro spoleÄnost. AI vÅ¡ak takÃ© pÅ™inÃ¡Å¡Ã­ vÃ½znamnÃ© vÃ½zvy a rizika, jako je ochrana soukromÃ­ dat, zaujatost, nedostatek vysvÄ›tlitelnosti a potenciÃ¡lnÃ­ zneuÅ¾itÃ­. Proto je zÃ¡sadnÃ­ zajistit, aby AI systÃ©my byly bezpeÄnÃ© a odpovÄ›dnÃ©, coÅ¾ znamenÃ¡, Å¾e dodrÅ¾ujÃ­ etickÃ© a prÃ¡vnÃ­ normy a mohou bÃ½t dÅ¯vÄ›ryhodnÃ© pro uÅ¾ivatele a zainteresovanÃ© strany.

BezpeÄnostnÃ­ testovÃ¡nÃ­ je proces hodnocenÃ­ bezpeÄnosti AI systÃ©mu nebo LLM identifikacÃ­ a vyuÅ¾Ã­vÃ¡nÃ­m jejich zranitelnostÃ­. Toto mÅ¯Å¾e provÃ¡dÄ›t vÃ½vojÃ¡Å™, uÅ¾ivatel nebo tÅ™etÃ­ strana, v zÃ¡vislosti na ÃºÄelu a rozsahu testovÃ¡nÃ­. NÄ›kterÃ© z nejbÄ›Å¾nÄ›jÅ¡Ã­ch metod bezpeÄnostnÃ­ho testovÃ¡nÃ­ AI systÃ©mÅ¯ a LLM jsou:

- **Sanitizace dat**: Proces odstraÅˆovÃ¡nÃ­ nebo anonymizace citlivÃ½ch Äi soukromÃ½ch informacÃ­ z trÃ©ninkovÃ½ch dat nebo vstupÅ¯ AI systÃ©mu Äi LLM. Sanitizace dat pomÃ¡hÃ¡ zabrÃ¡nit Ãºniku dat a Å¡kodlivÃ© manipulaci tÃ­m, Å¾e sniÅ¾uje expozici dÅ¯vÄ›rnÃ½ch nebo osobnÃ­ch ÃºdajÅ¯.
- **AdversariÃ¡lnÃ­ testovÃ¡nÃ­**: Proces generovÃ¡nÃ­ a aplikace adversariÃ¡lnÃ­ch pÅ™Ã­kladÅ¯ na vstup nebo vÃ½stup AI systÃ©mu Äi LLM za ÃºÄelem vyhodnocenÃ­ jeho odolnosti vÅ¯Äi adversariÃ¡lnÃ­m ÃºtokÅ¯m. PomÃ¡hÃ¡ identifikovat a zmÃ­rnit zranitelnosti a slabiny, kterÃ© by mohli ÃºtoÄnÃ­ci zneuÅ¾Ã­t.
- **OvÄ›Å™enÃ­ modelu**: Proces ovÄ›Å™ovÃ¡nÃ­ sprÃ¡vnosti a Ãºplnosti parametrÅ¯ nebo architektury modelu AI systÃ©mu Äi LLM. PomÃ¡hÃ¡ detekovat a zabrÃ¡nit krÃ¡deÅ¾i modelu tÃ­m, Å¾e zajistÃ­ jeho ochranu a autentizaci.
- **Validace vÃ½stupu**: Proces ovÄ›Å™ovÃ¡nÃ­ kvality a spolehlivosti vÃ½stupu AI systÃ©mu Äi LLM. PomÃ¡hÃ¡ odhalit a opravit Å¡kodlivou manipulaci tÃ­m, Å¾e zajistÃ­ konzistenci a pÅ™esnost vÃ½stupu.

OpenAI, lÃ­dr v oblasti AI systÃ©mÅ¯, zavedl sÃ©rii _bezpeÄnostnÃ­ch hodnocenÃ­_ v rÃ¡mci svÃ© iniciativy red teaming, zamÄ›Å™enÃ© na testovÃ¡nÃ­ vÃ½stupÅ¯ AI systÃ©mÅ¯ s cÃ­lem pÅ™ispÄ›t k bezpeÄnosti AI.

> HodnocenÃ­ mohou sahat od jednoduchÃ½ch otÃ¡zek a odpovÄ›dÃ­ aÅ¾ po sloÅ¾itÄ›jÅ¡Ã­ simulace. Jako konkrÃ©tnÃ­ pÅ™Ã­klady zde jsou ukÃ¡zkovÃ¡ hodnocenÃ­ vyvinutÃ¡ OpenAI pro posouzenÃ­ chovÃ¡nÃ­ AI z rÅ¯znÃ½ch ÃºhlÅ¯:

#### PÅ™esvÄ›dÄovÃ¡nÃ­

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m pÅ™imÄ›t jinÃ½ AI systÃ©m, aby Å™ekl tajnÃ© slovo?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m pÅ™esvÄ›dÄit jinÃ½ AI systÃ©m, aby daroval penÃ­ze?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m ovlivnit podporu politickÃ©ho nÃ¡vrhu jinÃ©ho AI systÃ©mu?

#### Steganografie (skrytÃ¡ zprÃ¡va)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m pÅ™edat tajnÃ© zprÃ¡vy, aniÅ¾ by byl odhalen jinÃ½m AI systÃ©mem?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m komprimovat a dekomprimovat zprÃ¡vy, aby umoÅ¾nil skrytÃ­ tajnÃ½ch zprÃ¡v?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Jak dobÅ™e dokÃ¡Å¾e AI systÃ©m koordinovat s jinÃ½m AI systÃ©mem bez pÅ™Ã­mÃ© komunikace?

### BezpeÄnost AI

Je nezbytnÃ© chrÃ¡nit AI systÃ©my pÅ™ed Å¡kodlivÃ½mi Ãºtoky, zneuÅ¾itÃ­m nebo nechtÄ›nÃ½mi dÅ¯sledky. To zahrnuje kroky k zajiÅ¡tÄ›nÃ­ bezpeÄnosti, spolehlivosti a dÅ¯vÄ›ryhodnosti AI systÃ©mÅ¯, napÅ™Ã­klad:

- ZabezpeÄenÃ­ dat a algoritmÅ¯ pouÅ¾Ã­vanÃ½ch k trÃ©ninku a provozu AI modelÅ¯
- Prevence neoprÃ¡vnÄ›nÃ©ho pÅ™Ã­stupu, manipulace nebo sabotÃ¡Å¾e AI systÃ©mÅ¯
- Detekce a zmÃ­rnÄ›nÃ­ zaujatosti, diskriminace nebo etickÃ½ch problÃ©mÅ¯ v AI systÃ©mech
- ZajiÅ¡tÄ›nÃ­ odpovÄ›dnosti, transparentnosti a vysvÄ›tlitelnosti rozhodnutÃ­ a akcÃ­ AI
- SlaÄovÃ¡nÃ­ cÃ­lÅ¯ a hodnot AI systÃ©mÅ¯ s hodnotami lidÃ­ a spoleÄnosti

BezpeÄnost AI je dÅ¯leÅ¾itÃ¡ pro zajiÅ¡tÄ›nÃ­ integrity, dostupnosti a dÅ¯vÄ›rnosti AI systÃ©mÅ¯ a dat. NÄ›kterÃ© vÃ½zvy a pÅ™Ã­leÅ¾itosti v oblasti bezpeÄnosti AI jsou:

- PÅ™Ã­leÅ¾itost: ZaÄlenÄ›nÃ­ AI do strategiÃ­ kybernetickÃ© bezpeÄnosti, protoÅ¾e mÅ¯Å¾e hrÃ¡t klÃ­Äovou roli pÅ™i identifikaci hrozeb a zlepÅ¡ovÃ¡nÃ­ reakÄnÃ­ch ÄasÅ¯. AI mÅ¯Å¾e pomoci automatizovat a rozÅ¡iÅ™ovat detekci a zmÃ­rnÄ›nÃ­ kybernetickÃ½ch ÃºtokÅ¯, jako jsou phishing, malware nebo ransomware.
- VÃ½zva: AI mÅ¯Å¾e bÃ½t takÃ© zneuÅ¾ita ÃºtoÄnÃ­ky k provÃ¡dÄ›nÃ­ sofistikovanÃ½ch ÃºtokÅ¯, jako je generovÃ¡nÃ­ faleÅ¡nÃ©ho nebo zavÃ¡dÄ›jÃ­cÃ­ho obsahu, vydÃ¡vÃ¡nÃ­ se za uÅ¾ivatele nebo zneuÅ¾Ã­vÃ¡nÃ­ zranitelnostÃ­ AI systÃ©mÅ¯. VÃ½vojÃ¡Å™i AI proto nesou jedineÄnou odpovÄ›dnost navrhovat systÃ©my, kterÃ© jsou odolnÃ© a robustnÃ­ vÅ¯Äi zneuÅ¾itÃ­.

### Ochrana dat

LLM mohou pÅ™edstavovat rizika pro soukromÃ­ a bezpeÄnost dat, kterÃ¡ pouÅ¾Ã­vajÃ­. NapÅ™Ã­klad LLM mohou potenciÃ¡lnÄ› zapamatovat a uniknout citlivÃ© informace ze svÃ½ch trÃ©ninkovÃ½ch dat, jako jsou osobnÃ­ jmÃ©na, adresy, hesla nebo ÄÃ­sla kreditnÃ­ch karet. Mohou bÃ½t takÃ© manipulovÃ¡ny nebo napadeny Å¡kodlivÃ½mi aktÃ©ry, kteÅ™Ã­ chtÄ›jÃ­ vyuÅ¾Ã­t jejich zranitelnosti nebo zaujatosti. Proto je dÅ¯leÅ¾itÃ© bÃ½t si tÄ›chto rizik vÄ›dom a pÅ™ijmout vhodnÃ¡ opatÅ™enÃ­ na ochranu dat pouÅ¾Ã­vanÃ½ch s LLM. NÄ›kterÃ© kroky, kterÃ© mÅ¯Å¾ete podniknout k ochranÄ› dat pouÅ¾Ã­vanÃ½ch s LLM, zahrnujÃ­:

- **Omezit mnoÅ¾stvÃ­ a typ dat sdÃ­lenÃ½ch s LLM**: SdÃ­lejte pouze data, kterÃ¡ jsou nezbytnÃ¡ a relevantnÃ­ pro zamÃ½Å¡lenÃ© ÃºÄely, a vyhnÄ›te se sdÃ­lenÃ­ citlivÃ½ch, dÅ¯vÄ›rnÃ½ch nebo osobnÃ­ch ÃºdajÅ¯. UÅ¾ivatelÃ© by takÃ© mÄ›li anonymizovat nebo Å¡ifrovat data sdÃ­lenÃ¡ s LLM, napÅ™Ã­klad odstranÄ›nÃ­m nebo zakrytÃ­m identifikaÄnÃ­ch informacÃ­ nebo pouÅ¾itÃ­m zabezpeÄenÃ½ch komunikaÄnÃ­ch kanÃ¡lÅ¯.
- **OvÄ›Å™ovat data generovanÃ¡ LLM**: VÅ¾dy kontrolujte pÅ™esnost a kvalitu vÃ½stupu generovanÃ©ho LLM, abyste zajistili, Å¾e neobsahuje neÅ¾Ã¡doucÃ­ nebo nevhodnÃ© informace.
- **HlÃ¡sit a upozorÅˆovat na jakÃ©koli Ãºniky dat nebo incidenty**: BuÄte ostraÅ¾itÃ­ vÅ¯Äi podezÅ™elÃ½m nebo abnormÃ¡lnÃ­m aktivitÃ¡m Äi chovÃ¡nÃ­ LLM, jako je generovÃ¡nÃ­ nerelevantnÃ­ch, nepÅ™esnÃ½ch, urÃ¡Å¾livÃ½ch nebo Å¡kodlivÃ½ch textÅ¯. To mÅ¯Å¾e bÃ½t znÃ¡mkou Ãºniku dat nebo bezpeÄnostnÃ­ho incidentu.

BezpeÄnost dat, sprÃ¡va a dodrÅ¾ovÃ¡nÃ­ pÅ™edpisÅ¯ jsou klÃ­ÄovÃ© pro kaÅ¾dou organizaci, kterÃ¡ chce vyuÅ¾Ã­vat sÃ­lu dat a AI v multi-cloudovÃ©m prostÅ™edÃ­. ZabezpeÄenÃ­ a sprÃ¡va vÅ¡ech vaÅ¡ich dat je sloÅ¾itÃ½ a mnohostrannÃ½ Ãºkol. MusÃ­te zabezpeÄit a spravovat rÅ¯znÃ© typy dat (strukturovanÃ¡, nestrukturovanÃ¡ a data generovanÃ¡ AI) na rÅ¯znÃ½ch mÃ­stech napÅ™Ã­Ä vÃ­ce cloudy a musÃ­te zohlednit stÃ¡vajÃ­cÃ­ i budoucÃ­ pÅ™edpisy tÃ½kajÃ­cÃ­ se bezpeÄnosti dat, sprÃ¡vy a AI. Pro ochranu vaÅ¡ich dat je tÅ™eba pÅ™ijmout nÄ›kterÃ© osvÄ›dÄenÃ© postupy a opatÅ™enÃ­, napÅ™Ã­klad:

- PouÅ¾Ã­vat cloudovÃ© sluÅ¾by nebo platformy, kterÃ© nabÃ­zejÃ­ funkce ochrany dat a soukromÃ­.
- PouÅ¾Ã­vat nÃ¡stroje pro kontrolu kvality a validaci dat k odhalenÃ­ chyb, nesrovnalostÃ­ nebo anomÃ¡liÃ­.
- PouÅ¾Ã­vat rÃ¡mce pro sprÃ¡vu dat a etiku, aby bylo zajiÅ¡tÄ›no odpovÄ›dnÃ© a transparentnÃ­ vyuÅ¾Ã­vÃ¡nÃ­ dat.

### Emulace reÃ¡lnÃ½ch hrozeb â€“ AI red teaming

Emulace reÃ¡lnÃ½ch
> Praxe AI red teamingu se vyvinula a zÃ­skala Å¡irÅ¡Ã­ vÃ½znam: nezahrnuje pouze hledÃ¡nÃ­ bezpeÄnostnÃ­ch zranitelnostÃ­, ale takÃ© testovÃ¡nÃ­ dalÅ¡Ã­ch selhÃ¡nÃ­ systÃ©mu, jako je generovÃ¡nÃ­ potenciÃ¡lnÄ› Å¡kodlivÃ©ho obsahu. AI systÃ©my pÅ™inÃ¡Å¡ejÃ­ novÃ¡ rizika a red teaming je klÃ­ÄovÃ½ pro pochopenÃ­ tÄ›chto novÃ½ch hrozeb, jako je prompt injection a vytvÃ¡Å™enÃ­ nepodloÅ¾enÃ©ho obsahu. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.cs.png)]()

NÃ­Å¾e jsou klÃ­ÄovÃ© poznatky, kterÃ© formovaly program AI Red Team spoleÄnosti Microsoft.

1. **Å irokÃ½ rozsah AI red teamingu:**  
   AI red teaming nynÃ­ zahrnuje jak bezpeÄnostnÃ­, tak i vÃ½sledky v oblasti Responsible AI (RAI). TradiÄnÄ› se red teaming zamÄ›Å™oval na bezpeÄnostnÃ­ aspekty, kdy byl model povaÅ¾ovÃ¡n za vektor Ãºtoku (napÅ™. krÃ¡deÅ¾ zÃ¡kladnÃ­ho modelu). AI systÃ©my vÅ¡ak pÅ™inÃ¡Å¡ejÃ­ novÃ© bezpeÄnostnÃ­ zranitelnosti (napÅ™. prompt injection, poisoning), kterÃ© vyÅ¾adujÃ­ zvlÃ¡Å¡tnÃ­ pozornost. KromÄ› bezpeÄnosti AI red teaming takÃ© zkoumÃ¡ otÃ¡zky spravedlnosti (napÅ™. stereotypizaci) a Å¡kodlivÃ½ obsah (napÅ™. glorifikaci nÃ¡silÃ­). VÄasnÃ© odhalenÃ­ tÄ›chto problÃ©mÅ¯ umoÅ¾Åˆuje prioritizovat investice do obrany.
2. **ZÃ¡mÄ›rnÃ© i neÃºmyslnÃ© selhÃ¡nÃ­:**  
   AI red teaming zohledÅˆuje selhÃ¡nÃ­ jak z pohledu Å¡kodlivÃ½ch ÃºmyslÅ¯, tak i neÅ¡kodnÃ½ch situacÃ­. NapÅ™Ã­klad pÅ™i red teamingu novÃ©ho Bingu zkoumÃ¡me nejen to, jak mohou Å¡kodlivÃ­ ÃºtoÄnÃ­ci systÃ©m zneuÅ¾Ã­t, ale takÃ© jak bÄ›Å¾nÃ­ uÅ¾ivatelÃ© mohou narazit na problematickÃ½ nebo Å¡kodlivÃ½ obsah. Na rozdÃ­l od tradiÄnÃ­ho bezpeÄnostnÃ­ho red teamingu, kterÃ½ se zamÄ›Å™uje hlavnÄ› na Å¡kodlivÃ© aktÃ©ry, AI red teaming bere v potaz Å¡irÅ¡Ã­ spektrum uÅ¾ivatelÅ¯ a moÅ¾nÃ½ch selhÃ¡nÃ­.
3. **DynamickÃ¡ povaha AI systÃ©mÅ¯:**  
   AI aplikace se neustÃ¡le vyvÃ­jejÃ­. U aplikacÃ­ zaloÅ¾enÃ½ch na velkÃ½ch jazykovÃ½ch modelech vÃ½vojÃ¡Å™i pÅ™izpÅ¯sobujÃ­ poÅ¾adavky. PrÅ¯bÄ›Å¾nÃ½ red teaming zajiÅ¡Å¥uje stÃ¡lou ostraÅ¾itost a pÅ™izpÅ¯sobenÃ­ se mÄ›nÃ­cÃ­m se rizikÅ¯m.

AI red teaming nenÃ­ vÅ¡emocnÃ½ a mÄ›l by bÃ½t povaÅ¾ovÃ¡n za doplÅˆkovÃ½ nÃ¡stroj k dalÅ¡Ã­m kontrolÃ¡m, jako je [role-based access control (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) a komplexnÃ­ Å™eÅ¡enÃ­ sprÃ¡vy dat. MÃ¡ doplnit bezpeÄnostnÃ­ strategii, kterÃ¡ se zamÄ›Å™uje na pouÅ¾Ã­vÃ¡nÃ­ bezpeÄnÃ½ch a odpovÄ›dnÃ½ch AI Å™eÅ¡enÃ­, jeÅ¾ zohledÅˆujÃ­ soukromÃ­ a bezpeÄnost a zÃ¡roveÅˆ usilujÃ­ o minimalizaci pÅ™edsudkÅ¯, Å¡kodlivÃ©ho obsahu a dezinformacÃ­, kterÃ© mohou podkopÃ¡vat dÅ¯vÄ›ru uÅ¾ivatelÅ¯.

Zde je seznam dalÅ¡Ã­ho ÄtenÃ­, kterÃ© vÃ¡m pomÅ¯Å¾e lÃ©pe pochopit, jak red teaming mÅ¯Å¾e pomoci identifikovat a zmÃ­rnit rizika ve vaÅ¡ich AI systÃ©mech:

- [PlÃ¡novÃ¡nÃ­ red teamingu pro velkÃ© jazykovÃ© modely (LLM) a jejich aplikace](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [Co je OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming â€“ klÃ­ÄovÃ¡ praxe pro budovÃ¡nÃ­ bezpeÄnÄ›jÅ¡Ã­ch a odpovÄ›dnÄ›jÅ¡Ã­ch AI Å™eÅ¡enÃ­](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), databÃ¡ze taktik a technik pouÅ¾Ã­vanÃ½ch ÃºtoÄnÃ­ky pÅ™i reÃ¡lnÃ½ch ÃºtocÃ­ch na AI systÃ©my.

## Kontrola znalostÃ­

JakÃ½ by mohl bÃ½t dobrÃ½ pÅ™Ã­stup k udrÅ¾enÃ­ integrity dat a prevenci jejich zneuÅ¾itÃ­?

1. MÃ­t silnÃ© role-based kontroly pro pÅ™Ã­stup k datÅ¯m a sprÃ¡vu dat  
1. Implementovat a auditovat oznaÄovÃ¡nÃ­ dat, aby se zabrÃ¡nilo jejich nesprÃ¡vnÃ© reprezentaci nebo zneuÅ¾itÃ­  
1. Zajistit, Å¾e vaÅ¡e AI infrastruktura podporuje filtrovÃ¡nÃ­ obsahu

OdpovÄ›Ä: 1, I kdyÅ¾ jsou vÅ¡echny tÅ™i doporuÄenÃ­ skvÄ›lÃ¡, sprÃ¡vnÃ© pÅ™iÅ™azenÃ­ pÅ™Ã­stupovÃ½ch prÃ¡v k datÅ¯m uÅ¾ivatelÅ¯m vÃ½raznÄ› pomÅ¯Å¾e zabrÃ¡nit manipulaci a nesprÃ¡vnÃ© reprezentaci dat pouÅ¾Ã­vanÃ½ch LLM.

## ğŸš€ VÃ½zva

PÅ™eÄtÄ›te si vÃ­ce o tom, jak mÅ¯Å¾ete [spravovat a chrÃ¡nit citlivÃ© informace](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) v dobÄ› AI.

## SkvÄ›lÃ¡ prÃ¡ce, pokraÄujte ve vzdÄ›lÃ¡vÃ¡nÃ­

Po dokonÄenÃ­ tÃ©to lekce si prohlÃ©dnÄ›te naÅ¡i [kolekci Generative AI Learning](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) a pokraÄujte ve zvyÅ¡ovÃ¡nÃ­ svÃ½ch znalostÃ­ o generativnÃ­ AI!

PÅ™ejdÄ›te k lekci 14, kde se podÃ­vÃ¡me na [Å¾ivotnÃ­ cyklus aplikacÃ­ generativnÃ­ AI](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**ProhlÃ¡Å¡enÃ­ o vylouÄenÃ­ odpovÄ›dnosti**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ AI pÅ™ekladatelskÃ© sluÅ¾by [Co-op Translator](https://github.com/Azure/co-op-translator). I kdyÅ¾ usilujeme o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatizovanÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho mateÅ™skÃ©m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. Nejsme odpovÄ›dnÃ­ za jakÃ©koliv nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© vÃ½klady vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.