{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## المقدمة\n",
    "\n",
    "ستتناول هذه الدرس:\n",
    "- ما هو استدعاء الدوال وحالات استخدامه\n",
    "- كيفية إنشاء استدعاء دالة باستخدام OpenAI\n",
    "- كيفية دمج استدعاء دالة في تطبيق\n",
    "\n",
    "## أهداف التعلم\n",
    "\n",
    "بعد الانتهاء من هذا الدرس ستعرف وتفهم:\n",
    "\n",
    "- الهدف من استخدام استدعاء الدوال\n",
    "- إعداد استدعاء دالة باستخدام خدمة OpenAI\n",
    "- تصميم استدعاءات دوال فعّالة لحالة استخدام تطبيقك\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## فهم استدعاء الدوال\n",
    "\n",
    "في هذا الدرس، نرغب في بناء ميزة لشركتنا الناشئة في مجال التعليم تتيح للمستخدمين استخدام روبوت دردشة للعثور على الدورات التقنية. سنوصي بالدورات التي تناسب مستوى مهاراتهم، ودورهم الحالي، والتقنية التي يهتمون بها.\n",
    "\n",
    "لإكمال ذلك سنستخدم مزيجاً من:\n",
    " - `OpenAI` لإنشاء تجربة دردشة للمستخدم\n",
    " - `Microsoft Learn Catalog API` لمساعدة المستخدمين في العثور على الدورات بناءً على طلبهم\n",
    " - `Function Calling` لأخذ استفسار المستخدم وإرساله إلى دالة لإجراء طلب API.\n",
    "\n",
    "لبدء العمل، دعونا نلقي نظرة على سبب رغبتنا في استخدام استدعاء الدوال من الأساس:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # الحصول على رد جديد من GPT حيث يمكنه رؤية استجابة الدالة\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### لماذا استدعاء الدوال\n",
    "\n",
    "إذا كنت قد أنهيت أي درس آخر في هذه الدورة، فمن المحتمل أنك تدرك قوة استخدام نماذج اللغة الكبيرة (LLMs). ونأمل أيضًا أنك لاحظت بعض القيود لديها.\n",
    "\n",
    "استدعاء الدوال هو ميزة في خدمة OpenAI تم تصميمها لمعالجة التحديات التالية:\n",
    "\n",
    "تنسيق الاستجابات غير المتسق:\n",
    "- قبل استدعاء الدوال، كانت الاستجابات من نموذج اللغة الكبير غير منظمة وغير متسقة. وكان على المطورين كتابة شيفرة تحقق معقدة للتعامل مع كل اختلاف في المخرجات.\n",
    "\n",
    "الدمج المحدود مع البيانات الخارجية:\n",
    "- قبل هذه الميزة، كان من الصعب دمج بيانات من أجزاء أخرى من التطبيق في سياق المحادثة.\n",
    "\n",
    "من خلال توحيد تنسيقات الاستجابات وتمكين التكامل السلس مع البيانات الخارجية، يبسط استدعاء الدوال عملية التطوير ويقلل الحاجة إلى منطق تحقق إضافي.\n",
    "\n",
    "لم يكن بإمكان المستخدمين الحصول على إجابات مثل \"ما هو الطقس الحالي في ستوكهولم؟\". وذلك لأن النماذج كانت محدودة بالوقت الذي تم فيه تدريب البيانات.\n",
    "\n",
    "دعونا نلقي نظرة على المثال أدناه الذي يوضح هذه المشكلة:\n",
    "\n",
    "لنفترض أننا نريد إنشاء قاعدة بيانات لبيانات الطلاب حتى نتمكن من اقتراح الدورة المناسبة لهم. في الأسفل لدينا وصفان لطالبين متشابهين جدًا في البيانات التي يحتويان عليها.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نريد إرسال هذا إلى نموذج لغوي كبير لتحليل البيانات. يمكن استخدام ذلك لاحقًا في تطبيقنا لإرساله إلى واجهة برمجة التطبيقات أو تخزينه في قاعدة بيانات.\n",
    "\n",
    "لنقم بإنشاء مطالبتين متطابقتين نوجه فيهما النموذج اللغوي حول المعلومات التي نهتم بها:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نريد إرسال هذا إلى نموذج لغوي كبير لتحليل الأجزاء المهمة لمنتجنا. حتى نتمكن من إنشاء مطالبتين متطابقتين لإعطاء التعليمات للنموذج اللغوي الكبير:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "بعد إنشاء هذين الطلبين، سنرسلهم إلى نموذج اللغة الكبير باستخدام `openai.ChatCompletion`. نقوم بتخزين الطلب في متغير `messages` ونعين الدور إلى `user`. هذا لمحاكاة رسالة من مستخدم يتم كتابتها إلى روبوت الدردشة.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الآن يمكننا إرسال كلا الطلبين إلى نموذج اللغة الكبير وفحص الاستجابة التي نتلقاها.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "على الرغم من أن التعليمات متشابهة والوصف متقارب، إلا أنه يمكننا الحصول على صيغ مختلفة لخاصية `Grades`.\n",
    "\n",
    "إذا قمت بتشغيل الخلية السابقة عدة مرات، قد يكون الشكل إما `3.7` أو `3.7 GPA`.\n",
    "\n",
    "يعود ذلك إلى أن نموذج اللغة الكبير يتعامل مع البيانات غير المنظمة على شكل تعليمات مكتوبة ويعيد أيضاً بيانات غير منظمة. نحن بحاجة إلى تنسيق منظم حتى نعرف ما الذي نتوقعه عند تخزين أو استخدام هذه البيانات.\n",
    "\n",
    "من خلال استخدام استدعاء الدوال، يمكننا التأكد من أننا نستلم بيانات منظمة. عند استخدام استدعاء الدوال، النموذج لا يقوم فعلياً باستدعاء أو تشغيل أي دوال. بدلاً من ذلك، نقوم بإنشاء هيكل للنموذج ليتبعه في ردوده. بعد ذلك نستخدم هذه الردود المنظمة لمعرفة أي دالة يجب تشغيلها في تطبيقاتنا.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![مخطط تدفق استدعاء الدالة](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.ar.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### حالات استخدام استدعاء الدوال\n",
    "\n",
    "**استدعاء الأدوات الخارجية**  \n",
    "تُعد الدردشات الذكية ممتازة في تقديم الإجابات على أسئلة المستخدمين. من خلال استخدام استدعاء الدوال، يمكن للدردشة الذكية استخدام رسائل المستخدمين لتنفيذ مهام معينة. على سبيل المثال، يمكن لطالب أن يطلب من الدردشة الذكية \"إرسال بريد إلكتروني إلى معلمي وأخبره أنني بحاجة إلى مزيد من المساعدة في هذا الموضوع\". في هذه الحالة، يمكن استدعاء دالة مثل `send_email(to: string, body: string)`\n",
    "\n",
    "**إنشاء استعلامات API أو قواعد بيانات**  \n",
    "يمكن للمستخدمين العثور على المعلومات باستخدام اللغة الطبيعية التي يتم تحويلها إلى استعلام منسق أو طلب API. مثال على ذلك هو معلم يطلب \"من هم الطلاب الذين أكملوا الواجب الأخير\"، حيث يمكن استدعاء دالة باسم `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**إنشاء بيانات منظمة**  \n",
    "يمكن للمستخدمين أخذ نص أو ملف CSV واستخدام نموذج اللغة الكبير لاستخراج المعلومات المهمة منه. على سبيل المثال، يمكن لطالب تحويل مقالة من ويكيبيديا عن اتفاقيات السلام إلى بطاقات مراجعة ذكية بالذكاء الاصطناعي. يمكن تحقيق ذلك باستخدام دالة مثل `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ٢. إنشاء أول استدعاء دالة لك\n",
    "\n",
    "تشمل عملية إنشاء استدعاء دالة ثلاث خطوات رئيسية:\n",
    "١. استدعاء واجهة برمجة تطبيقات إكمال الدردشة مع قائمة الدوال الخاصة بك ورسالة المستخدم\n",
    "٢. قراءة استجابة النموذج لتنفيذ إجراء مثل تنفيذ دالة أو استدعاء واجهة برمجة تطبيقات\n",
    "٣. إجراء استدعاء آخر لواجهة برمجة تطبيقات إكمال الدردشة مع استجابة دالتك لاستخدام تلك المعلومات في إنشاء رد للمستخدم.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![تدفق استدعاء الدالة](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.ar.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### عناصر استدعاء الدالة\n",
    "\n",
    "#### إدخال المستخدم\n",
    "\n",
    "الخطوة الأولى هي إنشاء رسالة من المستخدم. يمكن تعيين هذه الرسالة بشكل ديناميكي من خلال أخذ قيمة من حقل إدخال نصي أو يمكنك تعيين قيمة هنا مباشرة. إذا كانت هذه هي المرة الأولى التي تعمل فيها مع واجهة برمجة تطبيقات إكمال المحادثة، نحتاج إلى تحديد `role` و`content` للرسالة.\n",
    "\n",
    "يمكن أن تكون قيمة `role` إما `system` (لإنشاء القواعد)، أو `assistant` (النموذج)، أو `user` (المستخدم النهائي). في حالة استدعاء الدوال، سنحددها كـ `user` مع مثال على سؤال.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### إنشاء الدوال.\n",
    "\n",
    "بعد ذلك سنقوم بتعريف دالة والمعاملات الخاصة بهذه الدالة. سنستخدم هنا دالة واحدة فقط باسم `search_courses`، لكن يمكنك إنشاء عدة دوال إذا رغبت.\n",
    "\n",
    "**مهم**: يتم تضمين الدوال في رسالة النظام إلى نموذج اللغة الكبير (LLM) وسيتم احتسابها من ضمن عدد الرموز المتاحة لديك.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**التعريفات**\n",
    "\n",
    "هيكل تعريف الدالة يتكون من عدة مستويات، وكل مستوى له خصائصه الخاصة. فيما يلي شرح للهيكل المتداخل:\n",
    "\n",
    "**خصائص الدالة في المستوى الأعلى:**\n",
    "\n",
    "`name` - اسم الدالة التي نريد استدعاءها.\n",
    "\n",
    "`description` - هذا هو الوصف لكيفية عمل الدالة. من المهم هنا أن يكون الوصف محدد وواضح.\n",
    "\n",
    "`parameters` - قائمة بالقيم والصيغة التي تريد من النموذج أن ينتجها في رده.\n",
    "\n",
    "**خصائص كائن المعاملات:**\n",
    "\n",
    "`type` - نوع البيانات لكائن المعاملات (عادةً \"object\")\n",
    "\n",
    "`properties` - قائمة بالقيم المحددة التي سيستخدمها النموذج في رده.\n",
    "\n",
    "**خصائص كل معامل على حدة:**\n",
    "\n",
    "`name` - يتم تعريفه ضمنيًا بواسطة مفتاح الخاصية (مثلاً، \"role\"، \"product\"، \"level\")\n",
    "\n",
    "`type` - نوع البيانات لهذا المعامل المحدد (مثلاً، \"string\"، \"number\"، \"boolean\")\n",
    "\n",
    "`description` - وصف لهذا المعامل المحدد\n",
    "\n",
    "**الخصائص الاختيارية:**\n",
    "\n",
    "`required` - مصفوفة تسرد أي المعاملات مطلوبة لإتمام استدعاء الدالة\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### إجراء استدعاء الدالة\n",
    "بعد تعريف الدالة، نحتاج الآن إلى تضمينها في الاستدعاء إلى واجهة برمجة تطبيقات إكمال الدردشة. نقوم بذلك عن طريق إضافة `functions` إلى الطلب. في هذه الحالة نكتب `functions=functions`.\n",
    "\n",
    "هناك أيضًا خيار لتعيين `function_call` إلى `auto`. هذا يعني أننا سنترك للنموذج اللغوي الكبير تحديد أي دالة يجب استدعاؤها بناءً على رسالة المستخدم بدلاً من تعيينها بأنفسنا.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الآن دعونا نلقي نظرة على الاستجابة ونرى كيف تم تنسيقها:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "يمكنك أن ترى أن اسم الدالة تم استدعاؤه ومن خلال رسالة المستخدم، تمكن نموذج الذكاء الاصطناعي من إيجاد البيانات المناسبة لتعبئة معطيات الدالة.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. دمج استدعاءات الدوال في التطبيق\n",
    "\n",
    "بعد أن قمنا باختبار الاستجابة المنسقة من LLM، يمكننا الآن دمجها في التطبيق.\n",
    "\n",
    "### إدارة سير العمل\n",
    "\n",
    "لدمج هذا في تطبيقنا، دعونا نتبع الخطوات التالية:\n",
    "\n",
    "أولاً، سنقوم بإجراء الاستدعاء لخدمات OpenAI وتخزين الرسالة في متغير باسم `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "الآن سنعرّف الدالة التي ستستدعي واجهة برمجة تطبيقات Microsoft Learn للحصول على قائمة الدورات:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "كأفضل ممارسة، سنرى أولاً ما إذا كان النموذج يرغب في استدعاء دالة. بعد ذلك، سنقوم بإنشاء إحدى الدوال المتاحة ونطابقها مع الدالة التي يتم استدعاؤها.\n",
    "بعدها سنأخذ معاملات الدالة ونربطها بمعاملات من النموذج اللغوي الكبير.\n",
    "\n",
    "وأخيراً، سنضيف رسالة استدعاء الدالة والقيم التي تم إرجاعها بواسطة رسالة `search_courses`. هذا يمنح النموذج اللغوي الكبير كل المعلومات التي يحتاجها\n",
    "للرد على المستخدم باستخدام اللغة الطبيعية.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## تحدي البرمجة\n",
    "\n",
    "عمل رائع! لمواصلة تعلمك حول استدعاء الدوال في OpenAI يمكنك بناء: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - إضافة المزيد من المعاملات للدالة التي قد تساعد المتعلمين في العثور على المزيد من الدورات. يمكنك العثور على معاملات الـ API المتاحة هنا:\n",
    " - إنشاء استدعاء دالة آخر يأخذ معلومات إضافية من المتعلم مثل لغته الأم\n",
    " - إنشاء معالجة للأخطاء عندما لا يُرجع استدعاء الدالة و/أو استدعاء الـ API أي دورات مناسبة\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**إخلاء المسؤولية**:  \nتمت ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر الموثوق. للمعلومات الهامة، يُنصح بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسير خاطئ ينشأ عن استخدام هذه الترجمة.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T20:43:59+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "ar"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}