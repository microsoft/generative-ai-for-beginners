{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ضبط نماذج Open AI بدقة\n",
    "\n",
    "تعتمد هذه الدفتر على الإرشادات الحالية المقدمة في وثائق [الضبط الدقيق](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst) من Open AI.\n",
    "\n",
    "يعمل الضبط الدقيق على تحسين أداء نماذج الأساس لتطبيقك من خلال إعادة تدريبه ببيانات وسياق إضافي ذي صلة بحالة الاستخدام أو السيناريو المحدد. لاحظ أن تقنيات هندسة المطالبات مثل _التعلم بعدد قليل من الأمثلة_ و _التوليد المعزز بالاسترجاع_ تتيح لك تحسين المطالبة الافتراضية ببيانات ذات صلة لتحسين الجودة. ومع ذلك، فإن هذه الأساليب محدودة بحجم نافذة الرموز القصوى للنموذج الأساسي المستهدف.\n",
    "\n",
    "مع الضبط الدقيق، نحن فعليًا نعيد تدريب النموذج نفسه بالبيانات المطلوبة (مما يسمح لنا باستخدام عدد أكبر من الأمثلة مما يمكن أن يتسع في نافذة الرموز القصوى) - ونشر نسخة _مخصصة_ من النموذج التي لم تعد بحاجة إلى تقديم أمثلة في وقت الاستدلال. هذا لا يحسن فقط فعالية تصميم المطالبة لدينا (لدينا مرونة أكبر في استخدام نافذة الرموز لأشياء أخرى) ولكنه قد يحسن أيضًا تكاليفنا (عن طريق تقليل عدد الرموز التي نحتاج إلى إرسالها إلى النموذج في وقت الاستدلال).\n",
    "\n",
    "لضبط الدقيق أربع خطوات:\n",
    "1. إعداد بيانات التدريب وتحميلها.\n",
    "1. تشغيل مهمة التدريب للحصول على نموذج مضبوط بدقة.\n",
    "1. تقييم النموذج المضبوط بدقة والتكرار لتحسين الجودة.\n",
    "1. نشر النموذج المضبوط بدقة للاستدلال عند الرضا.\n",
    "\n",
    "لاحظ أن ليس كل نماذج الأساس تدعم الضبط الدقيق - [تحقق من وثائق OpenAI](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned?WT.mc_id=academic-105485-koreyst) لأحدث المعلومات. يمكنك أيضًا ضبط نموذج تم ضبطه مسبقًا بدقة. في هذا الدرس، سنستخدم `gpt-35-turbo` كنموذج الأساس المستهدف للضبط الدقيق.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### الخطوة 1.1: إعداد مجموعة البيانات الخاصة بك\n",
    "\n",
    "لنقم ببناء روبوت محادثة يساعدك على فهم الجدول الدوري للعناصر من خلال الإجابة على الأسئلة حول عنصر ما باستخدام ليمريك. في _هذا_ الدرس البسيط، سنقوم فقط بإنشاء مجموعة بيانات لتدريب النموذج مع بعض الأمثلة النموذجية للردود التي توضح التنسيق المتوقع للبيانات. في حالة الاستخدام الواقعي، ستحتاج إلى إنشاء مجموعة بيانات تحتوي على العديد من الأمثلة. قد تتمكن أيضًا من استخدام مجموعة بيانات مفتوحة (لمجال تطبيقك) إذا كانت موجودة، وإعادة تنسيقها لاستخدامها في التخصيص الدقيق.\n",
    "\n",
    "نظرًا لأننا نركز على `gpt-35-turbo` ونبحث عن استجابة ذات دور واحد (إكمال محادثة) يمكننا إنشاء أمثلة باستخدام [هذا التنسيق المقترح](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?WT.mc_id=academic-105485-koreyst) الذي يعكس متطلبات إكمال المحادثة من OpenAI. إذا كنت تتوقع محتوى محادثة متعدد الأدوار، فستستخدم [تنسيق المثال متعدد الأدوار](https://platform.openai.com/docs/guides/fine-tuning/multi-turn-chat-examples?WT.mc_id=academic-105485-koreyst) الذي يتضمن معلمة `weight` للإشارة إلى الرسائل التي يجب استخدامها (أو لا) في عملية التخصيص الدقيق.\n",
    "\n",
    "سنستخدم التنسيق الأبسط ذو الدور الواحد في درسنا هنا. البيانات بتنسيق [jsonl](https://jsonlines.org/?WT.mc_id=academic-105485-koreyst) مع سجل واحد لكل سطر، كل منها ممثل ككائن بصيغة JSON. المقتطف أدناه يعرض سجلين كمثال - انظر [training-data.jsonl](../../../../../18-fine-tuning/python/openai/training-data.jsonl) لمجموعة الأمثلة الكاملة (10 أمثلة) التي سنستخدمها في درس التخصيص الدقيق. **ملاحظة:** يجب تعريف كل سجل _في سطر واحد_ (ليس مقسماً عبر عدة أسطر كما هو معتاد في ملف JSON منسق)\n",
    "\n",
    "```json\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Gallium\"}, {\"role\": \"assistant\", \"content\": \"Gallium, oh gallium, so light - Melts in your hand, oh what a sight - At 86 degrees - Its liquid with ease - And in semiconductors, it's out of sight\"}]}\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Hydrogen\"}, {\"role\": \"assistant\", \"content\": \"Hydrogen, the first in the line - The lightest of all, so divine - It's in water, you see - And in stars, it's the key - The universe's most common sign\"}]}\n",
    "```\n",
    "\n",
    "في حالة الاستخدام الواقعي ستحتاج إلى مجموعة أمثلة أكبر بكثير للحصول على نتائج جيدة - سيكون المقابل هو بين جودة الردود والوقت/التكاليف للتخصيص الدقيق. نحن نستخدم مجموعة صغيرة حتى نتمكن من إكمال التخصيص بسرعة لتوضيح العملية. انظر [مثال OpenAI Cookbook هذا](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst) لدرس تخصيص دقيق أكثر تعقيدًا.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### الخطوة 1.2 رفع مجموعة البيانات الخاصة بك\n",
    "\n",
    "قم برفع البيانات باستخدام واجهة برمجة تطبيقات الملفات [كما هو موضح هنا](https://platform.openai.com/docs/guides/fine-tuning/upload-a-training-file). لاحظ أنه لتشغيل هذا الكود، يجب أن تكون قد قمت بالخطوات التالية أولاً:\n",
    " - تثبيت حزمة `openai` الخاصة بلغة بايثون (تأكد من استخدام إصدار >=0.28.0 للحصول على أحدث الميزات)\n",
    " - تعيين متغير البيئة `OPENAI_API_KEY` إلى مفتاح API الخاص بـ OpenAI\n",
    "لمعرفة المزيد، راجع [دليل الإعداد](./../../../00-course-setup/02-setup-local.md?WT.mc_id=academic-105485-koreyst) المقدم للدورة.\n",
    "\n",
    "الآن، قم بتشغيل الكود لإنشاء ملف للرفع من ملف JSONL المحلي الخاص بك.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-JdAJcagdOTG6ACNlFWzuzmyV', bytes=4021, created_at=1715566183, filename='training-data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
      "Training File ID: file-JdAJcagdOTG6ACNlFWzuzmyV\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_file = client.files.create(\n",
    "  file=open(\"./training-data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(ft_file)\n",
    "print(\"Training File ID: \" + ft_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### الخطوة 2.1: إنشاء مهمة الضبط الدقيق باستخدام SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', created_at=1715566184, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-EZ6ag0n0S6Zm8eV9BSWKmE6l', result_files=[], seed=830529052, status='validating_files', trained_tokens=None, training_file='file-JdAJcagdOTG6ACNlFWzuzmyV', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)\n",
      "Fine-tuning Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_filejob = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file.id, \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "print(ft_filejob)\n",
    "print(\"Fine-tuning Job ID: \" + ft_filejob.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### الخطوة 2.2: التحقق من حالة الوظيفة\n",
    "\n",
    "إليك بعض الأشياء التي يمكنك القيام بها باستخدام واجهة برمجة التطبيقات `client.fine_tuning.jobs`:\n",
    "- `client.fine_tuning.jobs.list(limit=<n>)` - سرد آخر n من وظائف الضبط الدقيق\n",
    "- `client.fine_tuning.jobs.retrieve(<job_id>)` - الحصول على تفاصيل وظيفة ضبط دقيق محددة\n",
    "- `client.fine_tuning.jobs.cancel(<job_id>)` - إلغاء وظيفة ضبط دقيق\n",
    "- `client.fine_tuning.jobs.list_events(fine_tuning_job_id=<job_id>, limit=<b>)` - سرد ما يصل إلى n من الأحداث من الوظيفة\n",
    "- `client.fine_tuning.jobs.create(model=\"gpt-35-turbo\", training_file=\"your-training-file.jsonl\", ...)`\n",
    "\n",
    "الخطوة الأولى من العملية هي _التحقق من صحة ملف التدريب_ للتأكد من أن البيانات في التنسيق الصحيح.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-GkWiDgZmOsuv4q5cSTEGscY6', created_at=1715566184, level='info', message='Validating training file: file-JdAJcagdOTG6ACNlFWzuzmyV', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-3899xdVTO3LN7Q7LkKLMJUnb', created_at=1715566184, level='info', message='Created fine-tuning job: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_filejob.id, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n",
      "Status: running\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "# Once the training data is validated\n",
    "# Track the job status to see if it is running and when it is complete\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(\"Trained Tokens:\", response.trained_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### الخطوة 2.3: تتبع الأحداث لمراقبة التقدم\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 85/100: training loss=0.14\n",
      "Step 86/100: training loss=0.00\n",
      "Step 87/100: training loss=0.00\n",
      "Step 88/100: training loss=0.07\n",
      "Step 89/100: training loss=0.00\n",
      "Step 90/100: training loss=0.00\n",
      "Step 91/100: training loss=0.00\n",
      "Step 92/100: training loss=0.00\n",
      "Step 93/100: training loss=0.00\n",
      "Step 94/100: training loss=0.00\n",
      "Step 95/100: training loss=0.08\n",
      "Step 96/100: training loss=0.05\n",
      "Step 97/100: training loss=0.00\n",
      "Step 98/100: training loss=0.00\n",
      "Step 99/100: training loss=0.00\n",
      "Step 100/100: training loss=0.00\n",
      "Checkpoint created at step 80 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyyF2:ckpt-step-80\n",
      "Checkpoint created at step 90 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyzhK:ckpt-step-90\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "# You can also track progress in a more granular way by checking for events\n",
    "# Refresh this code till you get the `The job has successfully completed` message\n",
    "response = client.fine_tuning.jobs.list_events(ft_filejob.id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### الخطوة 2.4: عرض الحالة في لوحة تحكم OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يمكنك أيضًا عرض الحالة بزيارة موقع OpenAI واستكشاف قسم _الضبط الدقيق_ في المنصة. سيُظهر لك هذا حالة الوظيفة الحالية، ويسمح لك أيضًا بتتبع تاريخ عمليات تنفيذ الوظائف السابقة. في هذه اللقطة، يمكنك أن ترى أن التنفيذ السابق فشل، وأن التشغيل الثاني نجح. للسياق، حدث هذا عندما استخدم التشغيل الأول ملف JSON يحتوي على سجلات بتنسيق غير صحيح - بمجرد إصلاحه، أكمل التشغيل الثاني بنجاح وجعل النموذج متاحًا للاستخدام.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/ar/fine-tuned-model-status.563271727bf7bfba.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يمكنك أيضًا عرض رسائل الحالة والقياسات عن طريق التمرير لأسفل أكثر في لوحة المعلومات المرئية كما هو موضح:\n",
    "\n",
    "| الرسائل | القياسات |\n",
    "|:---|:---|\n",
    "| ![Messages](../../../../../translated_images/ar/fine-tuned-messages-panel.4ed0c2da5ea1313b.webp) |  ![Metrics](../../../../../translated_images/ar/fine-tuned-metrics-panel.700d7e4995a65229.webp)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### الخطوة 3.1: استرجاع المعرف واختبار النموذج المحسن في الكود\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the identity of the fine-tuned model once ready\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "fine_tuned_model_id = response.fine_tuned_model\n",
    "print(\"Fine-tuned Model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Strontium, a metal so bright - It's in fireworks, a dazzling sight - It's in bones, you see - And in tea, it's the key - It's the fortieth, so pure, that's the right\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# You can then use that model to generate completions from the SDK as shown\n",
    "# Or you can load that model into the OpenAI Playground (in the UI) to validate it from there.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Elle, a factual chatbot that answers questions about elements in the periodic table with a limerick\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Strontium\"},\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### الخطوة 3.2: تحميل واختبار النموذج المُحسّن في الملعب\n",
    "\n",
    "يمكنك الآن اختبار النموذج المُحسّن بطريقتين. أولاً، يمكنك زيارة الملعب واستخدام قائمة النماذج المنسدلة لاختيار النموذج المُحسّن الجديد من بين الخيارات المدرجة. الخيار الآخر هو استخدام خيار \"الملعب\" المعروض في لوحة التخصيص (انظر لقطة الشاشة أعلاه) والذي يفتح العرض _المقارن_ التالي الذي يعرض إصدارات النموذج الأساسي والمُحسّن جنبًا إلى جنب لتقييم سريع.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/ar/fine-tuned-playground-compare.56e06f0ad8922016.webp)\n",
    "\n",
    "ببساطة املأ سياق النظام المستخدم في بيانات التدريب الخاصة بك وقدم سؤال الاختبار الخاص بك. ستلاحظ أن كلا الجانبين يتم تحديثهما بالسياق والسؤال المتطابقين. قم بتشغيل المقارنة وسترى الفرق في المخرجات بينهما. _لاحظ كيف يعرض النموذج المُحسّن الرد بالشكل الذي قدمته في أمثلتك بينما يتبع النموذج الأساسي ببساطة موجه النظام_.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/ar/fine-tuned-playground-launch.5a26495c983c6350.webp)\n",
    "\n",
    "ستلاحظ أن المقارنة توفر أيضًا عدد الرموز لكل نموذج، والوقت المستغرق للاستدلال. **هذا المثال المحدد بسيط ويهدف إلى إظهار العملية فقط وليس يعكس مجموعة بيانات أو سيناريو حقيقي**. قد تلاحظ أن كلا العينتين تظهران نفس عدد الرموز (سياق النظام وموجه المستخدم متطابقان) مع استغراق النموذج المُحسّن وقتًا أطول للاستدلال (النموذج المخصص).\n",
    "\n",
    "في السيناريوهات الواقعية، لن تستخدم مثالًا بسيطًا كهذا، بل ستقوم بالتخصيص مقابل بيانات حقيقية (مثل كتالوج المنتجات لخدمة العملاء) حيث ستكون جودة الاستجابة أكثر وضوحًا. في _ذلك_ السياق، الحصول على جودة استجابة معادلة باستخدام النموذج الأساسي سيتطلب هندسة موجه مخصصة أكثر مما سيزيد من استخدام الرموز وربما الوقت المتعلق بمعالجة الاستدلال. _لتجربة ذلك، اطلع على أمثلة التخصيص في كتاب وصفات OpenAI للبدء._\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**إخلاء المسؤولية**:  \nتمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق به. للمعلومات الهامة، يُنصح بالاعتماد على الترجمة البشرية المهنية. نحن غير مسؤولين عن أي سوء فهم أو تفسير ناتج عن استخدام هذه الترجمة.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "1725725c956564056baf895e6ca92aa5",
   "translation_date": "2025-12-19T08:55:13+00:00",
   "source_file": "18-fine-tuning/python/openai/oai-assignment.ipynb",
   "language_code": "ar"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}