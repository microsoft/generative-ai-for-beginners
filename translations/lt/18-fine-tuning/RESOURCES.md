<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2f423d1402f71ca3869ec135bb77d16",
  "translation_date": "2025-08-25T12:43:54+00:00",
  "source_file": "18-fine-tuning/RESOURCES.md",
  "language_code": "lt"
}
-->
# Savarankiško Mokymosi Ištekliai

Pamoka buvo sukurta naudojant pagrindinius OpenAI ir Azure OpenAI šaltinius kaip terminologijos ir mokymų pagrindą. Čia pateikiamas nebaigtinis sąrašas, kuris gali būti naudingas jūsų savarankiško mokymosi kelionėje.

## 1. Pagrindiniai Ištekliai

| Pavadinimas/Nuoroda                                                                                                                                                                                                                   | Aprašymas                                                                                                                                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning with OpenAI Models](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                       | Fine-tuning metodas pagerina few-shot mokymąsi, nes leidžia treniruoti modelį su daug daugiau pavyzdžių nei telpa į promptą – taip sumažinamos išlaidos, gerinama atsakymų kokybė ir užtikrinamas mažesnis vėlavimas. **Susipažinkite su OpenAI fine-tuning apžvalga.**                                                                                    |
| [What is Fine-Tuning with Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                   | Sužinokite, **kas yra fine-tuning (sąvoka)**, kodėl verta apie tai pagalvoti (problemos motyvacija), kokius duomenis naudoti (mokymui) ir kaip vertinti kokybę.                                                                                                                                                                           |
| [Customize a model with fine-tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Azure OpenAI Service leidžia pritaikyti modelius pagal jūsų duomenų rinkinius naudojant fine-tuning. Sužinokite, **kaip vyksta fine-tuning procesas** su Azure AI Studio, Python SDK ar REST API.                                                                                                                                |
| [Recommendations for LLM fine-tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                    | LLM modeliai gali nepakankamai gerai veikti tam tikrose srityse, užduotyse ar su specifiniais duomenų rinkiniais, arba pateikti netikslius ar klaidinančius rezultatus. **Kada verta apsvarstyti fine-tuning** kaip galimą sprendimą?                                                                                                                                  |
| [Continuous Fine Tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)             | Nuolatinis fine-tuning – tai iteracinis procesas, kai jau patobulintas modelis pasirenkamas kaip bazinis ir **toliau tobulinamas** su naujais mokymo pavyzdžiais.                                                                                                                                                     |
| [Fine-tuning and function calling](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                       | Modelio fine-tuning **su funkcijų kvietimo pavyzdžiais** gali pagerinti rezultatus – atsakymai bus tikslesni, nuoseklesni, vienodos struktūros ir sumažės kaštai.                                                                                                                                        |
| [Fine-tuning Models: Azure OpenAI Guidance](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                        | Šioje lentelėje rasite, **kokius modelius galima tobulinti** Azure OpenAI, kokiuose regionuose jie prieinami, jų token limitus ir mokymo duomenų galiojimo datas.                                                                                                                            |
| [To Fine Tune or Not To Fine Tune? That is the Question](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                      | Šioje 30 min. **2023 m. spalio** AI Show laidoje aptariami fine-tuning privalumai, trūkumai ir praktiniai patarimai, padedantys priimti sprendimą.                                                                                                                                                                                        |
| [Getting Started With LLM Fine-Tuning](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                             | Šis **AI Playbook** šaltinis padės suprasti duomenų reikalavimus, formatavimą, hiperparametrų derinimą ir iššūkius/limituotes, kuriuos verta žinoti.                                                                                                                                                                         |
| **Pamoka**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                  | Sužinokite, kaip sukurti pavyzdinį fine-tuning duomenų rinkinį, pasiruošti fine-tuning, sukurti fine-tuning užduotį ir diegti patobulintą modelį Azure aplinkoje.                                                                                                                                                                                    |
| **Pamoka**: [Fine-tune a Llama 2 model in Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                      | Azure AI Studio leidžia pritaikyti didelius kalbos modelius pagal savo duomenis _naudojant UI pagrįstą procesą, tinkantį mažai koduojantiems kūrėjams_. Peržiūrėkite šį pavyzdį.                                                                                                                                                               |
| **Pamoka**:[Fine-tune Hugging Face models for a single GPU on Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)               | Šiame straipsnyje aprašoma, kaip tobulinti Hugging Face modelį naudojant Hugging Face transformers biblioteką vienoje GPU su Azure DataBricks ir Hugging Face Trainer bibliotekomis.                                                                                                                                                |
| **Mokymai:** [Fine-tune a foundation model with Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)         | Azure Machine Learning modelių kataloge rasite daug atviro kodo modelių, kuriuos galite pritaikyti savo užduočiai. Išbandykite šį modulį iš [AzureML Generative AI Learning Path](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Pamoka:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                | GPT-3.5 ar GPT-4 modelių tobulinimas Microsoft Azure naudojant W&B leidžia detaliai stebėti ir analizuoti modelio veikimą. Šiame gide pateikiami konkretūs žingsniai ir funkcijos, pritaikytos Azure OpenAI, remiantis OpenAI Fine-Tuning gidu.                                                                         |
|                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                               |

## 2. Papildomi Ištekliai

Šioje dalyje pateikiami papildomi šaltiniai, kuriuos verta išnagrinėti, nors šioje pamokoje jų nespėjome aptarti. Jie gali būti įtraukti į būsimą pamoką arba pasiūlyti kaip papildoma užduotis vėliau. Kol kas naudokite juos gilindami savo žinias ir įgūdžius šioje srityje.

| Pavadinimas/Nuoroda                                                                                                                                                                                                            | Aprašymas                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Data preparation and analysis for chat model fine-tuning](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                      | Šis notebook skirtas paruošti ir analizuoti pokalbių duomenų rinkinį, naudojamą modelio tobulinimui. Jis tikrina formatavimo klaidas, pateikia pagrindinę statistiką ir apskaičiuoja tokenų kiekį, kad įvertintumėte fine-tuning kainą. Žiūrėkite: [Fine-tuning method for gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                   |
| **OpenAI Cookbook**: [Fine-Tuning for Retrieval Augmented Generation (RAG) with Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Šio notebook tikslas – parodyti, kaip išsamiai tobulinti OpenAI modelius Retrieval Augmented Generation (RAG) užduotims. Taip pat bus integruojamas Qdrant ir Few-Shot Learning, siekiant pagerinti modelio veikimą ir sumažinti netikslumų kiekį.                                                                                                                                                                                                                                                                |
| **OpenAI Cookbook**: [Fine-tuning GPT with Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) – tai AI kūrėjų platforma su įrankiais modelių mokymui, tobulinimui ir bazinių modelių naudojimui. Pirmiausia perskaitykite jų [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst) gidą, tada išbandykite Cookbook užduotį.                                                                                                                                                                                                                  |
| **Community Tutorial** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - fine-tuning for Small Language Models                                                   | Susipažinkite su [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), nauju Microsoft mažuoju modeliu, kuris stebina savo galia ir kompaktiškumu. Šioje pamokoje sužinosite, kaip tobulinti Phi-2, susikurti unikalų duomenų rinkinį ir atlikti fine-tuning naudojant QLoRA.                                                                                                                                                                       |
| **Hugging Face Tutorial** [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Šiame tinklaraščio įraše žingsnis po žingsnio parodoma, kaip tobulinti atvirus LLM modelius naudojant Hugging Face TRL, Transformers ir duomenų rinkinius 2024 metais. Apibrėžiate naudojimo atvejį, pasiruošiate aplinką, paruošiate duomenų rinkinį, tobulinate modelį, testuojate ir diegiate į produkciją.                                                                                                                                                                                                                                                                |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                            | Leidžia greičiau ir paprasčiau treniruoti bei diegti [pažangiausius mašininio mokymosi modelius](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Repo turi Colab pamokas su YouTube vaizdo įrašais, skirtas fine-tuning. **Atspindi naujausią [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst) atnaujinimą**. Skaitykite [AutoTrain dokumentaciją](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

---

**Atsakomybės atsisakymas**:  
Šis dokumentas buvo išverstas naudojant dirbtinio intelekto vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Svarbios informacijos atveju rekomenduojame profesionalų žmogaus vertimą. Mes neatsakome už nesusipratimus ar neteisingą interpretaciją, kilusią dėl šio vertimo naudojimo.