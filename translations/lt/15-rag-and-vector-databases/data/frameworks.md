<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-08-25T12:47:04+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "lt"
}
-->
# NeuroniniÅ³ tinklÅ³ karkasai

Kaip jau suÅ¾inojome, norint efektyviai treniruoti neuroninius tinklus, reikia dviejÅ³ dalykÅ³:

* Dirbti su tenzoriais, pvz., dauginti, sudÄ—ti ir skaiÄiuoti tam tikras funkcijas, tokias kaip sigmoidinÄ— ar softmax
* SkaiÄiuoti visÅ³ iÅ¡raiÅ¡kÅ³ gradientus, kad bÅ«tÅ³ galima atlikti gradientinÄ¯ nusileidimÄ… optimizavimui

Nors `numpy` biblioteka gali atlikti pirmÄ…jÄ… dalÄ¯, mums reikia mechanizmo gradientams skaiÄiuoti. MÅ«sÅ³ ankstesniame karkase, kurÄ¯ kÅ«rÄ—me praÄ—jusioje dalyje, visus iÅ¡vestiniÅ³ funkcijÅ³ skaiÄiavimus turÄ—jome programuoti rankiniu bÅ«du `backward` metode, kuris atlieka atgalinÄ¯ sklidimÄ…. Idealiu atveju, karkasas turÄ—tÅ³ suteikti galimybÄ™ skaiÄiuoti *bet kurios iÅ¡raiÅ¡kos* gradientus, kuriÄ… galime apibrÄ—Å¾ti.

Kitas svarbus dalykas â€“ galimybÄ— atlikti skaiÄiavimus GPU ar kitose specializuotose skaiÄiavimo Ä¯renginiuose, pvz., TPU. GiliÅ³jÅ³ neuroniniÅ³ tinklÅ³ mokymui reikia *labai daug* skaiÄiavimÅ³, todÄ—l galimybÄ— juos paralelizuoti GPU yra labai svarbi.

> âœ… Terminas â€paralelizuotiâ€œ reiÅ¡kia paskirstyti skaiÄiavimus per kelis Ä¯renginius.

Å iuo metu du populiariausi neuroniniÅ³ tinklÅ³ karkasai yra: TensorFlow ir PyTorch. Abu jie suteikia Å¾emo lygio API darbui su tenzoriais tiek CPU, tiek GPU. Ant Å¾emo lygio API yra ir aukÅ¡tesnio lygio API, atitinkamai vadinami Keras ir PyTorch Lightning.

Low-Level API | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
High-level API| Keras| Pytorch

**Å½emo lygio API** abiejuose karkasuose leidÅ¾ia kurti vadinamuosius **skaiÄiavimÅ³ grafus**. Å is grafas apibrÄ—Å¾ia, kaip apskaiÄiuoti iÅ¡vestÄ¯ (daÅ¾niausiai nuostoliÅ³ funkcijÄ…) su duotais Ä¯vesties parametrais ir gali bÅ«ti perduotas skaiÄiavimui GPU, jei jis prieinamas. Yra funkcijos, leidÅ¾ianÄios diferencijuoti Å¡Ä¯ skaiÄiavimÅ³ grafÄ… ir apskaiÄiuoti gradientus, kuriuos vÄ—liau galima naudoti modelio parametrÅ³ optimizavimui.

**AukÅ¡to lygio API** daÅ¾niausiai traktuoja neuroninius tinklus kaip **sluoksniÅ³ sekÄ…** ir leidÅ¾ia daug lengviau konstruoti daugumÄ… neuroniniÅ³ tinklÅ³. Modelio mokymas paprastai reikalauja paruoÅ¡ti duomenis ir tada iÅ¡kviesti `fit` funkcijÄ….

AukÅ¡to lygio API leidÅ¾ia labai greitai sukurti tipinius neuroninius tinklus, nesirÅ«pinant daugybe detaliÅ³. Tuo paÄiu metu Å¾emo lygio API suteikia daug daugiau kontrolÄ—s mokymo procesui, todÄ—l jie daÅ¾nai naudojami tyrimuose, kai dirbama su naujomis neuroniniÅ³ tinklÅ³ architektÅ«romis.

Taip pat svarbu suprasti, kad abi API galima naudoti kartu, pvz., galite sukurti savo tinklo sluoksnio architektÅ«rÄ… naudodami Å¾emo lygio API ir tada naudoti jÄ… didesniame tinkle, sukurtame ir apmokytame su aukÅ¡to lygio API. Arba galite apibrÄ—Å¾ti tinklÄ… kaip sluoksniÅ³ sekÄ… su aukÅ¡to lygio API ir tada naudoti savo Å¾emo lygio mokymo ciklÄ… optimizavimui. Abi API remiasi tais paÄiais pagrindiniais principais ir yra sukurtos taip, kad gerai veiktÅ³ kartu.

## Mokymasis

Å iame kurse daugumÄ… turinio pateikiame tiek PyTorch, tiek TensorFlow karkasams. Galite pasirinkti jums patinkantÄ¯ karkasÄ… ir dirbti tik su atitinkamais uÅ¾raÅ¡ais. Jei nesate tikri, kurÄ¯ karkasÄ… pasirinkti, pasiskaitykite diskusijas internete apie **PyTorch vs. TensorFlow**. Taip pat galite pasiÅ¾iÅ«rÄ—ti Ä¯ abu karkasus, kad geriau suprastumÄ—te skirtumus.

Kur Ä¯manoma, naudosime aukÅ¡to lygio API dÄ—l paprastumo. TaÄiau manome, kad svarbu suprasti, kaip neuroniniai tinklai veikia nuo pagrindÅ³, todÄ—l pradÅ¾ioje dirbsime su Å¾emo lygio API ir tenzoriais. Visgi, jei norite greitai pradÄ—ti ir nenorite gilintis Ä¯ Å¡ias detales, galite jas praleisti ir iÅ¡kart pereiti prie aukÅ¡to lygio API uÅ¾raÅ¡Å³.

## âœï¸ Pratimai: Karkasai

TÄ™skite mokymÄ…si Å¡iuose uÅ¾raÅ¡uose:

Low-Level API | TensorFlow+Keras Notebook | PyTorch
--------------|-------------------------------------|--------------------------------
High-level API| Keras | *PyTorch Lightning*

IÅ¡mokÄ™ dirbti su karkasais, prisiminkime, kas yra per didelis pritaikymas (overfitting).

# Per didelis pritaikymas (Overfitting)

Per didelis pritaikymas yra labai svarbi sÄ…voka maÅ¡ininio mokymosi srityje, ir labai svarbu jÄ… suprasti teisingai!

Apsvarstykite Å¡iÄ… uÅ¾duotÄ¯ â€“ aproksimuoti 5 taÅ¡kus (grafikuose Å¾ymimi â€xâ€œ):

!linear | overfit
-------------------------|--------------------------
**Tiesinis modelis, 2 parametrai** | **Netiesinis modelis, 7 parametrai**
Mokymo klaida = 5.3 | Mokymo klaida = 0
Validacijos klaida = 5.1 | Validacijos klaida = 20

* KairÄ—je matome gerÄ… tiesÄ—s aproksimacijÄ…. Kadangi parametrÅ³ skaiÄius tinkamas, modelis teisingai supranta taÅ¡kÅ³ pasiskirstymÄ….
* DeÅ¡inÄ—je modelis per stiprus. Kadangi turime tik 5 taÅ¡kus, o modelis turi 7 parametrus, jis gali prisitaikyti taip, kad praeitÅ³ per visus taÅ¡kus, todÄ—l mokymo klaida tampa 0. TaÄiau tai neleidÅ¾ia modeliui suprasti tikrojo duomenÅ³ dÄ—sningumo, todÄ—l validacijos klaida labai didelÄ—.

Labai svarbu rasti tinkamÄ… pusiausvyrÄ… tarp modelio sudÄ—tingumo (parametrÅ³ skaiÄiaus) ir mokymo pavyzdÅ¾iÅ³ kiekio.

## KodÄ—l atsiranda per didelis pritaikymas

  * Per maÅ¾ai mokymo duomenÅ³
  * Per sudÄ—tingas modelis
  * Per daug triukÅ¡mo Ä¯vesties duomenyse

## Kaip atpaÅ¾inti per didelÄ¯ pritaikymÄ…

Kaip matote iÅ¡ aukÅ¡Äiau pateikto grafiko, per didelÄ¯ pritaikymÄ… galima atpaÅ¾inti pagal labai maÅ¾Ä… mokymo klaidÄ… ir didelÄ™ validacijos klaidÄ…. Paprastai mokymo metu matysime, kad tiek mokymo, tiek validacijos klaidos pradeda maÅ¾Ä—ti, o vÄ—liau validacijos klaida gali nustoti maÅ¾Ä—ti ir pradÄ—ti didÄ—ti. Tai bus Å¾enklas, kad vyksta per didelis pritaikymas, ir signalas, kad reikÄ—tÅ³ sustabdyti mokymÄ… (arba bent jau iÅ¡saugoti modelio bÅ«senÄ…).

overfitting

## Kaip iÅ¡vengti per didelio pritaikymo

Jei matote, kad vyksta per didelis pritaikymas, galite imtis Å¡iÅ³ veiksmÅ³:

 * Padidinti mokymo duomenÅ³ kiekÄ¯
 * SumaÅ¾inti modelio sudÄ—tingumÄ…
 * Naudoti reguliavimo metodus, pvz., Dropout, kurÄ¯ aptarsime vÄ—liau.

## Per didelis pritaikymas ir Å¡aliÅ¡kumo-variacijos kompromisas

Per didelis pritaikymas iÅ¡ tikrÅ³jÅ³ yra platesnÄ—s statistikos problemos, vadinamos Å¡aliÅ¡kumo-variacijos kompromisu, atvejis. Jei paÅ¾velgsime Ä¯ galimus mÅ«sÅ³ modelio klaidÅ³ Å¡altinius, galime iÅ¡skirti du klaidÅ³ tipus:

* **Å aliÅ¡kumo klaidos** atsiranda, kai mÅ«sÅ³ algoritmas nesugeba teisingai atspindÄ—ti ryÅ¡io tarp mokymo duomenÅ³. Tai gali bÅ«ti dÄ—l to, kad modelis per silpnas (**per maÅ¾as pritaikymas**, underfitting).
* **Variacijos klaidos** atsiranda, kai modelis pradeda atkartoti triukÅ¡mÄ… Ä¯vesties duomenyse, o ne tikrÄ…jÄ¯ ryÅ¡Ä¯ (**per didelis pritaikymas**, overfitting).

Mokymo metu Å¡aliÅ¡kumo klaida maÅ¾Ä—ja (modelis mokosi aproksimuoti duomenis), o variacijos klaida didÄ—ja. Svarbu sustabdyti mokymÄ… â€“ arba rankiniu bÅ«du (pastebÄ—jus per didelÄ¯ pritaikymÄ…), arba automatiÅ¡kai (Ä¯vedant reguliavimÄ…) â€“ kad iÅ¡vengtume per didelio pritaikymo.

## IÅ¡vada

Å ioje pamokoje suÅ¾inojote apie skirtingus dviejÅ³ populiariausiÅ³ AI karkasÅ³ â€“ TensorFlow ir PyTorch â€“ API lygius. Taip pat susipaÅ¾inote su labai svarbia tema â€“ per dideliu pritaikymu.

## ğŸš€ IÅ¡Å¡Å«kis

Papildomuose uÅ¾raÅ¡uose apaÄioje rasite â€uÅ¾duotisâ€œ; pereikite per uÅ¾raÅ¡us ir atlikite uÅ¾duotis.

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

PasidomÄ—kite Å¡iomis temomis:

- TensorFlow
- PyTorch
- Per didelis pritaikymas (Overfitting)

UÅ¾duokite sau Å¡iuos klausimus:

- Kuo skiriasi TensorFlow ir PyTorch?
- Kuo skiriasi per didelis pritaikymas ir per maÅ¾as pritaikymas?

## UÅ¾duotis

Å ioje laboratorijoje turite iÅ¡sprÄ™sti dvi klasifikavimo uÅ¾duotis, naudodami vieno ir keliÅ³ sluoksniÅ³ pilnai sujungtus tinklus su PyTorch arba TensorFlow.

---

**AtsakomybÄ—s atsisakymas**:  
Å is dokumentas buvo iÅ¡verstas naudojant dirbtinio intelekto vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Svarbios informacijos atveju rekomenduojame profesionalÅ³ Å¾mogaus vertimÄ…. Mes neatsakome uÅ¾ nesusipratimus ar neteisingÄ… interpretavimÄ…, kilusÄ¯ dÄ—l Å¡io vertimo naudojimo.