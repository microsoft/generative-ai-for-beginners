<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-08-25T12:47:28+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "lt"
}
-->
# Ä®vadas Ä¯ neuroninius tinklus. Daugiasluoksnis perceptronas

Ankstesniame skyriuje susipaÅ¾inote su paÄiu paprasÄiausiu neuroninio tinklo modeliu â€“ vieno sluoksnio perceptronu, kuris yra linijinis dviejÅ³ klasiÅ³ klasifikatorius.

Å iame skyriuje iÅ¡plÄ—sime Å¡Ä¯ modelÄ¯ Ä¯ lankstesnÄ™ sistemÄ…, kuri leis mums:

* atlikti **daugiaklasÄ™ klasifikacijÄ…** be dviejÅ³ klasiÅ³ atskyrimo
* sprÄ™sti **regresijos uÅ¾davinius** be klasifikacijos
* atskirti klases, kurios nÄ—ra linijiÅ¡kai atskiriamos

Taip pat sukursime savo modulinÄ™ sistemÄ… Python kalba, kuri leis konstruoti Ä¯vairias neuroniniÅ³ tinklÅ³ architektÅ«ras.

## MaÅ¡ininio mokymosi formalizavimas

PradÄ—kime nuo maÅ¡ininio mokymosi uÅ¾davinio formalizavimo. Tarkime, turime mokymo duomenÅ³ rinkinÄ¯ **X** su Å¾ymÄ—mis **Y**, ir turime sukurti modelÄ¯ *f*, kuris darytÅ³ kuo tikslesnes prognozes. PrognoziÅ³ kokybÄ— matuojama **nuostoliÅ³ funkcija** â„’. DaÅ¾niausiai naudojamos Å¡ios nuostoliÅ³ funkcijos:

* Regresijos uÅ¾daviniui, kai reikia prognozuoti skaiÄiÅ³, galime naudoti **absoliutinÄ™ paklaidÄ…** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| arba **kvadratinÄ™ paklaidÄ…** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Klasifikacijai naudojame **0-1 nuostolÄ¯** (kuris iÅ¡ esmÄ—s atitinka modelio **tikslumÄ…**), arba **logistinÄ¯ nuostolÄ¯**.

Vieno sluoksnio perceptronui funkcija *f* buvo apibrÄ—Å¾ta kaip linijinÄ— funkcija *f(x)=wx+b* (Äia *w* â€“ svoriÅ³ matrica, *x* â€“ Ä¯vesties poÅ¾ymiÅ³ vektorius, o *b* â€“ poslinkio vektorius). Skirtingoms neuroniniÅ³ tinklÅ³ architektÅ«roms Å¡i funkcija gali bÅ«ti sudÄ—tingesnÄ—.

> Klasifikacijos atveju daÅ¾nai norime, kad tinklo iÅ¡vestis bÅ«tÅ³ atitinkamÅ³ klasiÅ³ tikimybÄ—s. Norint paversti bet kokius skaiÄius Ä¯ tikimybes (pvz., normalizuoti iÅ¡vestÄ¯), daÅ¾nai naudojama **softmax** funkcija Ïƒ, ir tada funkcija *f* tampa *f(x)=Ïƒ(wx+b)*

AukÅ¡Äiau pateiktoje *f* apibrÄ—Å¾tyje *w* ir *b* vadinami **parametrais** Î¸=âŸ¨*w,b*âŸ©. TurÄ—dami duomenÅ³ rinkinÄ¯ âŸ¨**X**,**Y**âŸ©, galime apskaiÄiuoti bendrÄ… klaidÄ… visame rinkinyje kaip parametrÅ³ Î¸ funkcijÄ….

> âœ… **Neuroninio tinklo mokymo tikslas â€“ sumaÅ¾inti klaidÄ… keiÄiant parametrus Î¸**

## Gradientinis nuolydÅ¾io maÅ¾inimas

Yra gerai Å¾inomas funkcijÅ³ optimizavimo metodas, vadinamas **gradientiniu nuolydÅ¾io maÅ¾inimu**. Jo esmÄ— â€“ galime apskaiÄiuoti nuostoliÅ³ funkcijos iÅ¡vestinÄ™ (daugiamatÄ—je erdvÄ—je vadinamÄ… **gradientu**) pagal parametrus ir keisti parametrus taip, kad klaida maÅ¾Ä—tÅ³. Tai galima formalizuoti taip:

* Inicializuojame parametrus atsitiktinÄ—mis reikÅ¡mÄ—mis w<sup>(0)</sup>, b<sup>(0)</sup>
* Kartojame Å¡Ä¯ Å¾ingsnÄ¯ daug kartÅ³:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Mokymo metu optimizavimo Å¾ingsniai turÄ—tÅ³ bÅ«ti skaiÄiuojami pagal visÄ… duomenÅ³ rinkinÄ¯ (prisiminkite, kad nuostolis skaiÄiuojamas kaip suma per visus mokymo pavyzdÅ¾ius). TaÄiau realybÄ—je imame maÅ¾as duomenÅ³ dalis, vadinamas **minipartijomis** (minibatches), ir gradientus skaiÄiuojame pagal duomenÅ³ pogrupÄ¯. Kadangi kiekvienÄ… kartÄ… pogrupis parenkamas atsitiktinai, Å¡is metodas vadinamas **stochastiniu gradientiniu nuolydÅ¾io maÅ¾inimu** (SGD).

## Daugiasluoksniai perceptronai ir atgalinis sklidimas

Vieno sluoksnio tinklas, kaip matÄ—me, gali klasifikuoti tik linijiÅ¡kai atskiriamas klases. NorÄ—dami sukurti sudÄ—tingesnÄ¯ modelÄ¯, galime sujungti kelis tinklo sluoksnius. MatematiÅ¡kai tai reiÅ¡kia, kad funkcija *f* bus sudÄ—tingesnÄ—s formos ir bus skaiÄiuojama keliais Å¾ingsniais:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

ÄŒia Î± yra **nelinijinÄ— aktyvacijos funkcija**, Ïƒ â€“ softmax funkcija, o parametrai Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradientinio nuolydÅ¾io maÅ¾inimo algoritmas iÅ¡lieka toks pats, taÄiau gradientus apskaiÄiuoti tampa sudÄ—tingiau. Pagal grandinÄ—s iÅ¡vestinÄ—s taisyklÄ™ galime iÅ¡vestines apskaiÄiuoti taip:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… GrandinÄ—s iÅ¡vestinÄ—s taisyklÄ— naudojama nuostoliÅ³ funkcijos iÅ¡vestinÄ—ms pagal parametrus apskaiÄiuoti.

Atkreipkite dÄ—mesÄ¯, kad visÅ³ Å¡iÅ³ iÅ¡raiÅ¡kÅ³ kairiausioji dalis yra ta pati, todÄ—l galime efektyviai skaiÄiuoti iÅ¡vestines pradedant nuo nuostoliÅ³ funkcijos ir einant â€atgalâ€œ per skaiÄiavimÅ³ grafÄ…. TodÄ—l daugiasluoksnio perceptrono mokymo metodas vadinamas **atgaliniu sklidimu** (angl. backpropagation) arba 'backprop'.



> TODO: paveikslÄ—lio Å¡altinis

> âœ… AtgalinÄ¯ sklidimÄ… detaliau nagrinÄ—sime mÅ«sÅ³ uÅ¾raÅ¡inÄ—je (notebook) pateiktame pavyzdyje.  

## IÅ¡vada

Å ioje pamokoje sukÅ«rÄ—me savo neuroniniÅ³ tinklÅ³ bibliotekÄ… ir panaudojome jÄ… paprastai dvimatÄ—s klasifikacijos uÅ¾duoÄiai.

## ğŸš€ IÅ¡Å¡Å«kis

Pridedamoje uÅ¾raÅ¡inÄ—je Ä¯gyvendinsite savo sistemÄ… daugiasluoksniÅ³ perceptronÅ³ kÅ«rimui ir mokymui. GalÄ—site detaliai pamatyti, kaip veikia Å¡iuolaikiniai neuroniniai tinklai.

Pereikite prie OwnFramework uÅ¾raÅ¡inÄ—s ir jÄ… iÅ¡sprÄ™skite.



## Kartojimas ir savarankiÅ¡kas mokymasis

Atgalinis sklidimas yra daÅ¾nai naudojamas algoritmas dirbtiniame intelekte ir maÅ¡ininio mokymosi srityje, todÄ—l verta jÄ¯ iÅ¡samiau iÅ¡studijuoti

## UÅ¾duotis

Å ioje laboratorijoje turite panaudoti Å¡ioje pamokoje sukurtÄ… sistemÄ… MNIST ranka raÅ¡ytÅ³ skaitmenÅ³ klasifikavimo uÅ¾daviniui iÅ¡sprÄ™sti.

* Instrukcijos
* UÅ¾raÅ¡inÄ—

---

**AtsakomybÄ—s atsisakymas**:  
Å is dokumentas buvo iÅ¡verstas naudojant dirbtinio intelekto vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Svarbios informacijos atveju rekomenduojame profesionalÅ³ Å¾mogaus vertimÄ…. Mes neatsakome uÅ¾ nesusipratimus ar neteisingÄ… interpretavimÄ…, kilusÄ¯ dÄ—l Å¡io vertimo naudojimo.