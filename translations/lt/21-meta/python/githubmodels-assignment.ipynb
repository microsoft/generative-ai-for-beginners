{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darbas su Meta šeimos modeliais\n",
    "\n",
    "## Įvadas\n",
    "\n",
    "Šioje pamokoje aptarsime:\n",
    "\n",
    "- Dviejų pagrindinių Meta šeimos modelių – Llama 3.1 ir Llama 3.2 – apžvalgą\n",
    "- Kiekvieno modelio naudojimo atvejus ir scenarijus\n",
    "- Kodo pavyzdį, parodantį kiekvieno modelio išskirtines savybes\n",
    "\n",
    "## Meta šeimos modeliai\n",
    "\n",
    "Šioje pamokoje susipažinsime su dviem Meta šeimos arba „Llama bandos“ modeliais – Llama 3.1 ir Llama 3.2\n",
    "\n",
    "Šie modeliai turi skirtingas versijas ir yra prieinami Github Modelų turgavietėje. Daugiau informacijos apie Github Models naudojimą [AI modelių prototipavimui](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Modelių variantai:\n",
    "- Llama 3.1 – 70B Instruct\n",
    "- Llama 3.1 – 405B Instruct\n",
    "- Llama 3.2 – 11B Vision Instruct\n",
    "- Llama 3.2 – 90B Vision Instruct\n",
    "\n",
    "*Pastaba: Llama 3 taip pat galima rasti Github Models, tačiau ši pamoka jos neapims*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Turėdama 405 milijardus parametrų, Llama 3.1 patenka į atvirojo kodo LLM kategoriją.\n",
    "\n",
    "Šis modelis yra ankstesnės Llama 3 versijos patobulinimas, siūlantis:\n",
    "\n",
    "- Didesnį konteksto langą – 128 tūkst. žodžių vietoj 8 tūkst.\n",
    "- Didesnį maksimalų išvesties žodžių skaičių – 4096 vietoj 2048\n",
    "- Geresnį daugiakalbį palaikymą – dėl padidėjusio mokymo duomenų kiekio\n",
    "\n",
    "Tai leidžia Llama 3.1 spręsti sudėtingesnes užduotis kuriant GenAI programas, įskaitant:\n",
    "- Natūralų funkcijų iškvietimą – galimybę kviesti išorinius įrankius ir funkcijas už LLM ribų\n",
    "- Geresnį RAG našumą – dėl didesnio konteksto lango\n",
    "- Sintetinių duomenų generavimą – galimybę kurti efektyvius duomenis užduotims, tokioms kaip modelio pritaikymas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gimtoji funkcijų iškvietimo galimybė\n",
    "\n",
    "Llama 3.1 buvo papildomai apmokyta, kad efektyviau naudotų funkcijų ar įrankių iškvietimus. Ji taip pat turi du įmontuotus įrankius, kuriuos modelis gali atpažinti kaip reikalingus naudoti pagal vartotojo užklausą. Šie įrankiai yra:\n",
    "\n",
    "- **Brave Search** – Gali būti naudojamas gauti naujausią informaciją, pavyzdžiui, orų prognozę, atliekant paiešką internete\n",
    "- **Wolfram Alpha** – Gali būti naudojamas sudėtingesniems matematiniams skaičiavimams, todėl nereikia rašyti savo funkcijų.\n",
    "\n",
    "Taip pat galite susikurti savo pasirinktinius įrankius, kuriuos LLM galės iškviesti.\n",
    "\n",
    "Žemiau pateiktame kodo pavyzdyje:\n",
    "\n",
    "- Sistemos užklausoje apibrėžiame galimus įrankius (brave_search, wolfram_alpha).\n",
    "- Išsiunčiame vartotojo užklausą, kuri klausia apie orus tam tikrame mieste.\n",
    "- LLM atsakys su įrankio iškvietimu į Brave Search įrankį, kuris atrodys taip: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Pastaba: Šiame pavyzdyje įrankis tik iškviečiamas, o jei norite gauti rezultatus, turėsite susikurti nemokamą paskyrą Brave API puslapyje ir apibrėžti pačią funkciją*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Nors Llama 3.1 yra didelis kalbos modelis, viena iš jo ribojimų yra multimodalumas. Tai reiškia, kad jis negali naudoti skirtingų įvesties tipų, pavyzdžiui, vaizdų, kaip užklausų ir pateikti atsakymų. Ši galimybė yra viena pagrindinių Llama 3.2 savybių. Kitos šios versijos naujovės:\n",
    "\n",
    "- Multimodalumas – gali apdoroti tiek tekstines, tiek vaizdines užklausas\n",
    "- Mažos ir vidutinės apimties variantai (11B ir 90B) – suteikia lankstumo diegiant modelį,\n",
    "- Tik tekstiniai variantai (1B ir 3B) – leidžia modelį naudoti kraštiniuose / mobiliuosiuose įrenginiuose ir užtikrina mažą delsą\n",
    "\n",
    "Multimodalinio palaikymo atsiradimas yra didelis žingsnis atvirojo kodo modelių pasaulyje. Žemiau pateiktame kodo pavyzdyje naudojamas tiek vaizdas, tiek tekstinė užklausa, kad Llama 3.2 90B pateiktų vaizdo analizę.\n",
    "\n",
    "### Multimodalinio palaikymas su Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mokymasis čia nesibaigia, tęskite kelionę\n",
    "\n",
    "Baigę šią pamoką, apsilankykite mūsų [Generatyvaus DI mokymosi kolekcijoje](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), kad toliau gilintumėte savo žinias apie generatyvųjį dirbtinį intelektą!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Atsakomybės atsisakymas**:  \nŠis dokumentas buvo išverstas naudojant dirbtinio intelekto vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, atkreipkite dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo pradinėje kalboje turėtų būti laikomas autoritetingu šaltiniu. Svarbiai informacijai rekomenduojame profesionalų žmogaus vertimą. Mes neprisiimame atsakomybės už bet kokius nesusipratimus ar neteisingą interpretavimą, kilusį naudojantis šiuo vertimu.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:51:12+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "lt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}