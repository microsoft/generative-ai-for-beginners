{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta 패밀리 모델로 빌드하기\n",
    "\n",
    "## 소개\n",
    "\n",
    "이 강의에서는 다음 내용을 다룹니다:\n",
    "\n",
    "- Meta 패밀리의 두 가지 주요 모델인 Llama 3.1과 Llama 3.2 살펴보기\n",
    "- 각 모델의 사용 사례와 적용 시나리오 이해하기\n",
    "- 각 모델의 고유한 기능을 보여주는 코드 예제\n",
    "\n",
    "## Meta 패밀리 모델\n",
    "\n",
    "이번 강의에서는 Meta 패밀리, 즉 \"Llama Herd\"의 두 가지 모델인 Llama 3.1과 Llama 3.2를 살펴봅니다.\n",
    "\n",
    "이 모델들은 다양한 버전으로 제공되며, Github Model 마켓플레이스에서 사용할 수 있습니다. Github Models를 활용해 [AI 모델로 프로토타입 만들기](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst)에 대한 자세한 내용도 참고하세요.\n",
    "\n",
    "모델 버전:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*참고: Llama 3도 Github Models에서 제공되지만, 이번 강의에서는 다루지 않습니다*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "4050억 개의 파라미터를 가진 Llama 3.1은 오픈 소스 LLM 범주에 속합니다.\n",
    "\n",
    "이 모델은 이전에 출시된 Llama 3에서 다음과 같은 업그레이드를 제공합니다:\n",
    "\n",
    "- 더 커진 컨텍스트 윈도우 - 128k 토큰(기존 8k 토큰 대비)\n",
    "- 더 많은 최대 출력 토큰 - 4096(기존 2048 대비)\n",
    "- 향상된 다국어 지원 - 학습 토큰 수 증가로 인한 효과\n",
    "\n",
    "이러한 기능 덕분에 Llama 3.1은 GenAI 애플리케이션을 구축할 때 더 복잡한 사용 사례를 처리할 수 있습니다. 예를 들어:\n",
    "- 네이티브 함수 호출 - LLM 워크플로우 외부의 외부 도구 및 함수 호출 가능\n",
    "- 향상된 RAG 성능 - 더 넓어진 컨텍스트 윈도우 덕분\n",
    "- 합성 데이터 생성 - 파인튜닝과 같은 작업에 효과적인 데이터를 생성할 수 있는 능력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이티브 함수 호출\n",
    "\n",
    "Llama 3.1은 함수나 도구 호출을 더 효과적으로 할 수 있도록 추가로 튜닝되었습니다. 또한, 모델이 사용자 프롬프트를 바탕으로 필요하다고 판단할 때 사용할 수 있는 두 가지 내장 도구가 있습니다. 이 도구들은 다음과 같습니다.\n",
    "\n",
    "- **Brave Search** - 웹 검색을 통해 날씨와 같은 최신 정보를 얻을 때 사용할 수 있습니다.\n",
    "- **Wolfram Alpha** - 복잡한 수학 계산을 할 때 사용할 수 있어, 직접 함수를 작성할 필요가 없습니다.\n",
    "\n",
    "또한, LLM이 호출할 수 있는 나만의 커스텀 도구를 만들 수도 있습니다.\n",
    "\n",
    "아래 코드 예시에서는:\n",
    "\n",
    "- 시스템 프롬프트에서 사용 가능한 도구(brave_search, wolfram_alpha)를 정의합니다.\n",
    "- 특정 도시의 날씨를 묻는 사용자 프롬프트를 보냅니다.\n",
    "- LLM은 Brave Search 도구를 호출하는 형태로 응답하며, 이는 다음과 같이 보입니다: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*참고: 이 예시는 도구 호출만 수행합니다. 결과를 얻으려면 Brave API 페이지에서 무료 계정을 만들고, 직접 함수를 정의해야 합니다.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Llama 3.1은 LLM임에도 불구하고 멀티모달리티라는 한계가 있습니다. 즉, 이미지를 비롯한 다양한 입력을 프롬프트로 사용하고 이에 대한 응답을 제공하는 기능이 부족합니다. 이 기능이 바로 Llama 3.2의 주요 특징 중 하나입니다. 주요 기능은 다음과 같습니다:\n",
    "\n",
    "- 멀티모달리티 - 텍스트와 이미지 프롬프트 모두를 평가할 수 있는 기능\n",
    "- 소형 및 중형 모델(11B와 90B) - 유연한 배포 옵션 제공\n",
    "- 텍스트 전용 모델(1B와 3B) - 엣지/모바일 기기에서 모델을 배포할 수 있고, 지연 시간이 낮음\n",
    "\n",
    "멀티모달 지원은 오픈소스 모델 분야에서 큰 진전입니다. 아래 코드 예시는 이미지와 텍스트 프롬프트를 함께 사용하여 Llama 3.2 90B로부터 이미지 분석을 받는 방법을 보여줍니다.\n",
    "\n",
    "### Llama 3.2의 멀티모달 지원\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습은 여기서 끝나지 않습니다, 여정을 계속하세요\n",
    "\n",
    "이 강의를 마친 후에는 [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)을 확인하여 생성형 AI에 대한 지식을 계속해서 쌓아보세요!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**면책 조항**:  \n이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서(원어)가 공식적인 기준임을 유의하시기 바랍니다. 중요한 정보의 경우, 전문 인간 번역을 권장합니다. 본 번역 사용으로 인해 발생하는 오해나 오역에 대해 당사는 책임을 지지 않습니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:39:35+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "ko"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}