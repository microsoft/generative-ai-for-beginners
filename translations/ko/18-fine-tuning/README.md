<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3772dcd23a98e2010f53ce8b9c583631",
  "translation_date": "2026-01-18T17:22:05+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "ko"
}
-->
[![Open Source Models](../../../../../translated_images/ko/18-lesson-banner.f30176815b1a5074.webp)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# LLM 세부 조정하기

대형 언어 모델을 사용하여 생성형 AI 애플리케이션을 구축하는 데는 새로운 과제가 있습니다. 핵심 문제는 주어진 사용자 요청에 대해 모델이 생성하는 콘텐츠의 응답 품질(정확성 및 적절성)을 보장하는 것입니다. 이전 수업들에서는 기존 모델에 대한 _프롬프트 입력을 수정하여_ 문제를 해결하려는 프롬프트 엔지니어링과 검색 증강 생성 같은 기법들을 다뤘습니다.

오늘 수업에서는 세 번째 기법인 **세부 조정(fine-tuning)** 에 대해 논의합니다. 이 방법은 추가 데이터를 사용하여 _모델 자체를 재학습_ 시켜 이 문제를 해결하려고 합니다. 자세히 살펴봅시다.

## 학습 목표

이 수업에서는 사전학습된 언어 모델을 위한 세부 조정 개념을 소개하고, 이 접근법의 이점과 과제를 탐구하며, 생성형 AI 모델의 성능을 향상시키기 위해 세부 조정을 언제, 어떻게 활용할지에 대한 지침을 제공합니다.

수업이 끝나면 다음 질문들에 답할 수 있어야 합니다:

- 언어 모델의 세부 조정이란 무엇인가?
- 세부 조정은 언제, 왜 유용한가?
- 사전 학습된 모델을 어떻게 세부 조정할 수 있나?
- 세부 조정의 한계는 무엇인가?

준비됐나요? 시작해 봅시다.

## 그림으로 보는 안내서

본격적으로 들어가기 전에 전체 개요를 보고 싶으신가요? 이 수업의 학습 여정을 도식화한 안내서를 확인하세요 - 세부 조정의 핵심 개념과 동기를 배우고, 세부 조정 작업의 과정과 모범 사례를 이해할 수 있습니다. 탐구할 만한 흥미로운 주제이니, 셀프 가이드 학습 여정을 지원하는 추가 링크를 보려면 [리소스](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) 페이지도 꼭 확인하세요!

![Illustrated Guide to Fine Tuning Language Models](../../../../../translated_images/ko/18-fine-tuning-sketchnote.11b21f9ec8a70346.webp)

## 언어 모델의 세부 조정이란 무엇인가?

대형 언어 모델은 정의상 인터넷을 포함한 다양한 출처에서 얻은 대량의 텍스트를 사용하여 _사전 학습_(pre-trained)됩니다. 이전 수업들에서 배운 바와 같이, 사용자 질문("프롬프트")에 대한 모델 응답 품질을 개선하기 위해서는 _프롬프트 엔지니어링_과 _검색 증강 생성_ 같은 기술이 필요합니다.

인기 있는 프롬프트 엔지니어링 기술은 모델이 응답에서 예상되는 내용을 더 잘 알 수 있도록 _명령_ (명시적 안내)을 제공하거나 _몇 가지 예시_ (암묵적 안내)를 주는 방식입니다. 이를 _few-shot 학습_이라 하지만 두 가지 한계가 있습니다:

- 모델 토큰 한계로 인해 제공할 수 있는 예시 수가 제한되고 효율성이 떨어집니다.
- 모델 토큰 비용 때문에 매 프롬프트마다 예시를 추가하는 것이 비쌀 수 있어 유연성이 제한됩니다.

세부 조정은 기계학습 시스템에서 일반적으로 사용하는 방법으로, 사전학습된 모델을 가져와서 특정 작업에 대한 성능을 향상시키기 위해 새로운 데이터로 다시 학습시키는 것입니다. 언어 모델의 경우, 우리는 특정 작업이나 애플리케이션 도메인에 대해 _특별히 선별된 예시 집합_ 을 사용하여 사전 학습된 모델을 세부 조정할 수 있으며, 이를 통해 해당 작업이나 도메인에 더 정확하고 적합한 **맞춤형 모델(custom model)** 을 만들 수 있습니다. 세부 조정의 부가적인 이점으로는 few-shot 학습에 필요한 예시 수를 줄여 토큰 사용량과 관련 비용도 감소시킬 수 있다는 점이 있습니다.

## 언제, 왜 모델을 세부 조정해야 할까?

_여기서_ 세부 조정은 원래 훈련 데이터에 없던 **새로운 데이터를 추가해서 재학습하는 감독 학습(Supervised) 세부 조정을 의미합니다. 이는 모델이 기존 데이터로만 다시 학습하지만 하이퍼파라미터를 다르게 설정하는 비감독 학습 세부 조정과는 다릅니다.

꼭 기억할 점은 세부 조정은 원하는 결과를 얻기 위해 일정 수준 이상의 전문 지식이 필요한 고급 기술이라는 것입니다. 잘못 수행하면 기대한 만큼 개선되지 않고 오히려 목표 도메인에서 모델 성능이 저하될 수 있습니다.

따라서 언어 모델 세부 조정을 "어떻게" 하는지 배우기 전에, 왜 이 경로를 선택해야 하는지 "왜", 언제 세부 조정을 시작해야 하는지 "언제"를 알아야 합니다. 먼저 다음 질문들을 스스로에게 물어보세요:

- **사용 사례**: 세부 조정을 하려는 _사용 사례_ 는 무엇인가? 현재 사전학습된 모델의 어떤 점을 개선하려는가?
- **대안**: 원하는 결과를 얻기 위해 _다른 기법_ 을 시도해봤는가? 이들을 기준선으로 활용하세요.
  - 프롬프트 엔지니어링: 관련 응답 예시가 포함된 few-shot 프롬프트를 시도하고 응답 품질을 평가하세요.
  - 검색 증강 생성: 데이터를 검색하여 검색 결과로 프롬프트를 증강하고 응답 품질을 평가하세요.
- **비용**: 세부 조정에 드는 비용을 파악했는가?
  - 조정 가능성 - 사전 학습된 모델이 세부 조정에 사용 가능한가?
  - 노력 - 학습 데이터 준비, 모델 평가 및 검토에 필요한 노력
  - 컴퓨팅 - 세부 조정 작업 실행 및 세부 조정 모델 배포 비용
  - 데이터 - 세부 조정 효과를 낼 만큼 충분한 품질의 예시 데이터에 접근 가능한가?
- **이점**: 세부 조정의 이점을 확인했는가?
  - 품질 - 세부 조정된 모델이 기준선 모델보다 뛰어난가?
  - 비용 - 프롬프트가 단순화되어 토큰 사용량이 줄었는가?
  - 확장성 - 기본 모델을 새로운 도메인에 재활용할 수 있는가?

이 질문들에 답하면 세부 조정이 사용 사례에 적합한 접근법인지 결정할 수 있습니다. 이상적으로는 이점이 비용보다 클 때만 선택하는 것이 바람직합니다. 진행하기로 결정했으면 이제 사전 학습된 모델을 _어떻게_ 세부 조정할지 생각할 시간입니다.

결정 과정에 대한 더 많은 통찰을 원한다면 [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)를 시청하세요.

## 사전 학습된 모델을 어떻게 세부 조정할 수 있나?

사전 학습된 모델을 세부 조정하기 위해서는 다음이 필요합니다:

- 세부 조정할 사전 학습된 모델
- 세부 조정에 사용할 데이터셋
- 세부 조정 작업을 실행할 학습 환경
- 세부 조정된 모델을 배포할 호스팅 환경

## 세부 조정 실제 사례

다음 리소스들은 특정 모델과 선별된 데이터셋을 사용하여 실제 예제를 단계별로 안내하는 튜토리얼을 제공합니다. 이 튜토리얼을 진행하려면 해당 제공자 계정과 관련 모델 및 데이터셋에 접근할 수 있어야 합니다.

| 제공자         | 튜토리얼                                                                                                                                                                   | 설명                                                                                                                                                                                                                                                                                                                                                                                                                              |
| -------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI         | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)              | 특정 도메인("레시피 어시스턴트")에 맞게 `gpt-35-turbo`를 세부 조정하는 방법을 배웁니다. 학습 데이터를 준비하고, 세부 조정 작업을 실행하고, 세부 조정된 모델을 추론에 사용하는 과정입니다.                                                                                                                                                                                                                                            |
| Azure OpenAI   | [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | Azure 상에서 `gpt-35-turbo-0613` 모델을 세부 조정하는 방법을 배우세요. 학습 데이터 생성 및 업로드, 세부 조정 작업 실행, 새로운 모델 배포 및 사용의 단계들을 다룹니다.                                                                                                                                                                                                                                                         |
| Hugging Face   | [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                           | Hugging Face의 [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) 라이브러리와 [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst)을 사용하여 _오픈 LLM_ (예: `CodeLlama 7B`)을 세부 조정하는 과정을 소개합니다. 관련 [데이터셋](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst)을 활용합니다.         |
|                |                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| 🤗 AutoTrain   | [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                     | AutoTrain (또는 AutoTrain Advanced)는 Hugging Face가 개발한 파이썬 라이브러리로, LLM 세부 조정을 포함해 다양한 작업에 대해 자동 세부 조정을 지원합니다. 노코드 솔루션이며 클라우드, Hugging Face Spaces, 로컬 환경에서 사용할 수 있습니다. 웹 기반 GUI, CLI 및 yaml 설정 파일을 통한 트레이닝을 지원합니다.                                                                                                                                                      |
|                |                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| 🦥 Unsloth     | [Fine-tuning LLMs with Unsloth](https://github.com/unslothai/unsloth)                                                                                                     | Unsloth는 LLM 세부 조정과 강화 학습(RL)을 지원하는 오픈 소스 프레임워크입니다. 준비된 [노트북](https://github.com/unslothai/notebooks)으로 로컬에서 학습, 평가 및 배포를 간편하게 할 수 있습니다. 텍스트 음성 변환(TTS), BERT 및 멀티모달 모델도 지원합니다. 시작하려면 단계별 [Fine-tuning LLMs Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide)를 참고하세요.                                                                                                  |
|                |                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
## 과제

위 튜토리얼 중 하나를 선택해 따라 해보세요. _이 저장소의 Jupyter Notebook에서 참고용으로 일부 튜토리얼을 재현할 수도 있으나, 최신 버전을 위해서는 원본 소스를 직접 이용하시기 바랍니다_.

## 훌륭합니다! 계속해서 학습하세요.

이 수업을 마치고 나면 [생성형 AI 학습 컬렉션](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)을 확인하여 생성형 AI 지식을 계속 향상시키세요!

축하합니다!! 이 과정의 v2 시리즈 마지막 수업을 완료했습니다! 학습과 구축을 계속 멈추지 마세요. \*\*이 주제에 대한 추가 제안을 보려면 [리소스](RESOURCES.md?WT.mc_id=academic-105485-koreyst) 페이지도 확인하세요.

v1 시리즈 수업도 과제와 개념이 더해져 업데이트 되었습니다. 잠시 시간을 내어 지식을 복습하고—커뮤니티를 위한 이 수업 개선에 도움을 주시려면 [질문과 피드백을 공유](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst)해주세요.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**면책 조항**:
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있음을 알려드립니다. 원본 문서의 원어가 권위 있는 출처로 간주되어야 합니다. 중요한 정보의 경우, 전문적인 사람 번역을 권장합니다. 이 번역의 사용으로 발생하는 오해나 잘못된 해석에 대해 당사는 어떠한 책임도 지지 않습니다.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->