{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open AI 모델 미세 조정\n",
    "\n",
    "이 노트북은 Open AI의 [미세 조정](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst) 문서에서 제공하는 최신 가이드를 기반으로 합니다.\n",
    "\n",
    "미세 조정은 특정 사용 사례나 시나리오와 관련된 추가 데이터와 컨텍스트로 재학습시켜 기초 모델의 성능을 향상시킵니다. _few shot learning_ 및 _retrieval augmented generation_과 같은 프롬프트 엔지니어링 기법을 사용하면 관련 데이터를 기본 프롬프트에 추가하여 품질을 개선할 수 있습니다. 그러나 이러한 접근법은 대상 기초 모델의 최대 토큰 창 크기에 의해 제한됩니다.\n",
    "\n",
    "미세 조정을 통해 필요한 데이터로 모델 자체를 효과적으로 재학습시키며(최대 토큰 창에 들어갈 수 있는 예제보다 훨씬 많은 예제를 사용할 수 있음) 추론 시점에 예제를 제공할 필요가 없는 _맞춤형_ 모델 버전을 배포합니다. 이는 프롬프트 설계의 효과성을 높일 뿐만 아니라(토큰 창을 다른 용도로 더 유연하게 사용할 수 있음) 추론 시 모델에 보내야 하는 토큰 수를 줄여 비용 절감에도 기여할 수 있습니다.\n",
    "\n",
    "미세 조정은 4단계로 이루어집니다:\n",
    "1. 학습 데이터를 준비하고 업로드합니다.\n",
    "1. 학습 작업을 실행하여 미세 조정된 모델을 얻습니다.\n",
    "1. 미세 조정된 모델을 평가하고 품질을 위해 반복합니다.\n",
    "1. 만족할 때 미세 조정된 모델을 추론용으로 배포합니다.\n",
    "\n",
    "모든 기초 모델이 미세 조정을 지원하는 것은 아니므로 최신 정보는 [OpenAI 문서](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned?WT.mc_id=academic-105485-koreyst)를 확인하세요. 이전에 미세 조정된 모델을 다시 미세 조정할 수도 있습니다. 이 튜토리얼에서는 미세 조정 대상 기초 모델로 `gpt-35-turbo`를 사용합니다.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1단계: 데이터셋 준비하기\n",
    "\n",
    "주기율표의 원소에 대해 질문하면 림릭(limerick)으로 답변해 주는 챗봇을 만들어 보겠습니다. 이 _간단한_ 튜토리얼에서는 모델을 학습시키기 위해 데이터의 예상 형식을 보여주는 몇 가지 샘플 응답 예제로 데이터셋을 만드는 것만 다룹니다. 실제 사용 사례에서는 훨씬 더 많은 예제로 데이터셋을 만들어야 합니다. 또한, 해당 응용 분야에 맞는 공개 데이터셋이 있다면 이를 사용하고 미세 조정에 맞게 재구성할 수도 있습니다.\n",
    "\n",
    "`gpt-35-turbo`에 집중하고 단일 턴 응답(챗 완료)을 목표로 하기 때문에 OpenAI 챗 완료 요구사항을 반영한 [이 권장 형식](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset?WT.mc_id=academic-105485-koreyst)을 사용해 예제를 만들 수 있습니다. 다중 턴 대화 내용을 예상한다면, 미세 조정 과정에서 어떤 메시지를 사용할지 신호를 보내는 `weight` 매개변수를 포함하는 [다중 턴 예제 형식](https://platform.openai.com/docs/guides/fine-tuning/multi-turn-chat-examples?WT.mc_id=academic-105485-koreyst)을 사용해야 합니다.\n",
    "\n",
    "여기서는 튜토리얼을 위해 더 간단한 단일 턴 형식을 사용합니다. 데이터는 한 줄에 하나의 레코드가 JSON 형식 객체로 표현된 [jsonl 형식](https://jsonlines.org/?WT.mc_id=academic-105485-koreyst)입니다. 아래 스니펫은 2개의 레코드를 샘플로 보여줍니다 - 전체 샘플 세트(10개 예제)는 [training-data.jsonl](../../../../../18-fine-tuning/python/openai/training-data.jsonl)에서 확인할 수 있습니다. **참고:** 각 레코드는 반드시 한 줄에 정의되어야 하며(일반적인 JSON 파일처럼 여러 줄로 나누지 말 것)\n",
    "\n",
    "```json\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Gallium\"}, {\"role\": \"assistant\", \"content\": \"Gallium, oh gallium, so light - Melts in your hand, oh what a sight - At 86 degrees - Its liquid with ease - And in semiconductors, it's out of sight\"}]}\n",
    "{ \"messages\": [{\"role\": \"system\", \"content\": \"Elle is a factual chatbot that answers questions about elements in the periodic table with a limerick\"}, {\"role\": \"user\", \"content\": \"Tell me about Hydrogen\"}, {\"role\": \"assistant\", \"content\": \"Hydrogen, the first in the line - The lightest of all, so divine - It's in water, you see - And in stars, it's the key - The universe's most common sign\"}]}\n",
    "```\n",
    "\n",
    "실제 사용 사례에서는 좋은 결과를 위해 훨씬 더 많은 예제 세트가 필요하며, 응답 품질과 미세 조정에 드는 시간/비용 간의 균형을 맞춰야 합니다. 우리는 과정을 빠르게 설명하기 위해 작은 세트를 사용해 미세 조정을 완료할 것입니다. 더 복잡한 미세 조정 튜토리얼은 [이 OpenAI Cookbook 예제](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)를 참고하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.2단계 데이터셋 업로드\n",
    "\n",
    "Files API를 사용하여 데이터를 업로드합니다 [여기 설명된 대로](https://platform.openai.com/docs/guides/fine-tuning/upload-a-training-file). 이 코드를 실행하려면 다음 단계를 먼저 완료해야 합니다:\n",
    " - `openai` Python 패키지를 설치했는지 확인하세요 (최신 기능을 위해 버전 >=0.28.0 사용 권장)\n",
    " - `OPENAI_API_KEY` 환경 변수를 OpenAI API 키로 설정하세요\n",
    "자세한 내용은 강좌에 제공된 [설정 가이드](./../../../00-course-setup/02-setup-local.md?WT.mc_id=academic-105485-koreyst)를 참조하세요.\n",
    "\n",
    "이제 로컬 JSONL 파일에서 업로드할 파일을 생성하는 코드를 실행하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-JdAJcagdOTG6ACNlFWzuzmyV', bytes=4021, created_at=1715566183, filename='training-data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
      "Training File ID: file-JdAJcagdOTG6ACNlFWzuzmyV\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_file = client.files.create(\n",
    "  file=open(\"./training-data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "print(ft_file)\n",
    "print(\"Training File ID: \" + ft_file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2.1: SDK로 파인튜닝 작업 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', created_at=1715566184, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-EZ6ag0n0S6Zm8eV9BSWKmE6l', result_files=[], seed=830529052, status='validating_files', trained_tokens=None, training_file='file-JdAJcagdOTG6ACNlFWzuzmyV', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)\n",
      "Fine-tuning Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "ft_filejob = client.fine_tuning.jobs.create(\n",
    "  training_file=ft_file.id, \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "print(ft_filejob)\n",
    "print(\"Fine-tuning Job ID: \" + ft_filejob.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2.2: 작업 상태 확인\n",
    "\n",
    "`client.fine_tuning.jobs` API로 할 수 있는 몇 가지 작업은 다음과 같습니다:\n",
    "- `client.fine_tuning.jobs.list(limit=<n>)` - 최근 n개의 파인튜닝 작업 목록 조회\n",
    "- `client.fine_tuning.jobs.retrieve(<job_id>)` - 특정 파인튜닝 작업의 세부 정보 조회\n",
    "- `client.fine_tuning.jobs.cancel(<job_id>)` - 파인튜닝 작업 취소\n",
    "- `client.fine_tuning.jobs.list_events(fine_tuning_job_id=<job_id>, limit=<b>)` - 작업에서 최대 n개의 이벤트 목록 조회\n",
    "- `client.fine_tuning.jobs.create(model=\"gpt-35-turbo\", training_file=\"your-training-file.jsonl\", ...)`\n",
    "\n",
    "프로세스의 첫 번째 단계는 _학습 파일을 검증_하여 데이터가 올바른 형식인지 확인하는 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-GkWiDgZmOsuv4q5cSTEGscY6', created_at=1715566184, level='info', message='Validating training file: file-JdAJcagdOTG6ACNlFWzuzmyV', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-3899xdVTO3LN7Q7LkKLMJUnb', created_at=1715566184, level='info', message='Created fine-tuning job: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=ft_filejob.id, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Usfb9RjasncaZ5Cjbuh1XSCh\n",
      "Status: running\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "# Once the training data is validated\n",
    "# Track the job status to see if it is running and when it is complete\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(\"Trained Tokens:\", response.trained_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3단계: 진행 상황을 모니터링하기 위한 이벤트 추적하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 85/100: training loss=0.14\n",
      "Step 86/100: training loss=0.00\n",
      "Step 87/100: training loss=0.00\n",
      "Step 88/100: training loss=0.07\n",
      "Step 89/100: training loss=0.00\n",
      "Step 90/100: training loss=0.00\n",
      "Step 91/100: training loss=0.00\n",
      "Step 92/100: training loss=0.00\n",
      "Step 93/100: training loss=0.00\n",
      "Step 94/100: training loss=0.00\n",
      "Step 95/100: training loss=0.08\n",
      "Step 96/100: training loss=0.05\n",
      "Step 97/100: training loss=0.00\n",
      "Step 98/100: training loss=0.00\n",
      "Step 99/100: training loss=0.00\n",
      "Step 100/100: training loss=0.00\n",
      "Checkpoint created at step 80 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyyF2:ckpt-step-80\n",
      "Checkpoint created at step 90 with Snapshot ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWyzhK:ckpt-step-90\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "# You can also track progress in a more granular way by checking for events\n",
    "# Refresh this code till you get the `The job has successfully completed` message\n",
    "response = client.fine_tuning.jobs.list_events(ft_filejob.id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4단계: OpenAI 대시보드에서 상태 보기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI 웹사이트를 방문하여 플랫폼의 _Fine-tuning_ 섹션을 탐색하면 상태를 확인할 수도 있습니다. 여기에서는 현재 작업의 상태를 보여주고 이전 작업 실행 기록도 추적할 수 있습니다. 이 스크린샷에서는 이전 실행이 실패했고 두 번째 실행이 성공한 것을 볼 수 있습니다. 참고로, 첫 번째 실행은 잘못 형식화된 레코드가 포함된 JSON 파일을 사용했을 때 발생했으며, 수정 후 두 번째 실행이 성공적으로 완료되어 모델을 사용할 수 있게 되었습니다.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fine-tuned-model-status.563271727bf7bfba7e3f73a201f8712fae3cea1c08f7c7f12ca469c06d234122.ko.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 시각적 대시보드에서 더 아래로 스크롤하여 상태 메시지와 메트릭을 확인할 수도 있습니다:\n",
    "\n",
    "| Messages | Metrics |\n",
    "|:---|:---|\n",
    "| ![Messages](../../../../../translated_images/fine-tuned-messages-panel.4ed0c2da5ea1313b3a706a66f66bf5007c379cd9219cfb74cb30c0b04b90c4c8.ko.png) |  ![Metrics](../../../../../translated_images/fine-tuned-metrics-panel.700d7e4995a652299584ab181536a6cfb67691a897a518b6c7a2aa0a17f1a30d.ko.png)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.1단계: ID 검색 및 코드에서 미세 조정된 모델 테스트하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ID: ft:gpt-3.5-turbo-0125:bitnbot::9OFWzNjz\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the identity of the fine-tuned model once ready\n",
    "response = client.fine_tuning.jobs.retrieve(ft_filejob.id)\n",
    "fine_tuned_model_id = response.fine_tuned_model\n",
    "print(\"Fine-tuned Model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Strontium, a metal so bright - It's in fireworks, a dazzling sight - It's in bones, you see - And in tea, it's the key - It's the fortieth, so pure, that's the right\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# You can then use that model to generate completions from the SDK as shown\n",
    "# Or you can load that model into the OpenAI Playground (in the UI) to validate it from there.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Elle, a factual chatbot that answers questions about elements in the periodic table with a limerick\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Strontium\"},\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.2단계: 플레이그라운드에서 미세 조정된 모델 로드 및 테스트\n",
    "\n",
    "이제 두 가지 방법으로 미세 조정된 모델을 테스트할 수 있습니다. 첫 번째는 플레이그라운드를 방문하여 모델 드롭다운에서 새로 미세 조정된 모델을 선택하는 방법입니다. 다른 방법은 미세 조정 패널에 표시된 \"Playground\" 옵션(위 스크린샷 참조)을 사용하는 것으로, 다음과 같은 _비교_ 뷰가 실행되어 기본 모델과 미세 조정된 모델 버전을 나란히 빠르게 평가할 수 있습니다.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fine-tuned-playground-compare.56e06f0ad8922016497d39ced3d84ea296eec89073503f2bf346ec9718f913b5.ko.png)\n",
    "\n",
    "학습 데이터에 사용된 시스템 컨텍스트를 입력하고 테스트 질문을 제공하기만 하면 됩니다. 양쪽 모두 동일한 컨텍스트와 질문으로 업데이트되는 것을 확인할 수 있습니다. 비교를 실행하면 출력 결과의 차이를 볼 수 있습니다. _미세 조정된 모델이 예제에서 제공한 형식으로 응답을 렌더링하는 반면, 기본 모델은 단순히 시스템 프롬프트를 따르는 점에 주목하세요_.\n",
    "\n",
    "![Fine-tuning job status](../../../../../translated_images/fine-tuned-playground-launch.5a26495c983c6350c227e05700a47a89002d132949a56fa4ff37f266ebe997b2.ko.png)\n",
    "\n",
    "비교 결과에는 각 모델의 토큰 수와 추론에 걸린 시간도 표시됩니다. **이 특정 예시는 과정을 보여주기 위한 단순한 예제로 실제 데이터셋이나 시나리오를 반영하지 않습니다**. 두 샘플 모두 동일한 토큰 수(시스템 컨텍스트와 사용자 프롬프트가 동일)를 보여주며, 미세 조정된 모델이 추론에 더 많은 시간이 걸리는 것을 확인할 수 있습니다(커스텀 모델).\n",
    "\n",
    "실제 시나리오에서는 이와 같은 간단한 예제를 사용하지 않고, 실제 데이터(예: 고객 서비스를 위한 제품 카탈로그)를 기반으로 미세 조정을 수행하여 응답 품질이 훨씬 더 명확해집니다. _그런 맥락에서는 기본 모델로 동등한 응답 품질을 얻으려면 더 많은 맞춤형 프롬프트 엔지니어링이 필요하며, 이는 토큰 사용량과 추론 처리 시간 증가로 이어질 수 있습니다._ _이를 직접 시도해 보려면 OpenAI Cookbook의 미세 조정 예제를 확인해 보세요._\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**면책 조항**:  \n이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확한 부분이 있을 수 있음을 유의하시기 바랍니다. 원문 문서가 권위 있는 출처로 간주되어야 합니다. 중요한 정보의 경우 전문적인 인간 번역을 권장합니다. 본 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "1725725c956564056baf895e6ca92aa5",
   "translation_date": "2025-12-19T09:31:21+00:00",
   "source_file": "18-fine-tuning/python/openai/oai-assignment.ipynb",
   "language_code": "ko"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}