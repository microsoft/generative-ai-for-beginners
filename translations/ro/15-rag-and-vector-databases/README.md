<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2210a0466c812d9defc4df2d9a709ff9",
  "translation_date": "2026-01-18T19:05:39+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "ro"
}
-->
# Generarea augmentatÄƒ prin recuperare (RAG) È™i bazele de date vectoriale

[![Generarea augmentatÄƒ prin recuperare (RAG) È™i bazele de date vectoriale](../../../../../translated_images/ro/15-lesson-banner.ac49e59506175d4f.webp)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

Ãn lecÈ›ia despre aplicaÈ›iile de cÄƒutare, am Ã®nvÄƒÈ›at pe scurt cum sÄƒ integrezi propriile date Ã®n modelele de limbaj mari (LLM). Ãn aceastÄƒ lecÈ›ie, vom aprofunda conceptele de fundamentare a datelor Ã®n aplicaÈ›ia ta LLM, mecanica procesului È™i metodele de stocare a datelor, inclusiv atÃ¢t Ã®ncorporÄƒrile, cÃ¢t È™i textul.

> **Video Ã®n curÃ¢nd**

## Introducere

Ãn aceastÄƒ lecÈ›ie vom acoperi urmÄƒtoarele:

- O introducere Ã®n RAG, ce este È™i de ce este folosit Ã®n AI (inteligenÈ›a artificialÄƒ).

- ÃnÈ›elegerea a ceea ce sunt bazele de date vectoriale È™i crearea uneia pentru aplicaÈ›ia noastrÄƒ.

- Un exemplu practic despre cum sÄƒ integrÄƒm RAG Ã®ntr-o aplicaÈ›ie.

## Obiectivele de Ã®nvÄƒÈ›are

DupÄƒ ce vei termina aceastÄƒ lecÈ›ie, vei putea:

- Explica importanÈ›a RAG Ã®n recuperarea È™i procesarea datelor.

- Configura o aplicaÈ›ie RAG È™i sÄƒ Ã®È›i fundamentezi datele pe un LLM.

- Integrarea eficientÄƒ a RAG È™i bazelor de date vectoriale Ã®n aplicaÈ›iile LLM.

## Scenariul nostru: Ã®mbunÄƒtÄƒÈ›irea LLM-urilor cu datele noastre

Pentru aceastÄƒ lecÈ›ie, vrem sÄƒ adÄƒugÄƒm propriile noastre notiÈ›e Ã®n startup-ul educaÈ›ional, care permite chatbot-ului sÄƒ obÈ›inÄƒ mai multe informaÈ›ii despre diferite subiecte. Folosind notiÈ›ele pe care le avem, cursanÈ›ii vor putea studia mai bine È™i Ã®nÈ›elege diferitele teme, fÄƒcÃ¢nd mai uÈ™oarÄƒ revizuirea pentru examene. Pentru a crea scenariul nostru, vom folosi:

- `Azure OpenAI:` LLM-ul pe care Ã®l vom folosi pentru a crea chatbot-ul

- `LecÈ›ia AI pentru Ã®ncepÄƒtori despre reÈ›ele neuronale:` acestea vor fi datele pe care ne vom fundamenta LLM-ul

- `Azure AI Search` È™i `Azure Cosmos DB:` baza de date vectorialÄƒ pentru a stoca datele È™i a crea un index de cÄƒutare

Utilizatorii vor putea crea teste practice din notiÈ›ele lor, carduri de revizuire È™i rezuma aceste notiÈ›e Ã®n sinteze concise. Pentru a Ã®ncepe, sÄƒ vedem ce este RAG È™i cum funcÈ›ioneazÄƒ:

## Generarea augmentatÄƒ prin recuperare (RAG)

Un chatbot alimentat de un LLM proceseazÄƒ solicitÄƒrile utilizatorului pentru a genera rÄƒspunsuri. Este conceput sÄƒ fie interactiv È™i sÄƒ interacÈ›ioneze cu utilizatorii pe o gamÄƒ largÄƒ de subiecte. TotuÈ™i, rÄƒspunsurile sale sunt limitate la contextul furnizat È™i la datele de antrenament fundamentale. De exemplu, limita de cunoÈ™tinÈ›e a GPT-4 este septembrie 2021, ceea ce Ã®nseamnÄƒ cÄƒ nu are cunoÈ™tinÈ›e despre evenimentele care au avut loc dupÄƒ aceastÄƒ datÄƒ. Ãn plus, datele folosite pentru antrenarea LLM-urilor exclud informaÈ›ii confidenÈ›iale, cum ar fi notiÈ›ele personale sau manualul de produs al unei companii.

### Cum funcÈ›ioneazÄƒ RAG-urile (Generarea augmentatÄƒ prin recuperare)

![drawing showing how RAGs work](../../../../../translated_images/ro/how-rag-works.f5d0ff63942bd3a6.webp)

SÄƒ presupunem cÄƒ doreÈ™ti sÄƒ implementezi un chatbot care creeazÄƒ teste din notiÈ›ele tale, vei avea nevoie de o conexiune la baza de cunoÈ™tinÈ›e. Aici intervine RAG. RAG-urile funcÈ›ioneazÄƒ astfel:

- **Baza de cunoÈ™tinÈ›e:** Ãnainte de recuperare, aceste documente trebuie ingerate È™i preprocesate, de obicei prin fragmentarea documentelor mari Ã®n bucÄƒÈ›i mai mici, transformarea lor Ã®n Ã®ncorporÄƒri textuale È™i stocarea lor Ã®ntr-o bazÄƒ de date.

- **Ãntrebarea utilizatorului:** utilizatorul pune o Ã®ntrebare

- **Recuperare:** cÃ¢nd utilizatorul pune o Ã®ntrebare, modelul de Ã®ncorporare recupereazÄƒ informaÈ›iile relevante din baza de cunoÈ™tinÈ›e pentru a oferi mai mult context care va fi Ã®ncorporat Ã®n prompt.

- **Generare augmentatÄƒ:** LLM-ul Ã®È™i Ã®mbunÄƒtÄƒÈ›eÈ™te rÄƒspunsul pe baza datelor recuperate. Acest lucru permite ca rÄƒspunsul generat sÄƒ nu fie bazat doar pe datele pre-antrenate, ci È™i pe informaÈ›ii relevante din contextul adÄƒugat. Datele recuperate sunt folosite pentru a augmenta rÄƒspunsurile LLM-ului. LLM-ul oferÄƒ apoi un rÄƒspuns Ã®ntrebÄƒrii utilizatorului.

![drawing showing how RAGs architecture](../../../../../translated_images/ro/encoder-decode.f2658c25d0eadee2.webp)

Arhitectura RAG-urilor este implementatÄƒ folosind transformatoare compuse din douÄƒ pÄƒrÈ›i: un encoder È™i un decoder. De exemplu, cÃ¢nd un utilizator pune o Ã®ntrebare, textul de intrare este â€encodatâ€ Ã®n vectori care surprind semnificaÈ›ia cuvintelor, iar vectorii sunt â€decodaÈ›iâ€ Ã®n indexul nostru de documente È™i genereazÄƒ text nou bazat pe Ã®ntrebarea utilizatorului. LLM-ul foloseÈ™te un model encoder-decoder pentru a genera ieÈ™irea.

DouÄƒ abordÄƒri Ã®n implementarea RAG conform lucrÄƒrii propuse: [Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sunt:

- **_RAG-Sequence_** foloseÈ™te documentele recuperate pentru a prezice cel mai bun rÄƒspuns posibil la o Ã®ntrebare a utilizatorului

- **RAG-Token** foloseÈ™te documentele pentru a genera urmÄƒtorul token, apoi le recupereazÄƒ pentru a rÄƒspunde la Ã®ntrebarea utilizatorului

### De ce sÄƒ foloseÈ™ti RAG-uri?Â 

- **BogÄƒÈ›ia informaÈ›iilor:** garanteazÄƒ cÄƒ rÄƒspunsurile text sunt actualizate È™i curente. Astfel, Ã®mbunÄƒtÄƒÈ›eÈ™te performanÈ›a pe sarcini specifice domeniului prin accesarea bazei interne de cunoÈ™tinÈ›e.

- Reduce fabricarea de informaÈ›ii prin utilizarea de **date verificabile** Ã®n baza de cunoÈ™tinÈ›e pentru a oferi context Ã®ntrebÄƒrilor utilizatorului.

- Este **rentabil** deoarece sunt mai economice comparativ cu ajustarea finÄƒ a unui LLM

## Crearea unei baze de cunoÈ™tinÈ›e

AplicaÈ›ia noastrÄƒ este bazatÄƒ pe datele personale, adicÄƒ lecÈ›ia despre ReÈ›eaua NeuronalÄƒ din curriculum-ul AI pentru ÃncepÄƒtori.

### Bazele de date vectoriale

O bazÄƒ de date vectorialÄƒ, spre deosebire de bazele de date tradiÈ›ionale, este o bazÄƒ de date specializatÄƒ creatÄƒ pentru a stoca, gestiona È™i cÄƒuta vectori Ã®ncorporaÈ›i. Aceasta stocheazÄƒ reprezentÄƒri numerice ale documentelor. Fragmentarea datelor Ã®n reprezentÄƒri numerice face mai uÈ™oarÄƒ Ã®nÈ›elegerea È™i procesarea datelor de cÄƒtre sistemul AI.

StocÄƒm Ã®ncorporÄƒrile noastre Ã®n bazele de date vectoriale deoarece LLM-urile au o limitÄƒ privind numÄƒrul de tokeni pe care Ã®i acceptÄƒ ca intrare. Deoarece nu poÈ›i trimite toate Ã®ncorporÄƒrile unui LLM, trebuie sÄƒ le Ã®mpÄƒrÈ›im Ã®n bucÄƒÈ›i, iar cÃ¢nd un utilizator pune o Ã®ntrebare, sunt returnate Ã®ncorporÄƒrile cele mai apropiate de Ã®ntrebare Ã®mpreunÄƒ cu promptul. Fragmentarea reduce È™i costurile legate de numÄƒrul de tokeni trimiÈ™i prin LLM.

Unele baze de date vectoriale populare includ Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant È™i DeepLake. PoÈ›i crea un model Azure Cosmos DB folosind Azure CLI cu urmÄƒtoarea comandÄƒ:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Din text Ã®n Ã®ncorporÄƒri

Ãnainte sÄƒ stocÄƒm datele, trebuie sÄƒ le convertim Ã®n Ã®ncorporÄƒri vectoriale Ã®nainte de a fi depozitate Ã®n baza de date. DacÄƒ lucrezi cu documente mari sau texte lungi, le poÈ›i fragmenta Ã®n funcÈ›ie de Ã®ntrebÄƒrile pe care le aÈ™tepÈ›i. Fragmentarea se poate face la nivel de propoziÈ›ie sau de paragraf. Deoarece fragmentarea derivÄƒ sensuri din cuvintele din jur, poÈ›i adÄƒuga context suplimentar la o bucatÄƒ, de exemplu, adÄƒugÃ¢nd titlul documentului sau includerea unor texte Ã®nainte sau dupÄƒ fragment. PoÈ›i fragmenta datele astfel:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # DacÄƒ ultima bucatÄƒ nu a atins lungimea minimÄƒ, adaugÄƒ-o oricum
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

OdatÄƒ fragmentate, putem Ã®ncorpora textul folosind diferite modele de Ã®ncorporare. Unele modele pe care le poÈ›i folosi includ: word2vec, ada-002 de la OpenAI, Azure Computer Vision È™i multe altele. Alegerea modelului depinde de limbile folosite, tipul de conÈ›inut codificat (text/imagine/audio), dimensiunea intrÄƒrii pe care o poate codifica È™i lungimea ieÈ™irii din Ã®ncorporare.

Un exemplu de text Ã®ncorporat folosind modelul `text-embedding-ada-002` de la OpenAI este:
![an embedding of the word cat](../../../../../translated_images/ro/cat.74cbd7946bc9ca38.webp)

## Recuperare È™i cÄƒutare vectorialÄƒ

CÃ¢nd un utilizator pune o Ã®ntrebare, sistemul de recuperare o transformÄƒ Ã®ntr-un vector folosind codificatorul de interogare, apoi cautÄƒ Ã®n indexul nostru de documente vectori relevanÈ›i din document legate de intrare. DupÄƒ aceea, converteÈ™te vectorii de intrare È™i vectorii documentelor Ã®n text È™i Ã®l transmite prin LLM.

### Recuperare

Recuperarea are loc cÃ¢nd sistemul Ã®ncearcÄƒ sÄƒ gÄƒseascÄƒ rapid documentele din index care corespund criteriilor de cÄƒutare. Scopul recuperatorului este sÄƒ obÈ›inÄƒ documente care vor fi folosite pentru a oferi context È™i a fundamenta LLM-ul pe datele tale.

ExistÄƒ mai multe moduri de a efectua cÄƒutarea Ã®n baza noastrÄƒ de date, cum ar fi:

- **CÄƒutare dupÄƒ cuvinte-cheie** - folositÄƒ pentru cÄƒutÄƒri textuale

- **CÄƒutare vectorialÄƒ** - converteÈ™te documentele din text Ã®n reprezentÄƒri vectoriale folosind modele de Ã®ncorporare, permiÈ›Ã¢nd o **cÄƒutare semanticÄƒ** bazatÄƒ pe semnificaÈ›ia cuvintelor. Recuperarea se face prin interogarea documentelor ale cÄƒror reprezentÄƒri vectoriale sunt cele mai apropiate de Ã®ntrebarea utilizatorului.

- **HibridÄƒ** - combinaÈ›ie Ã®ntre cÄƒutarea dupÄƒ cuvinte-cheie È™i cÄƒutarea vectorialÄƒ.

O provocare la recuperare apare atunci cÃ¢nd nu existÄƒ un rÄƒspuns similar Ã®n baza de date, sistemul va returna atunci cea mai bunÄƒ informaÈ›ie pe care o poate obÈ›ine, Ã®nsÄƒ poÈ›i folosi tactici precum stabilirea distanÈ›ei maxime pentru relevanÈ›Äƒ sau utilizarea cÄƒutÄƒrii hibride care combinÄƒ atÃ¢t cuvinte-cheie cÃ¢t È™i vectori. Ãn aceastÄƒ lecÈ›ie vom folosi cÄƒutarea hibridÄƒ, o combinaÈ›ie Ã®ntre cÄƒutarea vectorialÄƒ È™i cea dupÄƒ cuvinte-cheie. Vom stoca datele Ã®ntr-un dataframe cu coloane ce conÈ›in bucÄƒÈ›ile, precum È™i Ã®ncorporÄƒrile.

### Similaritatea vectorialÄƒ

Recuperatorul va cÄƒuta Ã®n baza de cunoÈ™tinÈ›e pentru Ã®ncorporÄƒri care sunt apropiate unele de altele, cel mai apropiat vecin, deoarece sunt texte similare. Ãn scenariu, cÃ¢nd un utilizator pune o Ã®ntrebare, aceasta este mai Ã®ntÃ¢i Ã®ncorporatÄƒ È™i apoi comparatÄƒ cu Ã®ncorporÄƒri similare. MÄƒsura comunÄƒ folositÄƒ pentru a determina cÃ¢t de similare sunt vectorii diferiÈ›i este similaritatea cosinus, bazatÄƒ pe unghiul dintre doi vectori.

Putem mÄƒsura similitudinea È™i cu alte alternative, cum ar fi distanÈ›a euclidianÄƒ, care este linia dreaptÄƒ dintre capetele vectorilor, È™i produsul scalar care mÄƒsoarÄƒ suma produselor elementelor corespunzÄƒtoare a doi vectori.

### Indexul de cÄƒutare

Pentru a face recuperare, trebuie sÄƒ construim un index de cÄƒutare pentru baza noastrÄƒ de cunoÈ™tinÈ›e Ã®nainte de a efectua cÄƒutarea. Un index va stoca Ã®ncorporÄƒrile È™i poate recupera rapid cele mai similare bucÄƒÈ›i chiar È™i Ã®ntr-o bazÄƒ de date mare. Putem crea indexul local folosind:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# CreeazÄƒ indexul de cÄƒutare
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Pentru a interoga indexul, poÈ›i folosi metoda kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```

### Re-rangarea

OdatÄƒ ce ai interogat baza de date, poate fi necesar sÄƒ sortezi rezultatele Ã®n ordinea relevanÈ›ei. Un LLM de rerangare utilizeazÄƒ Ã®nvÄƒÈ›area automatÄƒ pentru a Ã®mbunÄƒtÄƒÈ›i relevanÈ›a rezultatelor cÄƒutÄƒrii prin ordonarea lor de la cele mai relevante. Folosind Azure AI Search, rerangarea se face automat pentru tine folosind un reranker semantic. Un exemplu despre cum funcÈ›ioneazÄƒ rerangarea folosind cei mai apropiaÈ›i vecini:

```python
# GÄƒsiÈ›i cele mai similare documente
distances, indices = nbrs.kneighbors([query_vector])

index = []
# AfiÈ™aÈ›i cele mai similare documente
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## PunÃ¢nd totul cap la cap

Ultimul pas este sÄƒ adÄƒugÄƒm LLM-ul Ã®n ecuaÈ›ie pentru a putea obÈ›ine rÄƒspunsuri fundamentate pe datele noastre. Putem implementa acest lucru astfel:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # ConverteÈ™te Ã®ntrebarea Ã®ntr-un vector de interogare
    query_vector = create_embeddings(user_input)

    # GÄƒseÈ™te cele mai similare documente
    distances, indices = nbrs.kneighbors([query_vector])

    # adaugÄƒ documente la interogare pentru a oferi context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combinÄƒ istoricul È™i inputul utilizatorului
    history.append(user_input)

    # creeazÄƒ un obiect mesaj
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": "\n\n".join(history) }
    ]

    # foloseÈ™te completarea chatului pentru a genera un rÄƒspuns
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluarea aplicaÈ›iei noastre

### Metrici de evaluare

- Calitatea rÄƒspunsurilor furnizate, asigurÃ¢ndu-se cÄƒ sunÄƒ natural, fluent È™i asemÄƒnÄƒtor cu un om

- Fundamentarea datelor: evaluarea dacÄƒ rÄƒspunsul provine din documentele furnizate

- RelevanÈ›Äƒ: evaluarea dacÄƒ rÄƒspunsul corespunde È™i este legat de Ã®ntrebarea pusÄƒ

- FluenÈ›Äƒ - dacÄƒ rÄƒspunsul are sens din punct de vedere gramatical

## Cazuri de utilizare pentru folosirea RAG (Generarea augmentatÄƒ prin recuperare) È™i bazelor de date vectoriale

ExistÄƒ multe cazuri diferite Ã®n care apelurile de funcÈ›ii pot Ã®mbunÄƒtÄƒÈ›i aplicaÈ›ia ta, cum ar fi:

- ÃntrebÄƒri È™i rÄƒspunsuri: fundamentarea datelor companiei tale Ã®ntr-un chat ce poate fi folosit de angajaÈ›i pentru a pune Ã®ntrebÄƒri.

- Sisteme de recomandare: unde poÈ›i crea un sistem care potriveÈ™te cele mai similare valori, ex. filme, restaurante È™i multe altele.

- Servicii chatbot: poÈ›i stoca istoricul conversaÈ›iilor È™i personaliza conversaÈ›ia bazat pe datele utilizatorului.

- CÄƒutare imagine bazatÄƒ pe Ã®ncorporÄƒri vectoriale, utilÄƒ pentru recunoaÈ™terea imaginilor È™i detectarea anomaliilor.

## Rezumat

Am acoperit ariile fundamentale ale RAG de la adÄƒugarea datelor Ã®n aplicaÈ›ie, interogarea utilizatorului pÃ¢nÄƒ la ieÈ™ire. Pentru a simplifica crearea RAG, poÈ›i folosi cadre de lucru precum Semantic Kernel, Langchain sau Autogen.

## Tema

Pentru a continua Ã®nvÄƒÈ›area despre Generarea augmentatÄƒ prin recuperare (RAG), poÈ›i construi:

- Un front-end pentru aplicaÈ›ie folosind cadrul de lucru preferat

- FoloseÈ™te un framework, fie LangChain fie Semantic Kernel, È™i recreeazÄƒ aplicaÈ›ia ta.

FelicitÄƒri pentru terminarea lecÈ›iei ğŸ‘.

## ÃnvÄƒÈ›area nu se opreÈ™te aici, continuÄƒ cÄƒlÄƒtoria

DupÄƒ ce ai terminat aceastÄƒ lecÈ›ie, aruncÄƒ o privire la colecÈ›ia noastrÄƒ [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pentru a continua sÄƒ-È›i creÈ™ti cunoÈ™tinÈ›ele despre AI Generativ!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Declinarea responsabilitÄƒÈ›ii**:
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim pentru acurateÈ›e, vÄƒ rugÄƒm sÄƒ reÈ›ineÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa nativÄƒ trebuie considerat sursa autorizatÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm rÄƒspunderea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->