<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b4b0266fbadbba7ded891b6485adc66d",
  "translation_date": "2025-10-17T22:07:04+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "ro"
}
-->
# Generarea Augmentat캒 prin Recuperare (RAG) 탳i Baze de Date Vectoriale

[![Generarea Augmentat캒 prin Recuperare (RAG) 탳i Baze de Date Vectoriale](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.ro.png)](https://youtu.be/4l8zhHUBeyI?si=BmvDmL1fnHtgQYkL)

칉n lec탵ia despre aplica탵iile de c캒utare, am 칥nv캒탵at pe scurt cum s캒 integr캒m propriile date 칥n Modelele de Limbaj Extins (LLMs). 칉n aceast캒 lec탵ie, vom aprofunda conceptele de ancorare a datelor 칥n aplica탵ia LLM, mecanismele procesului 탳i metodele de stocare a datelor, inclusiv at칙t 칥ncorpor캒rile, c칙t 탳i textul.

> **Video 칥n cur칙nd**

## Introducere

칉n aceast캒 lec탵ie vom acoperi urm캒toarele:

- O introducere 칥n RAG, ce este 탳i de ce este utilizat 칥n AI (inteligen탵a artificial캒).

- 칉n탵elegerea bazelor de date vectoriale 탳i crearea uneia pentru aplica탵ia noastr캒.

- Un exemplu practic despre cum s캒 integr캒m RAG 칥ntr-o aplica탵ie.

## Obiective de 칥nv캒탵are

Dup캒 finalizarea acestei lec탵ii, vei putea:

- Explica semnifica탵ia RAG 칥n recuperarea 탳i procesarea datelor.

- Configura o aplica탵ie RAG 탳i s캒 ancorezi datele tale la un LLM.

- Integra eficient RAG 탳i Baze de Date Vectoriale 칥n aplica탵iile LLM.

## Scenariul nostru: 칥mbun캒t캒탵irea LLM-urilor cu datele noastre

Pentru aceast캒 lec탵ie, dorim s캒 ad캒ug캒m propriile noastre noti탵e 칥n startup-ul educa탵ional, ceea ce va permite chatbot-ului s캒 ob탵in캒 mai multe informa탵ii despre diferite subiecte. Folosind noti탵ele pe care le avem, cursan탵ii vor putea studia mai bine 탳i 칥n탵elege diferite teme, f캒c칙nd mai u탳oar캒 revizuirea pentru examenele lor. Pentru a crea acest scenariu, vom folosi:

- `Azure OpenAI:` LLM-ul pe care 칥l vom folosi pentru a crea chatbot-ul nostru.

- `Lec탵ia AI pentru 칥ncep캒tori despre Re탵ele Neurale:` acestea vor fi datele pe care le ancor캒m la LLM-ul nostru.

- `Azure AI Search` 탳i `Azure Cosmos DB:` baza de date vectorial캒 pentru a stoca datele noastre 탳i a crea un index de c캒utare.

Utilizatorii vor putea crea teste de practic캒 din noti탵ele lor, fi탳e de revizuire 탳i le vor putea rezuma 칥n prezent캒ri concise. Pentru a 칥ncepe, s캒 vedem ce este RAG 탳i cum func탵ioneaz캒:

## Generarea Augmentat캒 prin Recuperare (RAG)

Un chatbot alimentat de LLM proceseaz캒 solicit캒rile utilizatorilor pentru a genera r캒spunsuri. Este conceput s캒 fie interactiv 탳i s캒 interac탵ioneze cu utilizatorii pe o gam캒 larg캒 de subiecte. Totu탳i, r캒spunsurile sale sunt limitate la contextul oferit 탳i la datele sale de antrenament de baz캒. De exemplu, limita de cuno탳tin탵e a GPT-4 este septembrie 2021, ceea ce 칥nseamn캒 c캒 칥i lipsesc informa탵ii despre evenimentele care au avut loc dup캒 aceast캒 perioad캒. 칉n plus, datele utilizate pentru antrenarea LLM-urilor exclud informa탵iile confiden탵iale, cum ar fi noti탵ele personale sau manualul de produse al unei companii.

### Cum func탵ioneaz캒 RAG (Generarea Augmentat캒 prin Recuperare)

![desen care arat캒 cum func탵ioneaz캒 RAG](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.ro.png)

S캒 presupunem c캒 dore탳ti s캒 implementezi un chatbot care creeaz캒 teste din noti탵ele tale, vei avea nevoie de o conexiune la baza de cuno탳tin탵e. Aici intervine RAG. RAG func탵ioneaz캒 astfel:

- **Baza de cuno탳tin탵e:** 칉nainte de recuperare, aceste documente trebuie s캒 fie prelucrate 탳i preg캒tite, de obicei prin 칥mp캒r탵irea documentelor mari 칥n fragmente mai mici, transformarea lor 칥n 칥ncorpor캒ri textuale 탳i stocarea lor 칥ntr-o baz캒 de date.

- **Interogarea utilizatorului:** utilizatorul pune o 칥ntrebare.

- **Recuperare:** C칙nd utilizatorul pune o 칥ntrebare, modelul de 칥ncorporare recupereaz캒 informa탵ii relevante din baza noastr캒 de cuno탳tin탵e pentru a oferi mai mult context care va fi 칥ncorporat 칥n solicitare.

- **Generarea augmentat캒:** LLM 칥탳i 칥mbun캒t캒탵e탳te r캒spunsul pe baza datelor recuperate. Acest lucru permite ca r캒spunsul generat s캒 se bazeze nu doar pe datele pre-antrenate, ci 탳i pe informa탵ii relevante din contextul ad캒ugat. Datele recuperate sunt utilizate pentru a augmenta r캒spunsurile LLM-ului. LLM-ul returneaz캒 apoi un r캒spuns la 칥ntrebarea utilizatorului.

![desen care arat캒 arhitectura RAG](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.ro.png)

Arhitectura RAG este implementat캒 folosind transformatoare care constau din dou캒 p캒r탵i: un encoder 탳i un decoder. De exemplu, c칙nd un utilizator pune o 칥ntrebare, textul de intrare este 'encodat' 칥n vectori care capteaz캒 semnifica탵ia cuvintelor, iar vectorii sunt 'decoda탵i' 칥n indexul documentului nostru 탳i genereaz캒 text nou pe baza interog캒rii utilizatorului. LLM-ul folose탳te at칙t un model encoder-decoder pentru a genera rezultatul.

Dou캒 abord캒ri pentru implementarea RAG conform lucr캒rii propuse: [Generarea Augmentat캒 prin Recuperare pentru Sarcini NLP (procesare de limbaj natural) intensive 칥n cuno탳tin탵e](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sunt:

- **_RAG-Sequence_** utiliz칙nd documentele recuperate pentru a prezice cel mai bun r캒spuns posibil la o interogare a utilizatorului.

- **RAG-Token** utiliz칙nd documentele pentru a genera urm캒torul token, apoi recuper칙ndu-le pentru a r캒spunde la interogarea utilizatorului.

### De ce ai folosi RAG?

- **Bog캒탵ia informa탵iilor:** asigur캒 c캒 r캒spunsurile textuale sunt actualizate 탳i relevante. Prin urmare, 칥mbun캒t캒탵e탳te performan탵a 칥n sarcini specifice domeniului prin accesarea bazei de cuno탳tin탵e interne.

- Reduce fabricarea informa탵iilor prin utilizarea **datelor verificabile** din baza de cuno탳tin탵e pentru a oferi context interog캒rilor utilizatorilor.

- Este **cost-eficient** deoarece este mai economic comparativ cu ajustarea fin캒 a unui LLM.

## Crearea unei baze de cuno탳tin탵e

Aplica탵ia noastr캒 se bazeaz캒 pe datele noastre personale, adic캒 lec탵ia despre Re탵ele Neurale din curriculum-ul AI pentru 칉ncep캒tori.

### Baze de Date Vectoriale

O baz캒 de date vectorial캒, spre deosebire de bazele de date tradi탵ionale, este o baz캒 de date specializat캒 conceput캒 pentru a stoca, gestiona 탳i c캒uta vectori 칥ncorpora탵i. Aceasta stocheaz캒 reprezent캒ri numerice ale documentelor. Descompunerea datelor 칥n 칥ncorpor캒ri numerice face mai u탳or pentru sistemul nostru AI s캒 칥n탵eleag캒 탳i s캒 proceseze datele.

Stoc캒m 칥ncorpor캒rile noastre 칥n baze de date vectoriale deoarece LLM-urile au o limit캒 a num캒rului de token-uri pe care le accept캒 ca intrare. Deoarece nu putem transmite toate 칥ncorpor캒rile unui LLM, va trebui s캒 le 칥mp캒r탵im 칥n fragmente, iar c칙nd un utilizator pune o 칥ntrebare, 칥ncorpor캒rile cele mai asem캒n캒toare cu 칥ntrebarea vor fi returnate 칥mpreun캒 cu solicitarea. Fragmentarea reduce, de asemenea, costurile legate de num캒rul de token-uri transmise printr-un LLM.

Unele baze de date vectoriale populare includ Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant 탳i DeepLake. Po탵i crea un model Azure Cosmos DB folosind Azure CLI cu urm캒toarea comand캒:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### De la text la 칥ncorpor캒ri

칉nainte de a stoca datele noastre, va trebui s캒 le convertim 칥n 칥ncorpor캒ri vectoriale 칥nainte de a fi stocate 칥n baza de date. Dac캒 lucrezi cu documente mari sau texte lungi, le po탵i fragmenta pe baza interog캒rilor pe care le a탳tep탵i. Fragmentarea poate fi realizat캒 la nivel de propozi탵ie sau la nivel de paragraf. Deoarece fragmentarea deriv캒 semnifica탵ii din cuvintele din jurul lor, po탵i ad캒uga un alt context unui fragment, de exemplu, prin ad캒ugarea titlului documentului sau includerea unui text 칥nainte sau dup캒 fragment. Po탵i fragmenta datele astfel:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Odat캒 fragmentate, putem 칥ncorpora textul nostru folosind diferite modele de 칥ncorporare. Unele modele pe care le po탵i folosi includ: word2vec, ada-002 de la OpenAI, Azure Computer Vision 탳i multe altele. Selectarea unui model de utilizat va depinde de limbile pe care le folose탳ti, tipul de con탵inut codificat (text/imagine/audio), dimensiunea intr캒rii pe care o poate codifica 탳i lungimea ie탳irii 칥ncorpor캒rii.

Un exemplu de text 칥ncorporat folosind modelul `text-embedding-ada-002` de la OpenAI este:
![o 칥ncorporare a cuv칙ntului pisic캒](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.ro.png)

## Recuperare 탳i C캒utare Vectorial캒

C칙nd un utilizator pune o 칥ntrebare, recuperatorul o transform캒 칥ntr-un vector folosind encoder-ul de interogare, apoi caut캒 prin indexul nostru de c캒utare al documentelor pentru vectori relevan탵i din document care sunt lega탵i de intrare. Odat캒 ce este finalizat, converte탳te at칙t vectorul de intrare, c칙t 탳i vectorii documentului 칥n text 탳i 칥i transmite prin LLM.

### Recuperare

Recuperarea are loc atunci c칙nd sistemul 칥ncearc캒 s캒 g캒seasc캒 rapid documentele din index care 칥ndeplinesc criteriile de c캒utare. Scopul recuperatorului este de a ob탵ine documente care vor fi utilizate pentru a oferi context 탳i a ancora LLM-ul pe datele tale.

Exist캒 mai multe moduri de a efectua c캒ut캒ri 칥n baza noastr캒 de date, cum ar fi:

- **C캒utare dup캒 cuvinte cheie** - utilizat캒 pentru c캒ut캒ri textuale.

- **C캒utare semantic캒** - folose탳te semnifica탵ia semantic캒 a cuvintelor.

- **C캒utare vectorial캒** - converte탳te documentele din text 칥n reprezent캒ri vectoriale folosind modele de 칥ncorporare. Recuperarea se va face prin interogarea documentelor ale c캒ror reprezent캒ri vectoriale sunt cele mai apropiate de 칥ntrebarea utilizatorului.

- **Hibrid** - o combina탵ie 칥ntre c캒utarea dup캒 cuvinte cheie 탳i c캒utarea vectorial캒.

O provocare cu recuperarea apare atunci c칙nd nu exist캒 un r캒spuns similar la interogarea din baza de date, sistemul va returna cele mai bune informa탵ii pe care le poate ob탵ine, totu탳i, po탵i folosi tactici precum stabilirea distan탵ei maxime pentru relevan탵캒 sau utilizarea c캒ut캒rii hibride care combin캒 at칙t cuvintele cheie, c칙t 탳i c캒utarea vectorial캒. 칉n aceast캒 lec탵ie vom folosi c캒utarea hibrid캒, o combina탵ie 칥ntre c캒utarea vectorial캒 탳i dup캒 cuvinte cheie. Vom stoca datele noastre 칥ntr-un dataframe cu coloane care con탵in fragmentele, precum 탳i 칥ncorpor캒rile.

### Similaritatea Vectorial캒

Recuperatorul va c캒uta 칥n baza de cuno탳tin탵e pentru 칥ncorpor캒ri care sunt apropiate, cele mai apropiate vecine, deoarece sunt texte similare. 칉n scenariul 칥n care un utilizator pune o interogare, aceasta este mai 칥nt칙i 칥ncorporat캒, apoi potrivit캒 cu 칥ncorpor캒rile similare. M캒surarea comun캒 utilizat캒 pentru a determina c칙t de similare sunt diferite vectori este similaritatea cosinusului, care se bazeaz캒 pe unghiul dintre doi vectori.

Putem m캒sura similaritatea folosind alte alternative, cum ar fi distan탵a euclidian캒, care este linia dreapt캒 칥ntre punctele finale ale vectorilor, 탳i produsul scalar, care m캒soar캒 suma produselor elementelor corespunz캒toare ale doi vectori.

### Index de c캒utare

C칙nd facem recuperare, va trebui s캒 construim un index de c캒utare pentru baza noastr캒 de cuno탳tin탵e 칥nainte de a efectua c캒utarea. Un index va stoca 칥ncorpor캒rile noastre 탳i poate recupera rapid cele mai similare fragmente chiar 탳i 칥ntr-o baz캒 de date mare. Putem crea indexul nostru local folosind:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Reordonare

Odat캒 ce ai interogat baza de date, poate fi necesar s캒 sortezi rezultatele de la cele mai relevante. Un LLM de reordonare utilizeaz캒 칥nv캒탵area automat캒 pentru a 칥mbun캒t캒탵i relevan탵a rezultatelor c캒ut캒rii prin ordonarea lor de la cele mai relevante. Folosind Azure AI Search, reordonarea se face automat pentru tine folosind un reordonator semantic. Un exemplu despre cum func탵ioneaz캒 reordonarea folosind cei mai apropia탵i vecini:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Punerea tuturor 칥mpreun캒

Ultimul pas este ad캒ugarea LLM-ului nostru 칥n mix pentru a putea ob탵ine r캒spunsuri ancorate pe datele noastre. Putem implementa acest lucru astfel:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluarea aplica탵iei noastre

### Metrice de evaluare

- Calitatea r캒spunsurilor furnizate, asigur칙ndu-se c캒 sun캒 natural, fluent 탳i uman.

- Ancorarea datelor: evaluarea dac캒 r캒spunsul provine din documentele furnizate.

- Relevan탵a: evaluarea dac캒 r캒spunsul se potrive탳te 탳i este legat de 칥ntrebarea pus캒.

- Fluen탵a - dac캒 r캒spunsul are sens gramatical.

## Utiliz캒ri ale RAG (Generarea Augmentat캒 prin Recuperare) 탳i bazelor de date vectoriale

Exist캒 multe utiliz캒ri diferite 칥n care apelurile func탵ionale pot 칥mbun캒t캒탵i aplica탵ia ta, cum ar fi:

- 칉ntreb캒ri 탳i r캒spunsuri: ancorarea datelor companiei tale la un chat care poate fi utilizat de angaja탵i pentru a pune 칥ntreb캒ri.

- Sisteme de recomandare: unde po탵i crea un sistem care s캒 potriveasc캒 cele mai similare valori, de exemplu, filme, restaurante 탳i multe altele.

- Servicii de chatbot: po탵i stoca istoricul conversa탵iilor 탳i personaliza conversa탵ia pe baza datelor utilizatorului.

- C캒utare de imagini bazat캒 pe 칥ncorpor캒ri vectoriale, util캒 칥n recunoa탳terea imaginilor 탳i detectarea anomaliilor.

## Rezumat

Am acoperit aspectele fundamentale ale RAG, de la ad캒ugarea datelor noastre 칥n aplica탵ie, la interogarea utilizatorului 탳i rezultatul. Pentru a simplifica crearea RAG, po탵i folosi cadre precum Semantic Kernel, Langchain sau Autogen.

## Tem캒

Pentru a continua 칥nv캒탵area despre Generarea Augmentat캒 prin Recuperare (RAG), po탵i construi:

- O interfa탵캒 pentru aplica탵ie folosind cadrul ales de tine.

- Utiliza un cadru, fie LangChain, fie Semantic Kernel, 탳i s캒 recreezi aplica탵ia ta.

Felicit캒ri pentru finalizarea lec탵iei 游녪.

## 칉nv캒탵area nu se opre탳te aici, continu캒 c캒l캒toria

Dup캒 finalizarea acestei lec탵ii, consult캒 [Colec탵ia de 칉nv캒탵are AI Generativ캒](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pentru a continua s캒 칥탵i dezvol탵i cuno탳tin탵ele despre AI Generativ캒!

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). De탳i ne str캒duim s캒 asigur캒m acurate탵ea, v캒 rug캒m s캒 fi탵i con탳tien탵i c캒 traducerile automate pot con탵ine erori sau inexactit캒탵i. Documentul original 칥n limba sa matern캒 ar trebui considerat sursa autoritar캒. Pentru informa탵ii critice, se recomand캒 traducerea profesional캒 realizat캒 de un specialist uman. Nu ne asum캒m responsabilitatea pentru eventualele ne칥n탵elegeri sau interpret캒ri gre탳ite care pot ap캒rea din utilizarea acestei traduceri.