<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-05-20T01:42:26+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "ro"
}
-->
# Generarea Augmentat캒 de Reg캒sire (RAG) 탳i Baze de Date Vectoriale

[![Generarea Augmentat캒 de Reg캒sire (RAG) 탳i Baze de Date Vectoriale](../../../translated_images/15-lesson-banner.799d0cd2229970edb365f6667a4c7b3a0f526eb8698baa7d2e05c3bd49a5d83f.ro.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

칉n lec탵ia despre aplica탵iile de c캒utare, am 칥nv캒탵at pe scurt cum s캒 integrezi propriile tale date 칥n Modelele de Limbaj de Mare Anvergur캒 (LLMs). 칉n aceast캒 lec탵ie, vom aprofunda conceptele de fundamentare a datelor tale 칥n aplica탵ia LLM, mecanismele procesului 탳i metodele de stocare a datelor, inclusiv at칙t embedding-uri, c칙t 탳i text.

> **Video 칥n cur칙nd**

## Introducere

칉n aceast캒 lec탵ie vom acoperi urm캒toarele:

- O introducere 칥n RAG, ce este 탳i de ce este folosit 칥n inteligen탵a artificial캒 (IA).

- 칉n탵elegerea a ce sunt bazele de date vectoriale 탳i crearea uneia pentru aplica탵ia noastr캒.

- Un exemplu practic despre cum s캒 integrezi RAG 칥ntr-o aplica탵ie.

## Obiective de 칉nv캒탵are

Dup캒 finalizarea acestei lec탵ii, vei putea:

- Explica semnifica탵ia RAG 칥n reg캒sirea 탳i procesarea datelor.

- Configura aplica탵ia RAG 탳i fundamenta datele tale 칥ntr-un LLM

- Integrarea eficient캒 a RAG 탳i a Bazelor de Date Vectoriale 칥n Aplica탵iile LLM.

## Scenariul nostru: 칥mbun캒t캒탵irea LLM-urilor noastre cu datele proprii

Pentru aceast캒 lec탵ie, dorim s캒 ad캒ug캒m propriile noastre note 칥n startup-ul educa탵ional, care permite chatbot-ului s캒 ob탵in캒 mai multe informa탵ii despre diferitele subiecte. Folosind notele pe care le avem, cursan탵ii vor putea studia mai bine 탳i 칥n탵elege diferitele teme, facilit칙nd revizuirea pentru examenele lor. Pentru a crea scenariul nostru, vom folosi:

- `Azure OpenAI:` LLM-ul pe care 칥l vom folosi pentru a crea chatbot-ul nostru

- `AI for beginners' lesson on Neural Networks`: acestea vor fi datele pe care ne fundament캒m LLM-ul

- `Azure AI Search` 탳i `Azure Cosmos DB:` baza de date vectorial캒 pentru a stoca datele noastre 탳i a crea un index de c캒utare

Utilizatorii vor putea crea chestionare de practic캒 din notele lor, carduri de revizuire 탳i le pot rezuma 칥n prezent캒ri concise. Pentru a 칥ncepe, s캒 vedem ce este RAG 탳i cum func탵ioneaz캒:

## Generarea Augmentat캒 de Reg캒sire (RAG)

Un chatbot alimentat de un LLM proceseaz캒 solicit캒rile utilizatorilor pentru a genera r캒spunsuri. Este conceput s캒 fie interactiv 탳i s캒 interac탵ioneze cu utilizatorii pe o gam캒 larg캒 de subiecte. Totu탳i, r캒spunsurile sale sunt limitate la contextul furnizat 탳i la datele sale de baz캒 de instruire. De exemplu, data limit캒 de cunoa탳tere a GPT-4 este septembrie 2021, ceea ce 칥nseamn캒 c캒 nu are cuno탳tin탵e despre evenimentele care au avut loc dup캒 aceast캒 perioad캒. 칉n plus, datele folosite pentru a antrena LLM-urile exclud informa탵iile confiden탵iale, cum ar fi notele personale sau manualul de produse al unei companii.

### Cum func탵ioneaz캒 RAG-urile (Generarea Augmentat캒 de Reg캒sire)

![desen care arat캒 cum func탵ioneaz캒 RAG-urile](../../../translated_images/how-rag-works.d87a7ed9c30f43126bb9e8e259be5d66e16cd1fef65374e6914746ba9bfb0b2f.ro.png)

Presupun칙nd c캒 vrei s캒 implementezi un chatbot care creeaz캒 chestionare din notele tale, vei avea nevoie de o conexiune la baza de cuno탳tin탵e. Aici vine 칥n ajutor RAG. RAG-urile func탵ioneaz캒 astfel:

- **Baza de cuno탳tin탵e:** 칉nainte de reg캒sire, aceste documente trebuie s캒 fie ingerate 탳i preprocesate, de obicei descompun칙nd documentele mari 칥n buc캒탵i mai mici, transform칙ndu-le 칥n embedding-uri text 탳i stoc칙ndu-le 칥ntr-o baz캒 de date.

- **Interogarea utilizatorului:** utilizatorul pune o 칥ntrebare

- **Reg캒sirea:** C칙nd un utilizator pune o 칥ntrebare, modelul de embedding reg캒se탳te informa탵ii relevante din baza noastr캒 de cuno탳tin탵e pentru a oferi mai mult context care va fi inclus 칥n solicitare.

- **Generarea Augmentat캒:** LLM-ul 칥탳i 칥mbun캒t캒탵e탳te r캒spunsul pe baza datelor reg캒site. Permite ca r캒spunsul generat s캒 fie bazat nu doar pe datele pre-antrenate, ci 탳i pe informa탵ii relevante din contextul ad캒ugat. Datele reg캒site sunt folosite pentru a augmenta r캒spunsurile LLM-ului. LLM-ul returneaz캒 apoi un r캒spuns la 칥ntrebarea utilizatorului.

![desen care arat캒 arhitectura RAG-urilor](../../../translated_images/encoder-decode.75eebc7093ccefec17568eebc80d3d0b831ecf2ea204566377a04c77a5a57ebb.ro.png)

Arhitectura pentru RAG-uri este implementat캒 folosind transformatoare const칙nd din dou캒 p캒r탵i: un encoder 탳i un decoder. De exemplu, c칙nd un utilizator pune o 칥ntrebare, textul de intrare este 'encodat' 칥n vectori care captureaz캒 semnifica탵ia cuvintelor, iar vectorii sunt 'decoda탵i' 칥n indexul nostru de documente 탳i genereaz캒 text nou bazat pe interogarea utilizatorului. LLM-ul folose탳te at칙t un model encoder-decoder pentru a genera rezultatul.

Dou캒 abord캒ri c칙nd implementezi RAG conform lucr캒rii propuse: [Generarea Augmentat캒 de Reg캒sire pentru Sarcini NLP (procesare de limbaj natural) intensive 칥n cuno탳tin탵e](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sunt:

- **_RAG-Sequence_** folosind documentele reg캒site pentru a prezice cel mai bun r캒spuns posibil la o interogare a utilizatorului

- **RAG-Token** folosind documentele pentru a genera urm캒torul token, apoi le reg캒se탳te pentru a r캒spunde la interogarea utilizatorului

### De ce ai folosi RAG-uri?

- **Bog캒탵ia informa탵iilor:** asigur캒 c캒 r캒spunsurile text sunt actualizate 탳i curente. Prin urmare, 칥mbun캒t캒탵e탳te performan탵a 칥n sarcinile specifice domeniului prin accesarea bazei de cuno탳tin탵e interne.

- Reduce fabricarea prin utilizarea **datelor verificabile** 칥n baza de cuno탳tin탵e pentru a oferi context la interog캒rile utilizatorului.

- Este **cost-eficient** deoarece sunt mai economice comparativ cu ajustarea fin캒 a unui LLM

## Crearea unei baze de cuno탳tin탵e

Aplica탵ia noastr캒 se bazeaz캒 pe datele noastre personale, adic캒 lec탵ia despre Re탵ele Neuronale din curriculum-ul AI Pentru 칉ncep캒tori.

### Baze de Date Vectoriale

O baz캒 de date vectorial캒, spre deosebire de bazele de date tradi탵ionale, este o baz캒 de date specializat캒 conceput캒 pentru a stoca, gestiona 탳i c캒uta vectori embedda탵i. Stocheaz캒 reprezent캒ri numerice ale documentelor. Descompunerea datelor 칥n embedding-uri numerice face mai u탳or pentru sistemul nostru AI s캒 칥n탵eleag캒 탳i s캒 proceseze datele.

Stoc캒m embedding-urile noastre 칥n baze de date vectoriale deoarece LLM-urile au o limit캒 a num캒rului de tokeni pe care 칥i accept캒 ca intrare. Deoarece nu po탵i trece toate embedding-urile printr-un LLM, va trebui s캒 le descompui 칥n buc캒탵i 탳i c칙nd un utilizator pune o 칥ntrebare, embedding-urile cele mai asem캒n캒toare 칥ntreb캒rii vor fi returnate 칥mpreun캒 cu solicitarea. Descompunerea reduce, de asemenea, costurile legate de num캒rul de tokeni trecu탵i printr-un LLM.

Unele baze de date vectoriale populare includ Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant 탳i DeepLake. Po탵i crea un model Azure Cosmos DB folosind Azure CLI cu urm캒toarea comand캒:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### De la text la embedding-uri

칉nainte de a stoca datele noastre, va trebui s캒 le convertim 칥n embedding-uri vectoriale 칥nainte de a fi stocate 칥n baza de date. Dac캒 lucrezi cu documente mari sau texte lungi, le po탵i descompune pe baza interog캒rilor pe care le a탳tep탵i. Descompunerea se poate face la nivel de propozi탵ie sau la nivel de paragraf. Deoarece descompunerea deriv캒 semnifica탵ii din cuvintele din jurul lor, po탵i ad캒uga un alt context unei buc캒탵i, de exemplu, prin ad캒ugarea titlului documentului sau includerea unui text 칥nainte sau dup캒 bucata respectiv캒. Po탵i descompune datele astfel:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Odat캒 descompus, putem apoi s캒 embed캒m textul nostru folosind diferite modele de embedding. Unele modele pe care le po탵i folosi includ: word2vec, ada-002 de la OpenAI, Azure Computer Vision 탳i multe altele. Selectarea unui model de utilizat va depinde de limbile pe care le folose탳ti, tipul de con탵inut codificat (text/imagine/audio), dimensiunea intr캒rii pe care o poate codifica 탳i lungimea ie탳irii embedding-ului.

Un exemplu de text embedat folosind modelul `text-embedding-ada-002` de la OpenAI este:
![un embedding al cuv칙ntului pisic캒](../../../translated_images/cat.3db013cbca4fd5d90438ea7b312ad0364f7686cf79931ab15cd5922151aea53e.ro.png)

## Reg캒sire 탳i C캒utare Vectorial캒

C칙nd un utilizator pune o 칥ntrebare, sistemul de reg캒sire o transform캒 칥ntr-un vector folosind encoderul de interogare, apoi caut캒 칥n indexul nostru de c캒utare a documentelor vectorii relevan탵i din document care sunt lega탵i de intrare. Odat캒 ce este gata, converte탳te at칙t vectorul de intrare, c칙t 탳i vectorii documentului 칥n text 탳i 칥l trece prin LLM.

### Reg캒sire

Reg캒sirea se 칥nt칙mpl캒 atunci c칙nd sistemul 칥ncearc캒 s캒 g캒seasc캒 rapid documentele din index care satisfac criteriile de c캒utare. Scopul sistemului de reg캒sire este de a ob탵ine documente care vor fi folosite pentru a oferi context 탳i a fundamenta LLM-ul pe datele tale.

Exist캒 mai multe moduri de a efectua c캒ut캒ri 칥n baza noastr캒 de date, cum ar fi:

- **C캒utare dup캒 cuvinte cheie** - folosit캒 pentru c캒ut캒ri textuale

- **C캒utare semantic캒** - folose탳te semnifica탵ia semantic캒 a cuvintelor

- **C캒utare vectorial캒** - converte탳te documentele din text 칥n reprezent캒ri vectoriale folosind modele de embedding. Reg캒sirea se va face prin interogarea documentelor ale c캒ror reprezent캒ri vectoriale sunt cele mai apropiate de 칥ntrebarea utilizatorului.

- **Hibrid** - o combina탵ie de c캒utare dup캒 cuvinte cheie 탳i c캒utare vectorial캒.

O provocare cu reg캒sirea apare atunci c칙nd nu exist캒 un r캒spuns similar cu interogarea 칥n baza de date, sistemul va returna atunci cele mai bune informa탵ii pe care le pot ob탵ine, totu탳i, po탵i folosi tactici precum stabilirea distan탵ei maxime pentru relevan탵캒 sau folosirea c캒ut캒rii hibride care combin캒 at칙t cuvinte cheie, c칙t 탳i c캒utare vectorial캒. 칉n aceast캒 lec탵ie vom folosi c캒utarea hibrid캒, o combina탵ie de c캒utare vectorial캒 탳i dup캒 cuvinte cheie. Vom stoca datele noastre 칥ntr-un dataframe cu coloane care con탵in buc캒탵ile, precum 탳i embedding-urile.

### Similaritate Vectorial캒

Sistemul de reg캒sire va c캒uta 칥n baza de cuno탳tin탵e embedding-uri care sunt apropiate, cel mai apropiat vecin, deoarece sunt texte similare. 칉n scenariul 칥n care un utilizator pune o interogare, aceasta este mai 칥nt칙i embedat캒, apoi se potrive탳te cu embedding-uri similare. M캒surarea comun캒 care este folosit캒 pentru a g캒si c칙t de similare sunt diferite vectori este similaritatea cosinus, care se bazeaz캒 pe unghiul dintre doi vectori.

Putem m캒sura similaritatea folosind alte alternative, cum ar fi distan탵a euclidian캒, care este linia dreapt캒 칥ntre punctele finale ale vectorilor 탳i produsul punct care m캒soar캒 suma produselor elementelor corespunz캒toare ale doi vectori.

### Index de c캒utare

C칙nd facem reg캒sire, va trebui s캒 construim un index de c캒utare pentru baza noastr캒 de cuno탳tin탵e 칥nainte de a efectua c캒utarea. Un index va stoca embedding-urile noastre 탳i poate reg캒si rapid cele mai similare buc캒탵i chiar 탳i 칥ntr-o baz캒 de date mare. Putem crea indexul nostru local folosind:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Reordonare

Odat캒 ce ai interogat baza de date, s-ar putea s캒 fie nevoie s캒 sortezi rezultatele de la cele mai relevante. Un LLM de reordonare utilizeaz캒 칉nv캒탵area Automat캒 pentru a 칥mbun캒t캒탵i relevan탵a rezultatelor c캒ut캒rii prin ordonarea lor de la cele mai relevante. Folosind Azure AI Search, reordonarea se face automat pentru tine folosind un reordonator semantic. Un exemplu despre cum func탵ioneaz캒 reordonarea folosind cei mai apropia탵i vecini:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Pun칙nd totul 칥mpreun캒

Ultimul pas este ad캒ugarea LLM-ului nostru 칥n amestec pentru a putea ob탵ine r캒spunsuri care sunt fundamentate pe datele noastre. Putem implementa astfel:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluarea aplica탵iei noastre

### Metrici de Evaluare

- Calitatea r캒spunsurilor furnizate asigur칙ndu-se c캒 sun캒 natural, fluent 탳i uman

- Fundamentarea datelor: evalu칙nd dac캒 r캒spunsul provine din documentele furnizate

- Relevan탵캒: evalu칙nd dac캒 r캒spunsul se potrive탳te 탳i este legat de 칥ntrebarea pus캒

- Fluen탵캒 - dac캒 r캒spunsul are sens gramatical

## Cazuri de Utilizare pentru folosirea RAG (Generarea Augmentat캒 de Reg캒sire) 탳i bazele de date vectoriale

Exist캒 multe cazuri de utilizare diferite unde apelurile func탵ionale pot 칥mbun캒t캒탵i aplica탵ia ta, cum ar fi:

- 칉ntreb캒ri 탳i R캒spunsuri: fundamentarea datelor companiei tale la un chat care poate fi folosit de angaja탵i pentru a pune 칥ntreb캒ri.

- Sisteme de Recomandare: unde po탵i crea un sistem care potrive탳te cele mai similare valori, de exemplu, filme, restaurante 탳i multe altele.

- Servicii de Chatbot: po탵i stoca istoricul chatului 탳i personaliza conversa탵ia pe baza datelor utilizatorului.

- C캒utare de imagini bazat캒 pe embedding-uri vectoriale, util캒 atunci c칙nd faci recunoa탳tere de imagini 탳i detectare de anomalii.

## Rezumat

Am acoperit aspectele fundamentale ale RAG de la ad캒ugarea datelor noastre la aplica탵ie, interogarea utilizatorului 탳i ie탳irea. Pentru a simplifica crearea RAG, po탵i folosi cadre precum Semanti Kernel, Langchain sau Autogen.

## Sarcin캒

Pentru a continua 칥nv캒탵area Gener캒rii Augmentate de Reg캒sire (RAG) po탵i construi:

- Construie탳te o interfa탵캒 pentru aplica탵ie folosind cadrul la alegerea ta

- Utilizeaz캒 un cadru, fie LangChain, fie Semantic Kernel, 탳i recreeaz캒 aplica탵ia ta.

Felicit캒ri pentru finalizarea lec탵iei 游녪.

## 칉nv캒탵area nu se opre탳te aici, continu캒 c캒l캒toria

Dup캒 finalizarea acestei lec탵ii, verific캒 [colec탵ia noastr캒 de 칉nv캒탵are a AI Generative](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pentru a continua s캒 칥탵i 칥mbun캒t캒탵e탳ti cuno탳tin탵ele despre AI Generativ캒!

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). De탳i ne str캒duim s캒 ob탵inem acurate탵e, v캒 rug캒m s캒 fi탵i con탳tien탵i c캒 traducerile automate pot con탵ine erori sau inexactit캒탵i. Documentul original 칥n limba sa matern캒 ar trebui considerat sursa autoritar캒. Pentru informa탵ii critice, se recomand캒 traducerea uman캒 profesional캒. Nu ne asum캒m responsabilitatea pentru ne칥n탵elegeri sau interpret캒ri gre탳ite care decurg din utilizarea acestei traduceri.