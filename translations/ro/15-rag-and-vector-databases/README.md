<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-07-09T16:20:03+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "ro"
}
-->
# Retrieval Augmented Generation (RAG) È™i Baze de Date Vectoriale

[![Retrieval Augmented Generation (RAG) È™i Baze de Date Vectoriale](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.ro.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

Ãn lecÈ›ia despre aplicaÈ›ii de cÄƒutare, am Ã®nvÄƒÈ›at pe scurt cum sÄƒ integrezi propriile date Ã®n Modelele Mari de Limbaj (LLM-uri). Ãn aceastÄƒ lecÈ›ie, vom aprofunda conceptele de ancorare a datelor Ã®n aplicaÈ›ia ta LLM, mecanica procesului È™i metodele de stocare a datelor, inclusiv atÃ¢t embeddings, cÃ¢t È™i text.

> **Video Ã®n curÃ¢nd**

## Introducere

Ãn aceastÄƒ lecÈ›ie vom acoperi urmÄƒtoarele:

- O introducere Ã®n RAG, ce este È™i de ce este folosit Ã®n AI (inteligenÈ›Äƒ artificialÄƒ).

- ÃnÈ›elegerea bazelor de date vectoriale È™i crearea uneia pentru aplicaÈ›ia noastrÄƒ.

- Un exemplu practic despre cum sÄƒ integrezi RAG Ã®ntr-o aplicaÈ›ie.

## Obiective de Ã®nvÄƒÈ›are

DupÄƒ finalizarea acestei lecÈ›ii, vei putea:

- Explica importanÈ›a RAG Ã®n recuperarea È™i procesarea datelor.

- Configura o aplicaÈ›ie RAG È™i ancoreazÄƒ-È›i datele Ã®ntr-un LLM.

- Integrarea eficientÄƒ a RAG È™i bazelor de date vectoriale Ã®n aplicaÈ›iile LLM.

## Scenariul nostru: Ã®mbunÄƒtÄƒÈ›irea LLM-urilor cu propriile noastre date

Pentru aceastÄƒ lecÈ›ie, dorim sÄƒ adÄƒugÄƒm propriile noastre notiÈ›e Ã®n startup-ul educaÈ›ional, ceea ce permite chatbot-ului sÄƒ obÈ›inÄƒ mai multe informaÈ›ii despre diferite subiecte. Folosind notiÈ›ele pe care le avem, cursanÈ›ii vor putea studia mai bine È™i Ã®nÈ›elege diferitele teme, facilitÃ¢nd astfel revizuirea pentru examene. Pentru a crea scenariul nostru, vom folosi:

- `Azure OpenAI:` LLM-ul pe care Ã®l vom folosi pentru a crea chatbot-ul nostru

- `LecÈ›ia AI pentru Ã®ncepÄƒtori despre ReÈ›ele Neuronale:` aceasta va fi baza de date pe care ne vom ancora LLM-ul

- `Azure AI Search` È™i `Azure Cosmos DB:` baza de date vectorialÄƒ pentru stocarea datelor È™i crearea unui index de cÄƒutare

Utilizatorii vor putea crea teste practice din notiÈ›ele lor, carduri de revizuire È™i rezumate concise. Pentru a Ã®ncepe, sÄƒ vedem ce este RAG È™i cum funcÈ›ioneazÄƒ:

## Retrieval Augmented Generation (RAG)

Un chatbot alimentat de un LLM proceseazÄƒ prompturile utilizatorilor pentru a genera rÄƒspunsuri. Este conceput sÄƒ fie interactiv È™i sÄƒ interacÈ›ioneze cu utilizatorii pe o gamÄƒ largÄƒ de subiecte. TotuÈ™i, rÄƒspunsurile sale sunt limitate la contextul oferit È™i la datele de antrenament de bazÄƒ. De exemplu, cunoÈ™tinÈ›ele GPT-4 sunt actualizate pÃ¢nÄƒ Ã®n septembrie 2021, ceea ce Ã®nseamnÄƒ cÄƒ nu are informaÈ›ii despre evenimentele care au avut loc dupÄƒ aceastÄƒ perioadÄƒ. Ãn plus, datele folosite pentru antrenarea LLM-urilor exclud informaÈ›ii confidenÈ›iale, cum ar fi notiÈ›ele personale sau manualul de produs al unei companii.

### Cum funcÈ›ioneazÄƒ RAG-urile (Retrieval Augmented Generation)

![desen care aratÄƒ cum funcÈ›ioneazÄƒ RAG-urile](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.ro.png)

SÄƒ presupunem cÄƒ vrei sÄƒ lansezi un chatbot care creeazÄƒ teste din notiÈ›ele tale, vei avea nevoie de o conexiune la baza de cunoÈ™tinÈ›e. Aici intervine RAG. RAG-urile funcÈ›ioneazÄƒ astfel:

- **Baza de cunoÈ™tinÈ›e:** Ãnainte de recuperare, aceste documente trebuie preluate È™i preprocesate, de obicei prin Ã®mpÄƒrÈ›irea documentelor mari Ã®n bucÄƒÈ›i mai mici, transformarea lor Ã®n embeddings textuale È™i stocarea lor Ã®ntr-o bazÄƒ de date.

- **Ãntrebarea utilizatorului:** utilizatorul pune o Ã®ntrebare

- **Recuperare:** CÃ¢nd utilizatorul pune o Ã®ntrebare, modelul de embedding recupereazÄƒ informaÈ›ii relevante din baza noastrÄƒ de cunoÈ™tinÈ›e pentru a oferi mai mult context care va fi Ã®ncorporat Ã®n prompt.

- **Generare augmentatÄƒ:** LLM-ul Ã®È™i Ã®mbunÄƒtÄƒÈ›eÈ™te rÄƒspunsul pe baza datelor recuperate. Astfel, rÄƒspunsul generat nu se bazeazÄƒ doar pe datele pre-antrenate, ci È™i pe informaÈ›ii relevante din contextul adÄƒugat. Datele recuperate sunt folosite pentru a augmenta rÄƒspunsurile LLM-ului. LLM-ul returneazÄƒ apoi un rÄƒspuns la Ã®ntrebarea utilizatorului.

![desen care aratÄƒ arhitectura RAG-urilor](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.ro.png)

Arhitectura RAG-urilor este implementatÄƒ folosind transformere, constÃ¢nd din douÄƒ pÄƒrÈ›i: un encoder È™i un decoder. De exemplu, cÃ¢nd un utilizator pune o Ã®ntrebare, textul de intrare este â€encodatâ€ Ã®n vectori care surprind sensul cuvintelor, iar vectorii sunt â€decodaÈ›iâ€ Ã®n indexul nostru de documente È™i genereazÄƒ text nou bazat pe Ã®ntrebarea utilizatorului. LLM-ul foloseÈ™te un model encoder-decoder pentru a genera output-ul.

DouÄƒ abordÄƒri pentru implementarea RAG conform lucrÄƒrii propuse: [Retrieval-Augmented Generation for Knowledge intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) sunt:

- **_RAG-Sequence_** folosind documentele recuperate pentru a prezice cel mai bun rÄƒspuns posibil la o Ã®ntrebare a utilizatorului

- **RAG-Token** folosind documentele pentru a genera urmÄƒtorul token, apoi le recupereazÄƒ pentru a rÄƒspunde Ã®ntrebÄƒrii utilizatorului

### De ce ai folosi RAG-uri?

- **BogÄƒÈ›ia informaÈ›iilor:** asigurÄƒ cÄƒ rÄƒspunsurile textuale sunt actualizate È™i relevante. Astfel, Ã®mbunÄƒtÄƒÈ›eÈ™te performanÈ›a Ã®n sarcini specifice domeniului prin accesarea bazei interne de cunoÈ™tinÈ›e.

- Reduce fabricarea de informaÈ›ii prin utilizarea **datelor verificabile** din baza de cunoÈ™tinÈ›e pentru a oferi context Ã®ntrebÄƒrilor utilizatorilor.

- Este **cost-eficient** deoarece este mai economic comparativ cu fine-tuning-ul unui LLM.

## Crearea unei baze de cunoÈ™tinÈ›e

AplicaÈ›ia noastrÄƒ se bazeazÄƒ pe datele noastre personale, adicÄƒ lecÈ›ia despre ReÈ›ele Neuronale din curriculumul AI pentru ÃncepÄƒtori.

### Baze de date vectoriale

O bazÄƒ de date vectorialÄƒ, spre deosebire de bazele de date tradiÈ›ionale, este o bazÄƒ specializatÄƒ conceputÄƒ pentru a stoca, gestiona È™i cÄƒuta vectori Ã®ncorporaÈ›i. Aceasta stocheazÄƒ reprezentÄƒri numerice ale documentelor. ÃmpÄƒrÈ›irea datelor Ã®n embeddings numerice face mai uÈ™oarÄƒ Ã®nÈ›elegerea È™i procesarea datelor de cÄƒtre sistemul nostru AI.

StocÄƒm embeddings Ã®n baze de date vectoriale deoarece LLM-urile au o limitÄƒ a numÄƒrului de tokeni pe care Ã®i pot primi ca input. Deoarece nu poÈ›i transmite Ã®ntregul embedding cÄƒtre un LLM, va trebui sÄƒ le Ã®mpÄƒrÈ›im Ã®n bucÄƒÈ›i, iar cÃ¢nd un utilizator pune o Ã®ntrebare, embeddings cele mai apropiate de Ã®ntrebare vor fi returnate Ã®mpreunÄƒ cu promptul. ÃmpÄƒrÈ›irea Ã®n bucÄƒÈ›i reduce È™i costurile legate de numÄƒrul de tokeni procesaÈ›i de LLM.

Unele baze de date vectoriale populare includ Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant È™i DeepLake. PoÈ›i crea un model Azure Cosmos DB folosind Azure CLI cu urmÄƒtoarea comandÄƒ:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Din text Ã®n embeddings

Ãnainte sÄƒ stocÄƒm datele, trebuie sÄƒ le convertim Ã®n embeddings vectoriale Ã®nainte de a le salva Ã®n baza de date. DacÄƒ lucrezi cu documente mari sau texte lungi, le poÈ›i Ã®mpÄƒrÈ›i Ã®n bucÄƒÈ›i Ã®n funcÈ›ie de Ã®ntrebÄƒrile pe care le aÈ™tepÈ›i. ÃmpÄƒrÈ›irea poate fi fÄƒcutÄƒ la nivel de propoziÈ›ie sau paragraf. Deoarece Ã®mpÄƒrÈ›irea derivÄƒ sensul din cuvintele din jur, poÈ›i adÄƒuga context suplimentar unei bucÄƒÈ›i, de exemplu, titlul documentului sau un text Ã®nainte sau dupÄƒ bucata respectivÄƒ. PoÈ›i Ã®mpÄƒrÈ›i datele astfel:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

OdatÄƒ Ã®mpÄƒrÈ›ite, putem apoi sÄƒ Ã®ncorporÄƒm textul folosind diferite modele de embedding. Unele modele pe care le poÈ›i folosi includ: word2vec, ada-002 de la OpenAI, Azure Computer Vision È™i multe altele. Alegerea modelului depinde de limbile folosite, tipul de conÈ›inut codificat (text/imagine/audio), dimensiunea inputului pe care Ã®l poate codifica È™i lungimea output-ului embedding.

Un exemplu de text Ã®ncorporat folosind modelul `text-embedding-ada-002` de la OpenAI este:
![un embedding al cuvÃ¢ntului cat](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.ro.png)

## Recuperare È™i cÄƒutare vectorialÄƒ

CÃ¢nd un utilizator pune o Ã®ntrebare, retriever-ul o transformÄƒ Ã®ntr-un vector folosind encoder-ul de interogÄƒri, apoi cautÄƒ Ã®n indexul nostru de documente vectorii relevanÈ›i care sunt legaÈ›i de input. DupÄƒ aceea, converteÈ™te atÃ¢t vectorul de input, cÃ¢t È™i vectorii documentelor Ã®n text È™i le transmite LLM-ului.

### Recuperare

Recuperarea are loc cÃ¢nd sistemul Ã®ncearcÄƒ sÄƒ gÄƒseascÄƒ rapid documentele din index care satisfac criteriile de cÄƒutare. Scopul retriever-ului este sÄƒ obÈ›inÄƒ documente care vor fi folosite pentru a oferi context È™i a ancora LLM-ul pe datele tale.

ExistÄƒ mai multe metode de cÄƒutare Ã®n baza noastrÄƒ de date, cum ar fi:

- **CÄƒutare dupÄƒ cuvinte cheie** - folositÄƒ pentru cÄƒutÄƒri textuale

- **CÄƒutare semanticÄƒ** - foloseÈ™te sensul semantic al cuvintelor

- **CÄƒutare vectorialÄƒ** - converteÈ™te documentele din text Ã®n reprezentÄƒri vectoriale folosind modele de embedding. Recuperarea se face prin interogarea documentelor ale cÄƒror vectori sunt cei mai apropiaÈ›i de Ã®ntrebarea utilizatorului.

- **HibridÄƒ** - o combinaÈ›ie Ã®ntre cÄƒutarea dupÄƒ cuvinte cheie È™i cÄƒutarea vectorialÄƒ.

O provocare la recuperare apare cÃ¢nd nu existÄƒ un rÄƒspuns similar Ã®n baza de date; sistemul va returna atunci cele mai bune informaÈ›ii disponibile, Ã®nsÄƒ poÈ›i folosi tactici precum setarea unei distanÈ›e maxime pentru relevanÈ›Äƒ sau utilizarea cÄƒutÄƒrii hibride care combinÄƒ ambele metode. Ãn aceastÄƒ lecÈ›ie vom folosi cÄƒutarea hibridÄƒ, o combinaÈ›ie Ã®ntre cÄƒutarea vectorialÄƒ È™i cea dupÄƒ cuvinte cheie. Vom stoca datele Ã®ntr-un dataframe cu coloane care conÈ›in bucÄƒÈ›ile de text È™i embeddings.

### Similaritatea vectorialÄƒ

Retriever-ul va cÄƒuta Ã®n baza de cunoÈ™tinÈ›e embeddings care sunt apropiate Ã®ntre ele, cel mai apropiat vecin, deoarece sunt texte similare. Ãn scenariu, cÃ¢nd un utilizator pune o Ã®ntrebare, aceasta este mai Ã®ntÃ¢i Ã®ncorporatÄƒ, apoi comparatÄƒ cu embeddings similare. MÄƒsura comunÄƒ folositÄƒ pentru a determina cÃ¢t de similare sunt vectorii este similaritatea cosinus, bazatÄƒ pe unghiul dintre doi vectori.

Putem mÄƒsura similaritatea È™i folosind alte metode, cum ar fi distanÈ›a EuclidianÄƒ, care este linia dreaptÄƒ dintre capetele vectorilor, È™i produsul scalar, care mÄƒsoarÄƒ suma produselor elementelor corespunzÄƒtoare a doi vectori.

### Indexul de cÄƒutare

Pentru a face recuperarea, trebuie sÄƒ construim un index de cÄƒutare pentru baza noastrÄƒ de cunoÈ™tinÈ›e Ã®nainte de a efectua cÄƒutarea. Un index va stoca embeddings È™i poate recupera rapid cele mai similare bucÄƒÈ›i chiar È™i Ã®ntr-o bazÄƒ de date mare. Putem crea indexul local folosind:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Re-rangare

DupÄƒ ce ai interogat baza de date, poate fi necesar sÄƒ sortezi rezultatele de la cele mai relevante. Un LLM de re-rangare foloseÈ™te Machine Learning pentru a Ã®mbunÄƒtÄƒÈ›i relevanÈ›a rezultatelor de cÄƒutare prin ordonarea lor de la cele mai relevante. Folosind Azure AI Search, re-rangarea se face automat cu un semantic reranker. Un exemplu de cum funcÈ›ioneazÄƒ re-rangarea folosind cei mai apropiaÈ›i vecini:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## PunÃ¢nd totul cap la cap

Ultimul pas este sÄƒ adÄƒugÄƒm LLM-ul Ã®n ecuaÈ›ie pentru a putea obÈ›ine rÄƒspunsuri ancorate Ã®n datele noastre. Putem implementa astfel:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluarea aplicaÈ›iei noastre

### Metrici de evaluare

- Calitatea rÄƒspunsurilor oferite, asigurÃ¢ndu-se cÄƒ sunÄƒ natural, fluent È™i uman

- Ancorarea datelor: evaluarea dacÄƒ rÄƒspunsul provine din documentele furnizate

- RelevanÈ›a: evaluarea dacÄƒ rÄƒspunsul corespunde È™i este legat de Ã®ntrebarea pusÄƒ

- FluenÈ›a - dacÄƒ rÄƒspunsul are sens din punct de vedere gramatical

## Cazuri de utilizare pentru RAG (Retrieval Augmented Generation) È™i baze de date vectoriale

ExistÄƒ multe cazuri de utilizare Ã®n care apelurile funcÈ›iilor pot Ã®mbunÄƒtÄƒÈ›i aplicaÈ›ia ta, cum ar fi:

- ÃntrebÄƒri È™i rÄƒspunsuri: ancorarea datelor companiei tale Ã®ntr-un chat folosit de angajaÈ›i pentru a pune Ã®ntrebÄƒri.

- Sisteme de recomandare: unde poÈ›i crea un sistem care potriveÈ™te cele mai similare valori, de exemplu filme, restaurante È™i altele.

- Servicii chatbot: poÈ›i stoca istoricul conversaÈ›iilor È™i personaliza dialogul pe baza datelor utilizatorului.

- CÄƒutare de imagini bazatÄƒ pe embeddings vectoriale, utilÄƒ Ã®n recunoaÈ™terea imaginilor È™i detectarea anomaliilor.

## Rezumat

Am acoperit aspectele fundamentale ale RAG, de la adÄƒugarea datelor Ã®n aplicaÈ›ie, Ã®ntrebarea utilizatorului È™i output-ul. Pentru a simplifica crearea RAG, poÈ›i folosi framework-uri precum Semantic Kernel, Langchain sau Autogen.

## Tema

Pentru a continua Ã®nvÄƒÈ›area despre Retrieval Augmented Generation (RAG), poÈ›i construi:

- Un front-end pentru aplicaÈ›ie folosind framework-ul preferat

- FoloseÈ™te un framework, fie LangChain, fie Semantic Kernel, È™i recreeazÄƒ aplicaÈ›ia ta.

FelicitÄƒri pentru finalizarea lecÈ›iei ğŸ‘.

## ÃnvÄƒÈ›area nu se opreÈ™te aici, continuÄƒ cÄƒlÄƒtoria

DupÄƒ ce ai terminat aceastÄƒ lecÈ›ie, consultÄƒ colecÈ›ia noastrÄƒ [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pentru a-È›i continua dezvoltarea cunoÈ™tinÈ›elor Ã®n Generative AI!

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim pentru acurateÈ›e, vÄƒ rugÄƒm sÄƒ reÈ›ineÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa nativÄƒ trebuie considerat sursa autorizatÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm rÄƒspunderea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite rezultate din utilizarea acestei traduceri.