<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T02:07:18+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "ro"
}
-->
# Framework-uri pentru reÈ›ele neuronale

AÈ™a cum am Ã®nvÄƒÈ›at deja, pentru a putea antrena reÈ›ele neuronale eficient trebuie sÄƒ facem douÄƒ lucruri:

* SÄƒ operÄƒm pe tensori, de exemplu sÄƒ Ã®nmulÈ›im, sÄƒ adunÄƒm È™i sÄƒ calculÄƒm funcÈ›ii precum sigmoid sau softmax
* SÄƒ calculÄƒm gradientele tuturor expresiilor, pentru a efectua optimizarea prin gradient descent

DeÈ™i biblioteca `numpy` poate face prima parte, avem nevoie de un mecanism pentru a calcula gradientele. Ãn cadrul nostru pe care l-am dezvoltat Ã®n secÈ›iunea anterioarÄƒ, a trebuit sÄƒ programÄƒm manual toate funcÈ›iile derivate Ã®n metoda `backward`, care face backpropagation. Ideal, un cadru ar trebui sÄƒ ne ofere oportunitatea de a calcula gradientele *oricÄƒrei expresii* pe care o putem defini.

Un alt aspect important este sÄƒ putem efectua calcule pe GPU sau pe alte unitÄƒÈ›i de calcul specializate, precum TPU. Antrenamentul reÈ›elelor neuronale profunde necesitÄƒ *foarte multe* calcule, iar posibilitatea de a paraleliza aceste calcule pe GPU-uri este foarte importantÄƒ.

> âœ… Termenul 'paralelizare' Ã®nseamnÄƒ distribuirea calculelor pe mai multe dispozitive.

Ãn prezent, cele mai populare douÄƒ cadre pentru reÈ›ele neuronale sunt: TensorFlow È™i PyTorch. Ambele oferÄƒ o API de nivel scÄƒzut pentru a opera cu tensori atÃ¢t pe CPU, cÃ¢t È™i pe GPU. Pe lÃ¢ngÄƒ API-ul de nivel scÄƒzut, existÄƒ È™i un API de nivel Ã®nalt, numit Keras È™i, respectiv, PyTorch Lightning.

API de nivel scÄƒzut | TensorFlow | PyTorch
--------------------|-------------------------------------|--------------------------------
API de nivel Ã®nalt  | Keras | PyTorch Lightning

**API-urile de nivel scÄƒzut** din ambele cadre Ã®È›i permit sÄƒ construieÈ™ti aÈ™a-numitele **grafuri computaÈ›ionale**. Acest graf defineÈ™te cum sÄƒ se calculeze ieÈ™irea (de obicei funcÈ›ia de pierdere) cu parametrii de intrare daÈ›i È™i poate fi transferat pentru calcul pe GPU, dacÄƒ este disponibil. ExistÄƒ funcÈ›ii pentru a diferenÈ›ia acest graf computaÈ›ional È™i a calcula gradientele, care pot fi apoi folosite pentru optimizarea parametrilor modelului.

**API-urile de nivel Ã®nalt** considerÄƒ Ã®n mare parte reÈ›elele neuronale ca o **secvenÈ›Äƒ de straturi** È™i fac construirea majoritÄƒÈ›ii reÈ›elelor neuronale mult mai uÈ™oarÄƒ. Antrenarea modelului necesitÄƒ de obicei pregÄƒtirea datelor È™i apoi apelarea unei funcÈ›ii `fit` pentru a face treaba.

API-ul de nivel Ã®nalt Ã®È›i permite sÄƒ construieÈ™ti reÈ›ele neuronale tipice foarte rapid, fÄƒrÄƒ sÄƒ te Ã®ngrijorezi de multe detalii. Ãn acelaÈ™i timp, API-urile de nivel scÄƒzut oferÄƒ mult mai mult control asupra procesului de antrenament È™i astfel sunt folosite mult Ã®n cercetare, cÃ¢nd te ocupi de noi arhitecturi de reÈ›ele neuronale.

Este, de asemenea, important sÄƒ Ã®nÈ›elegi cÄƒ poÈ›i folosi ambele API-uri Ã®mpreunÄƒ, de exemplu, poÈ›i dezvolta propria arhitecturÄƒ de strat de reÈ›ea folosind API-ul de nivel scÄƒzut È™i apoi sÄƒ o foloseÈ™ti Ã®n reÈ›eaua mai mare construitÄƒ È™i antrenatÄƒ cu API-ul de nivel Ã®nalt. Sau poÈ›i defini o reÈ›ea folosind API-ul de nivel Ã®nalt ca o secvenÈ›Äƒ de straturi È™i apoi sÄƒ foloseÈ™ti propriul tÄƒu ciclu de antrenament de nivel scÄƒzut pentru a efectua optimizarea. Ambele API-uri folosesc aceleaÈ™i concepte de bazÄƒ È™i sunt proiectate sÄƒ funcÈ›ioneze bine Ã®mpreunÄƒ.

## ÃnvÄƒÈ›are

Ãn acest curs, oferim majoritatea conÈ›inutului atÃ¢t pentru PyTorch, cÃ¢t È™i pentru TensorFlow. PoÈ›i alege cadrul preferat È™i sÄƒ parcurgi doar notiÈ›ele corespunzÄƒtoare. DacÄƒ nu eÈ™ti sigur ce cadru sÄƒ alegi, citeÈ™te cÃ¢teva discuÈ›ii pe internet despre **PyTorch vs. TensorFlow**. PoÈ›i, de asemenea, sÄƒ arunci o privire asupra ambelor cadre pentru a obÈ›ine o Ã®nÈ›elegere mai bunÄƒ.

Acolo unde este posibil, vom folosi API-urile de nivel Ã®nalt pentru simplitate. TotuÈ™i, credem cÄƒ este important sÄƒ Ã®nÈ›elegi cum funcÈ›ioneazÄƒ reÈ›elele neuronale de la bazÄƒ, aÈ™a cÄƒ la Ã®nceput Ã®ncepem prin a lucra cu API-ul de nivel scÄƒzut È™i tensori. TotuÈ™i, dacÄƒ vrei sÄƒ Ã®ncepi rapid È™i nu vrei sÄƒ petreci mult timp Ã®nvÄƒÈ›Ã¢nd aceste detalii, poÈ›i sÄƒ le sari È™i sÄƒ mergi direct la notiÈ›ele cu API-ul de nivel Ã®nalt.

## âœï¸ ExerciÈ›ii: Framework-uri

ContinuÄƒ Ã®nvÄƒÈ›area Ã®n urmÄƒtoarele notiÈ›e:

API de nivel scÄƒzut | NotiÈ›Äƒ TensorFlow+Keras | PyTorch
--------------------|-------------------------------------|--------------------------------
API de nivel Ã®nalt  | Keras | *PyTorch Lightning*

DupÄƒ ce ai stÄƒpÃ¢nit cadrele, sÄƒ recapitulÄƒm noÈ›iunea de overfitting.

# Overfitting

Overfitting este un concept extrem de important Ã®n Ã®nvÄƒÈ›area automatÄƒ È™i este foarte important sÄƒ-l Ã®nÈ›elegi corect!

ConsiderÄƒ urmÄƒtoarea problemÄƒ de aproximare a 5 puncte (reprezentate de `x` pe graficele de mai jos):

!liniear | overfit
-------------------------|--------------------------
**Model liniar, 2 parametri** | **Model neliniar, 7 parametri**
Eroare antrenament = 5.3 | Eroare antrenament = 0
Eroare validare = 5.1 | Eroare validare = 20

* Ãn stÃ¢nga, vedem o bunÄƒ aproximare printr-o linie dreaptÄƒ. Deoarece numÄƒrul de parametri este adecvat, modelul Ã®nÈ›elege corect distribuÈ›ia punctelor.
* Ãn dreapta, modelul este prea puternic. Deoarece avem doar 5 puncte È™i modelul are 7 parametri, se poate ajusta astfel Ã®ncÃ¢t sÄƒ treacÄƒ prin toate punctele, fÄƒcÃ¢nd ca eroarea de antrenament sÄƒ fie 0. TotuÈ™i, acest lucru Ã®mpiedicÄƒ modelul sÄƒ Ã®nÈ›eleagÄƒ corect modelul datelor, astfel cÄƒ eroarea de validare este foarte mare.

Este foarte important sÄƒ gÄƒseÈ™ti un echilibru corect Ã®ntre bogÄƒÈ›ia modelului (numÄƒrul de parametri) È™i numÄƒrul de eÈ™antioane de antrenament.

## De ce apare overfitting-ul

  * Nu sunt suficiente date de antrenament
  * Model prea puternic
  * Prea mult zgomot Ã®n datele de intrare

## Cum sÄƒ detectezi overfitting-ul

DupÄƒ cum poÈ›i vedea din graficul de mai sus, overfitting-ul poate fi detectat printr-o eroare de antrenament foarte micÄƒ È™i o eroare de validare mare. Ãn mod normal, Ã®n timpul antrenamentului vom vedea atÃ¢t erorile de antrenament, cÃ¢t È™i cele de validare Ã®ncepÃ¢nd sÄƒ scadÄƒ, È™i apoi la un moment dat eroarea de validare s-ar putea opri din scÄƒdere È™i sÄƒ Ã®nceapÄƒ sÄƒ creascÄƒ. Acesta va fi un semn de overfitting È™i un indicator cÄƒ probabil ar trebui sÄƒ oprim antrenamentul Ã®n acest punct (sau cel puÈ›in sÄƒ facem o capturÄƒ de moment a modelului).

overfitting

## Cum sÄƒ previi overfitting-ul

DacÄƒ vezi cÄƒ apare overfitting-ul, poÈ›i face unul dintre urmÄƒtoarele lucruri:

 * CreÈ™te cantitatea de date de antrenament
 * Redu complexitatea modelului
 * FoloseÈ™te o tehnicÄƒ de regularizare, cum ar fi Dropout, pe care o vom considera mai tÃ¢rziu.

## Overfitting È™i compromisul Bias-VarianÈ›Äƒ

Overfitting-ul este de fapt un caz al unei probleme mai generale Ã®n statisticÄƒ numitÄƒ Compromisul Bias-VarianÈ›Äƒ. DacÄƒ luÄƒm Ã®n considerare sursele posibile de eroare Ã®n modelul nostru, putem vedea douÄƒ tipuri de erori:

* **Erorile de bias** sunt cauzate de faptul cÄƒ algoritmul nostru nu poate captura corect relaÈ›ia dintre datele de antrenament. Poate rezulta din faptul cÄƒ modelul nostru nu este suficient de puternic (**underfitting**).
* **Erorile de varianÈ›Äƒ**, care sunt cauzate de modelul care aproximeazÄƒ zgomotul din datele de intrare Ã®n loc de relaÈ›ia semnificativÄƒ (**overfitting**).

Ãn timpul antrenamentului, eroarea de bias scade (pe mÄƒsurÄƒ ce modelul nostru Ã®nvaÈ›Äƒ sÄƒ aproximeze datele), iar eroarea de varianÈ›Äƒ creÈ™te. Este important sÄƒ oprim antrenamentul - fie manual (cÃ¢nd detectÄƒm overfitting-ul) fie automat (introducÃ¢nd regularizare) - pentru a preveni overfitting-ul.

## Concluzie

Ãn aceastÄƒ lecÈ›ie, ai Ã®nvÄƒÈ›at despre diferenÈ›ele dintre diversele API-uri pentru cele mai populare douÄƒ cadre AI, TensorFlow È™i PyTorch. Ãn plus, ai Ã®nvÄƒÈ›at despre un subiect foarte important, overfitting-ul.

## ğŸš€ Provocare

Ãn notiÈ›ele Ã®nsoÈ›itoare, vei gÄƒsi 'sarcini' la final; parcurge notiÈ›ele È™i completeazÄƒ sarcinile.

## Recapitulare È™i auto-studiu

FÄƒ cÃ¢teva cercetÄƒri pe urmÄƒtoarele subiecte:

- TensorFlow
- PyTorch
- Overfitting

ÃntreabÄƒ-te urmÄƒtoarele Ã®ntrebÄƒri:

- Care este diferenÈ›a dintre TensorFlow È™i PyTorch?
- Care este diferenÈ›a dintre overfitting È™i underfitting?

## TemÄƒ

Ãn acest laborator, È›i se cere sÄƒ rezolvi douÄƒ probleme de clasificare folosind reÈ›ele complet conectate cu un singur strat È™i cu mai multe straturi folosind PyTorch sau TensorFlow.

**Declinarea responsabilitÄƒÈ›ii**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ umanÄƒ. Nu suntem responsabili pentru neÃ®nÈ›elegerile sau interpretÄƒrile greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.