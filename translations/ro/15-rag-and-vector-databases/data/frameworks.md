<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:37:08+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "ro"
}
-->
# Framework-uri pentru ReÈ›ele Neuronale

AÈ™a cum am Ã®nvÄƒÈ›at deja, pentru a putea antrena reÈ›ele neuronale eficient, trebuie sÄƒ facem douÄƒ lucruri:

* SÄƒ operÄƒm cu tensori, de exemplu sÄƒ Ã®nmulÈ›im, sÄƒ adunÄƒm È™i sÄƒ calculÄƒm funcÈ›ii precum sigmoid sau softmax
* SÄƒ calculÄƒm gradientele tuturor expresiilor, pentru a putea efectua optimizarea prin gradient descendent

DeÈ™i biblioteca `numpy` poate face prima parte, avem nevoie de un mecanism pentru a calcula gradientele. Ãn cadrul nostru, pe care l-am dezvoltat Ã®n secÈ›iunea anterioarÄƒ, a trebuit sÄƒ programÄƒm manual toate funcÈ›iile derivate Ã®n metoda `backward`, care realizeazÄƒ backpropagation. Ideal, un framework ar trebui sÄƒ ne ofere posibilitatea de a calcula gradientele *oricÄƒrei expresii* pe care o putem defini.

Un alt aspect important este sÄƒ putem efectua calcule pe GPU sau pe alte unitÄƒÈ›i specializate de calcul, cum ar fi TPU. Antrenarea reÈ›elelor neuronale profunde necesitÄƒ *foarte multe* calcule, iar posibilitatea de a paraleliza aceste calcule pe GPU-uri este esenÈ›ialÄƒ.

> âœ… Termenul â€paralelizaâ€ Ã®nseamnÄƒ distribuirea calculelor pe mai multe dispozitive.

Ãn prezent, cele douÄƒ framework-uri de reÈ›ele neuronale cele mai populare sunt: TensorFlow È™i PyTorch. Ambele oferÄƒ o API la nivel scÄƒzut pentru a opera cu tensori atÃ¢t pe CPU, cÃ¢t È™i pe GPU. Deasupra acestei API la nivel scÄƒzut, existÄƒ È™i o API la nivel Ã®nalt, numitÄƒ Keras, respectiv PyTorch Lightning.

Low-Level API | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
High-level API| Keras| PyTorch

**API-urile la nivel scÄƒzut** din ambele framework-uri permit construirea aÈ™a-numitelor **grafuri computaÈ›ionale**. Acest graf defineÈ™te cum sÄƒ calculÄƒm ieÈ™irea (de obicei funcÈ›ia de pierdere) pentru parametrii de intrare daÈ›i È™i poate fi trimis pentru calcul pe GPU, dacÄƒ este disponibil. ExistÄƒ funcÈ›ii pentru a diferenÈ›ia acest graf computaÈ›ional È™i a calcula gradientele, care pot fi apoi folosite pentru optimizarea parametrilor modelului.

**API-urile la nivel Ã®nalt** trateazÄƒ reÈ›elele neuronale ca o **secvenÈ›Äƒ de straturi** È™i faciliteazÄƒ mult construirea majoritÄƒÈ›ii reÈ›elelor neuronale. Antrenarea modelului necesitÄƒ de obicei pregÄƒtirea datelor È™i apoi apelarea funcÈ›iei `fit` pentru a face treaba.

API-ul la nivel Ã®nalt Ã®È›i permite sÄƒ construieÈ™ti rapid reÈ›ele neuronale tipice fÄƒrÄƒ sÄƒ te preocupi de multe detalii. Ãn acelaÈ™i timp, API-ul la nivel scÄƒzut oferÄƒ mult mai mult control asupra procesului de antrenare, motiv pentru care este folosit frecvent Ã®n cercetare, cÃ¢nd lucrezi cu arhitecturi noi de reÈ›ele neuronale.

Este important sÄƒ Ã®nÈ›elegi cÄƒ poÈ›i folosi ambele API-uri Ã®mpreunÄƒ, de exemplu poÈ›i dezvolta propria arhitecturÄƒ de strat de reÈ›ea folosind API-ul la nivel scÄƒzut È™i apoi sÄƒ o foloseÈ™ti Ã®ntr-o reÈ›ea mai mare construitÄƒ È™i antrenatÄƒ cu API-ul la nivel Ã®nalt. Sau poÈ›i defini o reÈ›ea folosind API-ul la nivel Ã®nalt ca o secvenÈ›Äƒ de straturi È™i apoi sÄƒ foloseÈ™ti propriul tÄƒu ciclu de antrenare la nivel scÄƒzut pentru optimizare. Ambele API-uri folosesc aceleaÈ™i concepte de bazÄƒ È™i sunt proiectate sÄƒ funcÈ›ioneze bine Ã®mpreunÄƒ.

## ÃnvÄƒÈ›are

Ãn acest curs, oferim majoritatea conÈ›inutului atÃ¢t pentru PyTorch, cÃ¢t È™i pentru TensorFlow. PoÈ›i alege framework-ul preferat È™i sÄƒ parcurgi doar notebook-urile corespunzÄƒtoare. DacÄƒ nu eÈ™ti sigur ce framework sÄƒ alegi, citeÈ™te cÃ¢teva discuÈ›ii de pe internet despre **PyTorch vs. TensorFlow**. PoÈ›i, de asemenea, sÄƒ arunci o privire la ambele framework-uri pentru a Ã®nÈ›elege mai bine.

Ori de cÃ¢te ori este posibil, vom folosi API-urile la nivel Ã®nalt pentru simplitate. TotuÈ™i, considerÄƒm cÄƒ este important sÄƒ Ã®nÈ›elegi cum funcÈ›ioneazÄƒ reÈ›elele neuronale de la bazÄƒ, aÈ™a cÄƒ la Ã®nceput vom lucra cu API-ul la nivel scÄƒzut È™i cu tensori. DacÄƒ vrei sÄƒ Ã®ncepi rapid È™i nu vrei sÄƒ pierzi mult timp cu aceste detalii, poÈ›i sÄƒri peste ele È™i sÄƒ mergi direct la notebook-urile cu API la nivel Ã®nalt.

## âœï¸ ExerciÈ›ii: Framework-uri

ContinuÄƒ-È›i Ã®nvÄƒÈ›area Ã®n urmÄƒtoarele notebook-uri:

Low-Level API | TensorFlow+Keras Notebook | PyTorch
--------------|-------------------------------------|--------------------------------
High-level API| Keras | *PyTorch Lightning*

DupÄƒ ce stÄƒpÃ¢neÈ™ti framework-urile, sÄƒ recapitulÄƒm noÈ›iunea de overfitting.

# Overfitting

Overfitting-ul este un concept extrem de important Ã®n Ã®nvÄƒÈ›area automatÄƒ È™i este esenÈ›ial sÄƒ Ã®l Ã®nÈ›elegem corect!

ConsiderÄƒ urmÄƒtoarea problemÄƒ de aproximare a 5 puncte (reprezentate prin `x` pe graficele de mai jos):

!linear | overfit
-------------------------|--------------------------
**Model liniar, 2 parametri** | **Model neliniar, 7 parametri**
Eroare de antrenare = 5.3 | Eroare de antrenare = 0
Eroare de validare = 5.1 | Eroare de validare = 20

* Ãn stÃ¢nga, vedem o aproximare bunÄƒ printr-o linie dreaptÄƒ. Deoarece numÄƒrul de parametri este adecvat, modelul surprinde corect distribuÈ›ia punctelor.
* Ãn dreapta, modelul este prea puternic. Pentru cÄƒ avem doar 5 puncte, iar modelul are 7 parametri, acesta se poate ajusta astfel Ã®ncÃ¢t sÄƒ treacÄƒ prin toate punctele, fÄƒcÃ¢nd eroarea de antrenare sÄƒ fie 0. TotuÈ™i, acest lucru Ã®mpiedicÄƒ modelul sÄƒ Ã®nÈ›eleagÄƒ corect tiparul din spatele datelor, astfel eroarea de validare este foarte mare.

Este foarte important sÄƒ gÄƒsim un echilibru corect Ã®ntre complexitatea modelului (numÄƒrul de parametri) È™i numÄƒrul de exemple de antrenament.

## De ce apare overfitting-ul

  * Date insuficiente pentru antrenament
  * Model prea complex
  * Prea mult zgomot Ã®n datele de intrare

## Cum detectÄƒm overfitting-ul

DupÄƒ cum se vede Ã®n graficul de mai sus, overfitting-ul poate fi detectat printr-o eroare foarte micÄƒ la antrenament È™i o eroare mare la validare. De obicei, Ã®n timpul antrenamentului, atÃ¢t eroarea de antrenament, cÃ¢t È™i cea de validare scad, iar apoi, la un moment dat, eroarea de validare se opreÈ™te din scÄƒzut È™i Ã®ncepe sÄƒ creascÄƒ. Acesta este un semn de overfitting È™i indicÄƒ faptul cÄƒ probabil ar trebui sÄƒ oprim antrenamentul Ã®n acel punct (sau cel puÈ›in sÄƒ salvÄƒm o copie a modelului).

overfitting

## Cum prevenim overfitting-ul

DacÄƒ observi cÄƒ apare overfitting, poÈ›i face una dintre urmÄƒtoarele:

 * CreÈ™te cantitatea de date pentru antrenament
 * Scade complexitatea modelului
 * FoloseÈ™te o tehnicÄƒ de regularizare, cum ar fi Dropout, pe care o vom analiza mai tÃ¢rziu.

## Overfitting È™i compromisului Bias-VarianÈ›Äƒ

Overfitting-ul este, de fapt, un caz particular al unei probleme mai generale din statisticÄƒ numitÄƒ compromis Bias-VarianÈ›Äƒ. DacÄƒ analizÄƒm sursele posibile de eroare Ã®n modelul nostru, putem identifica douÄƒ tipuri de erori:

* **Erorile de bias** sunt cauzate de incapacitatea algoritmului de a surprinde corect relaÈ›ia dintre datele de antrenament. Acestea pot apÄƒrea dacÄƒ modelul nu este suficient de puternic (**underfitting**).
* **Erorile de varianÈ›Äƒ**, care apar atunci cÃ¢nd modelul Ã®ncearcÄƒ sÄƒ aproximeze zgomotul din datele de intrare Ã®n loc de relaÈ›ia semnificativÄƒ (**overfitting**).

Ãn timpul antrenamentului, eroarea de bias scade (pe mÄƒsurÄƒ ce modelul Ã®nvaÈ›Äƒ sÄƒ aproximeze datele), iar eroarea de varianÈ›Äƒ creÈ™te. Este important sÄƒ oprim antrenamentul â€“ fie manual (cÃ¢nd detectÄƒm overfitting) sau automat (prin introducerea regularizÄƒrii) â€“ pentru a preveni overfitting-ul.

## Concluzie

Ãn aceastÄƒ lecÈ›ie, ai Ã®nvÄƒÈ›at despre diferenÈ›ele dintre diversele API-uri pentru cele douÄƒ framework-uri AI cele mai populare, TensorFlow È™i PyTorch. Ãn plus, ai aflat despre un subiect foarte important, overfitting-ul.

## ğŸš€ Provocare

Ãn notebook-urile Ã®nsoÈ›itoare vei gÄƒsi â€sarciniâ€ la final; parcurge notebook-urile È™i rezolvÄƒ sarcinile.

## Recapitulare & Studiu individual

CautÄƒ informaÈ›ii despre urmÄƒtoarele subiecte:

- TensorFlow
- PyTorch
- Overfitting

RÄƒspunde-È›i la urmÄƒtoarele Ã®ntrebÄƒri:

- Care este diferenÈ›a dintre TensorFlow È™i PyTorch?
- Care este diferenÈ›a dintre overfitting È™i underfitting?

## Tema

Ãn acest laborator, È›i se cere sÄƒ rezolvi douÄƒ probleme de clasificare folosind reÈ›ele complet conectate cu un singur strat È™i cu mai multe straturi, folosind PyTorch sau TensorFlow.

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim pentru acurateÈ›e, vÄƒ rugÄƒm sÄƒ reÈ›ineÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa nativÄƒ trebuie considerat sursa autorizatÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm rÄƒspunderea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite rezultate din utilizarea acestei traduceri.