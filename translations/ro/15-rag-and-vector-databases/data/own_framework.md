<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:51:07+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "ro"
}
-->
# Introducere Ã®n ReÈ›ele Neuronale. Perceptron Multi-Stratificat

Ãn secÈ›iunea anterioarÄƒ, ai Ã®nvÄƒÈ›at despre cel mai simplu model de reÈ›ea neuronalÄƒ - perceptronul cu un singur strat, un model liniar de clasificare binarÄƒ.

Ãn aceastÄƒ secÈ›iune vom extinde acest model Ã®ntr-un cadru mai flexibil, care ne va permite sÄƒ:

* realizÄƒm **clasificare multi-clasÄƒ** pe lÃ¢ngÄƒ clasificarea binarÄƒ
* rezolvÄƒm **probleme de regresie** pe lÃ¢ngÄƒ clasificare
* separÄƒm clase care nu sunt liniar separabile

De asemenea, vom dezvolta propriul nostru cadru modular Ã®n Python care ne va permite sÄƒ construim diferite arhitecturi de reÈ›ele neuronale.

## Formalizarea ÃnvÄƒÈ›Äƒrii Automate

SÄƒ Ã®ncepem prin a formaliza problema ÃnvÄƒÈ›Äƒrii Automate. Presupunem cÄƒ avem un set de date de antrenament **X** cu etichete **Y**, È™i trebuie sÄƒ construim un model *f* care sÄƒ facÄƒ predicÈ›ii cÃ¢t mai precise. Calitatea predicÈ›iilor este mÄƒsuratÄƒ prin **funcÈ›ia de pierdere** â„’. UrmÄƒtoarele funcÈ›ii de pierdere sunt frecvent utilizate:

* Pentru problema de regresie, cÃ¢nd trebuie sÄƒ prezicem un numÄƒr, putem folosi **eroarea absolutÄƒ** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, sau **eroarea pÄƒtraticÄƒ** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pentru clasificare, folosim **pierdere 0-1** (care este practic aceeaÈ™i cu **acurateÈ›ea** modelului), sau **pierdere logisticÄƒ**.

Pentru perceptronul cu un singur strat, funcÈ›ia *f* a fost definitÄƒ ca o funcÈ›ie liniarÄƒ *f(x)=wx+b* (aici *w* este matricea de greutÄƒÈ›i, *x* este vectorul caracteristicilor de intrare, iar *b* este vectorul de bias). Pentru diferite arhitecturi de reÈ›ele neuronale, aceastÄƒ funcÈ›ie poate lua o formÄƒ mai complexÄƒ.

> Ãn cazul clasificÄƒrii, este adesea de dorit sÄƒ obÈ›inem probabilitÄƒÈ›ile claselor corespunzÄƒtoare ca ieÈ™ire a reÈ›elei. Pentru a converti numere arbitrare Ã®n probabilitÄƒÈ›i (de exemplu, pentru a normaliza ieÈ™irea), folosim frecvent funcÈ›ia **softmax** Ïƒ, iar funcÈ›ia *f* devine *f(x)=Ïƒ(wx+b)*

Ãn definiÈ›ia lui *f* de mai sus, *w* È™i *b* sunt numiÈ›i **parametri** Î¸=âŸ¨*w,b*âŸ©. AvÃ¢nd setul de date âŸ¨**X**,**Y**âŸ©, putem calcula eroarea totalÄƒ pe Ã®ntregul set de date ca o funcÈ›ie a parametrilor Î¸.

> âœ… **Scopul antrenÄƒrii reÈ›elei neuronale este de a minimiza eroarea variind parametrii Î¸**

## Optimizarea prin Gradient Descendent

ExistÄƒ o metodÄƒ bine cunoscutÄƒ de optimizare a funcÈ›iilor numitÄƒ **gradient descendent**. Ideea este cÄƒ putem calcula derivata (Ã®n cazul multidimensional numitÄƒ **gradient**) a funcÈ›iei de pierdere Ã®n raport cu parametrii, È™i sÄƒ modificÄƒm parametrii astfel Ã®ncÃ¢t eroarea sÄƒ scadÄƒ. Acest lucru poate fi formalizat astfel:

* IniÈ›ializÄƒm parametrii cu niÈ™te valori aleatorii w<sup>(0)</sup>, b<sup>(0)</sup>
* RepetÄƒm urmÄƒtorul pas de multe ori:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Ãn timpul antrenÄƒrii, paÈ™ii de optimizare trebuie calculaÈ›i luÃ¢nd Ã®n considerare Ã®ntregul set de date (aminteÈ™te-È›i cÄƒ pierderea este calculatÄƒ ca sumÄƒ peste toate exemplele de antrenament). TotuÈ™i, Ã®n practicÄƒ luÄƒm porÈ›iuni mici din setul de date numite **minibatch-uri**, È™i calculÄƒm gradientul pe baza unui subset de date. Deoarece subsetul este ales aleator de fiecare datÄƒ, aceastÄƒ metodÄƒ se numeÈ™te **gradient descendent stocastic** (SGD).

## Perceptron Multi-Stratificat È™i Backpropagation

ReÈ›eaua cu un singur strat, aÈ™a cum am vÄƒzut mai sus, este capabilÄƒ sÄƒ clasifice clase liniar separabile. Pentru a construi un model mai complex, putem combina mai multe straturi ale reÈ›elei. Matematic, asta Ã®nseamnÄƒ cÄƒ funcÈ›ia *f* va avea o formÄƒ mai complexÄƒ È™i va fi calculatÄƒ Ã®n mai mulÈ›i paÈ™i:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Aici, Î± este o **funcÈ›ie de activare non-liniarÄƒ**, Ïƒ este funcÈ›ia softmax, iar parametrii Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritmul gradient descendent rÄƒmÃ¢ne acelaÈ™i, dar calculul gradientului devine mai dificil. AplicÃ¢nd regula de derivare Ã®n lanÈ›, putem calcula derivatele astfel:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Regula de derivare Ã®n lanÈ› este folositÄƒ pentru a calcula derivatele funcÈ›iei de pierdere Ã®n raport cu parametrii.

ObservÄƒ cÄƒ partea din stÃ¢nga a tuturor acestor expresii este aceeaÈ™i, astfel Ã®ncÃ¢t putem calcula eficient derivatele Ã®ncepÃ¢nd de la funcÈ›ia de pierdere È™i mergÃ¢nd â€Ã®napoiâ€ prin graful computaÈ›ional. Astfel, metoda de antrenare a perceptronului multi-stratificat se numeÈ™te **backpropagation**, sau â€backpropâ€.

> TODO: citare imagine

> âœ… Vom detalia backprop Ã®n exemplul din notebook-ul nostru.

## Concluzie

Ãn aceastÄƒ lecÈ›ie, am construit propria noastrÄƒ bibliotecÄƒ de reÈ›ele neuronale È™i am folosit-o pentru o sarcinÄƒ simplÄƒ de clasificare bidimensionalÄƒ.

## ğŸš€ Provocare

Ãn notebook-ul Ã®nsoÈ›itor, vei implementa propriul cadru pentru construirea È™i antrenarea perceptronilor multi-stratificaÈ›i. Vei putea vedea Ã®n detaliu cum funcÈ›ioneazÄƒ reÈ›elele neuronale moderne.

ContinuÄƒ cu notebook-ul OwnFramework È™i parcurge-l.

## Recapitulare & Studiu Individual

Backpropagation este un algoritm comun folosit Ã®n AI È™i ML, meritÄƒ studiat Ã®n detaliu.

## Tema

Ãn acest laborator, È›i se cere sÄƒ foloseÈ™ti cadrul pe care l-ai construit Ã®n aceastÄƒ lecÈ›ie pentru a rezolva clasificarea cifrelor scrise de mÃ¢nÄƒ din setul MNIST.

* InstrucÈ›iuni
* Notebook

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim pentru acurateÈ›e, vÄƒ rugÄƒm sÄƒ reÈ›ineÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa nativÄƒ trebuie considerat sursa autorizatÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm rÄƒspunderea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite rezultate din utilizarea acestei traduceri.