<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:26:45+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "ro"
}
-->
# Introducere Ã®n ReÈ›ele Neuronale. Perceptron Multi-Stratificat

Ãn secÈ›iunea anterioarÄƒ, ai Ã®nvÄƒÈ›at despre cel mai simplu model de reÈ›ea neuronalÄƒ - perceptronul cu un singur strat, un model liniar de clasificare Ã®n douÄƒ clase.

Ãn aceastÄƒ secÈ›iune vom extinde acest model Ã®ntr-un cadru mai flexibil, care ne va permite sÄƒ:

* efectuÄƒm **clasificare multi-clasÄƒ** pe lÃ¢ngÄƒ cea Ã®n douÄƒ clase
* rezolvÄƒm **probleme de regresie** pe lÃ¢ngÄƒ clasificare
* separÄƒm clase care nu sunt separabile liniar

Vom dezvolta, de asemenea, propriul nostru cadru modular Ã®n Python, care ne va permite sÄƒ construim diferite arhitecturi de reÈ›ele neuronale.

## Formalizarea ÃnvÄƒÈ›Äƒrii Automate

SÄƒ Ã®ncepem prin formalizarea problemei de ÃnvÄƒÈ›are AutomatÄƒ. Presupunem cÄƒ avem un set de date de antrenament **X** cu etichete **Y**, È™i trebuie sÄƒ construim un model *f* care sÄƒ facÄƒ predicÈ›ii cÃ¢t mai precise. Calitatea predicÈ›iilor este mÄƒsuratÄƒ prin **FuncÈ›ia de Pierdere** â„’. UrmÄƒtoarele funcÈ›ii de pierdere sunt adesea utilizate:

* Pentru problema de regresie, cÃ¢nd trebuie sÄƒ prezicem un numÄƒr, putem folosi **eroarea absolutÄƒ** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| sau **eroarea pÄƒtraticÄƒ** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Pentru clasificare, folosim **pierdere 0-1** (care este esenÈ›ial acelaÈ™i lucru cu **acurateÈ›ea** modelului) sau **pierdere logisticÄƒ**.

Pentru perceptronul cu un singur nivel, funcÈ›ia *f* a fost definitÄƒ ca o funcÈ›ie liniarÄƒ *f(x)=wx+b* (aici *w* este matricea de greutÄƒÈ›i, *x* este vectorul de caracteristici de intrare, È™i *b* este vectorul de bias). Pentru diferite arhitecturi de reÈ›ele neuronale, aceastÄƒ funcÈ›ie poate lua o formÄƒ mai complexÄƒ.

> Ãn cazul clasificÄƒrii, este adesea de dorit sÄƒ obÈ›inem probabilitÄƒÈ›ile claselor corespunzÄƒtoare ca rezultat al reÈ›elei. Pentru a converti numerele arbitrare Ã®n probabilitÄƒÈ›i (de exemplu, pentru a normaliza rezultatul), folosim adesea funcÈ›ia **softmax** Ïƒ, iar funcÈ›ia *f* devine *f(x)=Ïƒ(wx+b)*

Ãn definiÈ›ia *f* de mai sus, *w* È™i *b* sunt numite **parametri** Î¸=âŸ¨*w,b*âŸ©. Dat fiind setul de date âŸ¨**X**,**Y**âŸ©, putem calcula o eroare generalÄƒ pe Ã®ntregul set de date ca o funcÈ›ie a parametrilor Î¸.

> âœ… **Scopul antrenÄƒrii reÈ›elei neuronale este de a minimiza eroarea prin varierea parametrilor Î¸**

## Optimizarea prin Gradient Descent

ExistÄƒ o metodÄƒ bine cunoscutÄƒ de optimizare a funcÈ›iilor numitÄƒ **gradient descent**. Ideea este cÄƒ putem calcula o derivatÄƒ (Ã®n cazul multidimensional numitÄƒ **gradient**) a funcÈ›iei de pierdere Ã®n raport cu parametrii È™i sÄƒ variem parametrii astfel Ã®ncÃ¢t eroarea sÄƒ scadÄƒ. Acest lucru poate fi formalizat astfel:

* IniÈ›ializeazÄƒ parametrii cu unele valori aleatoare w<sup>(0)</sup>, b<sup>(0)</sup>
* RepetÄƒ urmÄƒtorul pas de mai multe ori:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Ãn timpul antrenÄƒrii, paÈ™ii de optimizare ar trebui sÄƒ fie calculaÈ›i luÃ¢nd Ã®n considerare Ã®ntregul set de date (amintiÈ›i-vÄƒ cÄƒ pierderea este calculatÄƒ ca o sumÄƒ pe toate exemplele de antrenament). TotuÈ™i, Ã®n viaÈ›a realÄƒ luÄƒm porÈ›iuni mici ale setului de date numite **minibatches** È™i calculÄƒm gradientele pe baza unui subset de date. Deoarece subsetul este luat aleatoriu de fiecare datÄƒ, aceastÄƒ metodÄƒ este numitÄƒ **stochastic gradient descent** (SGD).

## Perceptron Multi-Stratificat È™i Backpropagation

ReÈ›eaua cu un singur strat, aÈ™a cum am vÄƒzut mai sus, este capabilÄƒ sÄƒ clasifice clasele separabile liniar. Pentru a construi un model mai bogat, putem combina mai multe straturi ale reÈ›elei. Matematic, ar Ã®nsemna cÄƒ funcÈ›ia *f* ar avea o formÄƒ mai complexÄƒ È™i va fi calculatÄƒ Ã®n mai mulÈ›i paÈ™i:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Aici, Î± este o **funcÈ›ie de activare non-liniarÄƒ**, Ïƒ este o funcÈ›ie softmax, iar parametrii Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritmul de gradient descent ar rÄƒmÃ¢ne acelaÈ™i, dar ar fi mai dificil de calculat gradientele. DatÄƒ regula de diferenÈ›iere Ã®n lanÈ›, putem calcula derivatele astfel:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Regula de diferenÈ›iere Ã®n lanÈ› este folositÄƒ pentru a calcula derivatele funcÈ›iei de pierdere Ã®n raport cu parametrii.

ObservaÈ›i cÄƒ partea cea mai din stÃ¢nga a tuturor acestor expresii este aceeaÈ™i È™i, astfel, putem calcula eficient derivatele Ã®ncepÃ¢nd de la funcÈ›ia de pierdere È™i mergÃ¢nd "Ã®napoi" prin graful computaÈ›ional. Astfel, metoda de antrenare a unui perceptron multi-stratificat este numitÄƒ **backpropagation**, sau 'backprop'.

> TODO: citare imagine

> âœ… Vom aborda backprop Ã®n mult mai multe detalii Ã®n exemplul nostru din notebook.

## Concluzie

Ãn aceastÄƒ lecÈ›ie, am construit propria noastrÄƒ bibliotecÄƒ de reÈ›ele neuronale È™i am folosit-o pentru o sarcinÄƒ simplÄƒ de clasificare bidimensionalÄƒ.

## ğŸš€ Provocare

Ãn notebook-ul Ã®nsoÈ›itor, vei implementa propriul tÄƒu cadru pentru construirea È™i antrenarea perceptronilor multi-stratificaÈ›i. Vei putea vedea Ã®n detaliu cum funcÈ›ioneazÄƒ reÈ›elele neuronale moderne.

ContinuÄƒ cu notebook-ul OwnFramework È™i parcurge-l.

## Revizuire & Studiu Individual

Backpropagation este un algoritm comun utilizat Ã®n AI È™i ML, meritÄƒ studiat Ã®n detaliu.

## SarcinÄƒ

Ãn acest laborator, È›i se cere sÄƒ foloseÈ™ti cadrul pe care l-ai construit Ã®n aceastÄƒ lecÈ›ie pentru a rezolva clasificarea cifrelor scrise de mÃ¢nÄƒ MNIST.

* InstrucÈ›iuni
* Notebook

**Declinarea responsabilitÄƒÈ›ii**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa maternÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ umanÄƒ. Nu ne asumÄƒm responsabilitatea pentru neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.