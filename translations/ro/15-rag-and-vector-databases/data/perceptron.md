<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-07-09T17:01:02+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "ro"
}
-->
# Introducere Ã®n ReÈ›ele Neuronale: Perceptron

Una dintre primele Ã®ncercÄƒri de a implementa ceva similar cu o reÈ›ea neuronalÄƒ modernÄƒ a fost realizatÄƒ de Frank Rosenblatt de la Cornell Aeronautical Laboratory Ã®n 1957. A fost o implementare hardware numitÄƒ â€Mark-1â€, conceputÄƒ sÄƒ recunoascÄƒ figuri geometrice primitive, cum ar fi triunghiuri, pÄƒtrate È™i cercuri.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> Imagini de pe Wikipedia

O imagine de intrare era reprezentatÄƒ printr-un tablou de 20x20 celule foto, astfel Ã®ncÃ¢t reÈ›eaua neuronalÄƒ avea 400 de intrÄƒri È™i o ieÈ™ire binarÄƒ. O reÈ›ea simplÄƒ conÈ›inea un singur neuron, numit È™i **unitate logicÄƒ cu prag**. GreutÄƒÈ›ile reÈ›elei neuronale funcÈ›ionau ca potenÈ›iometre care necesitau ajustare manualÄƒ Ã®n timpul fazei de antrenament.

> âœ… Un potenÈ›iometru este un dispozitiv care permite utilizatorului sÄƒ ajusteze rezistenÈ›a unui circuit.

> The New York Times scria despre perceptron Ã®n acea perioadÄƒ: *embrionul unui calculator electronic care [Marina] se aÈ™teaptÄƒ sÄƒ poatÄƒ merge, vorbi, vedea, scrie, sÄƒ se reproducÄƒ È™i sÄƒ fie conÈ™tient de existenÈ›a sa.*

## Modelul Perceptronului

SÄƒ presupunem cÄƒ avem N caracteristici Ã®n modelul nostru, caz Ã®n care vectorul de intrare ar fi un vector de dimensiune N. Un perceptron este un model de **clasificare binarÄƒ**, adicÄƒ poate distinge Ã®ntre douÄƒ clase de date de intrare. Vom presupune cÄƒ pentru fiecare vector de intrare x, ieÈ™irea perceptronului nostru va fi fie +1, fie -1, Ã®n funcÈ›ie de clasÄƒ. IeÈ™irea va fi calculatÄƒ folosind formula:

y(x) = f(w<sup>T</sup>x)

unde f este o funcÈ›ie de activare treaptÄƒ

## Antrenarea Perceptronului

Pentru a antrena un perceptron trebuie sÄƒ gÄƒsim un vector de greutÄƒÈ›i w care sÄƒ clasifice corect majoritatea valorilor, adicÄƒ sÄƒ conducÄƒ la cea mai micÄƒ **eroare**. AceastÄƒ eroare este definitÄƒ prin **criteriul perceptronului** Ã®n felul urmÄƒtor:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

unde:

* suma se face peste acele puncte de date de antrenament i care duc la clasificare greÈ™itÄƒ
* x<sub>i</sub> este datele de intrare, iar t<sub>i</sub> este fie -1, fie +1 pentru exemple negative È™i pozitive, respectiv.

Acest criteriu este considerat o funcÈ›ie a greutÄƒÈ›ilor w, iar noi trebuie sÄƒ Ã®l minimizÄƒm. Adesea, se foloseÈ™te o metodÄƒ numitÄƒ **gradient descent** (coborÃ¢re pe gradient), Ã®n care Ã®ncepem cu niÈ™te greutÄƒÈ›i iniÈ›iale w<sup>(0)</sup>, iar apoi la fiecare pas actualizÄƒm greutÄƒÈ›ile conform formulei:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

Aici Î· este aÈ™a-numita **ratÄƒ de Ã®nvÄƒÈ›are**, iar âˆ‡E(w) reprezintÄƒ **gradientul** lui E. DupÄƒ ce calculÄƒm gradientul, ajungem la:

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Algoritmul Ã®n Python aratÄƒ astfel:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Concluzie

Ãn aceastÄƒ lecÈ›ie, ai Ã®nvÄƒÈ›at despre perceptron, un model de clasificare binarÄƒ, È™i cum sÄƒ Ã®l antrenezi folosind un vector de greutÄƒÈ›i.

## ğŸš€ Provocare

DacÄƒ vrei sÄƒ Ã®ncerci sÄƒ construieÈ™ti propriul perceptron, Ã®ncearcÄƒ acest laborator pe Microsoft Learn care foloseÈ™te Azure ML designer


## Recapitulare & Studiu Individual

Pentru a vedea cum putem folosi perceptronul pentru a rezolva o problemÄƒ simplÄƒ, dar È™i probleme din viaÈ›a realÄƒ, È™i pentru a continua sÄƒ Ã®nveÈ›i - acceseazÄƒ notebook-ul Perceptron.

IatÄƒ È™i un articol interesant despre perceptroni.

## Tema

Ãn aceastÄƒ lecÈ›ie, am implementat un perceptron pentru o sarcinÄƒ de clasificare binarÄƒ È™i l-am folosit pentru a clasifica Ã®ntre douÄƒ cifre scrise de mÃ¢nÄƒ. Ãn acest laborator, È›i se cere sÄƒ rezolvi problema clasificÄƒrii cifrelor Ã®n Ã®ntregime, adicÄƒ sÄƒ determini care cifrÄƒ este cea mai probabilÄƒ pentru o imagine datÄƒ.

* InstrucÈ›iuni
* Notebook

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim pentru acurateÈ›e, vÄƒ rugÄƒm sÄƒ reÈ›ineÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa nativÄƒ trebuie considerat sursa autorizatÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm rÄƒspunderea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite rezultate din utilizarea acestei traduceri.