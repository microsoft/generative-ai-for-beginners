<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-05-20T06:43:33+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "ro"
}
-->
# Introducere Ã®n ReÈ›ele Neurale: Perceptron

Una dintre primele Ã®ncercÄƒri de a implementa ceva asemÄƒnÄƒtor cu o reÈ›ea neuralÄƒ modernÄƒ a fost realizatÄƒ de Frank Rosenblatt de la Cornell Aeronautical Laboratory Ã®n 1957. A fost o implementare hardware numitÄƒ "Mark-1", proiectatÄƒ pentru a recunoaÈ™te figuri geometrice primitive, precum triunghiuri, pÄƒtrate È™i cercuri.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> Imagini de pe Wikipedia

O imagine de intrare era reprezentatÄƒ de o matrice de fotocelule de 20x20, astfel Ã®ncÃ¢t reÈ›eaua neuralÄƒ avea 400 de intrÄƒri È™i o ieÈ™ire binarÄƒ. O reÈ›ea simplÄƒ conÈ›inea un neuron, numit È™i **unitate logicÄƒ de prag**. GreutÄƒÈ›ile reÈ›elei neurale acÈ›ionau ca niÈ™te potenÈ›iometre care necesitau ajustare manualÄƒ Ã®n timpul fazei de antrenament.

> âœ… Un potenÈ›iometru este un dispozitiv care permite utilizatorului sÄƒ ajusteze rezistenÈ›a unui circuit.

> The New York Times a scris despre perceptron la acea vreme: *embrionul unui computer electronic pe care [Marina] se aÈ™teaptÄƒ sÄƒ fie capabil sÄƒ meargÄƒ, sÄƒ vorbeascÄƒ, sÄƒ vadÄƒ, sÄƒ scrie, sÄƒ se reproducÄƒ È™i sÄƒ fie conÈ™tient de existenÈ›a sa.*

## Modelul Perceptron

SÄƒ presupunem cÄƒ avem N caracteristici Ã®n modelul nostru, caz Ã®n care vectorul de intrare ar fi un vector de dimensiune N. Un perceptron este un model de **clasificare binarÄƒ**, adicÄƒ poate distinge Ã®ntre douÄƒ clase de date de intrare. Vom presupune cÄƒ pentru fiecare vector de intrare x, ieÈ™irea perceptronului nostru va fi fie +1, fie -1, Ã®n funcÈ›ie de clasÄƒ. IeÈ™irea va fi calculatÄƒ folosind formula:

y(x) = f(w<sup>T</sup>x)

unde f este o funcÈ›ie de activare treaptÄƒ

## Antrenarea Perceptronului

Pentru a antrena un perceptron, trebuie sÄƒ gÄƒsim un vector de greutÄƒÈ›i w care sÄƒ clasifice corect majoritatea valorilor, adicÄƒ sÄƒ rezulte Ã®n cel mai mic **eroare**. AceastÄƒ eroare este definitÄƒ de **criteriul perceptronului** Ã®n urmÄƒtorul mod:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

unde:

* suma se face pe acele puncte de date de antrenament i care duc la clasificare greÈ™itÄƒ
* x<sub>i</sub> este datele de intrare, iar t<sub>i</sub> este fie -1, fie +1 pentru exemple negative È™i pozitive, respectiv.

Acest criteriu este considerat ca o funcÈ›ie a greutÄƒÈ›ilor w È™i trebuie sÄƒ-l minimizÄƒm. Adesea, se foloseÈ™te o metodÄƒ numitÄƒ **descendentÄƒ a gradientului**, Ã®n care Ã®ncepem cu unele greutÄƒÈ›i iniÈ›iale w<sup>(0)</sup>, È™i apoi la fiecare pas actualizÄƒm greutÄƒÈ›ile conform formulei:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

Aici Î· este aÈ™a-numita **ratÄƒ de Ã®nvÄƒÈ›are**, iar âˆ‡E(w) denotÄƒ **gradientul** lui E. DupÄƒ ce calculÄƒm gradientul, ajungem la

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Algoritmul Ã®n Python aratÄƒ astfel:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Concluzie

Ãn aceastÄƒ lecÈ›ie, aÈ›i Ã®nvÄƒÈ›at despre un perceptron, care este un model de clasificare binarÄƒ, È™i cum sÄƒ-l antrenaÈ›i folosind un vector de greutÄƒÈ›i.

## ğŸš€ Provocare

DacÄƒ doriÈ›i sÄƒ Ã®ncercaÈ›i sÄƒ construiÈ›i propriul perceptron, Ã®ncercaÈ›i acest laborator pe Microsoft Learn care foloseÈ™te designerul Azure ML.

## Recenzie È™i Studiu Individual

Pentru a vedea cum putem folosi perceptronul pentru a rezolva o problemÄƒ de jucÄƒrie, precum È™i probleme din viaÈ›a realÄƒ, È™i pentru a continua Ã®nvÄƒÈ›area - mergeÈ›i la notebook-ul Perceptron.

IatÄƒ È™i un articol interesant despre perceptroni.

## TemÄƒ

Ãn aceastÄƒ lecÈ›ie, am implementat un perceptron pentru o sarcinÄƒ de clasificare binarÄƒ È™i l-am folosit pentru a clasifica Ã®ntre douÄƒ cifre scrise de mÃ¢nÄƒ. Ãn acest laborator, vi se cere sÄƒ rezolvaÈ›i problema clasificÄƒrii cifrelor Ã®n Ã®ntregime, adicÄƒ sÄƒ determinaÈ›i care cifrÄƒ este cel mai probabil sÄƒ corespundÄƒ unei imagini date.

* InstrucÈ›iuni
* Notebook

**Declinarea responsabilitÄƒÈ›ii**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa maternÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea umanÄƒ profesionalÄƒ. Nu ne asumÄƒm responsabilitatea pentru neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.