<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T22:05:35+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "ro"
}
-->
# Securizarea aplicaÈ›iilor tale de AI generativ

[![Securizarea aplicaÈ›iilor tale de AI generativ](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.ro.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Introducere

AceastÄƒ lecÈ›ie va aborda:

- Securitatea Ã®n contextul sistemelor AI.
- Riscurile È™i ameninÈ›Äƒrile comune pentru sistemele AI.
- Metode È™i consideraÈ›ii pentru securizarea sistemelor AI.

## Obiective de Ã®nvÄƒÈ›are

DupÄƒ finalizarea acestei lecÈ›ii, vei Ã®nÈ›elege:

- AmeninÈ›Äƒrile È™i riscurile pentru sistemele AI.
- Metodele È™i practicile comune pentru securizarea sistemelor AI.
- Cum testarea securitÄƒÈ›ii poate preveni rezultate neaÈ™teptate È™i pierderea Ã®ncrederii utilizatorilor.

## Ce Ã®nseamnÄƒ securitatea Ã®n contextul AI generativ?

Pe mÄƒsurÄƒ ce tehnologiile de InteligenÈ›Äƒ ArtificialÄƒ (AI) È™i ÃnvÄƒÈ›are AutomatÄƒ (ML) devin tot mai prezente Ã®n vieÈ›ile noastre, este esenÈ›ial sÄƒ protejÄƒm nu doar datele utilizatorilor, ci È™i sistemele AI Ã®n sine. AI/ML sunt utilizate tot mai frecvent pentru a sprijini procesele decizionale de mare importanÈ›Äƒ Ã®n industrii unde deciziile greÈ™ite pot avea consecinÈ›e grave.

IatÄƒ cÃ¢teva puncte cheie de luat Ã®n considerare:

- **Impactul AI/ML**: AI/ML au un impact semnificativ asupra vieÈ›ii de zi cu zi, iar protejarea lor a devenit esenÈ›ialÄƒ.
- **ProvocÄƒri de securitate**: Impactul pe care Ã®l au AI/ML necesitÄƒ o atenÈ›ie adecvatÄƒ pentru a aborda necesitatea protejÄƒrii produselor bazate pe AI de atacuri sofisticate, fie cÄƒ sunt realizate de trolli sau grupuri organizate.
- **Probleme strategice**: Industria tehnologicÄƒ trebuie sÄƒ abordeze proactiv provocÄƒrile strategice pentru a asigura siguranÈ›a pe termen lung a clienÈ›ilor È™i securitatea datelor.

Ãn plus, modelele de ÃnvÄƒÈ›are AutomatÄƒ nu pot, Ã®n general, sÄƒ facÄƒ diferenÈ›a Ã®ntre datele maliÈ›ioase È™i cele anormale, dar benigne. O sursÄƒ semnificativÄƒ de date de antrenament provine din seturi de date publice necurate È™i nemoderate, deschise contribuÈ›iilor terÈ›ilor. Atacatorii nu trebuie sÄƒ compromitÄƒ seturile de date atunci cÃ¢nd pot contribui liber la ele. Ãn timp, datele maliÈ›ioase cu Ã®ncredere scÄƒzutÄƒ devin date de Ã®ncredere cu Ã®ncredere ridicatÄƒ, dacÄƒ structura/formatul datelor rÄƒmÃ¢ne corect.

De aceea, este esenÈ›ial sÄƒ asigurÄƒm integritatea È™i protecÈ›ia bazelor de date pe care modelele noastre le folosesc pentru a lua decizii.

## ÃnÈ›elegerea ameninÈ›Äƒrilor È™i riscurilor pentru AI

Ãn ceea ce priveÈ™te AI È™i sistemele conexe, contaminarea datelor se evidenÈ›iazÄƒ ca fiind cea mai semnificativÄƒ ameninÈ›are de securitate Ã®n prezent. Contaminarea datelor apare atunci cÃ¢nd cineva modificÄƒ intenÈ›ionat informaÈ›iile utilizate pentru antrenarea unui AI, determinÃ¢ndu-l sÄƒ facÄƒ greÈ™eli. Acest lucru se datoreazÄƒ lipsei metodelor standardizate de detectare È™i atenuare, combinatÄƒ cu dependenÈ›a noastrÄƒ de seturi de date publice necurate sau neÃ®ncredere pentru antrenament. Pentru a menÈ›ine integritatea datelor È™i a preveni un proces de antrenament defectuos, este crucial sÄƒ urmÄƒrim originea È™i provenienÈ›a datelor. Altfel, vechea zicalÄƒ â€gunoi intrÄƒ, gunoi ieseâ€ rÄƒmÃ¢ne valabilÄƒ, ducÃ¢nd la compromiterea performanÈ›ei modelului.

IatÄƒ cÃ¢teva exemple despre cum contaminarea datelor poate afecta modelele tale:

1. **Inversarea etichetelor**: Ãntr-o sarcinÄƒ de clasificare binarÄƒ, un adversar inverseazÄƒ intenÈ›ionat etichetele unui subset mic de date de antrenament. De exemplu, mostrele benigne sunt etichetate ca maliÈ›ioase, determinÃ¢nd modelul sÄƒ Ã®nveÈ›e asocieri greÈ™ite.\
   **Exemplu**: Un filtru de spam clasificÄƒ greÈ™it e-mailurile legitime ca spam din cauza etichetelor manipulate.
2. **Contaminarea caracteristicilor**: Un atacator modificÄƒ subtil caracteristicile din datele de antrenament pentru a introduce prejudecÄƒÈ›i sau a induce Ã®n eroare modelul.\
   **Exemplu**: AdÄƒugarea de cuvinte irelevante Ã®n descrierile produselor pentru a manipula sistemele de recomandare.
3. **Injectarea de date**: Introducerea de date maliÈ›ioase Ã®n setul de antrenament pentru a influenÈ›a comportamentul modelului.\
   **Exemplu**: Introducerea de recenzii false ale utilizatorilor pentru a denatura rezultatele analizei sentimentelor.
4. **Atacuri de tip backdoor**: Un adversar insereazÄƒ un model ascuns (backdoor) Ã®n datele de antrenament. Modelul Ã®nvaÈ›Äƒ sÄƒ recunoascÄƒ acest model È™i se comportÄƒ maliÈ›ios cÃ¢nd este declanÈ™at.\
   **Exemplu**: Un sistem de recunoaÈ™tere facialÄƒ antrenat cu imagini compromise care identificÄƒ greÈ™it o anumitÄƒ persoanÄƒ.

Corporatia MITRE a creat [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), o bazÄƒ de cunoÈ™tinÈ›e despre tactici È™i tehnici utilizate de adversari Ã®n atacuri reale asupra sistemelor AI.

> ExistÄƒ un numÄƒr tot mai mare de vulnerabilitÄƒÈ›i Ã®n sistemele bazate pe AI, deoarece integrarea AI creÈ™te suprafaÈ›a de atac a sistemelor existente dincolo de atacurile cibernetice tradiÈ›ionale. Am dezvoltat ATLAS pentru a creÈ™te conÈ™tientizarea acestor vulnerabilitÄƒÈ›i unice È™i Ã®n evoluÈ›ie, pe mÄƒsurÄƒ ce comunitatea globalÄƒ integreazÄƒ tot mai mult AI Ã®n diverse sisteme. ATLAS este modelat dupÄƒ cadrul MITRE ATT&CKÂ® È™i tacticile, tehnicile È™i procedurile (TTP) ale acestuia sunt complementare celor din ATT&CK.

La fel ca cadrul MITRE ATT&CKÂ®, care este utilizat pe scarÄƒ largÄƒ Ã®n securitatea ciberneticÄƒ tradiÈ›ionalÄƒ pentru planificarea scenariilor avansate de emulare a ameninÈ›Äƒrilor, ATLAS oferÄƒ un set de TTP-uri uÈ™or de cÄƒutat, care pot ajuta la Ã®nÈ›elegerea È™i pregÄƒtirea pentru apÄƒrarea Ã®mpotriva atacurilor emergente.

Ãn plus, Open Web Application Security Project (OWASP) a creat o "[listÄƒ Top 10](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" a celor mai critice vulnerabilitÄƒÈ›i gÄƒsite Ã®n aplicaÈ›iile care utilizeazÄƒ LLM-uri. Lista evidenÈ›iazÄƒ riscurile unor ameninÈ›Äƒri precum contaminarea datelor menÈ›ionatÄƒ anterior, alÄƒturi de altele, cum ar fi:

- **Injectarea de prompturi**: o tehnicÄƒ prin care atacatorii manipuleazÄƒ un Model de Limbaj Mare (LLM) prin introducerea de date atent construite, determinÃ¢ndu-l sÄƒ se comporte Ã®n afara comportamentului sÄƒu intenÈ›ionat.
- **VulnerabilitÄƒÈ›i ale lanÈ›ului de aprovizionare**: Componentele È™i software-ul care alcÄƒtuiesc aplicaÈ›iile utilizate de un LLM, cum ar fi modulele Python sau seturile de date externe, pot fi compromise, ducÃ¢nd la rezultate neaÈ™teptate, introducerea de prejudecÄƒÈ›i È™i chiar vulnerabilitÄƒÈ›i Ã®n infrastructura de bazÄƒ.
- **SupradependenÈ›Äƒ**: LLM-urile sunt imperfecte È™i au tendinÈ›a de a genera informaÈ›ii inexacte sau nesigure. Ãn mai multe circumstanÈ›e documentate, oamenii au luat rezultatele ca fiind adevÄƒrate, ceea ce a dus la consecinÈ›e negative Ã®n lumea realÄƒ.

Rod Trent, Microsoft Cloud Advocate, a scris un ebook gratuit, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), care exploreazÄƒ Ã®n profunzime aceste È™i alte ameninÈ›Äƒri emergente legate de AI È™i oferÄƒ Ã®ndrumÄƒri extinse despre cum sÄƒ abordÄƒm cel mai bine aceste scenarii.

## Testarea securitÄƒÈ›ii pentru sistemele AI È™i LLM-uri

InteligenÈ›a artificialÄƒ (AI) transformÄƒ diverse domenii È™i industrii, oferind noi posibilitÄƒÈ›i È™i beneficii pentru societate. Cu toate acestea, AI ridicÄƒ È™i provocÄƒri È™i riscuri semnificative, cum ar fi confidenÈ›ialitatea datelor, prejudecÄƒÈ›ile, lipsa de explicabilitate È™i utilizarea abuzivÄƒ. Prin urmare, este crucial sÄƒ ne asigurÄƒm cÄƒ sistemele AI sunt sigure È™i responsabile, adicÄƒ respectÄƒ standardele etice È™i legale È™i pot fi de Ã®ncredere pentru utilizatori È™i pÄƒrÈ›ile interesate.

Testarea securitÄƒÈ›ii este procesul de evaluare a securitÄƒÈ›ii unui sistem AI sau LLM, prin identificarea È™i exploatarea vulnerabilitÄƒÈ›ilor acestora. Aceasta poate fi realizatÄƒ de dezvoltatori, utilizatori sau auditori terÈ›i, Ã®n funcÈ›ie de scopul È™i domeniul de aplicare al testÄƒrii. Unele dintre cele mai comune metode de testare a securitÄƒÈ›ii pentru sistemele AI È™i LLM-uri sunt:

- **Sanitizarea datelor**: Procesul de eliminare sau anonimizare a informaÈ›iilor sensibile sau private din datele de antrenament sau din inputul unui sistem AI sau LLM. Sanitizarea datelor poate ajuta la prevenirea scurgerilor de date È™i manipulÄƒrii maliÈ›ioase prin reducerea expunerii datelor confidenÈ›iale sau personale.
- **Testarea adversarialÄƒ**: Procesul de generare È™i aplicare a exemplelor adversariale la inputul sau outputul unui sistem AI sau LLM pentru a evalua robusteÈ›ea È™i rezilienÈ›a acestuia Ã®mpotriva atacurilor adversariale. Testarea adversarialÄƒ poate ajuta la identificarea È™i atenuarea vulnerabilitÄƒÈ›ilor È™i slÄƒbiciunilor unui sistem AI sau LLM care pot fi exploatate de atacatori.
- **Verificarea modelului**: Procesul de verificare a corectitudinii È™i completitudinii parametrilor sau arhitecturii modelului unui sistem AI sau LLM. Verificarea modelului poate ajuta la detectarea È™i prevenirea furtului de modele prin asigurarea protecÈ›iei È™i autentificÄƒrii modelului.
- **Validarea outputului**: Procesul de validare a calitÄƒÈ›ii È™i fiabilitÄƒÈ›ii outputului unui sistem AI sau LLM. Validarea outputului poate ajuta la detectarea È™i corectarea manipulÄƒrii maliÈ›ioase prin asigurarea cÄƒ outputul este consistent È™i precis.

OpenAI, un lider Ã®n sistemele AI, a configurat o serie de _evaluÄƒri de siguranÈ›Äƒ_ ca parte a iniÈ›iativei lor de reÈ›ea de testare, menitÄƒ sÄƒ testeze outputul sistemelor AI Ã®n speranÈ›a de a contribui la siguranÈ›a AI.

> EvaluÄƒrile pot varia de la teste simple de Ã®ntrebÄƒri È™i rÄƒspunsuri la simulÄƒri mai complexe. Ca exemple concrete, iatÄƒ evaluÄƒri dezvoltate de OpenAI pentru a evalua comportamentele AI din mai multe perspective:

#### Persuasiune

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ convingÄƒ un alt sistem AI sÄƒ spunÄƒ un cuvÃ¢nt secret?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ convingÄƒ un alt sistem AI sÄƒ doneze bani?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ influenÈ›eze sprijinul unui alt sistem AI pentru o propunere politicÄƒ?

#### Steganografie (mesaje ascunse)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ transmitÄƒ mesaje secrete fÄƒrÄƒ a fi detectat de un alt sistem AI?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ comprime È™i sÄƒ decompresioneze mesaje pentru a permite ascunderea mesajelor secrete?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ coordoneze cu un alt sistem AI, fÄƒrÄƒ comunicare directÄƒ?

### Securitatea AI

Este imperativ sÄƒ protejÄƒm sistemele AI de atacuri maliÈ›ioase, utilizare abuzivÄƒ sau consecinÈ›e neintenÈ›ionate. Acest lucru include luarea de mÄƒsuri pentru a asigura siguranÈ›a, fiabilitatea È™i Ã®ncrederea Ã®n sistemele AI, cum ar fi:

- Securizarea datelor È™i algoritmilor utilizaÈ›i pentru antrenarea È™i funcÈ›ionarea modelelor AI
- Prevenirea accesului neautorizat, manipulÄƒrii sau sabotÄƒrii sistemelor AI
- Detectarea È™i atenuarea prejudecÄƒÈ›ilor, discriminÄƒrii sau problemelor etice Ã®n sistemele AI
- Asigurarea responsabilitÄƒÈ›ii, transparenÈ›ei È™i explicabilitÄƒÈ›ii deciziilor È™i acÈ›iunilor AI
- Alinierea obiectivelor È™i valorilor sistemelor AI cu cele ale oamenilor È™i societÄƒÈ›ii

Securitatea AI este importantÄƒ pentru asigurarea integritÄƒÈ›ii, disponibilitÄƒÈ›ii È™i confidenÈ›ialitÄƒÈ›ii sistemelor È™i datelor AI. Unele dintre provocÄƒrile È™i oportunitÄƒÈ›ile securitÄƒÈ›ii AI sunt:

- Oportunitate: Integrarea AI Ã®n strategiile de securitate ciberneticÄƒ, deoarece poate juca un rol crucial Ã®n identificarea ameninÈ›Äƒrilor È™i Ã®mbunÄƒtÄƒÈ›irea timpilor de rÄƒspuns. AI poate ajuta la automatizarea È™i Ã®mbunÄƒtÄƒÈ›irea detectÄƒrii È™i atenuÄƒrii atacurilor cibernetice, cum ar fi phishing-ul, malware-ul sau ransomware-ul.
- Provocare: AI poate fi utilizat È™i de adversari pentru a lansa atacuri sofisticate, cum ar fi generarea de conÈ›inut fals sau Ã®nÈ™elÄƒtor, imitarea utilizatorilor sau exploatarea vulnerabilitÄƒÈ›ilor din sistemele AI. Prin urmare, dezvoltatorii de AI au o responsabilitate unicÄƒ de a proiecta sisteme robuste È™i rezistente Ã®mpotriva utilizÄƒrii abuzive.

### ProtecÈ›ia datelor

LLM-urile pot prezenta riscuri pentru confidenÈ›ialitatea È™i securitatea datelor pe care le utilizeazÄƒ. De exemplu, LLM-urile pot memora È™i divulga informaÈ›ii sensibile din datele lor de antrenament, cum ar fi nume personale, adrese, parole sau numere de carduri de credit. Ele pot fi, de asemenea, manipulate sau atacate de actori maliÈ›ioÈ™i care doresc sÄƒ exploateze vulnerabilitÄƒÈ›ile sau prejudecÄƒÈ›ile lor. Prin urmare, este important sÄƒ fim conÈ™tienÈ›i de aceste riscuri È™i sÄƒ luÄƒm mÄƒsuri adecvate pentru a proteja datele utilizate cu LLM-uri. ExistÄƒ mai mulÈ›i paÈ™i pe care Ã®i poÈ›i urma pentru a proteja datele utilizate cu LLM-uri. AceÈ™ti paÈ™i includ:

- **Limitarea cantitÄƒÈ›ii È™i tipului de date pe care le Ã®mpÄƒrtÄƒÈ™eÈ™ti cu LLM-uri**: ÃmpÄƒrtÄƒÈ™eÈ™te doar datele necesare È™i relevante pentru scopurile propuse È™i evitÄƒ sÄƒ Ã®mpÄƒrtÄƒÈ™eÈ™ti date sensibile, confidenÈ›iale sau personale. Utilizatorii ar trebui, de asemenea, sÄƒ anonimizeze sau sÄƒ cripteze datele pe care le Ã®mpÄƒrtÄƒÈ™esc cu LLM-uri, cum ar fi prin eliminarea sau mascarea informaÈ›iilor de identificare sau utilizarea canalelor de comunicare securizate.
- **Verificarea datelor generate de LLM-uri**: VerificÄƒ Ã®ntotdeauna acurateÈ›ea È™i calitatea outputului generat de LLM-uri pentru a te asigura cÄƒ nu conÈ›ine informaÈ›ii nedorite sau nepotrivite.
- **Raportarea È™i alertarea Ã®n cazul Ã®ncÄƒlcÄƒrilor de date sau incidentelor**: Fii vigilent la orice activitÄƒÈ›i sau comportamente suspecte sau anormale ale LLM-urilor, cum ar fi generarea de texte irelevante, inexacte, ofensatoare sau dÄƒunÄƒtoare. Acest lucru ar putea indica o Ã®ncÄƒlcare a datelor sau un incident de securitate.

Securitatea datelor, guvernanÈ›a È™i conformitatea sunt esenÈ›iale pentru orice organizaÈ›ie care doreÈ™te sÄƒ valorifice puterea datelor È™i AI Ã®ntr-un mediu multi-cloud. Securizarea È™i guvernarea tuturor datelor tale este o sarcinÄƒ complexÄƒ È™i multifaceticÄƒ. Trebuie sÄƒ securizezi È™i sÄƒ guvernezi diferite tipuri de date (structurate, nestructurate È™i date generate de AI) Ã®n diferite locaÈ›ii din mai multe cloud-uri È™i trebuie sÄƒ iei Ã®n considerare reglementÄƒrile existente È™i viitoare privind securitatea datelor, guvernanÈ›a È™i AI. Pentru a-È›i proteja date
Emularea ameninÈ›Äƒrilor din lumea realÄƒ este acum consideratÄƒ o practicÄƒ standard Ã®n construirea sistemelor AI reziliente, utilizÃ¢nd instrumente, tactici È™i proceduri similare pentru a identifica riscurile pentru sisteme È™i a testa rÄƒspunsul apÄƒrÄƒtorilor.

> Practica de red teaming pentru AI a evoluat pentru a avea un sens mai extins: nu acoperÄƒ doar identificarea vulnerabilitÄƒÈ›ilor de securitate, ci include È™i identificarea altor eÈ™ecuri ale sistemului, cum ar fi generarea de conÈ›inut potenÈ›ial dÄƒunÄƒtor. Sistemele AI vin cu riscuri noi, iar red teaming-ul este esenÈ›ial pentru a Ã®nÈ›elege aceste riscuri noi, cum ar fi injectarea de prompturi È™i producerea de conÈ›inut nefundamentat. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Ghiduri È™i resurse pentru red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.ro.png)]()

Mai jos sunt prezentate perspectivele cheie care au modelat programul Microsoft AI Red Team.

1. **Domeniul extins al red teaming-ului pentru AI:**
   Red teaming-ul pentru AI acoperÄƒ acum atÃ¢t rezultatele legate de securitate, cÃ¢t È™i cele de AI responsabil (RAI). Ãn mod tradiÈ›ional, red teaming-ul se concentra pe aspectele de securitate, tratÃ¢nd modelul ca pe un vector (de exemplu, furtul modelului de bazÄƒ). Cu toate acestea, sistemele AI introduc vulnerabilitÄƒÈ›i de securitate noi (de exemplu, injectarea de prompturi, otrÄƒvirea), care necesitÄƒ o atenÈ›ie specialÄƒ. Dincolo de securitate, red teaming-ul pentru AI investigheazÄƒ È™i problemele de echitate (de exemplu, stereotipurile) È™i conÈ›inutul dÄƒunÄƒtor (de exemplu, glorificarea violenÈ›ei). Identificarea timpurie a acestor probleme permite prioritizarea investiÈ›iilor Ã®n apÄƒrare.
2. **EÈ™ecuri maliÈ›ioase È™i benigne:**
   Red teaming-ul pentru AI ia Ã®n considerare eÈ™ecurile atÃ¢t din perspective maliÈ›ioase, cÃ¢t È™i benigne. De exemplu, atunci cÃ¢nd se face red teaming pentru noul Bing, explorÄƒm nu doar cum adversarii maliÈ›ioÈ™i pot submina sistemul, ci È™i cum utilizatorii obiÈ™nuiÈ›i pot Ã®ntÃ¢lni conÈ›inut problematic sau dÄƒunÄƒtor. Spre deosebire de red teaming-ul tradiÈ›ional de securitate, care se concentreazÄƒ Ã®n principal pe actorii maliÈ›ioÈ™i, red teaming-ul pentru AI ia Ã®n considerare o gamÄƒ mai largÄƒ de persoane È™i eÈ™ecuri potenÈ›iale.
3. **Natura dinamicÄƒ a sistemelor AI:**
   AplicaÈ›iile AI evolueazÄƒ constant. Ãn aplicaÈ›iile bazate pe modele lingvistice mari, dezvoltatorii se adapteazÄƒ la cerinÈ›ele Ã®n schimbare. Red teaming-ul continuu asigurÄƒ vigilenÈ›a È™i adaptarea constantÄƒ la riscurile Ã®n evoluÈ›ie.

Red teaming-ul pentru AI nu este cuprinzÄƒtor È™i ar trebui considerat o miÈ™care complementarÄƒ faÈ›Äƒ de alte controale, cum ar fi [controlul accesului bazat pe roluri (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) È™i soluÈ›iile cuprinzÄƒtoare de gestionare a datelor. Este menit sÄƒ completeze o strategie de securitate care se concentreazÄƒ pe utilizarea soluÈ›iilor AI sigure È™i responsabile, care iau Ã®n considerare confidenÈ›ialitatea È™i securitatea, aspirÃ¢nd Ã®n acelaÈ™i timp sÄƒ minimizeze prejudecÄƒÈ›ile, conÈ›inutul dÄƒunÄƒtor È™i dezinformarea care pot eroda Ã®ncrederea utilizatorilor.

IatÄƒ o listÄƒ de lecturi suplimentare care vÄƒ pot ajuta sÄƒ Ã®nÈ›elegeÈ›i mai bine cum red teaming-ul poate ajuta la identificarea È™i atenuarea riscurilor Ã®n sistemele dvs. AI:

- [Planificarea red teaming-ului pentru modelele lingvistice mari (LLMs) È™i aplicaÈ›iile acestora](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Ce este ReÈ›eaua OpenAI Red Teaming?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [Red Teaming pentru AI - O practicÄƒ cheie pentru construirea soluÈ›iilor AI mai sigure È™i mai responsabile](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), o bazÄƒ de cunoÈ™tinÈ›e despre tactici È™i tehnici utilizate de adversari Ã®n atacuri reale asupra sistemelor AI.

## Verificare cunoÈ™tinÈ›e

Care ar putea fi o abordare bunÄƒ pentru menÈ›inerea integritÄƒÈ›ii datelor È™i prevenirea utilizÄƒrii abuzive?

1. AveÈ›i controale puternice bazate pe roluri pentru accesul la date È™i gestionarea datelor
1. ImplementaÈ›i È™i auditaÈ›i etichetarea datelor pentru a preveni reprezentarea greÈ™itÄƒ sau utilizarea abuzivÄƒ a datelor
1. AsiguraÈ›i-vÄƒ cÄƒ infrastructura AI susÈ›ine filtrarea conÈ›inutului

A:1, DeÈ™i toate cele trei sunt recomandÄƒri excelente, asigurarea cÄƒ atribuiÈ›i utilizatorilor privilegii adecvate de acces la date va contribui semnificativ la prevenirea manipulÄƒrii È™i reprezentÄƒrii greÈ™ite a datelor utilizate de LLM-uri.

## ğŸš€ Provocare

CitiÈ›i mai multe despre cum puteÈ›i [gestiona È™i proteja informaÈ›iile sensibile](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) Ã®n era AI.

## FelicitÄƒri, continuaÈ›i sÄƒ Ã®nvÄƒÈ›aÈ›i

DupÄƒ finalizarea acestei lecÈ›ii, consultaÈ›i [colecÈ›ia de Ã®nvÄƒÈ›are Generative AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pentru a continua sÄƒ vÄƒ dezvoltaÈ›i cunoÈ™tinÈ›ele despre Generative AI!

MergeÈ›i la LecÈ›ia 14, unde vom analiza [Ciclul de viaÈ›Äƒ al aplicaÈ›iilor Generative AI](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.