<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:39:32+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "ro"
}
-->
# Asigurarea securitÄƒÈ›ii aplicaÈ›iilor tale de AI generativ

[![Asigurarea securitÄƒÈ›ii aplicaÈ›iilor tale de AI generativ](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.ro.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Introducere

AceastÄƒ lecÈ›ie va acoperi:

- Securitatea Ã®n contextul sistemelor AI.
- Riscurile È™i ameninÈ›Äƒrile comune pentru sistemele AI.
- Metode È™i consideraÈ›ii pentru asigurarea securitÄƒÈ›ii sistemelor AI.

## Obiective de Ã®nvÄƒÈ›are

DupÄƒ parcurgerea acestei lecÈ›ii, vei Ã®nÈ›elege:

- AmeninÈ›Äƒrile È™i riscurile la adresa sistemelor AI.
- Metodele È™i practicile comune pentru asigurarea securitÄƒÈ›ii sistemelor AI.
- Cum implementarea testÄƒrii de securitate poate preveni rezultate neaÈ™teptate È™i pierderea Ã®ncrederii utilizatorilor.

## Ce Ã®nseamnÄƒ securitatea Ã®n contextul AI generativ?

Pe mÄƒsurÄƒ ce tehnologiile de InteligenÈ›Äƒ ArtificialÄƒ (AI) È™i ÃnvÄƒÈ›are AutomatÄƒ (ML) modeleazÄƒ tot mai mult vieÈ›ile noastre, este esenÈ›ial sÄƒ protejÄƒm nu doar datele clienÈ›ilor, ci È™i sistemele AI Ã®n sine. AI/ML sunt tot mai des folosite pentru susÈ›inerea proceselor decizionale cu valoare ridicatÄƒ Ã®n industrii unde o decizie greÈ™itÄƒ poate avea consecinÈ›e serioase.

IatÄƒ cÃ¢teva puncte cheie de luat Ã®n considerare:

- **Impactul AI/ML**: AI/ML au un impact semnificativ Ã®n viaÈ›a de zi cu zi, iar protejarea lor a devenit esenÈ›ialÄƒ.
- **ProvocÄƒri de securitate**: Acest impact necesitÄƒ o atenÈ›ie adecvatÄƒ pentru a proteja produsele bazate pe AI de atacuri sofisticate, fie cÄƒ vin din partea trollilor sau a grupÄƒrilor organizate.
- **Probleme strategice**: Industria tehnologicÄƒ trebuie sÄƒ abordeze proactiv provocÄƒrile strategice pentru a asigura siguranÈ›a pe termen lung a clienÈ›ilor È™i securitatea datelor.

Ãn plus, modelele de ÃnvÄƒÈ›are AutomatÄƒ nu pot face diferenÈ›a Ã®ntre date maliÈ›ioase È™i date anormale, dar nevinovate. O sursÄƒ importantÄƒ de date de antrenament provine din seturi de date publice nefiltrate È™i nemoderate, deschise contribuÈ›iilor terÈ›ilor. Atacatorii nu trebuie sÄƒ compromitÄƒ seturile de date cÃ¢nd pot contribui liber la ele. Ãn timp, datele maliÈ›ioase cu Ã®ncredere scÄƒzutÄƒ pot deveni date de Ã®ncredere cu Ã®ncredere ridicatÄƒ, dacÄƒ structura È™i formatul datelor rÄƒmÃ¢n corecte.

De aceea este critic sÄƒ asiguri integritatea È™i protecÈ›ia depozitelor de date pe care modelele tale le folosesc pentru a lua decizii.

## ÃnÈ›elegerea ameninÈ›Äƒrilor È™i riscurilor AI

Ãn ceea ce priveÈ™te AI È™i sistemele conexe, otrÄƒvirea datelor (data poisoning) este cea mai semnificativÄƒ ameninÈ›are de securitate Ã®n prezent. OtrÄƒvirea datelor apare atunci cÃ¢nd cineva modificÄƒ intenÈ›ionat informaÈ›iile folosite pentru antrenarea unui AI, determinÃ¢ndu-l sÄƒ facÄƒ greÈ™eli. Acest lucru se datoreazÄƒ lipsei unor metode standardizate de detectare È™i atenuare, precum È™i dependenÈ›ei noastre de seturi de date publice neÃ®ncredere sau nefiltrate pentru antrenament. Pentru a menÈ›ine integritatea datelor È™i a preveni un proces de antrenament defectuos, este esenÈ›ial sÄƒ urmÄƒreÈ™ti originea È™i provenienÈ›a datelor tale. Altfel, vechea zicalÄƒ â€gunoi Ã®n, gunoi afarÄƒâ€ se aplicÄƒ, ducÃ¢nd la performanÈ›e compromise ale modelului.

IatÄƒ cÃ¢teva exemple despre cum otrÄƒvirea datelor poate afecta modelele tale:

1. **Schimbarea etichetelor (Label Flipping)**: Ãntr-o sarcinÄƒ de clasificare binarÄƒ, un adversar schimbÄƒ intenÈ›ionat etichetele unui subset mic de date de antrenament. De exemplu, mostrele benigne sunt etichetate ca maliÈ›ioase, ceea ce face ca modelul sÄƒ Ã®nveÈ›e asocieri greÈ™ite.\
   **Exemplu**: Un filtru de spam care clasificÄƒ greÈ™it emailuri legitime ca spam din cauza etichetelor manipulate.
2. **OtrÄƒvirea caracteristicilor (Feature Poisoning)**: Un atacator modificÄƒ subtil caracteristicile din datele de antrenament pentru a introduce un bias sau a induce Ã®n eroare modelul.\
   **Exemplu**: AdÄƒugarea de cuvinte cheie irelevante Ã®n descrierile produselor pentru a manipula sistemele de recomandare.
3. **Injectarea datelor (Data Injection)**: Injectarea de date maliÈ›ioase Ã®n setul de antrenament pentru a influenÈ›a comportamentul modelului.\
   **Exemplu**: Introducerea de recenzii false pentru a denatura rezultatele analizei sentimentelor.
4. **Atacuri cu uÈ™Äƒ din spate (Backdoor Attacks)**: Un adversar insereazÄƒ un tipar ascuns (backdoor) Ã®n datele de antrenament. Modelul Ã®nvaÈ›Äƒ sÄƒ recunoascÄƒ acest tipar È™i se comportÄƒ maliÈ›ios cÃ¢nd este declanÈ™at.\
   **Exemplu**: Un sistem de recunoaÈ™tere facialÄƒ antrenat cu imagini compromise care identificÄƒ greÈ™it o persoanÄƒ anume.

MITRE Corporation a creat [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), o bazÄƒ de cunoÈ™tinÈ›e despre tacticile È™i tehnicile folosite de adversari Ã®n atacurile reale asupra sistemelor AI.

> ExistÄƒ un numÄƒr tot mai mare de vulnerabilitÄƒÈ›i Ã®n sistemele cu AI, deoarece integrarea AI mÄƒreÈ™te suprafaÈ›a de atac a sistemelor existente dincolo de cele ale atacurilor cibernetice tradiÈ›ionale. Am dezvoltat ATLAS pentru a creÈ™te conÈ™tientizarea acestor vulnerabilitÄƒÈ›i unice È™i Ã®n evoluÈ›ie, pe mÄƒsurÄƒ ce comunitatea globalÄƒ integreazÄƒ AI Ã®n diverse sisteme. ATLAS este modelat dupÄƒ cadrul MITRE ATT&CKÂ® iar tacticile, tehnicile È™i procedurile (TTP) sunt complementare celor din ATT&CK.

La fel ca cadrul MITRE ATT&CKÂ®, folosit pe scarÄƒ largÄƒ Ã®n securitatea ciberneticÄƒ tradiÈ›ionalÄƒ pentru planificarea scenariilor avansate de emulare a ameninÈ›Äƒrilor, ATLAS oferÄƒ un set uÈ™or de cÄƒutat de TTP-uri care ajutÄƒ la Ã®nÈ›elegerea È™i pregÄƒtirea apÄƒrÄƒrii Ã®mpotriva atacurilor emergente.

Ãn plus, Open Web Application Security Project (OWASP) a creat o "[listÄƒ Top 10](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" a celor mai critice vulnerabilitÄƒÈ›i gÄƒsite Ã®n aplicaÈ›iile care utilizeazÄƒ LLM-uri. Lista evidenÈ›iazÄƒ riscurile unor ameninÈ›Äƒri precum otrÄƒvirea datelor menÈ›ionatÄƒ anterior, dar È™i altele precum:

- **Injectarea de prompturi (Prompt Injection)**: o tehnicÄƒ prin care atacatorii manipuleazÄƒ un Model de Limbaj Mare (LLM) prin inputuri atent construite, determinÃ¢ndu-l sÄƒ se comporte Ã®n afara comportamentului sÄƒu intenÈ›ionat.
- **VulnerabilitÄƒÈ›i Ã®n lanÈ›ul de aprovizionare**: Componentele È™i software-ul care alcÄƒtuiesc aplicaÈ›iile folosite de un LLM, cum ar fi module Python sau seturi de date externe, pot fi compromise, ducÃ¢nd la rezultate neaÈ™teptate, introducerea de bias-uri È™i chiar vulnerabilitÄƒÈ›i Ã®n infrastructura de bazÄƒ.
- **SupraÃ®ncrederea**: LLM-urile sunt imperfecte È™i au tendinÈ›a de a â€halucinaâ€, oferind rezultate inexacte sau nesigure. Ãn mai multe cazuri documentate, oamenii au luat aceste rezultate ca fiind adevÄƒrate, ceea ce a dus la consecinÈ›e negative neintenÈ›ionate Ã®n lumea realÄƒ.

Microsoft Cloud Advocate Rod Trent a scris un ebook gratuit, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), care aprofundeazÄƒ aceste È™i alte ameninÈ›Äƒri emergente Ã®n AI È™i oferÄƒ Ã®ndrumÄƒri extinse despre cum sÄƒ abordezi cel mai bine aceste scenarii.

## Testarea securitÄƒÈ›ii pentru sistemele AI È™i LLM-uri

InteligenÈ›a artificialÄƒ (AI) transformÄƒ diverse domenii È™i industrii, oferind noi posibilitÄƒÈ›i È™i beneficii pentru societate. TotuÈ™i, AI aduce È™i provocÄƒri È™i riscuri semnificative, cum ar fi confidenÈ›ialitatea datelor, bias-ul, lipsa de explicabilitate È™i potenÈ›ialul de utilizare abuzivÄƒ. Prin urmare, este crucial sÄƒ ne asigurÄƒm cÄƒ sistemele AI sunt sigure È™i responsabile, adicÄƒ respectÄƒ standardele etice È™i legale È™i pot fi de Ã®ncredere pentru utilizatori È™i pÄƒrÈ›ile interesate.

Testarea securitÄƒÈ›ii este procesul de evaluare a securitÄƒÈ›ii unui sistem AI sau LLM, prin identificarea È™i exploatarea vulnerabilitÄƒÈ›ilor acestora. Aceasta poate fi realizatÄƒ de dezvoltatori, utilizatori sau auditori terÈ›i, Ã®n funcÈ›ie de scopul È™i aria testÄƒrii. Unele dintre cele mai comune metode de testare a securitÄƒÈ›ii pentru sistemele AI È™i LLM-uri sunt:

- **Sanitizarea datelor**: Procesul de eliminare sau anonimizare a informaÈ›iilor sensibile sau private din datele de antrenament sau din inputul unui sistem AI sau LLM. Sanitizarea datelor ajutÄƒ la prevenirea scurgerilor de date È™i manipulÄƒrilor maliÈ›ioase prin reducerea expunerii datelor confidenÈ›iale sau personale.
- **Testarea adversarialÄƒ**: Generarea È™i aplicarea de exemple adversariale asupra inputului sau outputului unui sistem AI sau LLM pentru a evalua robusteÈ›ea È™i rezilienÈ›a acestuia Ã®n faÈ›a atacurilor adversariale. Testarea adversarialÄƒ ajutÄƒ la identificarea È™i atenuarea vulnerabilitÄƒÈ›ilor È™i slÄƒbiciunilor care pot fi exploatate de atacatori.
- **Verificarea modelului**: Procesul de verificare a corectitudinii È™i completitudinii parametrilor sau arhitecturii modelului unui sistem AI sau LLM. Verificarea modelului ajutÄƒ la detectarea È™i prevenirea furtului de model prin asigurarea protecÈ›iei È™i autentificÄƒrii acestuia.
- **Validarea outputului**: Procesul de validare a calitÄƒÈ›ii È™i fiabilitÄƒÈ›ii outputului unui sistem AI sau LLM. Validarea outputului ajutÄƒ la detectarea È™i corectarea manipulÄƒrilor maliÈ›ioase prin asigurarea consistenÈ›ei È™i acurateÈ›ei rezultatelor.

OpenAI, lider Ã®n sisteme AI, a creat o serie de _evaluÄƒri de siguranÈ›Äƒ_ ca parte a iniÈ›iativei lor de red teaming, menite sÄƒ testeze outputul sistemelor AI Ã®n speranÈ›a de a contribui la siguranÈ›a AI.

> EvaluÄƒrile pot varia de la teste simple de Ã®ntrebÄƒri È™i rÄƒspunsuri pÃ¢nÄƒ la simulÄƒri mai complexe. Ca exemple concrete, iatÄƒ cÃ¢teva evaluÄƒri dezvoltate de OpenAI pentru a evalua comportamentele AI din mai multe perspective:

#### Persuasiune

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ pÄƒcÄƒleascÄƒ un alt sistem AI sÄƒ spunÄƒ un cuvÃ¢nt secret?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ convingÄƒ un alt sistem AI sÄƒ doneze bani?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ influenÈ›eze susÈ›inerea unei propuneri politice de cÄƒtre un alt sistem AI?

#### Steganografie (mesaje ascunse)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ transmitÄƒ mesaje secrete fÄƒrÄƒ sÄƒ fie prins de un alt sistem AI?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ comprime È™i sÄƒ decomprime mesaje pentru a ascunde mesaje secrete?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): CÃ¢t de bine poate un sistem AI sÄƒ se coordoneze cu un alt sistem AI, fÄƒrÄƒ comunicare directÄƒ?

### Securitatea AI

Este imperativ sÄƒ ne propunem sÄƒ protejÄƒm sistemele AI de atacuri maliÈ›ioase, utilizare abuzivÄƒ sau consecinÈ›e neintenÈ›ionate. Aceasta include luarea de mÄƒsuri pentru a asigura siguranÈ›a, fiabilitatea È™i Ã®ncrederea Ã®n sistemele AI, cum ar fi:

- Asigurarea securitÄƒÈ›ii datelor È™i algoritmilor folosiÈ›i pentru antrenarea È™i rularea modelelor AI
- Prevenirea accesului neautorizat, manipulÄƒrii sau sabotajului sistemelor AI
- Detectarea È™i atenuarea bias-ului, discriminÄƒrii sau problemelor etice Ã®n sistemele AI
- Asigurarea responsabilitÄƒÈ›ii, transparenÈ›ei È™i explicabilitÄƒÈ›ii deciziilor È™i acÈ›iunilor AI
- Alinierea obiectivelor È™i valorilor sistemelor AI cu cele ale oamenilor È™i societÄƒÈ›ii

Securitatea AI este importantÄƒ pentru a garanta integritatea, disponibilitatea È™i confidenÈ›ialitatea sistemelor È™i datelor AI. Unele dintre provocÄƒrile È™i oportunitÄƒÈ›ile securitÄƒÈ›ii AI sunt:

- Oportunitate: Integrarea AI Ã®n strategiile de securitate ciberneticÄƒ, deoarece poate juca un rol crucial Ã®n identificarea ameninÈ›Äƒrilor È™i Ã®mbunÄƒtÄƒÈ›irea timpilor de rÄƒspuns. AI poate ajuta la automatizarea È™i augmentarea detectÄƒrii È™i atenuÄƒrii atacurilor cibernetice, cum ar fi phishing, malware sau ransomware.
- Provocare: AI poate fi folositÄƒ È™i de adversari pentru a lansa atacuri sofisticate, cum ar fi generarea de conÈ›inut fals sau Ã®nÈ™elÄƒtor, impersonarea utilizatorilor sau exploatarea vulnerabilitÄƒÈ›ilor din sistemele AI. Prin urmare, dezvoltatorii AI au o responsabilitate unicÄƒ de a proiecta sisteme robuste È™i reziliente Ã®mpotriva utilizÄƒrii abuzive.

### ProtecÈ›ia datelor

LLM-urile pot reprezenta riscuri pentru confidenÈ›ialitatea È™i securitatea datelor pe care le folosesc. De exemplu, LLM-urile pot memora È™i scurge informaÈ›ii sensibile din datele lor de antrenament, cum ar fi nume personale, adrese, parole sau numere de carduri de credit. De asemenea, pot fi manipulate sau atacate de actori maliÈ›ioÈ™i care doresc sÄƒ exploateze vulnerabilitÄƒÈ›ile sau bias-urile lor. Prin urmare, este important sÄƒ fim conÈ™tienÈ›i de aceste riscuri È™i sÄƒ luÄƒm mÄƒsuri adecvate pentru a proteja datele folosite cu LLM-uri. IatÄƒ cÃ¢teva mÄƒsuri pe care le poÈ›i lua pentru a proteja datele folosite cu LLM-uri:

- **Limitarea cantitÄƒÈ›ii È™i tipului de date partajate cu LLM-uri**: PartajeazÄƒ doar datele necesare È™i relevante pentru scopurile propuse È™i evitÄƒ sÄƒ partajezi date sensibile, confidenÈ›iale sau personale. Utilizatorii ar trebui sÄƒ anonimizeze sau sÄƒ cripteze datele partajate cu LLM-uri, de exemplu prin eliminarea sau mascarea oricÄƒror informaÈ›ii identificabile sau folosind canale de comunicare securizate.
- **Verificarea datelor generate de LLM-uri**: VerificÄƒ Ã®ntotdeauna acurateÈ›ea È™i calitatea outputului generat de LLM-uri pentru a te asigura cÄƒ nu conÈ›ine informaÈ›ii nedorite sau inadecvate.
- **Raportarea È™i alertarea oricÄƒror breÈ™e sau incidente de securitate a datelor**: Fii vigilent la orice activitÄƒÈ›i sau comportamente suspecte sau anormale ale LLM-urilor, cum ar fi generarea de texte irelevante, inexacte, ofensatoare sau dÄƒunÄƒtoare. Acestea pot indica o breÈ™Äƒ de securitate sau un incident.

Securitatea, guvernanÈ›a È™i conformitatea datelor sunt critice pentru orice organizaÈ›ie care doreÈ™te sÄƒ valorifice puterea datelor È™i AI Ã®ntr-un mediu multi-cloud. Asigurarea È™i guvernarea tuturor datelor este o sarcinÄƒ complexÄƒ È™i cu multe faÈ›ete. Trebuie sÄƒ asiguri securitatea È™i guvernanÈ›a diferitelor tipuri de date (structurate, nestructurate È™i generate de AI) Ã®n locaÈ›ii diferite, pe mai multe cloud-uri, È™i sÄƒ È›ii cont de reglementÄƒrile actuale È™i viitoare privind securitatea datelor, guvernanÈ›a È™i AI. Pentru a-È›i proteja datele, trebuie sÄƒ adopÈ›i bune practici È™i precauÈ›ii, cum ar fi:

- Folosirea serviciilor sau platformelor cloud care oferÄƒ funcÈ›ii de protecÈ›ie È™i confidenÈ›ialitate a
> Practica de red teaming Ã®n AI a evoluat È™i a cÄƒpÄƒtat un sens mai larg: nu se limiteazÄƒ doar la identificarea vulnerabilitÄƒÈ›ilor de securitate, ci include È™i testarea altor tipuri de defecÈ›iuni ale sistemului, cum ar fi generarea de conÈ›inut potenÈ›ial dÄƒunÄƒtor. Sistemele AI vin cu riscuri noi, iar red teaming este esenÈ›ial pentru a Ã®nÈ›elege aceste riscuri inedite, cum ar fi injecÈ›ia de prompturi È™i producerea de conÈ›inut nefundamentat. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.ro.png)]()

Mai jos sunt prezentate cÃ¢teva perspective cheie care au modelat programul AI Red Team al Microsoft.

1. **Domeniu extins al AI Red Teaming:**
   AI red teaming acoperÄƒ acum atÃ¢t aspectele de securitate, cÃ¢t È™i rezultatele legate de Responsible AI (RAI). Ãn mod tradiÈ›ional, red teaming-ul se concentra pe aspectele de securitate, tratÃ¢nd modelul ca un vector (de exemplu, furtul modelului de bazÄƒ). TotuÈ™i, sistemele AI aduc vulnerabilitÄƒÈ›i noi de securitate (de exemplu, injecÈ›ia de prompturi, otrÄƒvirea datelor), care necesitÄƒ o atenÈ›ie specialÄƒ. Dincolo de securitate, AI red teaming investigheazÄƒ È™i probleme legate de echitate (de exemplu, stereotipuri) È™i conÈ›inut dÄƒunÄƒtor (de exemplu, glorificarea violenÈ›ei). Identificarea timpurie a acestor probleme permite prioritizarea investiÈ›iilor Ã®n apÄƒrare.
2. **EÈ™ecuri maliÈ›ioase È™i benigne:**
   AI red teaming ia Ã®n considerare eÈ™ecurile atÃ¢t din perspectiva actorilor maliÈ›ioÈ™i, cÃ¢t È™i a celor benigni. De exemplu, atunci cÃ¢nd testÄƒm noul Bing, explorÄƒm nu doar cum adversarii rÄƒu intenÈ›ionaÈ›i pot submina sistemul, ci È™i cum utilizatorii obiÈ™nuiÈ›i pot Ã®ntÃ¢lni conÈ›inut problematic sau dÄƒunÄƒtor. Spre deosebire de red teaming-ul tradiÈ›ional de securitate, care se concentreazÄƒ Ã®n principal pe actorii maliÈ›ioÈ™i, AI red teaming ia Ã®n calcul o gamÄƒ mai largÄƒ de persoane È™i potenÈ›iale eÈ™ecuri.
3. **Natura dinamicÄƒ a sistemelor AI:**
   AplicaÈ›iile AI evolueazÄƒ constant. Ãn cazul aplicaÈ›iilor bazate pe modele mari de limbaj, dezvoltatorii se adapteazÄƒ cerinÈ›elor Ã®n schimbare. Red teaming-ul continuu asigurÄƒ o vigilenÈ›Äƒ permanentÄƒ È™i adaptarea la riscurile Ã®n evoluÈ›ie.

AI red teaming nu este o soluÈ›ie completÄƒ È™i ar trebui considerat o miÈ™care complementarÄƒ altor controale, cum ar fi [role-based access control (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) È™i soluÈ›ii cuprinzÄƒtoare de gestionare a datelor. Scopul sÄƒu este sÄƒ completeze o strategie de securitate care se concentreazÄƒ pe utilizarea unor soluÈ›ii AI sigure È™i responsabile, care È›in cont de confidenÈ›ialitate È™i securitate, Ã®n timp ce urmÄƒresc sÄƒ minimizeze prejudecÄƒÈ›ile, conÈ›inutul dÄƒunÄƒtor È™i dezinformarea care pot afecta Ã®ncrederea utilizatorilor.

IatÄƒ o listÄƒ de lecturi suplimentare care te pot ajuta sÄƒ Ã®nÈ›elegi mai bine cum red teaming-ul poate ajuta la identificarea È™i atenuarea riscurilor Ã®n sistemele tale AI:

- [Planificarea red teaming-ului pentru modelele mari de limbaj (LLM) È™i aplicaÈ›iile lor](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Ce este OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - O practicÄƒ esenÈ›ialÄƒ pentru construirea unor soluÈ›ii AI mai sigure È™i responsabile](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), o bazÄƒ de cunoÈ™tinÈ›e despre tacticile È™i tehnicile folosite de adversari Ã®n atacurile reale asupra sistemelor AI.

## Verificare cunoÈ™tinÈ›e

Care ar putea fi o abordare bunÄƒ pentru menÈ›inerea integritÄƒÈ›ii datelor È™i prevenirea utilizÄƒrii abuzive?

1. SÄƒ existe controale puternice bazate pe roluri pentru accesul È™i gestionarea datelor  
1. Implementarea È™i auditarea etichetÄƒrii datelor pentru a preveni reprezentarea greÈ™itÄƒ sau utilizarea abuzivÄƒ a datelor  
1. Asigurarea cÄƒ infrastructura AI suportÄƒ filtrarea conÈ›inutului  

RÄƒspuns: 1. DeÈ™i toate cele trei sunt recomandÄƒri excelente, asigurarea cÄƒ utilizatorilor li se acordÄƒ privilegiile corecte de acces la date va contribui semnificativ la prevenirea manipulÄƒrii È™i reprezentÄƒrii greÈ™ite a datelor folosite de LLM-uri.

## ğŸš€ Provocare

CiteÈ™te mai multe despre cum poÈ›i [guverna È™i proteja informaÈ›iile sensibile](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) Ã®n era AI.

## Bravo, ContinuÄƒ sÄƒ Ã®nveÈ›i

DupÄƒ ce ai terminat aceastÄƒ lecÈ›ie, consultÄƒ colecÈ›ia noastrÄƒ [Generative AI Learning](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pentru a-È›i aprofunda cunoÈ™tinÈ›ele despre Generative AI!

Mergi la LecÈ›ia 14, unde vom explora [Ciclul de viaÈ›Äƒ al aplicaÈ›iilor Generative AI](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim pentru acurateÈ›e, vÄƒ rugÄƒm sÄƒ reÈ›ineÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa nativÄƒ trebuie considerat sursa autorizatÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm rÄƒspunderea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite rezultate din utilizarea acestei traduceri.