{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Capitolul 7: Crearea aplicațiilor de chat\n",
    "## Ghid rapid pentru API-ul Github Models\n",
    "\n",
    "Acest notebook este adaptat din [Repository-ul de exemple Azure OpenAI](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst), care include notebook-uri ce accesează serviciile [Azure OpenAI](notebook-azure-openai.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Prezentare generală  \n",
    "„Modelele lingvistice mari sunt funcții care transformă textul în text. Pornind de la un șir de text introdus, un model lingvistic mare încearcă să prezică ce text va urma”(1). Acest notebook de tip „quickstart” îi va familiariza pe utilizatori cu conceptele de bază ale LLM, cerințele principale ale pachetului pentru a începe cu AML, o introducere ușoară în designul prompturilor și câteva exemple scurte pentru diferite scenarii de utilizare.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Cuprins  \n",
    "\n",
    "[Prezentare generală](../../../../07-building-chat-applications/python)  \n",
    "[Cum să folosești serviciul OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[1. Crearea serviciului OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[2. Instalare](../../../../07-building-chat-applications/python)    \n",
    "[3. Date de autentificare](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Utilizări](../../../../07-building-chat-applications/python)    \n",
    "[1. Rezumă textul](../../../../07-building-chat-applications/python)  \n",
    "[2. Clasifică textul](../../../../07-building-chat-applications/python)  \n",
    "[3. Generează nume noi de produse](../../../../07-building-chat-applications/python)  \n",
    "[4. Ajustează fin un clasificator](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Referințe](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Creează primul tău prompt  \n",
    "Acest exercițiu scurt oferă o introducere de bază pentru trimiterea de prompturi către un model în Github Models pentru o sarcină simplă de „rezumare”.\n",
    "\n",
    "\n",
    "**Pași**:  \n",
    "1. Instalează biblioteca `azure-ai-inference` în mediul tău python, dacă nu ai făcut-o deja.  \n",
    "2. Încarcă bibliotecile standard de ajutor și configurează acreditările pentru Github Models.  \n",
    "3. Alege un model pentru sarcina ta  \n",
    "4. Creează un prompt simplu pentru model  \n",
    "5. Trimite cererea ta către API-ul modelului!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Instalează `azure-ai-inference`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Găsirea modelului potrivit  \n",
    "Modelele GPT-3.5-turbo sau GPT-4 pot înțelege și genera limbaj natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Proiectarea prompturilor  \n",
    "\n",
    "„Magia modelelor lingvistice mari constă în faptul că, fiind antrenate să minimizeze această eroare de predicție pe cantități uriașe de text, modelele ajung să învețe concepte utile pentru aceste predicții. De exemplu, ele învață concepte precum”(1):\n",
    "\n",
    "* cum se scrie corect\n",
    "* cum funcționează gramatica\n",
    "* cum să reformuleze\n",
    "* cum să răspundă la întrebări\n",
    "* cum să poarte o conversație\n",
    "* cum să scrie în mai multe limbi\n",
    "* cum să scrie cod\n",
    "* etc.\n",
    "\n",
    "#### Cum controlezi un model lingvistic mare  \n",
    "„Dintre toate intrările pentru un model lingvistic mare, de departe cea mai influentă este promptul textului”(1).\n",
    "\n",
    "Modelele lingvistice mari pot fi stimulate să genereze rezultate în câteva moduri:\n",
    "\n",
    "Instrucțiune: Spune-i modelului ce vrei\n",
    "Completare: Induci modelul să continue începutul a ceea ce dorești\n",
    "Demonstrație: Arată-i modelului ce vrei, fie cu:\n",
    "Câteva exemple în prompt\n",
    "Câteva sute sau mii de exemple într-un set de date de antrenament pentru fine-tuning”\n",
    "\n",
    "\n",
    "\n",
    "#### Există trei reguli de bază pentru crearea prompturilor:\n",
    "\n",
    "**Arată și explică**. Fă clar ce vrei, fie prin instrucțiuni, exemple sau o combinație a celor două. Dacă vrei ca modelul să ordoneze o listă de elemente alfabetic sau să clasifice un paragraf după sentiment, arată-i exact asta.\n",
    "\n",
    "**Furnizează date de calitate**. Dacă încerci să construiești un clasificator sau să faci modelul să urmeze un anumit tipar, asigură-te că ai suficiente exemple. Verifică-ți exemplele — modelul este de obicei destul de inteligent să treacă peste greșeli de ortografie și să-ți dea un răspuns, dar ar putea presupune că acestea sunt intenționate și asta poate influența rezultatul.\n",
    "\n",
    "**Verifică setările.** Setările temperature și top_p controlează cât de determinist este modelul în generarea unui răspuns. Dacă îi ceri un răspuns unde există un singur răspuns corect, ar trebui să setezi aceste valori mai jos. Dacă vrei răspunsuri mai diverse, le poți seta mai sus. Cea mai frecventă greșeală pe care o fac oamenii cu aceste setări este să creadă că ele controlează „inteligența” sau „creativitatea” modelului.\n",
    "\n",
    "\n",
    "Sursa: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Rezumă textul  \n",
    "#### Provocare  \n",
    "Rezumă un text adăugând 'tl;dr:' la finalul unui pasaj. Observă cum modelul înțelege să realizeze mai multe sarcini fără instrucțiuni suplimentare. Poți experimenta cu solicitări mai descriptive decât tl;dr pentru a modifica comportamentul modelului și a personaliza rezumatul primit(3).  \n",
    "\n",
    "Cercetări recente au arătat îmbunătățiri semnificative în multe sarcini și benchmark-uri NLP prin pre-antrenarea pe un corpus mare de texte, urmată de ajustarea fină pe o sarcină specifică. Deși această metodă are o arhitectură, în general, independentă de sarcină, tot necesită seturi de date specifice pentru ajustare fină, cu mii sau zeci de mii de exemple. În schimb, oamenii pot realiza, de obicei, o nouă sarcină lingvistică doar din câteva exemple sau instrucțiuni simple – lucru cu care sistemele NLP actuale încă se confruntă. Aici arătăm că extinderea modelelor lingvistice îmbunătățește semnificativ performanța generală, cu puține exemple, uneori ajungând chiar să concureze cu abordările anterioare de ajustare fină de ultimă generație.\n",
    "\n",
    "\n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Exerciții pentru mai multe scenarii de utilizare  \n",
    "1. Rezumă textul  \n",
    "2. Clasifică textul  \n",
    "3. Generează nume noi de produse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Clasifică Textul  \n",
    "#### Provocare  \n",
    "Clasifică elementele în categorii furnizate în momentul inferenței. În exemplul de mai jos, atât categoriile cât și textul de clasificat sunt incluse în prompt (*playground_reference).\n",
    "\n",
    "Solicitare client: Bună ziua, una dintre tastele de la tastatura laptopului meu s-a stricat recent și am nevoie de una nouă:\n",
    "\n",
    "Categorie clasificată:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Generează Nume Noi de Produse\n",
    "#### Provocare\n",
    "Creează nume de produse pornind de la cuvinte exemplu. În acest exercițiu includem în prompt informații despre produsul pentru care dorim să generăm nume. Oferim și un exemplu similar pentru a arăta modelul pe care îl dorim. De asemenea, am setat o valoare mare pentru temperatură pentru a crește gradul de inovație și răspunsuri mai creative.\n",
    "\n",
    "Descriere produs: Un aparat de făcut milkshake-uri acasă  \n",
    "Cuvinte cheie: rapid, sănătos, compact.  \n",
    "Nume de produse: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Descriere produs: O pereche de pantofi care se potrivește oricărei mărimi de picior.  \n",
    "Cuvinte cheie: adaptabil, potrivire, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Referințe  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Exemple OpenAI Studio](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Cele mai bune practici pentru fine-tuning GPT-3 pentru clasificarea textului](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Pentru mai mult ajutor  \n",
    "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Contribuitori\n",
    "* [Chew-Yean Yam](https://www.linkedin.com/in/cyyam/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Declinarea responsabilității**:\nAcest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși depunem eforturi pentru acuratețe, vă rugăm să rețineți că traducerile automate pot conține erori sau inexactități. Documentul original, în limba sa nativă, trebuie considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de oameni. Nu ne asumăm răspunderea pentru eventuale neînțelegeri sau interpretări greșite care pot apărea în urma utilizării acestei traduceri.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "3eeb8e5cad61b52a8366f6259a53ed49",
   "translation_date": "2025-08-25T17:49:32+00:00",
   "source_file": "07-building-chat-applications/python/githubmodels-assignment.ipynb",
   "language_code": "ro"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}