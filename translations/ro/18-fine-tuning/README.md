<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-07-09T17:49:35+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "ro"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.ro.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Ajustarea finÄƒ a LLM-ului tÄƒu

Folosirea modelelor mari de limbaj pentru a construi aplicaÈ›ii AI generative vine cu noi provocÄƒri. O problemÄƒ esenÈ›ialÄƒ este asigurarea calitÄƒÈ›ii rÄƒspunsurilor (acurateÈ›e È™i relevanÈ›Äƒ) Ã®n conÈ›inutul generat de model pentru o anumitÄƒ cerere a utilizatorului. Ãn lecÈ›iile anterioare, am discutat tehnici precum ingineria promptului È™i generarea augmentatÄƒ cu recuperare, care Ã®ncearcÄƒ sÄƒ rezolve problema prin _modificarea inputului promptului_ cÄƒtre modelul existent.

Ãn lecÈ›ia de astÄƒzi, discutÄƒm o a treia tehnicÄƒ, **ajustarea finÄƒ**, care Ã®ncearcÄƒ sÄƒ abordeze provocarea prin _re-antrenarea modelului Ã®n sine_ cu date suplimentare. HaideÈ›i sÄƒ intrÄƒm Ã®n detalii.

## Obiective de Ã®nvÄƒÈ›are

AceastÄƒ lecÈ›ie introduce conceptul de ajustare finÄƒ pentru modelele de limbaj pre-antrenate, exploreazÄƒ beneficiile È™i provocÄƒrile acestei abordÄƒri È™i oferÄƒ Ã®ndrumÄƒri despre cÃ¢nd È™i cum sÄƒ foloseÈ™ti ajustarea finÄƒ pentru a Ã®mbunÄƒtÄƒÈ›i performanÈ›a modelelor tale AI generative.

La finalul acestei lecÈ›ii, ar trebui sÄƒ poÈ›i rÄƒspunde la urmÄƒtoarele Ã®ntrebÄƒri:

- Ce este ajustarea finÄƒ pentru modelele de limbaj?
- CÃ¢nd È™i de ce este utilÄƒ ajustarea finÄƒ?
- Cum pot ajusta fin un model pre-antrenat?
- Care sunt limitÄƒrile ajustÄƒrii fine?

EÈ™ti gata? SÄƒ Ã®ncepem.

## Ghid ilustrat

Vrei sÄƒ ai o imagine de ansamblu a ceea ce vom acoperi Ã®nainte sÄƒ intrÄƒm Ã®n detalii? ConsultÄƒ acest ghid ilustrat care descrie parcursul de Ã®nvÄƒÈ›are pentru aceastÄƒ lecÈ›ie â€“ de la Ã®nÈ›elegerea conceptelor de bazÄƒ È™i motivaÈ›ia pentru ajustarea finÄƒ, pÃ¢nÄƒ la procesul È™i cele mai bune practici pentru executarea sarcinii de ajustare finÄƒ. Este un subiect fascinant pentru explorare, aÈ™a cÄƒ nu uita sÄƒ verifici pagina [Resurse](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) pentru linkuri suplimentare care sÄƒ susÈ›inÄƒ cÄƒlÄƒtoria ta de Ã®nvÄƒÈ›are autodirijatÄƒ!

![Ghid ilustrat pentru ajustarea finÄƒ a modelelor de limbaj](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.ro.png)

## Ce este ajustarea finÄƒ pentru modelele de limbaj?

Prin definiÈ›ie, modelele mari de limbaj sunt _pre-antrenate_ pe cantitÄƒÈ›i mari de text provenit din surse diverse, inclusiv internetul. AÈ™a cum am Ã®nvÄƒÈ›at Ã®n lecÈ›iile anterioare, avem nevoie de tehnici precum _ingineria promptului_ È™i _generarea augmentatÄƒ cu recuperare_ pentru a Ã®mbunÄƒtÄƒÈ›i calitatea rÄƒspunsurilor modelului la Ã®ntrebÄƒrile utilizatorului (â€promptsâ€).

O tehnicÄƒ popularÄƒ de inginerie a promptului implicÄƒ oferirea modelului mai multÄƒ Ã®ndrumare despre ce se aÈ™teaptÄƒ Ã®n rÄƒspuns, fie prin furnizarea de _instrucÈ›iuni_ (Ã®ndrumare explicitÄƒ), fie prin _oferirea cÃ¢torva exemple_ (Ã®ndrumare implicitÄƒ). Aceasta se numeÈ™te _Ã®nvÄƒÈ›are cu puÈ›ine exemple_ (few-shot learning), dar are douÄƒ limitÄƒri:

- Limitele de tokeni ale modelului pot restricÈ›iona numÄƒrul de exemple pe care le poÈ›i oferi È™i pot limita eficienÈ›a.
- Costurile tokenilor pot face adÄƒugarea de exemple la fiecare prompt costisitoare È™i pot limita flexibilitatea.

Ajustarea finÄƒ este o practicÄƒ comunÄƒ Ã®n sistemele de Ã®nvÄƒÈ›are automatÄƒ, unde luÄƒm un model pre-antrenat È™i Ã®l re-antrenÄƒm cu date noi pentru a-i Ã®mbunÄƒtÄƒÈ›i performanÈ›a pe o sarcinÄƒ specificÄƒ. Ãn contextul modelelor de limbaj, putem ajusta fin modelul pre-antrenat _cu un set selectat de exemple pentru o anumitÄƒ sarcinÄƒ sau domeniu de aplicare_ pentru a crea un **model personalizat** care poate fi mai precis È™i mai relevant pentru acea sarcinÄƒ sau domeniu specific. Un beneficiu suplimentar al ajustÄƒrii fine este cÄƒ poate reduce È™i numÄƒrul de exemple necesare pentru Ã®nvÄƒÈ›area cu puÈ›ine exemple â€“ reducÃ¢nd astfel utilizarea tokenilor È™i costurile aferente.

## CÃ¢nd È™i de ce ar trebui sÄƒ ajustÄƒm fin modelele?

Ãn _acest_ context, cÃ¢nd vorbim despre ajustarea finÄƒ, ne referim la ajustarea finÄƒ **supravegheatÄƒ**, unde re-antrenarea se face prin **adÄƒugarea de date noi** care nu au fÄƒcut parte din setul original de antrenament. Aceasta este diferitÄƒ de o abordare de ajustare finÄƒ nesupravegheatÄƒ, unde modelul este re-antrenat pe datele originale, dar cu hiperparametri diferiÈ›i.

Ce este important de reÈ›inut este cÄƒ ajustarea finÄƒ este o tehnicÄƒ avansatÄƒ care necesitÄƒ un anumit nivel de expertizÄƒ pentru a obÈ›ine rezultatele dorite. DacÄƒ este fÄƒcutÄƒ incorect, poate sÄƒ nu aducÄƒ Ã®mbunÄƒtÄƒÈ›irile aÈ™teptate È™i chiar sÄƒ degradeze performanÈ›a modelului pentru domeniul tÄƒu È›intÄƒ.

AÈ™adar, Ã®nainte sÄƒ Ã®nveÈ›i â€cumâ€ sÄƒ ajustezi fin modelele de limbaj, trebuie sÄƒ È™tii â€de ceâ€ ar trebui sÄƒ alegi aceastÄƒ cale È™i â€cÃ¢ndâ€ sÄƒ Ã®ncepi procesul de ajustare finÄƒ. Ãncepe prin a-È›i pune urmÄƒtoarele Ã®ntrebÄƒri:

- **Caz de utilizare**: Care este _cazul tÄƒu de utilizare_ pentru ajustarea finÄƒ? Ce aspect al modelului pre-antrenat actual vrei sÄƒ Ã®mbunÄƒtÄƒÈ›eÈ™ti?
- **Alternative**: Ai Ã®ncercat _alte tehnici_ pentru a obÈ›ine rezultatele dorite? FoloseÈ™te-le pentru a crea un punct de referinÈ›Äƒ pentru comparaÈ›ie.
  - Ingineria promptului: ÃncearcÄƒ tehnici precum few-shot prompting cu exemple relevante de rÄƒspunsuri la prompturi. EvalueazÄƒ calitatea rÄƒspunsurilor.
  - Generare augmentatÄƒ cu recuperare: ÃncearcÄƒ sÄƒ completezi prompturile cu rezultate obÈ›inute prin cÄƒutarea Ã®n datele tale. EvalueazÄƒ calitatea rÄƒspunsurilor.
- **Costuri**: Ai identificat costurile pentru ajustarea finÄƒ?
  - Posibilitatea de ajustare â€“ este modelul pre-antrenat disponibil pentru ajustare finÄƒ?
  - Efort â€“ pentru pregÄƒtirea datelor de antrenament, evaluarea È™i rafinarea modelului.
  - Resurse de calcul â€“ pentru rularea joburilor de ajustare finÄƒ È™i pentru implementarea modelului ajustat.
  - Date â€“ acces la exemple de calitate suficientÄƒ pentru impactul ajustÄƒrii fine.
- **Beneficii**: Ai confirmat beneficiile ajustÄƒrii fine?
  - Calitate â€“ modelul ajustat fin a depÄƒÈ™it punctul de referinÈ›Äƒ?
  - Cost â€“ reduce utilizarea tokenilor prin simplificarea prompturilor?
  - Extensibilitate â€“ poÈ›i reutiliza modelul de bazÄƒ pentru noi domenii?

RÄƒspunzÃ¢nd acestor Ã®ntrebÄƒri, ar trebui sÄƒ poÈ›i decide dacÄƒ ajustarea finÄƒ este abordarea potrivitÄƒ pentru cazul tÄƒu de utilizare. Ideal, aceastÄƒ abordare este valabilÄƒ doar dacÄƒ beneficiile depÄƒÈ™esc costurile. OdatÄƒ ce decizi sÄƒ continui, este timpul sÄƒ te gÃ¢ndeÈ™ti _cum_ poÈ›i ajusta fin modelul pre-antrenat.

Vrei sÄƒ afli mai multe despre procesul decizional? UrmÄƒreÈ™te [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Cum putem ajusta fin un model pre-antrenat?

Pentru a ajusta fin un model pre-antrenat, ai nevoie de:

- un model pre-antrenat pentru ajustare finÄƒ
- un set de date pentru ajustare finÄƒ
- un mediu de antrenament pentru a rula jobul de ajustare finÄƒ
- un mediu de gÄƒzduire pentru a implementa modelul ajustat fin

## Ajustarea finÄƒ Ã®n practicÄƒ

Resursele urmÄƒtoare oferÄƒ tutoriale pas cu pas care te ghideazÄƒ printr-un exemplu real folosind un model selectat cu un set de date curat. Pentru a parcurge aceste tutoriale, ai nevoie de un cont la furnizorul specific, Ã®mpreunÄƒ cu acces la modelul È™i seturile de date relevante.

| Furnizor    | Tutorial                                                                                                                                                                       | Descriere                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI      | [Cum sÄƒ ajustezi fin modelele de chat](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)          | ÃnvaÈ›Äƒ cum sÄƒ ajustezi fin un `gpt-35-turbo` pentru un domeniu specific (â€asistent de reÈ›eteâ€) prin pregÄƒtirea datelor de antrenament, rularea jobului de ajustare finÄƒ È™i folosirea modelului ajustat pentru inferenÈ›Äƒ.                                                                                                                                                                                                           |
| Azure OpenAI| [Tutorial de ajustare finÄƒ GPT 3.5 Turbo](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | ÃnvaÈ›Äƒ cum sÄƒ ajustezi fin un model `gpt-35-turbo-0613` **pe Azure** prin paÈ™i pentru crearea È™i Ã®ncÄƒrcarea datelor de antrenament, rularea jobului de ajustare finÄƒ. Implementarea È™i utilizarea noului model.                                                                                                                                                                                                                      |
| Hugging Face| [Ajustarea finÄƒ a LLM-urilor cu Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                     | Acest articol te ghideazÄƒ prin ajustarea finÄƒ a unui _LLM deschis_ (ex: `CodeLlama 7B`) folosind biblioteca [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) È™i [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) cu seturi de date deschise pe Hugging Face.                                                                 |
|             |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ğŸ¤— AutoTrain| [Ajustarea finÄƒ a LLM-urilor cu AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                               | AutoTrain (sau AutoTrain Advanced) este o bibliotecÄƒ Python dezvoltatÄƒ de Hugging Face care permite ajustarea finÄƒ pentru multe sarcini diferite, inclusiv ajustarea finÄƒ a LLM-urilor. AutoTrain este o soluÈ›ie fÄƒrÄƒ cod, iar ajustarea finÄƒ poate fi fÄƒcutÄƒ Ã®n propriul tÄƒu cloud, pe Hugging Face Spaces sau local. SuportÄƒ interfaÈ›Äƒ web, CLI È™i antrenament prin fiÈ™iere de configurare yaml.                                         |
|             |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                  |

## Tema

Alege unul dintre tutorialele de mai sus È™i parcurge-l. _Este posibil sÄƒ replicÄƒm o versiune a acestor tutoriale Ã®n Jupyter Notebooks Ã®n acest repo doar pentru referinÈ›Äƒ. Te rugÄƒm sÄƒ foloseÈ™ti sursele originale direct pentru a obÈ›ine cele mai recente versiuni_.

## Bravo! ContinuÄƒ sÄƒ Ã®nveÈ›i.

DupÄƒ ce ai terminat aceastÄƒ lecÈ›ie, consultÄƒ colecÈ›ia noastrÄƒ [Generative AI Learning](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pentru a-È›i continua dezvoltarea cunoÈ™tinÈ›elor despre AI generativ!

FelicitÄƒri!! Ai finalizat ultima lecÈ›ie din seria v2 pentru acest curs! Nu te opri din Ã®nvÄƒÈ›at È™i construit. \*\*VerificÄƒ pagina [RESURSE](RESOURCES.md?WT.mc_id=academic-105485-koreyst) pentru o listÄƒ de sugestii suplimentare doar pentru acest subiect.

Seria noastrÄƒ v1 de lecÈ›ii a fost de asemenea actualizatÄƒ cu mai multe teme È™i concepte. AÈ™a cÄƒ ia-È›i un moment sÄƒ-È›i reÃ®mprospÄƒtezi cunoÈ™tinÈ›ele â€“ È™i te rugÄƒm sÄƒ [Ã®mpÄƒrtÄƒÈ™eÈ™ti Ã®ntrebÄƒrile È™i feedback-ul tÄƒu](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) pentru a ne ajuta sÄƒ Ã®mbunÄƒtÄƒÈ›im aceste lecÈ›ii pentru comunitate.

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim pentru acurateÈ›e, vÄƒ rugÄƒm sÄƒ reÈ›ineÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa nativÄƒ trebuie considerat sursa autorizatÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm rÄƒspunderea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite rezultate din utilizarea acestei traduceri.