<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-05-20T07:58:21+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "ro"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.8487555c3e3225eefc1dc84e72c8e00bce1ee76db867a080628fb0fbb04aa0d2.ro.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Ajustarea fin캒 a LLM-ului t캒u

Utilizarea modelelor mari de limbaj pentru a construi aplica탵ii AI generative vine cu noi provoc캒ri. O problem캒 cheie este asigurarea calit캒탵ii r캒spunsurilor (acurate탵e 탳i relevan탵캒) 칥n con탵inutul generat de model pentru o cerere dat캒 a utilizatorului. 칉n lec탵iile anterioare, am discutat tehnici precum ingineria prompturilor 탳i generarea augmentat캒 prin reg캒sire, care 칥ncearc캒 s캒 rezolve problema prin _modificarea intr캒rii promptului_ 칥n modelul existent.

칉n lec탵ia de ast캒zi, discut캒m o a treia tehnic캒, **ajustarea fin캒**, care 칥ncearc캒 s캒 abordeze provocarea prin _re-antrenarea modelului 칥n sine_ cu date suplimentare. S캒 intr캒m 칥n detalii.

## Obiectivele 칉nv캒탵캒rii

Aceast캒 lec탵ie introduce conceptul de ajustare fin캒 pentru modelele de limbaj pre-antrenate, exploreaz캒 beneficiile 탳i provoc캒rile acestei abord캒ri 탳i ofer캒 칥ndrum캒ri despre c칙nd 탳i cum s캒 folose탳ti ajustarea fin캒 pentru a 칥mbun캒t캒탵i performan탵a modelelor tale AI generative.

P칙n캒 la sf칙r탳itul acestei lec탵ii, ar trebui s캒 po탵i r캒spunde la urm캒toarele 칥ntreb캒ri:

- Ce este ajustarea fin캒 pentru modelele de limbaj?
- C칙nd 탳i de ce este util캒 ajustarea fin캒?
- Cum pot ajusta fin un model pre-antrenat?
- Care sunt limit캒rile ajust캒rii fine?

Gata? S캒 칥ncepem.

## Ghid Ilustrat

Vrei s캒 ob탵ii o imagine de ansamblu asupra a ceea ce vom acoperi 칥nainte de a intra 칥n detalii? Verific캒 acest ghid ilustrat care descrie parcursul de 칥nv캒탵are pentru aceast캒 lec탵ie - de la 칥nv캒탵area conceptelor de baz캒 탳i motiva탵ia pentru ajustarea fin캒, p칙n캒 la 칥n탵elegerea procesului 탳i a celor mai bune practici pentru realizarea sarcinii de ajustare fin캒. Este un subiect fascinant de explorat, a탳a c캒 nu uita s캒 verifici pagina de [Resurse](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) pentru linkuri suplimentare care s캒 sprijine parcursul t캒u de 칥nv캒탵are autodidact캒!

![Ghid Ilustrat pentru Ajustarea Fin캒 a Modelelor de Limbaj](../../../translated_images/18-fine-tuning-sketchnote.92733966235199dd260184b1aae3a84b877c7496bc872d8e63ad6fa2dd96bafc.ro.png)

## Ce este ajustarea fin캒 pentru modelele de limbaj?

Prin defini탵ie, modelele mari de limbaj sunt _pre-antrenate_ pe cantit캒탵i mari de text provenind din surse diverse, inclusiv internetul. A탳a cum am 칥nv캒탵at 칥n lec탵iile anterioare, avem nevoie de tehnici precum _ingineria prompturilor_ 탳i _generarea augmentat캒 prin reg캒sire_ pentru a 칥mbun캒t캒탵i calitatea r캒spunsurilor modelului la 칥ntreb캒rile utilizatorului ("prompturi").

O tehnic캒 popular캒 de inginerie a prompturilor implic캒 oferirea modelului de mai multe indica탵ii despre ce se a탳teapt캒 칥n r캒spuns, fie prin oferirea de _instruc탵iuni_ (ghidare explicit캒) sau _oferindu-i c칙teva exemple_ (ghidare implicit캒). Aceasta este cunoscut캒 sub numele de _칥nv캒탵are cu pu탵ine exemple_, dar are dou캒 limit캒ri:

- Limitele de token ale modelului pot restric탵iona num캒rul de exemple pe care le po탵i oferi 탳i limiteaz캒 eficacitatea.
- Costurile token ale modelului pot face ca ad캒ugarea de exemple la fiecare prompt s캒 fie costisitoare 탳i s캒 limiteze flexibilitatea.

Ajustarea fin캒 este o practic캒 obi탳nuit캒 칥n sistemele de 칥nv캒탵are automat캒, unde lu캒m un model pre-antrenat 탳i 칥l re-antren캒m cu date noi pentru a-i 칥mbun캒t캒탵i performan탵a pe o sarcin캒 specific캒. 칉n contextul modelelor de limbaj, putem ajusta fin modelul pre-antrenat _cu un set curat de exemple pentru o sarcin캒 sau un domeniu de aplica탵ie dat_ pentru a crea un **model personalizat** care poate fi mai precis 탳i mai relevant pentru acea sarcin캒 sau domeniu specific. Un beneficiu secundar al ajust캒rii fine este c캒 poate reduce 탳i num캒rul de exemple necesare pentru 칥nv캒탵area cu pu탵ine exemple - reduc칙nd utilizarea tokenilor 탳i costurile asociate.

## C칙nd 탳i de ce ar trebui s캒 ajust캒m fin modelele?

칉n _acest_ context, c칙nd vorbim despre ajustarea fin캒, ne referim la ajustarea fin캒 **supervizat캒**, unde re-antrenarea se face prin **ad캒ugarea de date noi** care nu f캒ceau parte din setul de date ini탵ial de antrenament. Acest lucru este diferit de o abordare nesupervizat캒 de ajustare fin캒, unde modelul este re-antrenat pe datele originale, dar cu hiperparametri diferi탵i.

Lucrul esen탵ial de re탵inut este c캒 ajustarea fin캒 este o tehnic캒 avansat캒 care necesit캒 un anumit nivel de expertiz캒 pentru a ob탵ine rezultatele dorite. Dac캒 este f캒cut캒 incorect, poate s캒 nu ofere 칥mbun캒t캒탵irile a탳teptate 탳i poate chiar s캒 degradeze performan탵a modelului pentru domeniul t캒u 탵int캒.

A탳adar, 칥nainte de a 칥nv캒탵a "cum" s캒 ajustezi fin modelele de limbaj, trebuie s캒 탳tii "de ce" ar trebui s캒 alegi aceast캒 cale 탳i "c칙nd" s캒 칥ncepi procesul de ajustare fin캒. 칉ncepe prin a-탵i pune aceste 칥ntreb캒ri:

- **Caz de utilizare**: Care este _cazul t캒u de utilizare_ pentru ajustarea fin캒? Ce aspect al modelului pre-antrenat actual dore탳ti s캒 칥mbun캒t캒탵e탳ti?
- **Alternative**: Ai 칥ncercat _alte tehnici_ pentru a ob탵ine rezultatele dorite? Folose탳te-le pentru a crea o baz캒 de compara탵ie.
  - Ingineria prompturilor: 칉ncearc캒 tehnici precum prompting cu pu탵ine exemple cu exemple de r캒spunsuri relevante la prompturi. Evalueaz캒 calitatea r캒spunsurilor.
  - Generarea Augmentat캒 prin Reg캒sire: 칉ncearc캒 s캒 adaugi prompturi cu rezultate de c캒utare ob탵inute prin c캒utarea datelor tale. Evalueaz캒 calitatea r캒spunsurilor.
- **Costuri**: Ai identificat costurile pentru ajustarea fin캒?
  - Capacitate de ajustare - este modelul pre-antrenat disponibil pentru ajustare fin캒?
  - Efort - pentru preg캒tirea datelor de antrenament, evaluarea 탳i rafinarea modelului.
  - Resurse de calcul - pentru rularea sarcinilor de ajustare fin캒 탳i implementarea modelului ajustat fin
  - Date - acces la exemple de calitate suficient캒 pentru a avea impact asupra ajust캒rii fine
- **Beneficii**: Ai confirmat beneficiile ajust캒rii fine?
  - Calitate - a dep캒탳it modelul ajustat fin baza de compara탵ie?
  - Cost - reduce utilizarea tokenilor prin simplificarea prompturilor?
  - Extensibilitate - po탵i reutiliza modelul de baz캒 pentru domenii noi?

Prin r캒spunsul la aceste 칥ntreb캒ri, ar trebui s캒 po탵i decide dac캒 ajustarea fin캒 este abordarea potrivit캒 pentru cazul t캒u de utilizare. Ideal, abordarea este valabil캒 doar dac캒 beneficiile dep캒탳esc costurile. Odat캒 ce decizi s캒 continui, este timpul s캒 te g칙nde탳ti la _cum_ po탵i ajusta fin modelul pre-antrenat.

Vrei s캒 ob탵ii mai multe informa탵ii despre procesul de luare a deciziilor? Urm캒re탳te [S캒 ajustezi fin sau nu](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Cum putem ajusta fin un model pre-antrenat?

Pentru a ajusta fin un model pre-antrenat, ai nevoie de:

- un model pre-antrenat pentru a fi ajustat fin
- un set de date pentru a fi folosit la ajustarea fin캒
- un mediu de antrenament pentru a rula sarcina de ajustare fin캒
- un mediu de g캒zduire pentru a implementa modelul ajustat fin

## Ajustarea Fin캒 칥n Ac탵iune

Urm캒toarele resurse ofer캒 tutoriale pas cu pas pentru a te ghida printr-un exemplu real folosind un model selectat cu un set de date curat. Pentru a parcurge aceste tutoriale, ai nevoie de un cont la furnizorul specific, 칥mpreun캒 cu acces la modelul 탳i seturile de date relevante.

| Furnizor     | Tutorial                                                                                                                                                                       | Descriere                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [Cum s캒 ajustezi fin modelele de chat](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                | 칉nva탵캒 s캒 ajustezi fin un `gpt-35-turbo` pentru un domeniu specific ("asistent de re탵ete") prin preg캒tirea datelor de antrenament, rularea sarcinii de ajustare fin캒 탳i folosirea modelului ajustat fin pentru inferen탵캒.                                                                                                                                                                                                                                              |
| Azure OpenAI | [Tutorial de ajustare fin캒 GPT 3.5 Turbo](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | 칉nva탵캒 s캒 ajustezi fin un model `gpt-35-turbo-0613` **pe Azure** parcurg칙nd pa탳ii de creare 탳i 칥nc캒rcare a datelor de antrenament, rularea sarcinii de ajustare fin캒. Implementeaz캒 탳i folose탳te noul model.                                                                                                                                                                                                                                                                 |
| Hugging Face | [Ajustarea fin캒 a LLM-urilor cu Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Acest articol de blog te ghideaz캒 칥n ajustarea fin캒 a unui _LLM deschis_ (ex: `CodeLlama 7B`) folosind biblioteca [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) 탳i [칉nv캒탵area prin Recompens캒 a Transformatoarelor (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) cu seturi de date deschise [datasets](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) pe Hugging Face. |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 游뱅 AutoTrain | [Ajustarea fin캒 a LLM-urilor cu AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                         | AutoTrain (sau AutoTrain Advanced) este o bibliotec캒 Python dezvoltat캒 de Hugging Face care permite ajustarea fin캒 pentru multe sarcini diferite, inclusiv ajustarea fin캒 a LLM-urilor. AutoTrain este o solu탵ie f캒r캒 cod 탳i ajustarea fin캒 poate fi realizat캒 칥n propriul t캒u cloud, pe Hugging Face Spaces sau local. Suport캒 at칙t o interfa탵캒 grafic캒 web, c칙t 탳i interfa탵a de linie de comand캒 탳i antrenamentul prin fi탳iere de configurare yaml.                                                                               |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Tem캒

Selecteaz캒 unul dintre tutorialele de mai sus 탳i parcurge-le. _Putem replica o versiune a acestor tutoriale 칥n Jupyter Notebooks 칥n acest repo doar pentru referin탵캒. Te rug캒m s캒 folose탳ti sursele originale direct pentru a ob탵ine cele mai recente versiuni_.

## Lucru Grozav! Continu캒-탵i 칉nv캒탵area.

Dup캒 ce ai completat aceast캒 lec탵ie, verific캒 colec탵ia noastr캒 [Generative AI Learning](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pentru a continua s캒 칥탵i 칥mbun캒t캒탵e탳ti cuno탳tin탵ele 칥n AI generativ캒!

Felicit캒ri!! Ai finalizat lec탵ia final캒 din seria v2 pentru acest curs! Nu te opri din 칥nv캒탵at 탳i construit. \*\*Verific캒 pagina de [RESURSE](RESOURCES.md?WT.mc_id=academic-105485-koreyst) pentru o list캒 de sugestii suplimentare doar pentru acest subiect.

Seria noastr캒 v1 de lec탵ii a fost de asemenea actualizat캒 cu mai multe teme 탳i concepte. A탳adar, ia-탵i un minut s캒-탵i re칥mprosp캒tezi cuno탳tin탵ele - 탳i te rug캒m [s캒 칥mp캒rt캒탳e탳ti 칥ntreb캒rile 탳i feedback-ul t캒u](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) pentru a ne ajuta s캒 칥mbun캒t캒탵im aceste lec탵ii pentru comunitate.

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). De탳i ne str캒duim s캒 asigur캒m acurate탵ea, v캒 rug캒m s캒 fi탵i con탳tien탵i de faptul c캒 traducerile automate pot con탵ine erori sau inexactit캒탵i. Documentul original 칥n limba sa matern캒 ar trebui s캒 fie considerat sursa autoritar캒. Pentru informa탵ii critice, se recomand캒 traducerea profesional캒 uman캒. Nu ne asum캒m responsabilitatea pentru eventualele ne칥n탵elegeri sau interpret캒ri gre탳ite care pot ap캒rea din utilizarea acestei traduceri.