{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construirea cu modelele din familia Meta\n",
    "\n",
    "## Introducere\n",
    "\n",
    "Această lecție va acoperi:\n",
    "\n",
    "- Explorarea celor două modele principale din familia Meta - Llama 3.1 și Llama 3.2\n",
    "- Înțelegerea scenariilor și a cazurilor de utilizare pentru fiecare model\n",
    "- Exemplu de cod pentru a evidenția caracteristicile unice ale fiecărui model\n",
    "\n",
    "## Familia de modele Meta\n",
    "\n",
    "În această lecție, vom explora 2 modele din familia Meta sau „Llama Herd” - Llama 3.1 și Llama 3.2\n",
    "\n",
    "Aceste modele vin în variante diferite și sunt disponibile pe piața de modele Github. Găsești mai multe detalii despre folosirea Github Models pentru a [prototipa cu modele AI](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Variante de model:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Notă: Llama 3 este disponibil și pe Github Models, dar nu va fi acoperit în această lecție*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Cu 405 miliarde de parametri, Llama 3.1 se încadrează în categoria LLM open source.\n",
    "\n",
    "Acest model este o îmbunătățire față de versiunea anterioară Llama 3, oferind:\n",
    "\n",
    "- Fereastră de context mai mare - 128k tokeni față de 8k tokeni\n",
    "- Număr maxim de tokeni generați mai mare - 4096 față de 2048\n",
    "- Suport multilingv mai bun - datorită creșterii numărului de tokeni folosiți la antrenare\n",
    "\n",
    "Aceste îmbunătățiri permit Llama 3.1 să gestioneze cazuri de utilizare mai complexe atunci când se construiesc aplicații GenAI, inclusiv:\n",
    "- Apelare nativă de funcții - posibilitatea de a apela instrumente și funcții externe în afara fluxului de lucru al LLM-ului\n",
    "- Performanță RAG îmbunătățită - datorită ferestrei de context mai mari\n",
    "- Generare de date sintetice - capacitatea de a crea date eficiente pentru sarcini precum fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apelarea nativă a funcțiilor\n",
    "\n",
    "Llama 3.1 a fost ajustat pentru a fi mai eficient la efectuarea de apeluri către funcții sau instrumente. De asemenea, are două instrumente integrate pe care modelul le poate identifica ca fiind necesare, în funcție de cererea utilizatorului. Aceste instrumente sunt:\n",
    "\n",
    "- **Brave Search** – Poate fi folosit pentru a obține informații actualizate, cum ar fi vremea, printr-o căutare pe web\n",
    "- **Wolfram Alpha** – Poate fi folosit pentru calcule matematice mai complexe, astfel încât nu este nevoie să scrii propriile funcții.\n",
    "\n",
    "Poți crea și propriile tale instrumente personalizate pe care LLM le poate apela.\n",
    "\n",
    "În exemplul de cod de mai jos:\n",
    "\n",
    "- Definim instrumentele disponibile (brave_search, wolfram_alpha) în promptul de sistem.\n",
    "- Trimitem un prompt de la utilizator care întreabă despre vremea dintr-un anumit oraș.\n",
    "- LLM va răspunde cu un apel către instrumentul Brave Search, care va arăta astfel: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Notă: Acest exemplu doar face apelul către instrument; dacă vrei să obții rezultatele, va trebui să îți creezi un cont gratuit pe pagina Brave API și să definești funcția propriu-zisă*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Deși este un LLM, una dintre limitările pe care le are Llama 3.1 este multimodalitatea. Mai exact, capacitatea de a folosi diferite tipuri de input, cum ar fi imaginile, ca prompturi și de a oferi răspunsuri. Această abilitate este una dintre principalele caracteristici ale Llama 3.2. Aceste funcționalități includ și:\n",
    "\n",
    "- Multimodalitate – are capacitatea de a evalua atât prompturi text, cât și imagini\n",
    "- Variante de dimensiuni mici și medii (11B și 90B) – oferă opțiuni flexibile de implementare,\n",
    "- Variante doar text (1B și 3B) – permit modelului să fie implementat pe dispozitive edge sau mobile și asigură latență redusă\n",
    "\n",
    "Suportul multimodal reprezintă un pas important în lumea modelelor open source. Exemplul de cod de mai jos folosește atât o imagine, cât și un prompt text pentru a obține o analiză a imaginii de la Llama 3.2 90B.\n",
    "\n",
    "### Suport multimodal cu Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Învățarea nu se oprește aici, continuă-ți drumul\n",
    "\n",
    "După ce ai terminat această lecție, aruncă o privire la [colecția noastră de învățare despre AI generativă](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pentru a-ți dezvolta în continuare cunoștințele despre AI generativă!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Declinarea responsabilității**:\nAcest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși depunem eforturi pentru acuratețe, vă rugăm să rețineți că traducerile automate pot conține erori sau inexactități. Documentul original, în limba sa nativă, trebuie considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de oameni. Nu ne asumăm răspunderea pentru eventualele neînțelegeri sau interpretări greșite care pot apărea în urma utilizării acestei traduceri.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:48:41+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "ro"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}