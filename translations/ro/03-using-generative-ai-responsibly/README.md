<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "13084c6321a2092841b9a081b29497ba",
  "translation_date": "2025-05-19T14:50:21+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "ro"
}
-->
# Utilizarea AI Generativ 칥n Mod Responsabil

> _Apas캒 pe imaginea de mai sus pentru a viziona videoclipul acestei lec탵ii_

Este u탳or s캒 fii fascinat de AI 탳i, 칥n special, de AI generativ, dar trebuie s캒 iei 칥n considerare cum s캒-l utilizezi 칥n mod responsabil. Trebuie s캒 te g칙nde탳ti la lucruri precum cum s캒 te asiguri c캒 rezultatele sunt corecte, non-d캒un캒toare 탳i multe altele. Acest capitol 칥탳i propune s캒 칥탵i ofere contextul men탵ionat, ce s캒 iei 칥n considerare 탳i cum s캒 faci pa탳i activi pentru a 칥mbun캒t캒탵i utilizarea AI.

## Introducere

Aceast캒 lec탵ie va acoperi:

- De ce ar trebui s캒 prioritizezi AI Responsabil atunci c칙nd construie탳ti aplica탵ii de AI Generativ.
- Principiile de baz캒 ale AI Responsabil 탳i cum se raporteaz캒 la AI Generativ.
- Cum s캒 aplici aceste principii ale AI Responsabil prin strategie 탳i instrumente.

## Obiective de 칉nv캒탵are

Dup캒 ce vei finaliza aceast캒 lec탵ie vei 탳ti:

- Importan탵a AI Responsabil atunci c칙nd construie탳ti aplica탵ii de AI Generativ.
- C칙nd s캒 g칙nde탳ti 탳i s캒 aplici principiile de baz캒 ale AI Responsabil atunci c칙nd construie탳ti aplica탵ii de AI Generativ.
- Ce instrumente 탳i strategii sunt disponibile pentru a pune 칥n practic캒 conceptul de AI Responsabil.

## Principiile AI Responsabil

Entuziasmul pentru AI Generativ nu a fost niciodat캒 mai mare. Acest entuziasm a adus mul탵i dezvoltatori noi, aten탵ie 탳i finan탵are 칥n acest domeniu. De탳i acest lucru este foarte pozitiv pentru oricine dore탳te s캒 construiasc캒 produse 탳i companii folosind AI Generativ, este, de asemenea, important s캒 proced캒m responsabil.

Pe parcursul acestui curs, ne concentr캒m pe construirea startup-ului nostru 탳i a produsului nostru educa탵ional AI. Vom folosi principiile AI Responsabil: Echitate, Incluziune, Fiabilitate/Siguran탵캒, Securitate 탳i Confiden탵ialitate, Transparen탵캒 탳i Responsabilitate. Cu aceste principii, vom explora cum se raporteaz캒 la utilizarea AI Generativ 칥n produsele noastre.

## De ce ar trebui s캒 prioritizezi AI Responsabil

C칙nd construie탳ti un produs, adopt칙nd o abordare centrat캒 pe oameni 탳i av칙nd 칥n vedere interesul utilizatorului duce la cele mai bune rezultate.

Unicitatea AI Generativ const캒 칥n puterea sa de a crea r캒spunsuri utile, informa탵ii, ghiduri 탳i con탵inut pentru utilizatori. Acest lucru poate fi realizat f캒r캒 multe etape manuale, ceea ce poate duce la rezultate foarte impresionante. F캒r캒 planificare 탳i strategii adecvate, poate, din p캒cate, duce la rezultate d캒un캒toare pentru utilizatorii t캒i, produsul t캒u 탳i societatea 칥n ansamblu.

S캒 ne uit캒m la c칙teva (dar nu toate) dintre aceste rezultate poten탵ial d캒un캒toare:

### Halucina탵ii

Halucina탵iile sunt un termen folosit pentru a descrie c칙nd un LLM produce con탵inut care este fie complet nonsensical, fie ceva despre care 탳tim c캒 este gre탳it factual pe baza altor surse de informa탵ii.

S캒 lu캒m, de exemplu, construim o func탵ie pentru startup-ul nostru care permite studen탵ilor s캒 pun캒 칥ntreb캒ri istorice unui model. Un student pune 칥ntrebarea `Who was the sole survivor of Titanic?`

Modelul produce un r캒spuns precum cel de mai jos:

Aceasta este un r캒spuns foarte 칥ncrez캒tor 탳i detaliat. Din p캒cate, este incorect. Chiar 탳i cu o cantitate minim캒 de cercetare, cineva ar descoperi c캒 au fost mai mul탵i supravie탵uitori ai dezastrului Titanic. Pentru un student care abia 칥ncepe s캒 cerceteze acest subiect, acest r캒spuns poate fi suficient de conving캒tor pentru a nu fi pus la 칥ndoial캒 탳i tratat ca un fapt. Consecin탵ele acestui lucru pot duce la sistemul AI fiind nesigur 탳i pot afecta negativ reputa탵ia startup-ului nostru.

Cu fiecare itera탵ie a unui LLM dat, am observat 칥mbun캒t캒탵iri ale performan탵ei 칥n minimizarea halucina탵iilor. Chiar 탳i cu aceast캒 칥mbun캒t캒탵ire, noi, ca dezvoltatori de aplica탵ii 탳i utilizatori, trebuie s캒 r캒m칙nem con탳tien탵i de aceste limit캒ri.

### Con탵inut D캒un캒tor

Am acoperit 칥n sec탵iunea anterioar캒 c칙nd un LLM produce r캒spunsuri incorecte sau nonsensicale. Un alt risc de care trebuie s캒 fim con탳tien탵i este c칙nd un model r캒spunde cu con탵inut d캒un캒tor.

Con탵inutul d캒un캒tor poate fi definit ca:

- Oferirea de instruc탵iuni sau 칥ncurajarea auto-v캒t캒m캒rii sau v캒t캒m캒rii anumitor grupuri.
- Con탵inutul ur칙t sau denigrator.
- Ghidarea planific캒rii oric캒rui tip de atac sau acte violente.
- Oferirea de instruc탵iuni despre cum s캒 g캒se탳ti con탵inut ilegal sau s캒 comi탵i acte ilegale.
- Afi탳area con탵inutului sexual explicit.

Pentru startup-ul nostru, dorim s캒 ne asigur캒m c캒 avem instrumentele 탳i strategiile potrivite pentru a preveni ca acest tip de con탵inut s캒 fie v캒zut de studen탵i.

### Lipsa Echit캒탵ii

Echitatea este definit캒 ca "asigurarea c캒 un sistem AI este liber de prejudec캒탵i 탳i discriminare 탳i c캒 trateaz캒 pe toat캒 lumea corect 탳i egal." 칉n lumea AI Generativ, dorim s캒 ne asigur캒m c캒 viziunile excluzionare ale grupurilor marginalizate nu sunt 칥nt캒rite de rezultatele modelului.

Aceste tipuri de rezultate nu sunt doar distructive pentru construirea de experien탵e de produs pozitive pentru utilizatorii no탳tri, dar cauzeaz캒 탳i un prejudiciu social suplimentar. Ca dezvoltatori de aplica탵ii, ar trebui s캒 avem 칥ntotdeauna 칥n vedere o baz캒 larg캒 탳i divers캒 de utilizatori atunci c칙nd construim solu탵ii cu AI Generativ.

## Cum s캒 folose탳ti AI Generativ 칥n mod Responsabil

Acum c캒 am identificat importan탵a AI Generativ Responsabil, s캒 ne uit캒m la 4 pa탳i pe care 칥i putem face pentru a construi solu탵iile noastre AI 칥n mod responsabil:

### M캒surarea Poten탵ialelor Daune

칉n testarea software-ului, test캒m ac탵iunile a탳teptate ale unui utilizator asupra unei aplica탵ii. 칉n mod similar, testarea unui set divers de solicit캒ri pe care utilizatorii sunt cel mai probabil s캒 le foloseasc캒 este o modalitate bun캒 de a m캒sura daunele poten탵iale.

Deoarece startup-ul nostru construie탳te un produs educa탵ional, ar fi bine s캒 preg캒tim o list캒 de solicit캒ri legate de educa탵ie. Acest lucru ar putea fi pentru a acoperi un anumit subiect, fapte istorice 탳i solicit캒ri despre via탵a de student.

### Atenuarea Poten탵ialelor Daune

Acum este timpul s캒 g캒sim modalit캒탵i prin care putem preveni sau limita daunele poten탵iale cauzate de model 탳i de r캒spunsurile acestuia. Putem privi acest lucru 칥n 4 straturi diferite:

- **Model**. Alegerea modelului potrivit pentru cazul de utilizare potrivit. Modelele mai mari 탳i mai complexe, cum ar fi GPT-4, pot cauza un risc mai mare de con탵inut d캒un캒tor atunci c칙nd sunt aplicate la cazuri de utilizare mai mici 탳i mai specifice. Utilizarea datelor tale de instruire pentru a ajusta fin reduce, de asemenea, riscul de con탵inut d캒un캒tor.

- **Sistem de Siguran탵캒**. Un sistem de siguran탵캒 este un set de instrumente 탳i configura탵ii pe platforma care serve탳te modelul 탳i care ajut캒 la atenuarea daunelor. Un exemplu 칥n acest sens este sistemul de filtrare a con탵inutului de pe serviciul Azure OpenAI. Sistemele ar trebui s캒 detecteze, de asemenea, atacurile de jailbreak 탳i activit캒탵ile nedorite, cum ar fi solicit캒rile de la robo탵i.

- **Metaprompt**. Metapromptele 탳i ancorarea sunt modalit캒탵i prin care putem direc탵iona sau limita modelul pe baza anumitor comportamente 탳i informa탵ii. Acest lucru ar putea fi folosirea intr캒rilor sistemului pentru a defini anumite limite ale modelului. 칉n plus, oferirea de rezultate care sunt mai relevante pentru domeniul sau domeniul sistemului.

De asemenea, poate fi utilizarea tehnicilor precum Generarea Augmentat캒 de Recuperare (RAG) pentru a face ca modelul s캒 extrag캒 informa탵ii doar dintr-o selec탵ie de surse de 칥ncredere. Exist캒 o lec탵ie mai t칙rziu 칥n acest curs pentru [construirea aplica탵iilor de c캒utare](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Experien탵a Utilizatorului**. Ultimul strat este locul unde utilizatorul interac탵ioneaz캒 direct cu modelul prin intermediul interfe탵ei aplica탵iei noastre 칥ntr-un fel. 칉n acest mod, putem proiecta UI/UX pentru a limita utilizatorul 칥n tipurile de intr캒ri pe care le pot trimite modelului, precum 탳i textul sau imaginile afi탳ate utilizatorului. C칙nd implement캒m aplica탵ia AI, trebuie s캒 fim, de asemenea, transparen탵i cu privire la ce poate 탳i ce nu poate face aplica탵ia noastr캒 AI Generativ.

Avem o lec탵ie 칥ntreag캒 dedicat캒 [Proiect캒rii UX pentru Aplica탵ii AI](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Evaluarea modelului**. Lucrul cu LLM-urile poate fi provocator deoarece nu avem 칥ntotdeauna control asupra datelor pe care modelul a fost instruit. Indiferent, ar trebui s캒 evalu캒m 칥ntotdeauna performan탵a 탳i rezultatele modelului. Este important s캒 m캒sur캒m acurate탵ea, similaritatea, ancorarea 탳i relevan탵a rezultatului modelului. Acest lucru ajut캒 la oferirea de transparen탵캒 탳i 칥ncredere pentru p캒r탵ile interesate 탳i utilizatori.

### Operarea unei solu탵ii AI Generativ Responsabile

Construirea unei practici opera탵ionale 칥n jurul aplica탵iilor tale AI este etapa final캒. Aceasta include colaborarea cu alte p캒r탵i ale startup-ului nostru, cum ar fi Legal 탳i Securitate, pentru a ne asigura c캒 suntem conformi cu toate politicile de reglementare. 칉nainte de lansare, dorim, de asemenea, s캒 construim planuri 칥n jurul livr캒rii, gestion캒rii incidentelor 탳i retragerii pentru a preveni orice daune pentru utilizatorii no탳tri de la cre탳tere.

## Instrumente

De탳i munca de dezvoltare a solu탵iilor AI Responsabile poate p캒rea mult캒, este o munc캒 care merit캒 efortul. Pe m캒sur캒 ce domeniul AI Generativ cre탳te, mai multe instrumente pentru a ajuta dezvoltatorii s캒 integreze eficient responsabilitatea 칥n fluxurile lor de lucru vor evolua. De exemplu, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) poate ajuta la detectarea con탵inutului 탳i imaginilor d캒un캒toare printr-o cerere API.

## Verificare a Cuno탳tin탵elor

Care sunt unele lucruri de care trebuie s캒 te 칥ngrije탳ti pentru a asigura utilizarea responsabil캒 a AI?

1. Ca r캒spunsul s캒 fie corect.
1. Utilizarea d캒un캒toare, ca AI s캒 nu fie folosit pentru scopuri criminale.
1. Asigurarea c캒 AI este liber de prejudec캒탵i 탳i discriminare.

A: 2 탳i 3 sunt corecte. AI Responsabil te ajut캒 s캒 consideri cum s캒 atenuezi efectele d캒un캒toare 탳i prejudec캒탵ile 탳i multe altele.

## 游 Provocare

Cite탳te despre [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) 탳i vezi ce po탵i adopta pentru utilizarea ta.

## Felicit캒ri, Continu캒 칉nv캒탵area Ta

Dup캒 ce ai finalizat aceast캒 lec탵ie, verific캒 colec탵ia noastr캒 de [칉nv캒탵are AI Generativ캒](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) pentru a continua s캒 칥탵i 칥mbun캒t캒탵e탳ti cuno탳tin탵ele despre AI Generativ!

Mergi la Lec탵ia 4 unde vom privi [Fundamentele Ingineriei Solicit캒rilor](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!

**Declinarea responsabilit캒탵ii**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). De탳i ne str캒duim s캒 asigur캒m acurate탵ea, v캒 rug캒m s캒 fi탵i con탳tien탵i de faptul c캒 traducerile automate pot con탵ine erori sau inexactit캒탵i. Documentul original 칥n limba sa matern캒 ar trebui considerat sursa autoritar캒. Pentru informa탵ii critice, se recomand캒 traducerea uman캒 profesional캒. Nu suntem responsabili pentru eventualele ne칥n탵elegeri sau interpret캒ri gre탳ite care pot ap캒rea din utilizarea acestei traduceri.