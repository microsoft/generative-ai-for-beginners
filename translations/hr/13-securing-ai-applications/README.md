<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-18T01:30:34+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "hr"
}
-->
# Osiguranje va코ih generativnih AI aplikacija

[![Osiguranje va코ih generativnih AI aplikacija](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.hr.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## Uvod

Ova lekcija obuhva캖a:

- Sigurnost u kontekstu AI sustava.
- Uobi캜ajene rizike i prijetnje za AI sustave.
- Metode i razmatranja za osiguranje AI sustava.

## Ciljevi u캜enja

Nakon zavr코etka ove lekcije, razumjet 캖ete:

- Prijetnje i rizike za AI sustave.
- Uobi캜ajene metode i prakse za osiguranje AI sustava.
- Kako implementacija sigurnosnog testiranja mo쬰 sprije캜iti neo캜ekivane rezultate i gubitak povjerenja korisnika.

## 맚o zna캜i sigurnost u kontekstu generativne umjetne inteligencije?

Kako tehnologije umjetne inteligencije (AI) i strojnog u캜enja (ML) sve vi코e oblikuju na코e 쬴vote, klju캜no je za코tititi ne samo podatke korisnika, ve캖 i same AI sustave. AI/ML se sve vi코e koristi u podr코ci dono코enja odluka od velike va쬹osti u industrijama gdje pogre코na odluka mo쬰 imati ozbiljne posljedice.

Evo klju캜nih to캜aka koje treba uzeti u obzir:

- **Utjecaj AI/ML-a**: AI/ML imaju zna캜ajan utjecaj na svakodnevni 쬴vot, zbog 캜ega je njihova za코tita postala neophodna.
- **Izazovi sigurnosti**: Zbog tog utjecaja, potrebno je posvetiti odgovaraju캖u pa쬹ju za코titi AI proizvoda od sofisticiranih napada, bilo od strane trolova ili organiziranih grupa.
- **Strate코ki problemi**: Tehnolo코ka industrija mora proaktivno rje코avati strate코ke izazove kako bi osigurala dugoro캜nu sigurnost korisnika i za코titu podataka.

Osim toga, modeli strojnog u캜enja uglavnom nisu sposobni razlikovati zlonamjerne ulaze od bezopasnih anomalnih podataka. Zna캜ajan izvor podataka za treniranje dolazi iz neure캠enih, nemoderiranih javnih skupova podataka, koji su otvoreni za doprinose tre캖ih strana. Napada캜i ne moraju kompromitirati skupove podataka kada im je dopu코teno da u njih slobodno doprinose. S vremenom, podaci niske pouzdanosti postaju podaci visoke pouzdanosti ako njihova struktura/format ostane ispravan.

Zbog toga je klju캜no osigurati integritet i za코titu spremi코ta podataka koje va코i modeli koriste za dono코enje odluka.

## Razumijevanje prijetnji i rizika za AI

U kontekstu AI sustava, trovanje podataka isti캜e se kao najzna캜ajnija sigurnosna prijetnja danas. Trovanje podataka doga캠a se kada netko namjerno mijenja informacije koje se koriste za treniranje AI-a, uzrokuju캖i da sustav pravi gre코ke. To je posljedica nedostatka standardiziranih metoda za otkrivanje i ubla쬬vanje, u kombinaciji s oslanjanjem na nepouzdane ili neure캠ene javne skupove podataka za treniranje. Kako bi se odr쬬o integritet podataka i sprije캜io pogre코an proces treniranja, klju캜no je pratiti podrijetlo i porijeklo va코ih podataka. Ina캜e, stara izreka "sme캖e unutra, sme캖e van" vrijedi, 코to dovodi do kompromitiranih performansi modela.

Evo primjera kako trovanje podataka mo쬰 utjecati na va코e modele:

1. **Preokretanje oznaka**: Kod zadatka binarne klasifikacije, napada캜 namjerno preokre캖e oznake malog dijela podataka za treniranje. Na primjer, bezopasni uzorci ozna캜eni su kao zlonamjerni, 코to dovodi do toga da model u캜i pogre코ne asocijacije.\
   **Primjer**: Filter za ne쬰ljenu po코tu pogre코no klasificira legitimne e-mailove kao ne쬰ljene zbog manipuliranih oznaka.
2. **Trovanje zna캜ajki**: Napada캜 suptilno mijenja zna캜ajke u podacima za treniranje kako bi uveo pristranost ili zaveo model.\
   **Primjer**: Dodavanje neva쬹ih klju캜nih rije캜i opisima proizvoda kako bi se manipuliralo sustavima preporuka.
3. **Umetanje podataka**: Umetanje zlonamjernih podataka u skup za treniranje kako bi se utjecalo na pona코anje modela.\
   **Primjer**: Uvo캠enje la쬹ih korisni캜kih recenzija kako bi se iskrivili rezultati analize sentimenta.
4. **Napadi stra쬹jih vrata**: Napada캜 ubacuje skriveni uzorak (stra쬹ja vrata) u podatke za treniranje. Model u캜i prepoznati taj uzorak i pona코a se zlonamjerno kada se aktivira.\
   **Primjer**: Sustav prepoznavanja lica treniran s slikama koje sadr쬰 stra쬹ja vrata, 코to dovodi do pogre코nog prepoznavanja odre캠ene osobe.

MITRE Corporation je kreirao [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), bazu znanja o taktikama i tehnikama koje koriste napada캜i u stvarnim napadima na AI sustave.

> Postoji sve ve캖i broj ranjivosti u sustavima koji koriste AI, jer integracija AI-a pove캖ava povr코inu napada postoje캖ih sustava izvan tradicionalnih cyber-napada. Razvili smo ATLAS kako bismo podigli svijest o ovim jedinstvenim i evoluiraju캖im ranjivostima, dok globalna zajednica sve vi코e uklju캜uje AI u razli캜ite sustave. ATLAS je modeliran prema MITRE ATT&CK춽 okviru, a njegove taktike, tehnike i procedure (TTPs) komplementarne su onima u ATT&CK-u.

Sli캜no MITRE ATT&CK춽 okviru, koji se opse쬹o koristi u tradicionalnoj kiberneti캜koj sigurnosti za planiranje scenarija napredne emulacije prijetnji, ATLAS pru쬬 lako pretra쬴vi skup TTP-ova koji mogu pomo캖i u boljem razumijevanju i pripremi za obranu od novih napada.

Osim toga, Open Web Application Security Project (OWASP) je kreirao "[Top 10 listu](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" najkriti캜nijih ranjivosti prona캠enih u aplikacijama koje koriste LLM-ove. Lista isti캜e rizike prijetnji poput spomenutog trovanja podataka, kao i drugih poput:

- **Umetanje upita**: tehnika u kojoj napada캜i manipuliraju velikim jezi캜nim modelom (LLM) pa쬷jivo osmi코ljenim unosima, uzrokuju캖i da se pona코a izvan svoje namjeravane funkcije.
- **Ranjivosti u lancu opskrbe**: Komponente i softver koji 캜ine aplikacije koje koristi LLM, poput Python modula ili vanjskih skupova podataka, mogu biti kompromitirani, 코to dovodi do neo캜ekivanih rezultata, uvedenih pristranosti pa 캜ak i ranjivosti u osnovnoj infrastrukturi.
- **Pretjerano oslanjanje**: LLM-ovi su podlo쬹i gre코kama i skloni su "halucinacijama", pru쬬ju캖i neto캜ne ili nesigurne rezultate. U nekoliko dokumentiranih slu캜ajeva, ljudi su prihvatili rezultate zdravo za gotovo, 코to je dovelo do ne쬰ljenih negativnih posljedica u stvarnom svijetu.

Microsoft Cloud Advocate Rod Trent napisao je besplatnu e-knjigu, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), koja detaljno istra쬿je ove i druge nove prijetnje AI-u te pru쬬 opse쬹e smjernice o tome kako najbolje rije코iti ove scenarije.

## Sigurnosno testiranje za AI sustave i LLM-ove

Umjetna inteligencija (AI) transformira razli캜ite domene i industrije, nude캖i nove mogu캖nosti i koristi za dru코tvo. Me캠utim, AI tako캠er predstavlja zna캜ajne izazove i rizike, poput privatnosti podataka, pristranosti, nedostatka obja코njivosti i potencijalne zloupotrebe. Stoga je klju캜no osigurati da AI sustavi budu sigurni i odgovorni, 코to zna캜i da se pridr쬬vaju eti캜kih i pravnih standarda te da im korisnici i dionici mogu vjerovati.

Sigurnosno testiranje je proces procjene sigurnosti AI sustava ili LLM-a, identificiranjem i iskori코tavanjem njihovih ranjivosti. To mogu provoditi programeri, korisnici ili neovisni revizori, ovisno o svrsi i opsegu testiranja. Neke od naj캜e코캖ih metoda sigurnosnog testiranja za AI sustave i LLM-ove su:

- **Sanitacija podataka**: Proces uklanjanja ili anonimiziranja osjetljivih ili privatnih informacija iz podataka za treniranje ili unosa AI sustava ili LLM-a. Sanitacija podataka mo쬰 pomo캖i u sprje캜avanju curenja podataka i zlonamjerne manipulacije smanjenjem izlo쬰nosti povjerljivih ili osobnih podataka.
- **Adversarijalno testiranje**: Proces generiranja i primjene adversarijalnih primjera na unos ili izlaz AI sustava ili LLM-a kako bi se procijenila njihova otpornost na adversarijalne napade. Adversarijalno testiranje mo쬰 pomo캖i u identifikaciji i ubla쬬vanju ranjivosti i slabosti AI sustava ili LLM-a koje napada캜i mogu iskoristiti.
- **Verifikacija modela**: Proces provjere ispravnosti i cjelovitosti parametara ili arhitekture modela AI sustava ili LLM-a. Verifikacija modela mo쬰 pomo캖i u otkrivanju i sprje캜avanju kra캠e modela osiguravanjem da je model za코ti캖en i autentificiran.
- **Validacija izlaza**: Proces validacije kvalitete i pouzdanosti izlaza AI sustava ili LLM-a. Validacija izlaza mo쬰 pomo캖i u otkrivanju i ispravljanju zlonamjerne manipulacije osiguravanjem da je izlaz dosljedan i to캜an.

OpenAI, lider u AI sustavima, postavio je niz _evaluacija sigurnosti_ kao dio svoje inicijative za red teaming mre쬿, s ciljem testiranja izlaza AI sustava u nadi da 캖e doprinijeti sigurnosti AI-a.

> Evaluacije mogu varirati od jednostavnih Q&A testova do slo쬰nijih simulacija. Kao konkretni primjeri, evo uzoraka evaluacija koje je razvio OpenAI za procjenu pona코anja AI-a iz razli캜itih perspektiva:

#### Uvjeravanje

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav mo쬰 navesti drugi AI sustav da izgovori tajnu rije캜?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav mo쬰 uvjeriti drugi AI sustav da donira novac?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav mo쬰 utjecati na podr코ku drugog AI sustava za politi캜ki prijedlog?

#### Steganografija (skriveno slanje poruka)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav mo쬰 prenijeti tajne poruke bez da ga drugi AI sustav otkrije?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav mo쬰 komprimirati i dekomprimirati poruke kako bi omogu캖io skrivanje tajnih poruka?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav mo쬰 koordinirati s drugim AI sustavom bez izravne komunikacije?

### Sigurnost AI-a

Klju캜no je za코tititi AI sustave od zlonamjernih napada, zloupotrebe ili ne쬰ljenih posljedica. To uklju캜uje poduzimanje koraka za osiguranje sigurnosti, pouzdanosti i povjerenja u AI sustave, kao 코to su:

- Osiguranje podataka i algoritama koji se koriste za treniranje i rad AI modela
- Sprje캜avanje neovla코tenog pristupa, manipulacije ili sabota쬰 AI sustava
- Otkrivanje i ubla쬬vanje pristranosti, diskriminacije ili eti캜kih problema u AI sustavima
- Osiguranje odgovornosti, transparentnosti i obja코njivosti AI odluka i radnji
- Uskla캠ivanje ciljeva i vrijednosti AI sustava s onima ljudi i dru코tva

Sigurnost AI-a va쬹a je za osiguranje integriteta, dostupnosti i povjerljivosti AI sustava i podataka. Neki od izazova i prilika sigurnosti AI-a su:

- Prilika: Uklju캜ivanje AI-a u strategije kiberneti캜ke sigurnosti jer mo쬰 igrati klju캜nu ulogu u identificiranju prijetnji i pobolj코anju vremena reakcije. AI mo쬰 pomo캖i u automatizaciji i pobolj코anju otkrivanja i ubla쬬vanja kiberneti캜kih napada, poput phishinga, zlonamjernog softvera ili ransomwarea.
- Izazov: AI tako캠er mo쬰 biti kori코ten od strane napada캜a za pokretanje sofisticiranih napada, poput generiranja la쬹og ili obmanjuju캖eg sadr쬬ja, imitacije korisnika ili iskori코tavanja ranjivosti u AI sustavima. Stoga, AI programeri imaju jedinstvenu odgovornost dizajnirati sustave koji su robusni i otporni na zloupotrebu.

### Za코tita podataka

LLM-ovi mogu predstavljati rizik za privatnost i sigurnost podataka koje koriste. Na primjer, LLM-ovi mogu potencijalno zapamtiti i otkriti osjetljive informacije iz svojih podataka za treniranje, poput osobnih imena, adresa, lozinki ili brojeva kreditnih kartica. Tako캠er mogu biti manipulirani ili napadnuti od strane zlonamjernih aktera koji 쬰le iskoristiti njihove ranjivosti ili pristranosti. Stoga je va쬹o biti svjestan ovih rizika i poduzeti odgovaraju캖e mjere za za코titu podataka koji se koriste s LLM-ovima. Postoji nekoliko koraka koje mo쬰te poduzeti za za코titu podataka koji se koriste s LLM-ovima. Ti koraci uklju캜uju:

- **Ograni캜avanje koli캜ine i vrste podataka koje dijelite s LLM-ovima**: Dijelite samo podatke koji su nu쬹i i relevantni za predvi캠ene svrhe, i izbjegavajte dijeljenje bilo kakvih podataka koji su osjetljivi, povjerljivi ili osobni. Korisnici tako캠er trebaju anonimizirati ili 코ifrirati podatke koje dijele s LLM-ovima, poput uklanjanja ili maskiranja bilo kakvih identifikacijskih informacija, ili kori코tenja sigurnih komunikacijskih kanala.
- **Provjeravanje podataka koje LLM-ovi generiraju**: Uvijek provjerite to캜nost i kvalitetu izlaza koji generiraju LLM-ovi kako biste osigurali da ne sadr쬰 ne쬰ljene ili neprikladne informacije.
- **Prijavljivanje i upozoravanje na bilo kakve povrede podataka ili incidente**: Budite oprezni prema bilo kakvim sumnjivim ili abnormalnim aktivnostima ili pona코anjima LLM-ova, poput generiranja tekstova koji su irelevantni, neto캜ni, uvredljivi ili 코tetni. To bi moglo biti indikacija povrede podataka ili sigurnosnog incidenta.

Sigurnost podataka, upravljanje i uskla캠enost klju캜ni su za svaku organizaciju koja 쬰li iskoristiti mo캖 podataka i AI-a u multi-cloud okru쬰nju. Osiguranje i upravljanje svim va코im podacima slo쬰n je i vi코estran zadatak. Potrebno je osigurati i upravljati razli캜itim vrstama podataka (strukturirani, nestrukturirani i podaci generirani AI-om) na razli캜itim lokacijama u vi코e oblaka, te uzeti u obzir postoje캖e i budu캖e propise o sigurnosti podataka, upravljanju i AI-u. Kako biste za코titili svoje podatke, trebate usvojiti neke najbolje prakse i mjere opreza, poput:

- Koristite cloud usluge ili platforme koje nude zna캜ajke za코tite podataka i privatnosti.
- Koristite alate za kvalitetu i validaciju podataka kako biste provjerili svoje podatke na pogre코ke, nedosljednosti ili anomalije.
- Koristite okvire za upravljanje podacima i etiku kako biste osigurali da se va코i podaci koriste na odgovoran i transparentan na캜in.

### Emulacija prijetnji iz stvarnog svijeta - AI
Opona코anje prijetnji iz stvarnog svijeta sada se smatra standardnom praksom u izgradnji otpornijih AI sustava kori코tenjem sli캜nih alata, taktika i procedura za identifikaciju rizika za sustave i testiranje odgovora branitelja.

> Praksa AI red teaminga evoluirala je i dobila 코ire zna캜enje: ne pokriva samo ispitivanje sigurnosnih ranjivosti, ve캖 uklju캜uje i ispitivanje drugih neuspjeha sustava, poput generiranja potencijalno 코tetnog sadr쬬ja. AI sustavi donose nove rizike, a red teaming je klju캜an za razumijevanje tih novih rizika, poput ubrizgavanja upita i stvaranja neosnovanog sadr쬬ja. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Smjernice i resursi za red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.hr.png)]()

Ispod su klju캜ni uvidi koji su oblikovali Microsoftov AI Red Team program.

1. **말rok opseg AI Red Teaminga:**
   AI red teaming sada obuhva캖a i sigurnosne i odgovorne AI (RAI) ishode. Tradicionalno, red teaming se fokusirao na sigurnosne aspekte, tretiraju캖i model kao vektor (npr. kra캠a osnovnog modela). Me캠utim, AI sustavi uvode nove sigurnosne ranjivosti (npr. ubrizgavanje upita, trovanje), 코to zahtijeva posebnu pa쬹ju. Osim sigurnosti, AI red teaming tako캠er ispituje pitanja pravednosti (npr. stereotipiziranje) i 코tetnog sadr쬬ja (npr. veli캜anje nasilja). Rano prepoznavanje ovih problema omogu캖uje prioritizaciju ulaganja u obranu.
2. **Zlonamjerni i bezazleni neuspjesi:**
   AI red teaming razmatra neuspjehe iz perspektive zlonamjernih i bezazlenih scenarija. Na primjer, prilikom red teaminga novog Binga, istra쬿jemo ne samo kako zlonamjerni protivnici mogu potkopati sustav, ve캖 i kako obi캜ni korisnici mogu nai캖i na problemati캜an ili 코tetan sadr쬬j. Za razliku od tradicionalnog sigurnosnog red teaminga, koji se uglavnom fokusira na zlonamjerne aktere, AI red teaming uzima u obzir 코iri raspon persona i potencijalnih neuspjeha.
3. **Dinami캜na priroda AI sustava:**
   AI aplikacije neprestano evoluiraju. U aplikacijama velikih jezi캜nih modela, programeri se prilago캠avaju promjenjivim zahtjevima. Kontinuirani red teaming osigurava stalnu budnost i prilagodbu evoluiraju캖im rizicima.

AI red teaming nije sveobuhvatan i treba ga smatrati dopunom dodatnim kontrolama poput [kontrole pristupa temeljenog na ulogama (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) i sveobuhvatnih rje코enja za upravljanje podacima. Namijenjen je dopuni sigurnosne strategije koja se fokusira na primjenu sigurnih i odgovornih AI rje코enja koja uzimaju u obzir privatnost i sigurnost, dok nastoje minimizirati pristranosti, 코tetni sadr쬬j i dezinformacije koje mogu naru코iti povjerenje korisnika.

Evo popisa dodatnih materijala za 캜itanje koji vam mogu pomo캖i da bolje razumijete kako red teaming mo쬰 pomo캖i u identifikaciji i ubla쬬vanju rizika u va코im AI sustavima:

- [Planiranje red teaminga za velike jezi캜ne modele (LLM) i njihove aplikacije](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [맚o je OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - Klju캜na praksa za izgradnju sigurnijih i odgovornijih AI rje코enja](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), baza znanja o taktikama i tehnikama koje koriste protivnici u stvarnim napadima na AI sustave.

## Provjera znanja

Koji bi mogao biti dobar pristup za odr쬬vanje integriteta podataka i sprje캜avanje zloupotrebe?

1. Imati sna쬹e kontrole pristupa temeljenog na ulogama za upravljanje podacima
1. Provoditi i provjeravati ozna캜avanje podataka kako bi se sprije캜ilo pogre코no predstavljanje ili zloupotreba podataka
1. Osigurati da va코a AI infrastruktura podr쬬va filtriranje sadr쬬ja

A:1, Iako su sve tri preporuke odli캜ne, osiguravanje pravilnih privilegija pristupa podacima korisnicima zna캜ajno 캖e doprinijeti sprje캜avanju manipulacije i pogre코nog predstavljanja podataka koje koriste LLM-ovi.

## 游 Izazov

Pro캜itajte vi코e o tome kako mo쬰te [upravljati i 코tititi osjetljive informacije](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) u doba AI-a.

## Sjajan posao, nastavite u캜iti

Nakon 코to zavr코ite ovu lekciju, pogledajte na코u [Generative AI Learning kolekciju](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) kako biste nastavili unapre캠ivati svoje znanje o generativnom AI-u!

Prije캠ite na Lekciju 14 gdje 캖emo pogledati [콯ivotni ciklus aplikacija generativnog AI-a](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

---

**Izjava o odricanju odgovornosti**:  
Ovaj dokument je preveden pomo캖u AI usluge za prevo캠enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati to캜nost, imajte na umu da automatski prijevodi mogu sadr쬬vati pogre코ke ili neto캜nosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za klju캜ne informacije preporu캜uje se profesionalni prijevod od strane 캜ovjeka. Ne preuzimamo odgovornost za nesporazume ili pogre코na tuma캜enja koja proizlaze iz kori코tenja ovog prijevoda.