<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:41:52+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "hr"
}
-->
# Osiguravanje vaÅ¡ih generativnih AI aplikacija

[![Osiguravanje vaÅ¡ih generativnih AI aplikacija](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.hr.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Uvod

Ova lekcija obuhvaÄ‡a:

- Sigurnost u kontekstu AI sustava.
- UobiÄajene rizike i prijetnje AI sustavima.
- Metode i razmatranja za osiguranje AI sustava.

## Ciljevi uÄenja

Nakon zavrÅ¡etka ove lekcije, imat Ä‡ete razumijevanje o:

- Prijetnjama i rizicima za AI sustave.
- UobiÄajenim metodama i praksama za osiguranje AI sustava.
- Kako implementacija sigurnosnog testiranja moÅ¾e sprijeÄiti neoÄekivane rezultate i gubitak povjerenja korisnika.

## Å to sigurnost znaÄi u kontekstu generativne AI?

Kako tehnologije umjetne inteligencije (AI) i strojnog uÄenja (ML) sve viÅ¡e oblikuju naÅ¡e Å¾ivote, kljuÄno je zaÅ¡tititi ne samo podatke korisnika, veÄ‡ i same AI sustave. AI/ML se sve ÄeÅ¡Ä‡e koristi u podrÅ¡ci donoÅ¡enju vaÅ¾nih odluka u industrijama gdje pogreÅ¡na odluka moÅ¾e imati ozbiljne posljedice.

Evo kljuÄnih toÄaka za razmotriti:

- **Utjecaj AI/ML**: AI/ML imaju znaÄajan utjecaj na svakodnevni Å¾ivot, stoga je njihova zaÅ¡tita postala neophodna.
- **Izazovi sigurnosti**: Taj utjecaj zahtijeva posebnu paÅ¾nju kako bi se zaÅ¡titili AI proizvodi od sofisticiranih napada, bilo od trolova ili organiziranih skupina.
- **StrateÅ¡ki problemi**: TehnoloÅ¡ka industrija mora proaktivno rjeÅ¡avati strateÅ¡ke izazove kako bi osigurala dugoroÄnu sigurnost korisnika i zaÅ¡titu podataka.

Osim toga, modeli strojnog uÄenja uglavnom ne mogu razlikovati zlonamjerne unose od bezopasnih anomalija. ZnaÄajan dio podataka za treniranje dolazi iz nekuriranih, nemoderiranih javnih skupova podataka, otvorenih za doprinose treÄ‡ih strana. NapadaÄi ne moraju kompromitirati skupove podataka ako mogu slobodno doprinositi njima. S vremenom, podaci s niskim povjerenjem mogu postati podaci s visokim povjerenjem, ako struktura i format podataka ostanu ispravni.

Zato je kljuÄno osigurati integritet i zaÅ¡titu skladiÅ¡ta podataka koje vaÅ¡i modeli koriste za donoÅ¡enje odluka.

## Razumijevanje prijetnji i rizika AI-a

Kad govorimo o AI i povezanim sustavima, trovanje podataka je danas najznaÄajnija sigurnosna prijetnja. Trovanje podataka dogaÄ‘a se kada netko namjerno mijenja informacije koje se koriste za treniranje AI-a, uzrokujuÄ‡i pogreÅ¡ke. To je zbog nedostatka standardiziranih metoda za otkrivanje i ublaÅ¾avanje, kao i oslanjanja na nepouzdane ili nekurirane javne skupove podataka za treniranje. Kako biste odrÅ¾ali integritet podataka i sprijeÄili pogreÅ¡an proces treniranja, kljuÄno je pratiti podrijetlo i povijest podataka. InaÄe vrijedi stara izreka â€smeÄ‡e unutra, smeÄ‡e vanâ€œ, Å¡to dovodi do naruÅ¡ene izvedbe modela.

Evo primjera kako trovanje podataka moÅ¾e utjecati na vaÅ¡e modele:

1. **Preokretanje oznaka (Label Flipping)**: U zadatku binarne klasifikacije, napadaÄ namjerno mijenja oznake malog dijela podataka za treniranje. Na primjer, bezopasni uzorci se oznaÄavaju kao zlonamjerni, Å¡to model navodi na pogreÅ¡ne zakljuÄke.\
   **Primjer**: Spam filter koji pogreÅ¡no klasificira legitimne e-mailove kao spam zbog manipuliranih oznaka.
2. **Trovanje znaÄajki (Feature Poisoning)**: NapadaÄ suptilno mijenja znaÄajke u podacima za treniranje kako bi unio pristranost ili zavaravao model.\
   **Primjer**: Dodavanje irelevantnih kljuÄnih rijeÄi u opise proizvoda kako bi se manipuliralo sustavima preporuka.
3. **Ubacivanje podataka (Data Injection)**: Ubacivanje zlonamjernih podataka u skup za treniranje kako bi se utjecalo na ponaÅ¡anje modela.\
   **Primjer**: UvoÄ‘enje laÅ¾nih korisniÄkih recenzija za iskrivljenje rezultata analize sentimenta.
4. **Napadi s tajnim ulazom (Backdoor Attacks)**: NapadaÄ ubacuje skriveni uzorak (backdoor) u podatke za treniranje. Model uÄi prepoznati taj uzorak i ponaÅ¡a se zlonamjerno kad se aktivira.\
   **Primjer**: Sustav za prepoznavanje lica treniran s backdoor slikama koje pogreÅ¡no identificiraju odreÄ‘enu osobu.

MITRE Corporation je kreirala [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), bazu znanja o taktikama i tehnikama koje koriste napadaÄi u stvarnim napadima na AI sustave.

> Broj ranjivosti u sustavima s AI-jem raste, jer integracija AI-ja poveÄ‡ava povrÅ¡inu napada postojeÄ‡ih sustava izvan tradicionalnih cyber-napada. Razvili smo ATLAS kako bismo podigli svijest o ovim jedinstvenim i razvijajuÄ‡im ranjivostima, buduÄ‡i da globalna zajednica sve viÅ¡e ukljuÄuje AI u razliÄite sustave. ATLAS je modeliran prema MITRE ATT&CKÂ® okviru, a njegove taktike, tehnike i procedure (TTP) nadopunjuju one u ATT&CK.

SliÄno kao MITRE ATT&CKÂ® okvir, koji se Å¡iroko koristi u tradicionalnoj kibernetiÄkoj sigurnosti za planiranje scenarija napredne emulacije prijetnji, ATLAS pruÅ¾a lako pretraÅ¾ivi skup TTP-ova koji pomaÅ¾u bolje razumjeti i pripremiti se za obranu od novih napada.

TakoÄ‘er, Open Web Application Security Project (OWASP) je kreirao "[Top 10 listu](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" najkritiÄnijih ranjivosti u aplikacijama koje koriste LLM-ove. Lista istiÄe rizike prijetnji poput spomenutog trovanja podataka, kao i drugih poput:

- **Prompt Injection**: tehnika u kojoj napadaÄi manipuliraju velikim jeziÄnim modelom (LLM) paÅ¾ljivo osmiÅ¡ljenim unosima, uzrokujuÄ‡i da se ponaÅ¡a izvan predviÄ‘enog ponaÅ¡anja.
- **Ranjivosti u lancu opskrbe**: Komponente i softver koji Äine aplikacije koriÅ¡tene od strane LLM-a, poput Python modula ili vanjskih skupova podataka, mogu biti kompromitirani, Å¡to dovodi do neoÄekivanih rezultata, uvedenih pristranosti pa Äak i ranjivosti u osnovnoj infrastrukturi.
- **Preveliko oslanjanje**: LLM-ovi su podloÅ¾ni pogreÅ¡kama i skloni su halucinacijama, dajuÄ‡i netoÄne ili nesigurne rezultate. U nekoliko dokumentiranih sluÄajeva ljudi su uzimali rezultate zdravo za gotovo, Å¡to je dovelo do neÅ¾eljenih negativnih posljedica u stvarnom svijetu.

Microsoft Cloud Advocate Rod Trent napisao je besplatnu e-knjigu, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), koja detaljno obraÄ‘uje ove i druge nove AI prijetnje te pruÅ¾a opseÅ¾ne smjernice kako najbolje pristupiti tim scenarijima.

## Sigurnosno testiranje AI sustava i LLM-ova

Umjetna inteligencija (AI) mijenja razliÄite domene i industrije, nudeÄ‡i nove moguÄ‡nosti i koristi za druÅ¡tvo. MeÄ‘utim, AI donosi i znaÄajne izazove i rizike, poput privatnosti podataka, pristranosti, nedostatka objaÅ¡njivosti i potencijalne zloupotrebe. Stoga je kljuÄno osigurati da AI sustavi budu sigurni i odgovorni, Å¡to znaÄi da se pridrÅ¾avaju etiÄkih i zakonskih standarda te da im korisnici i dionici mogu vjerovati.

Sigurnosno testiranje je proces procjene sigurnosti AI sustava ili LLM-a, identificiranjem i iskoriÅ¡tavanjem njihovih ranjivosti. To mogu provoditi programeri, korisnici ili neovisni revizori, ovisno o svrsi i opsegu testiranja. Neki od najÄeÅ¡Ä‡ih metoda sigurnosnog testiranja AI sustava i LLM-ova su:

- **Sanitacija podataka**: Proces uklanjanja ili anonimizacije osjetljivih ili privatnih informacija iz podataka za treniranje ili unosa AI sustava ili LLM-a. Sanitacija podataka pomaÅ¾e sprijeÄiti curenje podataka i zlonamjernu manipulaciju smanjenjem izloÅ¾enosti povjerljivim ili osobnim podacima.
- **Adversarialno testiranje**: Proces generiranja i primjene adversarialnih primjera na ulaz ili izlaz AI sustava ili LLM-a kako bi se procijenila njegova otpornost i izdrÅ¾ljivost protiv napada. Adversarialno testiranje pomaÅ¾e identificirati i ublaÅ¾iti ranjivosti i slabosti AI sustava ili LLM-a koje napadaÄi mogu iskoristiti.
- **Verifikacija modela**: Proces provjere ispravnosti i potpunosti parametara modela ili arhitekture AI sustava ili LLM-a. Verifikacija modela pomaÅ¾e otkriti i sprijeÄiti kraÄ‘u modela osiguravajuÄ‡i da je model zaÅ¡tiÄ‡en i autentificiran.
- **Validacija izlaza**: Proces provjere kvalitete i pouzdanosti izlaza AI sustava ili LLM-a. Validacija izlaza pomaÅ¾e otkriti i ispraviti zlonamjernu manipulaciju osiguravajuÄ‡i da je izlaz dosljedan i toÄan.

OpenAI, vodeÄ‡i u AI sustavima, uspostavio je niz _sigurnosnih evaluacija_ kao dio svoje inicijative red teaming mreÅ¾e, s ciljem testiranja izlaza AI sustava u nadi da Ä‡e doprinijeti sigurnosti AI-ja.

> Evaluacije mogu varirati od jednostavnih Q&A testova do sloÅ¾enijih simulacija. Kao konkretne primjere, evo nekoliko evaluacija koje je OpenAI razvio za procjenu ponaÅ¡anja AI-ja iz razliÄitih perspektiva:

#### Uvjeravanje

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav moÅ¾e prevariti drugi AI sustav da izgovori tajnu rijeÄ?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav moÅ¾e uvjeriti drugi AI sustav da donira novac?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav moÅ¾e utjecati na podrÅ¡ku drugog AI sustava politiÄkoj inicijativi?

#### Steganografija (skrivena poruka)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav moÅ¾e prenijeti tajne poruke bez da ga drugi AI sustav otkrije?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav moÅ¾e komprimirati i dekomprimirati poruke kako bi omoguÄ‡io skrivanje tajnih poruka?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Koliko dobro AI sustav moÅ¾e koordinirati s drugim AI sustavom bez izravne komunikacije?

### Sigurnost AI-ja

Neophodno je zaÅ¡tititi AI sustave od zlonamjernih napada, zloupotrebe ili neÅ¾eljenih posljedica. To ukljuÄuje poduzimanje koraka za osiguranje sigurnosti, pouzdanosti i vjerodostojnosti AI sustava, kao Å¡to su:

- Osiguranje podataka i algoritama koji se koriste za treniranje i rad AI modela
- SprjeÄavanje neovlaÅ¡tenog pristupa, manipulacije ili sabotaÅ¾e AI sustava
- Otkrivanje i ublaÅ¾avanje pristranosti, diskriminacije ili etiÄkih problema u AI sustavima
- Osiguranje odgovornosti, transparentnosti i objaÅ¡njivosti AI odluka i postupaka
- UsuglaÅ¡avanje ciljeva i vrijednosti AI sustava s onima ljudi i druÅ¡tva

Sigurnost AI-ja vaÅ¾na je za oÄuvanje integriteta, dostupnosti i povjerljivosti AI sustava i podataka. Neki od izazova i prilika sigurnosti AI-ja su:

- Prilika: UkljuÄivanje AI-ja u strategije kibernetiÄke sigurnosti jer moÅ¾e igrati kljuÄnu ulogu u prepoznavanju prijetnji i poboljÅ¡anju vremena odgovora. AI moÅ¾e pomoÄ‡i u automatizaciji i unapreÄ‘enju otkrivanja i ublaÅ¾avanja cyber-napada, poput phishinga, zlonamjernog softvera ili ransomwarea.
- Izazov: AI takoÄ‘er mogu koristiti napadaÄi za izvoÄ‘enje sofisticiranih napada, poput generiranja laÅ¾nog ili obmanjujuÄ‡eg sadrÅ¾aja, laÅ¾nog predstavljanja korisnika ili iskoriÅ¡tavanja ranjivosti u AI sustavima. Stoga programeri AI-ja imaju jedinstvenu odgovornost dizajnirati sustave koji su robusni i otporni na zloupotrebu.

### ZaÅ¡tita podataka

LLM-ovi mogu predstavljati rizike za privatnost i sigurnost podataka koje koriste. Na primjer, LLM-ovi mogu potencijalno zapamtiti i otkriti osjetljive informacije iz svojih podataka za treniranje, poput osobnih imena, adresa, lozinki ili brojeva kreditnih kartica. TakoÄ‘er ih mogu manipulirati ili napadati zlonamjerni akteri koji Å¾ele iskoristiti njihove ranjivosti ili pristranosti. Stoga je vaÅ¾no biti svjestan ovih rizika i poduzeti odgovarajuÄ‡e mjere za zaÅ¡titu podataka koriÅ¡tenih s LLM-ovima. Neki od koraka koje moÅ¾ete poduzeti za zaÅ¡titu podataka koriÅ¡tenih s LLM-ovima ukljuÄuju:

- **OgraniÄavanje koliÄine i vrste podataka koje dijelite s LLM-ovima**: Dijelite samo podatke koji su nuÅ¾ni i relevantni za predviÄ‘ene svrhe, izbjegavajuÄ‡i dijeljenje osjetljivih, povjerljivih ili osobnih podataka. Korisnici bi takoÄ‘er trebali anonimizirati ili Å¡ifrirati podatke koje dijele s LLM-ovima, primjerice uklanjanjem ili maskiranjem identifikacijskih podataka ili koriÅ¡tenjem sigurnih komunikacijskih kanala.
- **Provjera podataka koje LLM-ovi generiraju**: Uvijek provjeravajte toÄnost i kvalitetu izlaza koje generiraju LLM-ovi kako biste osigurali da ne sadrÅ¾e neÅ¾eljene ili neprikladne informacije.
- **Prijavljivanje i upozoravanje na bilo kakve povrede podataka ili incidente**: Budite oprezni prema sumnjivim ili abnormalnim aktivnostima ili ponaÅ¡anjima LLM-ova, poput generiranja tekstova koji su irelevantni, netoÄni, uvredljivi ili Å¡tetni. To moÅ¾e biti znak povrede podataka ili sigurnosnog incidenta.

Sigurnost podataka, upravljanje i usklaÄ‘enost kljuÄni su za svaku organizaciju koja Å¾eli iskoristiti snagu podataka i AI-ja u multi-cloud okruÅ¾enju. Osiguranje i upravljanje svim vaÅ¡im podacima sloÅ¾en je i viÅ¡eslojan zadatak. Potrebno je osigurati i upravljati razliÄitim vrstama podataka (strukturirani, nestrukturirani i podaci generirani AI-jem) na razliÄitim lokacijama u viÅ¡e cloudova, uzimajuÄ‡i u obzir postojeÄ‡e i buduÄ‡e propise o sigurnosti podataka, upravljanju i AI-ju. Za zaÅ¡titu podataka potrebno je usvojiti najbolje prakse i mjere opreza, kao Å¡to su:

- KoriÅ¡tenje cloud usluga ili platformi koje nude znaÄajke zaÅ¡tite podataka i privatnosti.
- KoriÅ¡tenje alata za provjeru kvalitete i validaciju podataka kako biste provjerili podatke na pogreÅ¡ke, nedosljednosti ili anomalije.
- KoriÅ¡tenje okvira za upravljanje podacima i etiku kako biste osigurali odgovornu i transparentnu upotrebu podataka.

### Emulacija prijetnji iz stvarnog svijeta - AI red teaming

Emulacija prijetnji iz stvarnog svijeta sada se smatra standardnom praksom u izgradnji otpornijih AI sustava, koristeÄ‡i sliÄne alate, taktike i procedure za identificiranje rizika sustava i testiranje reakcije branitelja.
> Praksa AI red teaminga razvila se i sada ima Å¡ire znaÄenje: ne odnosi se samo na traÅ¾enje sigurnosnih ranjivosti, veÄ‡ ukljuÄuje i ispitivanje drugih kvarova sustava, poput generiranja potencijalno Å¡tetnog sadrÅ¾aja. AI sustavi donose nove rizike, a red teaming je kljuÄan za razumijevanje tih novih prijetnji, poput prompt injectiona i stvaranja neutemeljenog sadrÅ¾aja. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.hr.png)]()

Ispod su kljuÄni uvidi koji su oblikovali Microsoftov AI Red Team program.

1. **Å irok opseg AI red teaminga:**  
   AI red teaming sada obuhvaÄ‡a i sigurnosne i Responsible AI (RAI) ishode. Tradicionalno, red teaming se fokusirao na sigurnosne aspekte, tretirajuÄ‡i model kao vektor (npr. kraÄ‘a osnovnog modela). MeÄ‘utim, AI sustavi uvode nove sigurnosne ranjivosti (npr. prompt injection, trovanje), Å¡to zahtijeva posebnu paÅ¾nju. Osim sigurnosti, AI red teaming takoÄ‘er istraÅ¾uje pitanja pravednosti (npr. stereotipizaciju) i Å¡tetnog sadrÅ¾aja (npr. glorifikaciju nasilja). Rana identifikacija ovih problema omoguÄ‡uje prioritetno ulaganje u obranu.  
2. **Zlonamjerne i nenamjerne pogreÅ¡ke:**  
   AI red teaming razmatra pogreÅ¡ke iz zlonamjerne i nenamjerne perspektive. Na primjer, prilikom red teaminga novog Binga, istraÅ¾ujemo ne samo kako zlonamjerni napadaÄi mogu podmititi sustav, veÄ‡ i kako obiÄni korisnici mogu naiÄ‡i na problematiÄan ili Å¡tetan sadrÅ¾aj. Za razliku od tradicionalnog sigurnosnog red teaminga, koji se uglavnom fokusira na zlonamjerne aktere, AI red teaming uzima u obzir Å¡iri spektar korisniÄkih profila i potencijalnih pogreÅ¡aka.  
3. **DinamiÄna priroda AI sustava:**  
   AI aplikacije se stalno razvijaju. U aplikacijama velikih jeziÄnih modela, developeri se prilagoÄ‘avaju promjenjivim zahtjevima. Kontinuirani red teaming osigurava stalnu budnost i prilagodbu novim rizicima.

AI red teaming nije sveobuhvatan i treba ga smatrati dopunskom aktivnoÅ¡Ä‡u uz dodatne kontrole poput [role-based access control (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) i sveobuhvatna rjeÅ¡enja za upravljanje podacima. Namijenjen je kao dodatak sigurnosnoj strategiji koja se fokusira na koriÅ¡tenje sigurnih i odgovornih AI rjeÅ¡enja koja uzimaju u obzir privatnost i sigurnost, a istovremeno nastoje minimizirati pristranosti, Å¡tetan sadrÅ¾aj i dezinformacije koje mogu naruÅ¡iti povjerenje korisnika.

Evo popisa dodatnih materijala koji vam mogu pomoÄ‡i bolje razumjeti kako red teaming moÅ¾e pomoÄ‡i u prepoznavanju i ublaÅ¾avanju rizika u vaÅ¡im AI sustavima:

- [Planiranje red teaminga za velike jeziÄne modele (LLM) i njihove primjene](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [Å to je OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming - KljuÄna praksa za izgradnju sigurnijih i odgovornih AI rjeÅ¡enja](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), baza znanja o taktikama i tehnikama koje koriste napadaÄi u stvarnim napadima na AI sustave.

## Provjera znanja

Koji bi mogao biti dobar pristup odrÅ¾avanju integriteta podataka i sprjeÄavanju zloupotrebe?

1. Imati snaÅ¾ne kontrole pristupa podacima temeljene na ulogama i upravljanje podacima  
1. Implementirati i nadzirati oznaÄavanje podataka kako bi se sprijeÄila pogreÅ¡na reprezentacija ili zloupotreba podataka  
1. Osigurati da vaÅ¡a AI infrastruktura podrÅ¾ava filtriranje sadrÅ¾aja

A:1, Iako su sva tri odliÄne preporuke, osiguravanje da korisnicima dodjeljujete odgovarajuÄ‡e privilegije pristupa podacima znatno Ä‡e pomoÄ‡i u sprjeÄavanju manipulacije i pogreÅ¡ne reprezentacije podataka koje koriste LLM-ovi.

## ğŸš€ Izazov

ProÄitajte viÅ¡e o tome kako moÅ¾ete [upravljati i Å¡tititi osjetljive informacije](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) u doba AI.

## OdliÄan posao, nastavite s uÄenjem

Nakon Å¡to zavrÅ¡ite ovu lekciju, pogledajte naÅ¡u [Generative AI Learning kolekciju](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) kako biste nastavili unapreÄ‘ivati svoje znanje o Generativnoj AI!

Krenite na Lekciju 14 gdje Ä‡emo pogledati [Å½ivotni ciklus Generativnih AI aplikacija](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden koriÅ¡tenjem AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako teÅ¾imo toÄnosti, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati sluÅ¾benim i autoritativnim izvorom. Za kritiÄne informacije preporuÄuje se profesionalni ljudski prijevod. Ne snosimo odgovornost za bilo kakva nesporazuma ili pogreÅ¡na tumaÄenja koja proizlaze iz koriÅ¡tenja ovog prijevoda.