<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-05-20T01:45:25+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "hr"
}
-->
# Generiranje uz pomo캖 pretra쬴vanja (RAG) i vektorske baze podataka

U lekciji o aplikacijama za pretra쬴vanje, ukratko smo nau캜ili kako integrirati vlastite podatke u velike jezi캜ne modele (LLM). U ovoj lekciji 캖emo detaljnije istra쬴ti koncepte uzemljenja va코ih podataka u va코oj LLM aplikaciji, mehaniku procesa i metode za pohranu podataka, uklju캜uju캖i i ugra캠ene i tekstualne podatke.

## Uvod

U ovoj lekciji obradit 캖emo sljede캖e:

- Uvod u RAG, 코to je to i za코to se koristi u AI (umjetnoj inteligenciji).

- Razumijevanje 코to su vektorske baze podataka i stvaranje jedne za na코u aplikaciju.

- Prakti캜ni primjer kako integrirati RAG u aplikaciju.

## Ciljevi u캜enja

Nakon zavr코etka ove lekcije, mo캖i 캖ete:

- Objasniti zna캜aj RAG-a u pretra쬴vanju i obradi podataka.

- Postaviti RAG aplikaciju i uzemljiti svoje podatke na LLM.

- U캜inkovita integracija RAG-a i vektorskih baza podataka u LLM aplikacije.

## Na코 scenarij: pobolj코anje na코ih LLM-a s vlastitim podacima

Za ovu lekciju 쬰limo dodati vlastite bilje코ke u edukativni startup, koji omogu캖uje chatbotu da dobije vi코e informacija o razli캜itim predmetima. Koriste캖i bilje코ke koje imamo, u캜enici 캖e mo캖i bolje u캜iti i razumjeti razli캜ite teme, 코to 캖e im olak코ati pripremu za ispite. Da bismo kreirali na코 scenarij, koristit 캖emo:

- `Azure OpenAI:` LLM koji 캖emo koristiti za kreiranje na코eg chatbota

- `AI for beginners' lesson on Neural Networks`: ovo 캖e biti podaci na koje uzemljujemo na코 LLM

- `Azure AI Search` i `Azure Cosmos DB:` vektorska baza podataka za pohranu na코ih podataka i kreiranje indeksa pretra쬴vanja

Korisnici 캖e mo캖i kreirati vje쬭ovne kvizove iz svojih bilje코ki, kartice za ponavljanje i sa쬰ti ih u koncizne preglede. Da bismo zapo캜eli, pogledajmo 코to je RAG i kako funkcionira:

## Generiranje uz pomo캖 pretra쬴vanja (RAG)

Chatbot pokretan LLM-om obra캠uje korisni캜ke upite kako bi generirao odgovore. Dizajniran je da bude interaktivan i komunicira s korisnicima o 코irokom rasponu tema. Me캠utim, njegovi odgovori su ograni캜eni na kontekst koji je pru쬰n i osnovne podatke za treniranje. Na primjer, GPT-4 ima prekid znanja u rujnu 2021., 코to zna캜i da nema znanja o doga캠ajima koji su se dogodili nakon tog razdoblja. Osim toga, podaci kori코teni za treniranje LLM-ova isklju캜uju povjerljive informacije kao 코to su osobne bilje코ke ili priru캜nik za proizvode tvrtke.

### Kako RAG-ovi (Generiranje uz pomo캖 pretra쬴vanja) rade

Pretpostavimo da 쬰lite implementirati chatbot koji kreira kvizove iz va코ih bilje코ki, trebat 캖e vam veza s bazom znanja. Tu RAG dolazi u pomo캖. RAG-ovi djeluju na sljede캖i na캜in:

- **Baza znanja:** Prije pretra쬴vanja, ovi dokumenti moraju biti uneseni i prethodno obra캠eni, obi캜no razbijaju캖i velike dokumente u manje dijelove, transformiraju캖i ih u tekstualne ugradnje i pohranjuju캖i ih u bazu podataka.

- **Upit korisnika:** korisnik postavlja pitanje

- **Pretra쬴vanje:** Kada korisnik postavi pitanje, model ugradnje pretra쬿je relevantne informacije iz na코e baze znanja kako bi pru쬴o vi코e konteksta koji 캖e biti uklju캜en u upit.

- **Generiranje uz pomo캖 pretra쬴vanja:** LLM pobolj코ava svoj odgovor na temelju pretra쬰nih podataka. Omogu캖uje da generirani odgovor bude ne samo temeljen na prethodno treniranim podacima ve캖 i relevantnim informacijama iz dodanog konteksta. Pretra쬰ni podaci koriste se za pobolj코anje odgovora LLM-a. LLM tada vra캖a odgovor na korisni캜ko pitanje.

Arhitektura za RAG-ove se implementira pomo캖u transformatora koji se sastoje od dva dijela: kodera i dekodera. Na primjer, kada korisnik postavi pitanje, ulazni tekst se 'kodira' u vektore koji hvataju zna캜enje rije캜i, a vektori se 'dekodiraju' u na코 indeks dokumenata i generiraju novi tekst temeljen na korisni캜kom upitu. LLM koristi model kodera-dekodera za generiranje izlaza.

Dva pristupa pri implementaciji RAG-a prema predlo쬰nom radu: [Generiranje uz pomo캖 pretra쬴vanja za zadatke intenzivne obrade znanja u NLP-u (softver za obradu prirodnog jezika)](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) su:

- **_RAG-Sequence_** koriste캖i pretra쬰ne dokumente za predvi캠anje najboljeg mogu캖eg odgovora na korisni캜ki upit

- **RAG-Token** koriste캖i dokumente za generiranje sljede캖eg tokena, zatim ih pretra쬿je kako bi odgovorio na korisni캜ki upit

### Za코to biste koristili RAG-ove?

- **Bogatstvo informacija:** osigurava da tekstualni odgovori budu a쬿rirani i aktualni. Stoga pobolj코ava izvedbu na zadacima specifi캜nim za domenu pristupaju캖i unutarnjoj bazi znanja.

- Smanjuje izmi코ljanje koriste캖i **provjerljive podatke** u bazi znanja kako bi pru쬴o kontekst korisni캜kim upitima.

- **Isplativo je** jer su ekonomi캜niji u usporedbi s finim pode코avanjem LLM-a.

## Stvaranje baze znanja

Na코a aplikacija temelji se na na코im osobnim podacima tj. lekciji o neuronskim mre쬬ma u kurikulumu AI za po캜etnike.

### Vektorske baze podataka

Vektorska baza podataka, za razliku od tradicionalnih baza podataka, je specijalizirana baza podataka dizajnirana za pohranu, upravljanje i pretra쬴vanje ugra캠enih vektora. Pohranjuje numeri캜ke reprezentacije dokumenata. Razbijanje podataka na numeri캜ke ugradnje olak코ava na코em AI sustavu razumijevanje i obradu podataka.

Pohranjujemo na코e ugradnje u vektorske baze podataka jer LLM-ovi imaju ograni캜enje broja tokena koje prihva캖aju kao ulaz. Kako ne mo쬰te prenijeti cijele ugradnje u LLM, morat 캖emo ih razbiti na dijelove i kada korisnik postavi pitanje, ugradnje koje su najvi코e sli캜ne pitanju bit 캖e vra캖ene zajedno s upitom. Razbijanje tako캠er smanjuje tro코kove broja tokena koji prolaze kroz LLM.

Neke popularne vektorske baze podataka uklju캜uju Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant i DeepLake. Mo쬰te kreirati model Azure Cosmos DB pomo캖u Azure CLI s sljede캖om naredbom:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od teksta do ugradnji

Prije nego pohranimo na코e podatke, morat 캖emo ih pretvoriti u vektorske ugradnje prije nego 코to ih pohranimo u bazu podataka. Ako radite s velikim dokumentima ili dugim tekstovima, mo쬰te ih razbiti na temelju upita koje o캜ekujete. Razbijanje se mo쬰 obaviti na razini re캜enice ili na razini paragrafa. Kako razbijanje izvodi zna캜enja iz rije캜i oko njih, mo쬰te dodati neki drugi kontekst dijelu, na primjer, dodavanjem naslova dokumenta ili uklju캜ivanjem nekog teksta prije ili poslije dijela. Mo쬰te razbiti podatke na sljede캖i na캜in:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Jednom kada su razbijeni, mo쬰mo zatim ugraditi na코 tekst koriste캖i razli캜ite modele ugradnje. Neki modeli koje mo쬰te koristiti uklju캜uju: word2vec, ada-002 od OpenAI, Azure Computer Vision i mnoge druge. Odabir modela koji 캖ete koristiti ovisit 캖e o jezicima koje koristite, vrsti sadr쬬ja koji se kodira (tekst/slike/audio), veli캜ini ulaza koji mo쬰 kodirati i duljini izlaza ugradnje.

Primjer ugra캠enog teksta koriste캖i OpenAI-jev model `text-embedding-ada-002` je:

## Pretra쬴vanje i vektorsko pretra쬴vanje

Kada korisnik postavi pitanje, retriver ga transformira u vektor koriste캖i kodera upita, zatim pretra쬿je kroz na코 indeks pretra쬴vanja dokumenata za relevantne vektore u dokumentu koji su povezani s ulazom. Kada je gotovo, pretvara i ulazni vektor i vektore dokumenata u tekst i proslje캠uje ga kroz LLM.

### Pretra쬴vanje

Pretra쬴vanje se doga캠a kada sustav poku코ava brzo prona캖i dokumente iz indeksa koji zadovoljavaju kriterije pretra쬴vanja. Cilj retrivera je dobiti dokumente koji 캖e se koristiti za pru쬬nje konteksta i uzemljenje LLM-a na va코im podacima.

Postoji nekoliko na캜ina za izvo캠enje pretra쬴vanja unutar na코e baze podataka kao 코to su:

- **Pretra쬴vanje po klju캜nim rije캜ima** - koristi se za tekstualna pretra쬴vanja

- **Semanti캜ko pretra쬴vanje** - koristi semanti캜ko zna캜enje rije캜i

- **Vektorsko pretra쬴vanje** - pretvara dokumente iz teksta u vektorske reprezentacije koriste캖i modele ugradnje. Pretra쬴vanje 캖e se obaviti upitom dokumenta 캜ije su vektorske reprezentacije najbli쬰 korisni캜kom pitanju.

- **Hibridno** - kombinacija pretra쬴vanja po klju캜nim rije캜ima i vektorskog pretra쬴vanja.

Izazov s pretra쬴vanjem dolazi kada nema sli캜nog odgovora na upit u bazi podataka, sustav 캖e tada vratiti najbolje informacije koje mogu dobiti, me캠utim, mo쬰te koristiti taktike kao 코to su postavljanje maksimalne udaljenosti za relevantnost ili kori코tenje hibridnog pretra쬴vanja koje kombinira i klju캜ne rije캜i i vektorsko pretra쬴vanje. U ovoj lekciji 캖emo koristiti hibridno pretra쬴vanje, kombinaciju vektorskog i pretra쬴vanja po klju캜nim rije캜ima. Pohranit 캖emo na코e podatke u dataframe s stupcima koji sadr쬰 dijelove kao i ugradnje.

### Vektorska sli캜nost

Retriver 캖e pretra쬴vati kroz bazu znanja za ugradnje koje su blizu jedna drugoj, najbli쬴 susjed, jer su tekstovi sli캜ni. U scenariju kada korisnik postavi upit, prvo se ugra캠uje, zatim se podudara sa sli캜nim ugradnjama. Uobi캜ajeno mjerenje koje se koristi za pronala쬰nje koliko su razli캜iti vektori sli캜ni je kosinusna sli캜nost koja se temelji na kutu izme캠u dvaju vektora.

Mo쬰mo mjeriti sli캜nost koriste캖i druge alternative koje mo쬰mo koristiti su euklidska udaljenost koja je ravna linija izme캠u krajnjih to캜aka vektora i skalarni produkt koji mjeri zbroj proizvoda odgovaraju캖ih elemenata dvaju vektora.

### Indeks pretra쬴vanja

Kada radimo pretra쬴vanje, trebat 캖emo izgraditi indeks pretra쬴vanja za na코u bazu znanja prije nego 코to obavimo pretra쬴vanje. Indeks 캖e pohraniti na코e ugradnje i mo쬰 brzo pretra쬴ti najvi코e sli캜ne dijelove 캜ak i u velikoj bazi podataka. Mo쬰mo kreirati na코 indeks lokalno koriste캖i:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Ponovno rangiranje

Nakon 코to ste upitali bazu podataka, mo쬯a 캖ete trebati sortirati rezultate od najrelevantnijih. LLM za ponovno rangiranje koristi strojno u캜enje za pobolj코anje relevantnosti rezultata pretra쬴vanja rangiranjem od najrelevantnijih. Koriste캖i Azure AI Search, ponovno rangiranje se automatski obavlja za vas koriste캖i semanti캜ki ponovni rangir. Primjer kako ponovno rangiranje radi koriste캖i najbli쬰 susjede:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Sve zajedno

Posljednji korak je dodavanje na코eg LLM-a u mje코avinu kako bismo mogli dobiti odgovore koji su uzemljeni na na코im podacima. Mo쬰mo ga implementirati na sljede캖i na캜in:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluacija na코e aplikacije

### Evaluacijske metrike

- Kvaliteta dostavljenih odgovora osiguravaju캖i da zvu캜i prirodno, te캜no i ljudski

- Uzemljenost podataka: procjena je li odgovor do코ao iz dostavljenih dokumenata

- Relevantnost: procjena odgovora koji se podudara i odnosi se na postavljeno pitanje

- Te캜nost - je li odgovor gramati캜ki smislen

## Primjeri upotrebe za kori코tenje RAG-a (Generiranje uz pomo캖 pretra쬴vanja) i vektorskih baza podataka

Postoji mnogo razli캜itih primjera upotrebe gdje pozivi funkcija mogu pobolj코ati va코u aplikaciju kao 코to su:

- Postavljanje pitanja i odgovora: uzemljenje va코ih podataka tvrtke na chat koji zaposlenici mogu koristiti za postavljanje pitanja.

- Sustavi preporuka: gdje mo쬰te kreirati sustav koji podudara najsli캜nije vrijednosti npr. filmove, restorane i mnoge druge.

- Usluge chatbota: mo쬰te pohraniti povijest chata i personalizirati razgovor na temelju korisni캜kih podataka.

- Pretra쬴vanje slika temeljeno na vektorskim ugradnjama, korisno kada radite prepoznavanje slika i detekciju anomalija.

## Sa쬰tak

Obradili smo temeljna podru캜ja RAG-a od dodavanja na코ih podataka u aplikaciju, korisni캜kog upita i izlaza. Da biste pojednostavili kreiranje RAG-a, mo쬰te koristiti okvire kao 코to su Semanti Kernel, Langchain ili Autogen.

## Zadatak

Da biste nastavili s u캜enjem Generiranja uz pomo캖 pretra쬴vanja (RAG) mo쬰te izgraditi:

- Izgradite front-end za aplikaciju koriste캖i okvir po va코em izboru

- Iskoristite okvir, bilo LangChain ili Semanti Kernel, i rekreirajte svoju aplikaciju.

캛estitamo na zavr코etku lekcije 游녪.

## U캜enje ne prestaje ovdje, nastavite putovanje

Nakon zavr코etka ove lekcije, pogledajte na코u [Generativnu AI kolekciju za u캜enje](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) kako biste nastavili unapre캠ivati svoje znanje o generativnoj AI!

**Izjava o odricanju odgovornosti**:  
Ovaj dokument je preveden koriste캖i AI uslugu prevo캠enja [Co-op Translator](https://github.com/Azure/co-op-translator). Iako te쬴mo ka to캜nosti, imajte na umu da automatizirani prijevodi mogu sadr쬬vati pogre코ke ili neto캜nosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za kriti캜ne informacije preporu캜uje se profesionalni prijevod od strane 캜ovjeka. Ne odgovaramo za nesporazume ili pogre코na tuma캜enja koja proizlaze iz kori코tenja ovog prijevoda.