<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-07-09T16:21:33+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "hr"
}
-->
# Retrieval Augmented Generation (RAG) i vektorske baze podataka

[![Retrieval Augmented Generation (RAG) i vektorske baze podataka](../../../translated_images/15-lesson-banner.ac49e59506175d4fc6ce521561dab2f9ccc6187410236376cfaed13cde371b90.hr.png)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

U lekciji o aplikacijama za pretra쬴vanje, ukratko smo nau캜ili kako integrirati vlastite podatke u Large Language Models (LLM). U ovoj lekciji 캖emo dublje istra쬴ti koncepte povezivanja va코ih podataka u LLM aplikaciji, mehaniku procesa i metode pohrane podataka, uklju캜uju캖i i embeddings i tekst.

> **Video uskoro**

## Uvod

U ovoj lekciji 캖emo obraditi sljede캖e:

- Uvod u RAG, 코to je i za코to se koristi u AI (umjetnoj inteligenciji).

- Razumijevanje 코to su vektorske baze podataka i kako stvoriti jednu za na코u aplikaciju.

- Prakti캜ni primjer kako integrirati RAG u aplikaciju.

## Ciljevi u캜enja

Nakon zavr코etka ove lekcije, mo캖i 캖ete:

- Objasniti va쬹ost RAG-a u dohva캖anju i obradi podataka.

- Postaviti RAG aplikaciju i povezati svoje podatke s LLM-om.

- U캜inkovito integrirati RAG i vektorske baze podataka u LLM aplikacije.

## Na코 scenarij: unapre캠enje na코ih LLM-ova vlastitim podacima

Za ovu lekciju 쬰limo dodati vlastite bilje코ke u edukacijski startup, 코to omogu캖uje chatbotu da dobije vi코e informacija o razli캜itim predmetima. Koriste캖i bilje코ke koje imamo, u캜enici 캖e mo캖i bolje u캜iti i razumjeti razli캜ite teme, 코to 캖e im olak코ati pripremu za ispite. Za kreiranje na코eg scenarija koristit 캖emo:

- `Azure OpenAI:` LLM koji 캖emo koristiti za izradu chatbota

- `AI for beginners' lesson on Neural Networks:` podaci na kojima 캖emo temeljiti na코 LLM

- `Azure AI Search` i `Azure Cosmos DB:` vektorska baza podataka za pohranu podataka i kreiranje indeksa za pretra쬴vanje

Korisnici 캖e mo캖i stvarati kvizove za vje쬭u iz svojih bilje코ki, kartice za ponavljanje i sa쬰tke u pregledne prikaze. Za po캜etak, pogledajmo 코to je RAG i kako funkcionira:

## Retrieval Augmented Generation (RAG)

Chatbot pokretan LLM-om obra캠uje korisni캜ke upite kako bi generirao odgovore. Dizajniran je da bude interaktivan i komunicira s korisnicima o 코irokom spektru tema. Me캠utim, njegovi odgovori ograni캜eni su na kontekst koji mu je dan i na osnovne podatke na kojima je treniran. Na primjer, GPT-4 ima cutoff znanja do rujna 2021., 코to zna캜i da nema informacije o doga캠ajima nakon tog razdoblja. Osim toga, podaci kori코teni za treniranje LLM-ova ne uklju캜uju povjerljive informacije poput osobnih bilje코ki ili priru캜nika za proizvode tvrtke.

### Kako RAG (Retrieval Augmented Generation) funkcionira

![crte koji prikazuje kako RAG funkcionira](../../../translated_images/how-rag-works.f5d0ff63942bd3a638e7efee7a6fce7f0787f6d7a1fca4e43f2a7a4d03cde3e0.hr.png)

Pretpostavimo da 쬰lite implementirati chatbota koji stvara kvizove iz va코ih bilje코ki, trebat 캖e vam veza s bazom znanja. Tu RAG dolazi kao rje코enje. RAG radi na sljede캖i na캜in:

- **Baza znanja:** Prije dohva캖anja, dokumenti se moraju unijeti i prethodno obraditi, obi캜no razbijanjem velikih dokumenata na manje dijelove, pretvaranjem u tekstualne embeddings i pohranom u bazu podataka.

- **Korisni캜ki upit:** korisnik postavlja pitanje

- **Dohva캖anje:** Kada korisnik postavi pitanje, embedding model pronalazi relevantne informacije iz baze znanja kako bi pru쬴o dodatni kontekst koji 캖e se uklju캜iti u upit.

- **Augmentirana generacija:** LLM pobolj코ava svoj odgovor na temelju dohva캖enih podataka. To omogu캖uje da odgovor nije samo baziran na prethodno treniranim podacima, ve캖 i na relevantnim informacijama iz dodanog konteksta. Dohva캖eni podaci koriste se za oboga캖ivanje odgovora LLM-a. LLM zatim vra캖a odgovor na korisni캜ki upit.

![crte koji prikazuje arhitekturu RAG](../../../translated_images/encoder-decode.f2658c25d0eadee2377bb28cf3aee8b67aa9249bf64d3d57bb9be077c4bc4e1a.hr.png)

Arhitektura RAG-a implementirana je pomo캖u transformera koji se sastoje od dva dijela: enkodera i dekodera. Na primjer, kada korisnik postavi pitanje, ulazni tekst se 'enkodira' u vektore koji hvataju zna캜enje rije캜i, a ti se vektori 'dekodiraju' u indeks dokumenata i generira se novi tekst na temelju korisni캜kog upita. LLM koristi model enkoder-dekoder za generiranje izlaza.

Dvije su pristupa implementaciji RAG-a prema predlo쬰nom radu: [Retrieval-Augmented Generation for Knowledge intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst):

- **_RAG-Sequence_** koristi dohva캖ene dokumente za predvi캠anje najboljeg mogu캖eg odgovora na korisni캜ki upit

- **RAG-Token** koristi dokumente za generiranje sljede캖eg tokena, zatim ih dohva캖a za odgovor na korisni캜ki upit

### Za코to koristiti RAG?

- **Bogatstvo informacija:** osigurava da su tekstualni odgovori a쬿rni i aktualni. Time se pobolj코ava izvedba na zadacima specifi캜nim za odre캠eno podru캜je pristupom internoj bazi znanja.

- Smanjuje izmi코ljanje podataka kori코tenjem **provjerljivih podataka** iz baze znanja za pru쬬nje konteksta korisni캜kim upitima.

- **Isplativo je** jer je ekonomi캜nije od finog pode코avanja LLM-a.

## Kreiranje baze znanja

Na코a aplikacija temelji se na osobnim podacima, tj. lekciji o neuronskim mre쬬ma iz kurikuluma AI For Beginners.

### Vektorske baze podataka

Vektorska baza podataka, za razliku od tradicionalnih baza, je specijalizirana baza dizajnirana za pohranu, upravljanje i pretra쬴vanje ugra캠enih vektora. Pohranjuje numeri캜ke prikaze dokumenata. Razbijanje podataka u numeri캜ke embeddings olak코ava na코em AI sustavu razumijevanje i obradu podataka.

Embeddings pohranjujemo u vektorske baze podataka jer LLM-ovi imaju ograni캜enje broja tokena koje mogu primiti kao ulaz. Kako ne mo쬰mo proslijediti cijele embeddings LLM-u, moramo ih razbiti na dijelove, a kada korisnik postavi pitanje, vra캖aju se embeddings najbli쬴 pitanju zajedno s upitom. Razbijanje na dijelove tako캠er smanjuje tro코kove vezane uz broj tokena koji se 코alju LLM-u.

Neke popularne vektorske baze podataka uklju캜uju Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant i DeepLake. Mo쬰te kreirati Azure Cosmos DB model koriste캖i Azure CLI s naredbom:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### Od teksta do embeddings

Prije nego 코to pohranimo podatke, moramo ih pretvoriti u vektorske embeddings. Ako radite s velikim dokumentima ili dugim tekstovima, mo쬰te ih razbiti na dijelove prema o캜ekivanim upitima. Razbijanje mo쬰 biti na razini re캜enice ili paragrafa. Budu캖i da razbijanje izvla캜i zna캜enje iz rije캜i oko njih, mo쬰te dodati i dodatni kontekst dijelu, na primjer, naslov dokumenta ili neki tekst prije ili poslije dijela. Podatke mo쬰te razbiti na sljede캖i na캜in:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

Nakon razbijanja, tekst mo쬰mo ugraditi koriste캖i razli캜ite modele za embeddings. Neki modeli koje mo쬰te koristiti su: word2vec, ada-002 od OpenAI, Azure Computer Vision i mnogi drugi. Izbor modela ovisi o jezicima koje koristite, vrsti sadr쬬ja (tekst/slike/audio), veli캜ini ulaza koji mo쬰 kodirati i duljini izlaza embeddingsa.

Primjer ugra캠enog teksta kori코tenjem OpenAI modela `text-embedding-ada-002` je:
![embedding rije캜i cat](../../../translated_images/cat.74cbd7946bc9ca380a8894c4de0c706a4f85b16296ffabbf52d6175df6bf841e.hr.png)

## Dohva캖anje i vektorsko pretra쬴vanje

Kada korisnik postavi pitanje, retriver ga pretvara u vektor koriste캖i query encoder, zatim pretra쬿je indeks dokumenata za relevantne vektore povezane s upitom. Nakon toga, ulazni vektor i vektori dokumenata pretvaraju se u tekst i proslje캠uju LLM-u.

### Dohva캖anje

Dohva캖anje se doga캠a kada sustav brzo pronalazi dokumente iz indeksa koji zadovoljavaju kriterije pretra쬴vanja. Cilj retrivera je prona캖i dokumente koji 캖e se koristiti za pru쬬nje konteksta i povezivanje LLM-a s va코im podacima.

Postoji nekoliko na캜ina za pretra쬴vanje u na코oj bazi podataka, kao 코to su:

- **Pretra쬴vanje po klju캜nim rije캜ima** - koristi se za tekstualna pretra쬴vanja

- **Semanti캜ko pretra쬴vanje** - koristi semanti캜ko zna캜enje rije캜i

- **Vektorsko pretra쬴vanje** - pretvara dokumente iz teksta u vektorske prikaze koriste캖i modele za embeddings. Dohva캖anje se vr코i upitom dokumenata 캜iji su vektorski prikazi najbli쬴 korisni캜kom pitanju.

- **Hibridno** - kombinacija pretra쬴vanja po klju캜nim rije캜ima i vektorskog pretra쬴vanja.

Izazov kod dohva캖anja nastaje kada u bazi nema sli캜nog odgovora na upit, tada sustav vra캖a najbolje mogu캖e informacije, no mo쬰te koristiti taktike poput postavljanja maksimalne udaljenosti za relevantnost ili koristiti hibridno pretra쬴vanje koje kombinira klju캜ne rije캜i i vektorsko pretra쬴vanje. U ovoj lekciji koristit 캖emo hibridno pretra쬴vanje, kombinaciju vektorskog i pretra쬴vanja po klju캜nim rije캜ima. Podatke 캖emo pohraniti u dataframe s kolonama koje sadr쬰 dijelove teksta i embeddings.

### Vektorska sli캜nost

Retriver 캖e pretra쬴vati bazu znanja za embeddings koji su blizu jedan drugome, najbli쬴 susjed, jer su to sli캜ni tekstovi. U scenariju kada korisnik postavi upit, on se prvo ugra캠uje, a zatim se uspore캠uje sa sli캜nim embeddingsima. Naj캜e코캖a mjera za odre캠ivanje sli캜nosti vektora je kosinusna sli캜nost, koja se temelji na kutu izme캠u dva vektora.

Sli캜nost mo쬰mo mjeriti i drugim metodama poput Euklidske udaljenosti, koja je najkra캖a linija izme캠u krajeva vektora, ili skalarne produkcije koja mjeri zbroj proizvoda odgovaraju캖ih elemenata dva vektora.

### Indeks pretra쬴vanja

Prije dohva캖anja, potrebno je izgraditi indeks pretra쬴vanja za bazu znanja. Indeks pohranjuje na코e embeddings i mo쬰 brzo dohvatiti najsli캜nije dijelove 캜ak i u velikoj bazi podataka. Indeks mo쬰mo kreirati lokalno koriste캖i:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
```

### Ponovno rangiranje

Nakon 코to ste izvr코ili upit u bazu, mo쬯a 캖ete htjeti sortirati rezultate od najrelevantnijih. Reranking LLM koristi strojno u캜enje za pobolj코anje relevantnosti rezultata pretra쬴vanja tako da ih poredak postavi od najrelevantnijih. Koriste캖i Azure AI Search, reranking se automatski obavlja pomo캖u semanti캜kog rerankera. Primjer kako reranking funkcionira koriste캖i najbli쬰 susjede:

```python
# Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Index {index} not found in DataFrame")
```

## Sve zajedno

Zadnji korak je dodati na코 LLM u cijelu pri캜u kako bismo mogli dobiti odgovore koji su povezani s na코im podacima. Mo쬰mo ga implementirati na sljede캖i na캜in:

```python
user_input = "what is a perceptron?"

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps with AI questions."},
        {"role": "user", "content": history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Evaluacija na코e aplikacije

### Mjerne vrijednosti evaluacije

- Kvaliteta odgovora: osigurati da zvu캜e prirodno, te캜no i ljudski

- Povezanost podataka: procjena je li odgovor do코ao iz dostavljenih dokumenata

- Relevantnost: procjena podudara li se odgovor i odnosi li se na postavljeno pitanje

- Te캜nost - da li odgovor gramati캜ki ima smisla

## Primjeri upotrebe RAG-a i vektorskih baza podataka

Postoji mnogo razli캜itih primjena gdje pozivi funkcija mogu pobolj코ati va코u aplikaciju, kao 코to su:

- Pitanja i odgovori: povezivanje podataka va코e tvrtke s chatom koji zaposlenici mogu koristiti za postavljanje pitanja.

- Sustavi preporuka: gdje mo쬰te kreirati sustav koji pronalazi najsli캜nije vrijednosti, npr. filmove, restorane i sli캜no.

- Chatbot usluge: mo쬰te pohraniti povijest razgovora i personalizirati konverzaciju na temelju korisni캜kih podataka.

- Pretra쬴vanje slika na temelju vektorskih embeddings, korisno za prepoznavanje slika i otkrivanje anomalija.

## Sa쬰tak

Obradili smo osnovna podru캜ja RAG-a, od dodavanja podataka u aplikaciju, korisni캜kog upita do izlaza. Za jednostavniju izradu RAG-a mo쬰te koristiti okvire poput Semantic Kernel, Langchain ili Autogen.

## Zadatak

Za nastavak u캜enja Retrieval Augmented Generation (RAG) mo쬰te izgraditi:

- Front-end aplikacije koriste캖i okvir po va코em izboru

- Iskoristiti neki od okvira, LangChain ili Semantic Kernel, i ponovno izraditi svoju aplikaciju.

캛estitamo na zavr코etku lekcije 游녪.

## U캜enje ne prestaje ovdje, nastavite putovanje

Nakon zavr코etka ove lekcije, pogledajte na코u [Generative AI Learning kolekciju](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) i nastavite podizati svoje znanje o Generativnoj AI!

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden kori코tenjem AI usluge za prevo캠enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako te쬴mo to캜nosti, imajte na umu da automatski prijevodi mogu sadr쬬vati pogre코ke ili neto캜nosti. Izvorni dokument na izvornom jeziku treba smatrati slu쬭enim i autoritativnim izvorom. Za kriti캜ne informacije preporu캜uje se profesionalni ljudski prijevod. Ne snosimo odgovornost za bilo kakva nesporazuma ili pogre코na tuma캜enja koja proizlaze iz kori코tenja ovog prijevoda.