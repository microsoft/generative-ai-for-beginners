<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-05-20T02:09:03+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "hr"
}
-->
# Okviri za neuronske mreÅ¾e

Kao Å¡to smo veÄ‡ nauÄili, kako bismo mogli uÄinkovito trenirati neuronske mreÅ¾e, trebamo uÄiniti dvije stvari:

* Raditi s tenzorima, npr. mnoÅ¾iti, zbrajati i izraÄunavati neke funkcije poput sigmoida ili softmaxa
* IzraÄunati gradijente svih izraza kako bismo mogli provesti optimizaciju gradijentnim spuÅ¡tanjem

Iako biblioteka `numpy` moÅ¾e obaviti prvi dio, trebamo neki mehanizam za izraÄunavanje gradijenata. U naÅ¡em okviru koji smo razvili u prethodnom odjeljku morali smo ruÄno programirati sve funkcije derivacija unutar metode `backward`, koja provodi unazadno Å¡irenje pogreÅ¡ke. Idealno, okvir bi nam trebao pruÅ¾iti moguÄ‡nost izraÄunavanja gradijenata *bilo kojeg izraza* koji moÅ¾emo definirati.

JoÅ¡ jedna vaÅ¾na stvar je moguÄ‡nost izvoÄ‘enja proraÄuna na GPU-u ili bilo kojoj drugoj specijaliziranoj jedinici za proraÄun, kao Å¡to je TPU. Trening dubokih neuronskih mreÅ¾a zahtijeva *puno* proraÄuna, i moguÄ‡nost paraleliziranja tih proraÄuna na GPU-ima je vrlo vaÅ¾na.

> âœ… Pojam 'paralelizirati' znaÄi raspodijeliti proraÄune na viÅ¡e ureÄ‘aja.

Trenutno su dva najpopularnija okvira za neuronske mreÅ¾e: TensorFlow i PyTorch. Oba pruÅ¾aju niskorazinski API za rad s tenzorima na CPU-u i GPU-u. Povrh niskorazinskog API-ja, postoji i visokorazinski API, nazvan Keras i PyTorch Lightning.

Niskorazinski API | TensorFlow| PyTorch
--------------|-------------------------------------|--------------------------------
Visokorazinski API| Keras| PyTorch Lightning

**Niskorazinski API-ji** u oba okvira omoguÄ‡uju vam izgradnju takozvanih **proraÄunskih grafova**. Ovaj graf definira kako izraÄunati izlaz (obiÄno funkciju gubitka) s danim ulaznim parametrima i moÅ¾e se poslati na proraÄun na GPU, ako je dostupan. Postoje funkcije za diferenciranje ovog proraÄunskog grafa i izraÄunavanje gradijenata, koji se zatim mogu koristiti za optimizaciju parametara modela.

**Visokorazinski API-ji** uvelike smatraju neuronske mreÅ¾e **sekvencom slojeva**, i Äine konstruiranje veÄ‡ine neuronskih mreÅ¾a puno lakÅ¡im. Trening modela obiÄno zahtijeva pripremu podataka i zatim pozivanje funkcije `fit` da obavi posao.

Visokorazinski API omoguÄ‡uje vam brzo konstruiranje tipiÄnih neuronskih mreÅ¾a bez brige o puno detalja. Istovremeno, niskorazinski API nudi mnogo viÅ¡e kontrole nad procesom treniranja, i stoga se puno koristi u istraÅ¾ivanju, kada se bavite novim arhitekturama neuronskih mreÅ¾a.

TakoÄ‘er je vaÅ¾no razumjeti da moÅ¾ete koristiti oba API-ja zajedno, npr. moÅ¾ete razviti vlastitu arhitekturu slojeva mreÅ¾e koristeÄ‡i niskorazinski API, a zatim je koristiti unutar veÄ‡e mreÅ¾e konstruirane i trenirane s visokorazinskim API-jem. Ili moÅ¾ete definirati mreÅ¾u koristeÄ‡i visokorazinski API kao sekvencu slojeva, a zatim koristiti vlastitu niskorazinsku petlju treniranja za provoÄ‘enje optimizacije. Oba API-ja koriste iste osnovne koncepte i dizajnirana su da dobro suraÄ‘uju.

## UÄenje

U ovom teÄaju nudimo veÄ‡inu sadrÅ¾aja i za PyTorch i za TensorFlow. MoÅ¾ete odabrati svoj preferirani okvir i proÄ‡i samo odgovarajuÄ‡e biljeÅ¾nice. Ako niste sigurni koji okvir odabrati, proÄitajte neke rasprave na internetu o **PyTorch vs. TensorFlow**. TakoÄ‘er moÅ¾ete pogledati oba okvira kako biste bolje razumjeli.

Gdje god je moguÄ‡e, koristit Ä‡emo visokorazinske API-je radi jednostavnosti. MeÄ‘utim, vjerujemo da je vaÅ¾no razumjeti kako neuronske mreÅ¾e rade od temelja, stoga na poÄetku poÄinjemo raditi s niskorazinskim API-jem i tenzorima. MeÄ‘utim, ako Å¾elite brzo poÄeti i ne Å¾elite troÅ¡iti puno vremena na uÄenje tih detalja, moÅ¾ete ih preskoÄiti i odmah prijeÄ‡i na biljeÅ¾nice s visokorazinskim API-jem.

## âœï¸ VjeÅ¾be: Okviri

Nastavite svoje uÄenje u sljedeÄ‡im biljeÅ¾nicama:

Niskorazinski API | TensorFlow+Keras BiljeÅ¾nica | PyTorch
--------------|-------------------------------------|--------------------------------
Visokorazinski API| Keras | *PyTorch Lightning*

Nakon Å¡to svladate okvire, ponovimo pojam prenauÄenosti.

# PrenauÄenost

PrenauÄenost je izuzetno vaÅ¾an koncept u strojnom uÄenju, i vrlo je vaÅ¾no toÄno ga shvatiti!

Razmotrite sljedeÄ‡i problem aproksimacije 5 toÄaka (predstavljenih s `x` na grafovima ispod):

!linear | prenauÄenost
-------------------------|--------------------------
**Linearni model, 2 parametra** | **Nelinearni model, 7 parametara**
GreÅ¡ka na treningu = 5.3 | GreÅ¡ka na treningu = 0
GreÅ¡ka na validaciji = 5.1 | GreÅ¡ka na validaciji = 20

* S lijeve strane vidimo dobru aproksimaciju pravom linijom. BuduÄ‡i da je broj parametara adekvatan, model ispravno shvaÄ‡a distribuciju toÄaka.
* S desne strane, model je previÅ¡e moÄ‡an. BuduÄ‡i da imamo samo 5 toÄaka, a model ima 7 parametara, moÅ¾e se prilagoditi tako da prolazi kroz sve toÄke, ÄineÄ‡i greÅ¡ku na treningu 0. MeÄ‘utim, to sprjeÄava model da razumije ispravan uzorak podataka, stoga je greÅ¡ka na validaciji vrlo visoka.

Vrlo je vaÅ¾no postiÄ‡i ispravnu ravnoteÅ¾u izmeÄ‘u bogatstva modela (broja parametara) i broja uzoraka za trening.

## ZaÅ¡to dolazi do prenauÄenosti

  * Nedovoljno podataka za trening
  * PreviÅ¡e moÄ‡an model
  * PreviÅ¡e Å¡uma u ulaznim podacima

## Kako otkriti prenauÄenost

Kao Å¡to moÅ¾ete vidjeti iz gornjeg grafa, prenauÄenost se moÅ¾e otkriti vrlo niskom greÅ¡kom na treningu i visokom greÅ¡kom na validaciji. Normalno, tijekom treninga vidjet Ä‡emo kako greÅ¡ke na treningu i validaciji poÄinju opadati, a zatim u nekom trenutku greÅ¡ka na validaciji moÅ¾e prestati opadati i poÄeti rasti. Ovo Ä‡e biti znak prenauÄenosti i pokazatelj da bismo vjerojatno trebali zaustaviti trening u tom trenutku (ili barem napraviti snimku modela).

## Kako sprijeÄiti prenauÄenost

Ako vidite da dolazi do prenauÄenosti, moÅ¾ete uÄiniti jedno od sljedeÄ‡eg:

 * PoveÄ‡ajte koliÄinu podataka za trening
 * Smanjite sloÅ¾enost modela
 * Koristite neku tehniku regularizacije, kao Å¡to je Dropout, koju Ä‡emo razmotriti kasnije.

## PrenauÄenost i kompromis pristranosti-varijance

PrenauÄenost je zapravo sluÄaj opÄ‡enitijeg problema u statistici nazvanog kompromis pristranosti-varijance. Ako razmotrimo moguÄ‡e izvore pogreÅ¡ke u naÅ¡em modelu, moÅ¾emo vidjeti dvije vrste pogreÅ¡aka:

* **PogreÅ¡ke pristranosti** uzrokovane su time Å¡to naÅ¡ algoritam ne moÅ¾e ispravno uhvatiti odnos izmeÄ‘u podataka za trening. To moÅ¾e biti rezultat Äinjenice da naÅ¡ model nije dovoljno moÄ‡an (**nedovoljno uÄenje**).
* **PogreÅ¡ke varijance**, koje su uzrokovane time Å¡to model aproksimira Å¡um u ulaznim podacima umjesto smislenog odnosa (**prenauÄenost**).

Tijekom treninga, pogreÅ¡ka pristranosti opada (kako naÅ¡ model uÄi aproksimirati podatke), a pogreÅ¡ka varijance raste. VaÅ¾no je zaustaviti trening - bilo ruÄno (kada otkrijemo prenauÄenost) ili automatski (uvoÄ‘enjem regularizacije) - kako bismo sprijeÄili prenauÄenost.

## ZakljuÄak

U ovoj lekciji nauÄili ste o razlikama izmeÄ‘u raznih API-ja za dva najpopularnija AI okvira, TensorFlow i PyTorch. Osim toga, nauÄili ste o vrlo vaÅ¾noj temi, prenauÄenosti.

## ğŸš€ Izazov

U prateÄ‡im biljeÅ¾nicama pronaÄ‡i Ä‡ete 'zadaci' na dnu; proÄ‘ite kroz biljeÅ¾nice i dovrÅ¡ite zadatke.

## Pregled i samostalno uÄenje

IstraÅ¾ite sljedeÄ‡e teme:

- TensorFlow
- PyTorch
- PrenauÄenost

Postavite si sljedeÄ‡a pitanja:

- Koja je razlika izmeÄ‘u TensorFlow i PyTorch?
- Koja je razlika izmeÄ‘u prenauÄenosti i nedovoljnog uÄenja?

## Zadatak

U ovom laboratoriju traÅ¾i se da rijeÅ¡ite dva problema klasifikacije koristeÄ‡i jednostruke i viÅ¡eslojne potpuno povezane mreÅ¾e koristeÄ‡i PyTorch ili TensorFlow.

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden koriÅ¡tenjem AI usluge prevoÄ‘enja [Co-op Translator](https://github.com/Azure/co-op-translator). Iako teÅ¾imo toÄnosti, imajte na umu da automatizirani prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne odgovaramo za nesporazume ili pogreÅ¡ne interpretacije proizaÅ¡le iz koriÅ¡tenja ovog prijevoda.