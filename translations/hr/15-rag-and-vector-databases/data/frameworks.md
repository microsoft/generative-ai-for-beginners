<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:38:08+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "hr"
}
-->
# Neural Network Frameworks

Kao Å¡to smo veÄ‡ nauÄili, da bismo mogli uÄinkovito trenirati neuronske mreÅ¾e, potrebno je napraviti dvije stvari:

* Raditi s tenzorima, npr. mnoÅ¾iti, zbrajati i izraÄunavati funkcije poput sigmoid ili softmax
* IzraÄunati gradijente svih izraza kako bismo mogli provesti optimizaciju gradijentnim spustom

Iako `numpy` biblioteka moÅ¾e obaviti prvi dio, potrebna nam je mehanika za izraÄun gradijenata. U naÅ¡em okviru koji smo razvili u prethodnom poglavlju morali smo ruÄno programirati sve funkcije derivacija unutar metode `backward`, koja provodi backpropagation. Idealno bi bilo da nam okvir omoguÄ‡i izraÄun gradijenata *bilo kojeg izraza* koji definiramo.

JoÅ¡ jedna vaÅ¾na stvar je moguÄ‡nost izvoÄ‘enja izraÄuna na GPU-u ili drugim specijaliziranim raÄunalnim jedinicama, poput TPU-a. Trening dubokih neuronskih mreÅ¾a zahtijeva *puno* izraÄuna, a moguÄ‡nost paralelizacije tih izraÄuna na GPU-ima je vrlo vaÅ¾na.

> âœ… Pojam 'paralelizirati' znaÄi raspodijeliti izraÄune na viÅ¡e ureÄ‘aja.

Trenutno su dva najpopularnija okvira za neuronske mreÅ¾e: TensorFlow i PyTorch. Oba nude niskorazinski API za rad s tenzorima na CPU-u i GPU-u. Iznad niskorazinskog API-ja nalaze se i viÅ¡i API-ji, nazvani Keras i PyTorch Lightning.

Niskorazinski API | TensorFlow | PyTorch
-----------------|------------|---------
ViÅ¡erazinski API | Keras      | PyTorch

**Niskorazinski API-ji** u oba okvira omoguÄ‡uju izgradnju tzv. **raÄunalnih grafova**. Taj graf definira kako izraÄunati izlaz (obiÄno funkciju gubitka) za zadane ulazne parametre, i moÅ¾e se poslati na izraÄun na GPU ako je dostupan. Postoje funkcije za diferenciranje tog raÄunalnog grafa i izraÄun gradijenata, koji se potom koriste za optimizaciju parametara modela.

**ViÅ¡erazinski API-ji** tretiraju neuronske mreÅ¾e kao **niz slojeva** i znatno olakÅ¡avaju izgradnju veÄ‡ine neuronskih mreÅ¾a. Trening modela obiÄno zahtijeva pripremu podataka, a zatim pozivanje funkcije `fit` koja obavlja trening.

ViÅ¡erazinski API omoguÄ‡uje brzo sastavljanje tipiÄnih neuronskih mreÅ¾a bez brige o mnogim detaljima. Istovremeno, niskorazinski API pruÅ¾a puno veÄ‡u kontrolu nad procesom treniranja, zbog Äega se Äesto koristi u istraÅ¾ivanjima i radu s novim arhitekturama neuronskih mreÅ¾a.

VaÅ¾no je razumjeti da se oba API-ja mogu koristiti zajedno, npr. moÅ¾ete razviti vlastitu arhitekturu sloja koristeÄ‡i niskorazinski API, a zatim je koristiti unutar veÄ‡e mreÅ¾e sastavljene i trenirane viÅ¡erazinskim API-jem. Ili moÅ¾ete definirati mreÅ¾u kao niz slojeva viÅ¡erazinskim API-jem, a zatim koristiti vlastitu niskorazinsku petlju treniranja za optimizaciju. Oba API-ja dijele iste osnovne koncepte i dizajnirani su da dobro suraÄ‘uju.

## UÄenje

U ovom teÄaju nudimo veÄ‡inu sadrÅ¾aja za PyTorch i TensorFlow. MoÅ¾ete odabrati okvir koji vam viÅ¡e odgovara i pratiti samo pripadajuÄ‡e biljeÅ¾nice. Ako niste sigurni koji okvir odabrati, proÄitajte rasprave na internetu o **PyTorch vs. TensorFlow**. TakoÄ‘er moÅ¾ete pogledati oba okvira kako biste bolje razumjeli razlike.

Kad god je moguÄ‡e, koristit Ä‡emo viÅ¡erazinske API-je radi jednostavnosti. No smatramo da je vaÅ¾no razumjeti kako neuronske mreÅ¾e funkcioniraju od samih poÄetaka, pa stoga u poÄetku radimo s niskorazinskim API-jem i tenzorima. Ako Å¾elite brzo krenuti i ne Å¾elite troÅ¡iti puno vremena na detalje, moÅ¾ete preskoÄiti taj dio i odmah prijeÄ‡i na biljeÅ¾nice s viÅ¡erazinskim API-jem.

## âœï¸ VjeÅ¾be: Frameworks

Nastavite s uÄenjem u sljedeÄ‡im biljeÅ¾nicama:

Niskorazinski API | TensorFlow+Keras biljeÅ¾nica | PyTorch
-----------------|-------------------------------|---------
ViÅ¡erazinski API | Keras                         | *PyTorch Lightning*

Nakon Å¡to savladate okvire, ponovimo pojam overfittinga.

# Overfitting

Overfitting je iznimno vaÅ¾an pojam u strojnome uÄenju i vaÅ¾no ga je pravilno razumjeti!

Razmotrimo problem aproksimacije 5 toÄaka (oznaÄenih s `x` na donjim grafovima):

!linear | overfit
-------------------------|--------------------------
**Linearni model, 2 parametra** | **Nelinearni model, 7 parametara**
GreÅ¡ka na treningu = 5.3 | GreÅ¡ka na treningu = 0
GreÅ¡ka na validaciji = 5.1 | GreÅ¡ka na validaciji = 20

* S lijeve strane vidimo dobru aproksimaciju pravom linijom. BuduÄ‡i da je broj parametara primjeren, model dobro hvata raspored toÄaka.
* S desne strane model je premoÄ‡an. BuduÄ‡i da imamo samo 5 toÄaka, a model ima 7 parametara, moÅ¾e se prilagoditi tako da proÄ‘e kroz sve toÄke, ÄineÄ‡i greÅ¡ku na treningu 0. MeÄ‘utim, to sprjeÄava model da razumije pravi obrazac podataka, zbog Äega je greÅ¡ka na validaciji vrlo visoka.

Vrlo je vaÅ¾no pronaÄ‡i pravi balans izmeÄ‘u sloÅ¾enosti modela (broja parametara) i broja uzoraka za trening.

## ZaÅ¡to dolazi do overfittinga

  * Nedovoljno podataka za trening
  * PremoÄ‡an model
  * PreviÅ¡e Å¡uma u ulaznim podacima

## Kako otkriti overfitting

Kao Å¡to se vidi na gornjem grafu, overfitting se moÅ¾e prepoznati po vrlo niskoj greÅ¡ci na treningu i visokoj greÅ¡ci na validaciji. Tijekom treniranja obiÄno vidimo da se greÅ¡ke na treningu i validaciji smanjuju, a zatim u nekom trenutku greÅ¡ka na validaciji prestane padati i poÄne rasti. To je znak overfittinga i pokazatelj da bismo vjerojatno trebali prekinuti treniranje u tom trenutku (ili barem napraviti snimku modela).

overfitting

## Kako sprijeÄiti overfitting

Ako primijetite da dolazi do overfittinga, moÅ¾ete uÄiniti jedno od sljedeÄ‡eg:

 * PoveÄ‡ati koliÄinu podataka za trening
 * Smanjiti sloÅ¾enost modela
 * Koristiti neku tehniku regularizacije, poput Dropout-a, koju Ä‡emo kasnije razmotriti.

## Overfitting i kompromis izmeÄ‘u pristranosti i varijance

Overfitting je zapravo poseban sluÄaj opÄ‡eg problema u statistici nazvanog kompromis izmeÄ‘u pristranosti i varijance (Bias-Variance Tradeoff). Ako razmotrimo moguÄ‡e izvore pogreÅ¡ke u naÅ¡em modelu, moÅ¾emo razlikovati dvije vrste pogreÅ¡aka:

* **PogreÅ¡ke pristranosti (Bias errors)** nastaju kada naÅ¡ algoritam ne moÅ¾e pravilno uhvatiti odnos u podacima za trening. To moÅ¾e biti zato Å¡to model nije dovoljno moÄ‡an (**underfitting**).
* **PogreÅ¡ke varijance (Variance errors)** nastaju kada model aproksimira Å¡um u ulaznim podacima umjesto stvarnog odnosa (**overfitting**).

Tijekom treniranja, pogreÅ¡ka pristranosti opada (kako model uÄi podatke), a pogreÅ¡ka varijance raste. VaÅ¾no je prekinuti treniranje â€“ ruÄno (kad otkrijemo overfitting) ili automatski (uvoÄ‘enjem regularizacije) â€“ kako bismo sprijeÄili overfitting.

## ZakljuÄak

U ovoj lekciji ste nauÄili o razlikama izmeÄ‘u razliÄitih API-ja za dva najpopularnija AI okvira, TensorFlow i PyTorch. TakoÄ‘er ste upoznati s vrlo vaÅ¾nom temom, overfittingom.

## ğŸš€ Izazov

U prateÄ‡im biljeÅ¾nicama naÄ‡i Ä‡ete 'zadace' na dnu; radite kroz biljeÅ¾nice i dovrÅ¡ite zadatke.

## Pregled i samostalno uÄenje

IstraÅ¾ite sljedeÄ‡e teme:

- TensorFlow
- PyTorch
- Overfitting

Postavite si sljedeÄ‡a pitanja:

- Koja je razlika izmeÄ‘u TensorFlow i PyTorch?
- Koja je razlika izmeÄ‘u overfittinga i underfittinga?

## Zadatak

U ovom laboratoriju trebate rijeÅ¡iti dva problema klasifikacije koristeÄ‡i jednoslojne i viÅ¡eslojne potpuno povezane mreÅ¾e koristeÄ‡i PyTorch ili TensorFlow.

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden koriÅ¡tenjem AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako teÅ¾imo toÄnosti, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kritiÄne informacije preporuÄuje se profesionalni ljudski prijevod. Ne snosimo odgovornost za bilo kakva nesporazuma ili pogreÅ¡na tumaÄenja koja proizlaze iz koriÅ¡tenja ovog prijevoda.