<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-07-09T17:01:41+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "hr"
}
-->
# Uvod u neuronske mreÅ¾e: Perceptron

Jedan od prvih pokuÅ¡aja implementacije neÄega sliÄnog modernoj neuronskoj mreÅ¾i napravio je Frank Rosenblatt iz Cornell Aeronautical Laboratory 1957. godine. Bila je to hardverska implementacija nazvana "Mark-1", dizajnirana za prepoznavanje primitivnih geometrijskih oblika, poput trokuta, kvadrata i krugova.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> Slike s Wikipedije

Ulazna slika predstavljena je nizom od 20x20 fotocelija, tako da je neuronska mreÅ¾a imala 400 ulaza i jedan binarni izlaz. Jednostavna mreÅ¾a sadrÅ¾avala je jedan neuron, takoÄ‘er nazvan **threshold logic unit**. TeÅ¾ine neuronske mreÅ¾e djelovale su poput potenciometara koji su zahtijevali ruÄno podeÅ¡avanje tijekom faze uÄenja.

> âœ… Potenciometar je ureÄ‘aj koji korisniku omoguÄ‡uje podeÅ¡avanje otpora u krugu.

> The New York Times je tada pisao o perceptronu: *zametak elektroniÄkog raÄunala za koje [Mornarica] oÄekuje da Ä‡e moÄ‡i hodati, govoriti, vidjeti, pisati, reproducirati se i biti svjesno svog postojanja.*

## Model perceptrona

Pretpostavimo da u naÅ¡em modelu imamo N znaÄajki, u kojem sluÄaju bi ulazni vektor bio vektor veliÄine N. Perceptron je model **binarne klasifikacije**, tj. moÅ¾e razlikovati dvije klase ulaznih podataka. Pretpostavit Ä‡emo da za svaki ulazni vektor x izlaz naÅ¡eg perceptrona moÅ¾e biti +1 ili -1, ovisno o klasi. Izlaz se raÄuna pomoÄ‡u formule:

y(x) = f(w<sup>T</sup>x)

gdje je f funkcija aktivacije stepenice

## Trening perceptrona

Za treniranje perceptrona potrebno je pronaÄ‡i vektor teÅ¾ina w koji ispravno klasificira veÄ‡inu vrijednosti, tj. rezultira najmanjom **pogreÅ¡kom**. Ta se pogreÅ¡ka definira pomoÄ‡u **perceptron kriterija** na sljedeÄ‡i naÄin:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

gdje:

* zbroj se raÄuna preko onih podataka za uÄenje i koji rezultiraju pogreÅ¡nom klasifikacijom
* x<sub>i</sub> su ulazni podaci, a t<sub>i</sub> je ili -1 ili +1 za negativne odnosno pozitivne primjere.

Ovaj kriterij se smatra funkcijom teÅ¾ina w, i potrebno ga je minimizirati. ÄŒesto se koristi metoda zvana **gradijentni spust**, pri Äemu zapoÄinjemo s nekim poÄetnim teÅ¾inama w<sup>(0)</sup>, a zatim na svakom koraku aÅ¾uriramo teÅ¾ine prema formuli:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

Ovdje je Î· tzv. **stopa uÄenja**, a âˆ‡E(w) oznaÄava **gradijent** funkcije E. Nakon izraÄuna gradijenta, dobivamo

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Algoritam u Pythonu izgleda ovako:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## ZakljuÄak

U ovoj lekciji ste nauÄili o perceptronu, modelu binarne klasifikacije, i kako ga trenirati koristeÄ‡i vektor teÅ¾ina.

## ğŸš€ Izazov

Ako Å¾elite pokuÅ¡ati izgraditi vlastiti perceptron, isprobajte ovaj laboratorij na Microsoft Learn koji koristi Azure ML dizajner


## Pregled i samostalno uÄenje

Da biste vidjeli kako moÅ¾emo koristiti perceptron za rjeÅ¡avanje jednostavnih problema kao i problema iz stvarnog Å¾ivota, te nastavili s uÄenjem - posjetite Perceptron biljeÅ¾nicu.

Evo i zanimljivog Älanka o perceptronima.

## Zadatak

U ovoj lekciji implementirali smo perceptron za zadatak binarne klasifikacije i koristili ga za klasifikaciju izmeÄ‘u dvije rukom napisane znamenke. U ovom laboratoriju traÅ¾i se da u potpunosti rijeÅ¡ite problem klasifikacije znamenki, tj. odredite koja znamenka najvjerojatnije odgovara danoj slici.

* Upute
* BiljeÅ¾nica

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden koriÅ¡tenjem AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako teÅ¾imo toÄnosti, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kritiÄne informacije preporuÄuje se profesionalni ljudski prijevod. Ne snosimo odgovornost za bilo kakve nesporazume ili pogreÅ¡na tumaÄenja koja proizlaze iz koriÅ¡tenja ovog prijevoda.