<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-05-20T06:44:28+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "hr"
}
-->
# Uvod u Neuronske MreÅ¾e: Perceptron

Jedan od prvih pokuÅ¡aja implementacije neÄega sliÄnog modernoj neuronskoj mreÅ¾i napravio je Frank Rosenblatt iz Cornell Aeronautical Laboratory 1957. godine. Bila je to hardverska implementacija nazvana "Mark-1", dizajnirana za prepoznavanje primitivnih geometrijskih figura, kao Å¡to su trokuti, kvadrati i krugovi.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> Slike s Wikipedije

Ulazna slika bila je predstavljena nizom fotocelija 20x20, tako da je neuronska mreÅ¾a imala 400 ulaza i jedan binarni izlaz. Jednostavna mreÅ¾a sadrÅ¾avala je jedan neuron, takoÄ‘er nazvan **jedinica logiÄkog praga**. TeÅ¾ine neuronske mreÅ¾e djelovale su kao potenciometri koji su zahtijevali ruÄno podeÅ¡avanje tijekom faze treniranja.

> âœ… Potenciometar je ureÄ‘aj koji omoguÄ‡uje korisniku podeÅ¡avanje otpornosti kruga.

> The New York Times je tada pisao o perceptronu: *embrij elektroniÄkog raÄunala za koje [mornarica] oÄekuje da Ä‡e moÄ‡i hodati, govoriti, vidjeti, pisati, reproducirati se i biti svjestan svog postojanja.*

## Model Perceptrona

Pretpostavimo da imamo N znaÄajki u naÅ¡em modelu, u kojem sluÄaju ulazni vektor bi bio vektor veliÄine N. Perceptron je model **binarne klasifikacije**, tj. moÅ¾e razlikovati dvije klase ulaznih podataka. Pretpostavit Ä‡emo da za svaki ulazni vektor x izlaz naÅ¡eg perceptrona bi bio ili +1 ili -1, ovisno o klasi. Izlaz Ä‡e se izraÄunati pomoÄ‡u formule:

y(x) = f(w<sup>T</sup>x)

gdje je f funkcija aktivacije koraka

## Treniranje Perceptrona

Da bismo trenirali perceptron, moramo pronaÄ‡i vektor teÅ¾ina w koji ispravno klasificira veÄ‡inu vrijednosti, tj. rezultira najmanjom **greÅ¡kom**. Ova greÅ¡ka je definirana **kriterijem perceptrona** na sljedeÄ‡i naÄin:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

gdje:

* suma se uzima na onim toÄkama podataka za treniranje i koje rezultiraju pogreÅ¡nom klasifikacijom
* x<sub>i</sub> je ulazni podatak, a t<sub>i</sub> je ili -1 ili +1 za negativne i pozitivne primjere.

Ovaj kriterij se smatra funkcijom teÅ¾ina w, i trebamo ga minimizirati. ÄŒesto se koristi metoda nazvana **gradijentni spust**, u kojoj poÄinjemo s nekim poÄetnim teÅ¾inama w<sup>(0)</sup>, a zatim u svakom koraku aÅ¾uriramo teÅ¾ine prema formuli:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

Ovdje je Î· tzv. **stopa uÄenja**, a âˆ‡E(w) oznaÄava **gradijent** E. Nakon Å¡to izraÄunamo gradijent, zavrÅ¡avamo s

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Algoritam u Pythonu izgleda ovako:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## ZakljuÄak

U ovoj lekciji ste nauÄili o perceptronu, koji je model binarne klasifikacije, i kako ga trenirati koristeÄ‡i vektor teÅ¾ina.

## ğŸš€ Izazov

Ako Å¾elite pokuÅ¡ati izgraditi vlastiti perceptron, isprobajte ovaj laboratorij na Microsoft Learn koji koristi Azure ML designer.

## Pregled i Samostalno UÄenje

Da biste vidjeli kako moÅ¾emo koristiti perceptron za rjeÅ¡avanje jednostavnih problema kao i problema iz stvarnog Å¾ivota, i nastaviti s uÄenjem - posjetite Perceptron biljeÅ¾nicu.

Evo zanimljivog Älanka o perceptronima.

## Zadatak

U ovoj lekciji smo implementirali perceptron za zadatak binarne klasifikacije, i koristili smo ga za klasifikaciju izmeÄ‘u dvije rukom pisane znamenke. U ovom laboratoriju, od vas se traÅ¾i da rijeÅ¡ite problem klasifikacije znamenki u cijelosti, tj. odredite koja je znamenka najvjerojatnije povezana s danom slikom.

* Upute
* BiljeÅ¾nica

**Odricanje odgovornosti**:  
Ovaj dokument je preveden koristeÄ‡i AI uslugu prevoÄ‘enja [Co-op Translator](https://github.com/Azure/co-op-translator). Iako teÅ¾imo toÄnosti, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kritiÄne informacije preporuÄuje se profesionalni prijevod od strane Äovjeka. Ne odgovaramo za bilo kakve nesporazume ili pogreÅ¡ne interpretacije koje proizlaze iz koriÅ¡tenja ovog prijevoda.