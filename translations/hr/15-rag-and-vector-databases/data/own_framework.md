<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:27:59+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "hr"
}
-->
# Uvod u Neuronske MreÅ¾e. ViÅ¡eslojni Perceptron

U prethodnom dijelu nauÄili ste o najjednostavnijem modelu neuronske mreÅ¾e - jednoslojnom perceptronu, linearnom modelu za klasifikaciju s dvije klase.

U ovom dijelu proÅ¡irit Ä‡emo ovaj model u fleksibilniji okvir, Å¡to nam omoguÄ‡uje:

* izvoÄ‘enje **viÅ¡eklasne klasifikacije** uz klasifikaciju s dvije klase
* rjeÅ¡avanje **problema regresije** uz klasifikaciju
* razdvajanje klasa koje nisu linearno razdvojive

TakoÄ‘er Ä‡emo razviti vlastiti modularni okvir u Pythonu koji Ä‡e nam omoguÄ‡iti konstrukciju razliÄitih arhitektura neuronskih mreÅ¾a.

## Formalizacija Strojnog UÄenja

PoÄnimo s formalizacijom problema Strojnog UÄenja. Pretpostavimo da imamo skup podataka za treniranje **X** s oznakama **Y**, i trebamo izgraditi model *f* koji Ä‡e davati najtoÄnije predikcije. Kvaliteta predikcija mjeri se pomoÄ‡u **funkcije gubitka** â„’. SljedeÄ‡e funkcije gubitka Äesto se koriste:

* Za problem regresije, kada trebamo predvidjeti broj, moÅ¾emo koristiti **apsolutnu pogreÅ¡ku** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, ili **kvadratnu pogreÅ¡ku** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Za klasifikaciju, koristimo **0-1 gubitak** (Å¡to je u biti isto Å¡to i **toÄnost** modela), ili **logistiÄki gubitak**.

Za jednoslojni perceptron, funkcija *f* bila je definirana kao linearna funkcija *f(x)=wx+b* (ovdje je *w* matrica teÅ¾ina, *x* je vektor ulaznih znaÄajki, a *b* je vektor pristranosti). Za razliÄite arhitekture neuronskih mreÅ¾a, ova funkcija moÅ¾e poprimiti sloÅ¾eniji oblik.

> U sluÄaju klasifikacije, Äesto je poÅ¾eljno dobiti vjerojatnosti odgovarajuÄ‡ih klasa kao izlaz mreÅ¾e. Za pretvaranje proizvoljnih brojeva u vjerojatnosti (npr. za normalizaciju izlaza), Äesto koristimo **softmax** funkciju Ïƒ, i funkcija *f* postaje *f(x)=Ïƒ(wx+b)*

U definiciji *f* iznad, *w* i *b* nazivaju se **parametri** Î¸=âŸ¨*w,b*âŸ©. S obzirom na skup podataka âŸ¨**X**,**Y**âŸ©, moÅ¾emo izraÄunati ukupnu pogreÅ¡ku na cijelom skupu podataka kao funkciju parametara Î¸.

> âœ… **Cilj treniranja neuronske mreÅ¾e je minimizirati pogreÅ¡ku mijenjajuÄ‡i parametre Î¸**

## Optimizacija Gradijentnim Spustom

Postoji dobro poznata metoda optimizacije funkcije nazvana **gradijentni spust**. Ideja je da moÅ¾emo izraÄunati derivat (u viÅ¡edimenzionalnom sluÄaju zvan **gradijent**) funkcije gubitka s obzirom na parametre, i mijenjati parametre na naÄin da se pogreÅ¡ka smanjuje. Ovo se moÅ¾e formalizirati na sljedeÄ‡i naÄin:

* Inicijalizirajte parametre s nekim sluÄajnim vrijednostima w<sup>(0)</sup>, b<sup>(0)</sup>
* Ponavljajte sljedeÄ‡i korak mnogo puta:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Tijekom treniranja, koraci optimizacije trebali bi se raÄunati uzimajuÄ‡i u obzir cijeli skup podataka (sjetite se da se gubitak raÄuna kao zbroj kroz sve uzorke za treniranje). MeÄ‘utim, u stvarnom Å¾ivotu uzimamo male dijelove skupa podataka zvane **minibatches**, i raÄunamo gradijente na temelju podskupa podataka. BuduÄ‡i da se podskup uzima nasumiÄno svaki put, takva metoda naziva se **stohastiÄki gradijentni spust** (SGD).

## ViÅ¡eslojni Perceptroni i Unatragna Propagacija

Jednoslojna mreÅ¾a, kao Å¡to smo vidjeli gore, sposobna je klasificirati linearno razdvojive klase. Da bismo izgradili bogatiji model, moÅ¾emo kombinirati nekoliko slojeva mreÅ¾e. MatematiÄki bi to znaÄilo da bi funkcija *f* imala sloÅ¾eniji oblik i bila bi izraÄunata u nekoliko koraka:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Ovdje je Î± **nelinearna aktivacijska funkcija**, Ïƒ je softmax funkcija, a parametri Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritam gradijentnog spusta ostao bi isti, ali bi bilo teÅ¾e izraÄunati gradijente. S obzirom na pravilo lanÄanog diferenciranja, moÅ¾emo izraÄunati derivate kao:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Pravilo lanÄanog diferenciranja koristi se za izraÄunavanje derivata funkcije gubitka s obzirom na parametre.

Napomena da je lijevi dio svih tih izraza isti, i stoga moÅ¾emo uÄinkovito izraÄunati derivate poÄevÅ¡i od funkcije gubitka i iduÄ‡i "unatrag" kroz raÄunski graf. Stoga se metoda treniranja viÅ¡eslojnog perceptrona naziva **unatragna propagacija**, ili 'backprop'.

> TODO: citiranje slike

> âœ… Detaljnije Ä‡emo obraditi unatragnu propagaciju u naÅ¡em primjeru biljeÅ¾nice.

## ZakljuÄak

U ovoj lekciji izgradili smo vlastitu biblioteku neuronskih mreÅ¾a i koristili smo je za jednostavan zadatak klasifikacije u dvije dimenzije.

## ğŸš€ Izazov

U prateÄ‡oj biljeÅ¾nici implementirat Ä‡ete vlastiti okvir za izgradnju i treniranje viÅ¡eslojnih perceptrona. MoÄ‡i Ä‡ete vidjeti detaljno kako moderne neuronske mreÅ¾e djeluju.

PrijeÄ‘ite na biljeÅ¾nicu OwnFramework i proÄ‘ite kroz nju.

## Pregled i Samostalno UÄenje

Unatragna propagacija je uobiÄajeni algoritam koji se koristi u AI i ML, vrijedan detaljnijeg prouÄavanja.

## Zadatak

U ovom laboratoriju, traÅ¾i se da koristite okvir koji ste izgradili u ovoj lekciji za rjeÅ¡avanje klasifikacije rukom pisanih znamenki MNIST.

* Upute
* BiljeÅ¾nica

**Odricanje odgovornosti**:  
Ovaj dokument je preveden koristeÄ‡i AI uslugu prevoÄ‘enja [Co-op Translator](https://github.com/Azure/co-op-translator). Iako se trudimo da postignemo taÄnost, molimo vas da budete svjesni da automatski prijevodi mogu sadrÅ¾avati greÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane Äovjeka. Ne snosimo odgovornost za nesporazume ili pogreÅ¡ne interpretacije koji mogu proizaÄ‡i iz koriÅ¡tenja ovog prijevoda.