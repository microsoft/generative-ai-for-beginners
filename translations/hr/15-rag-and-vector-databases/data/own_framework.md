<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:51:56+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "hr"
}
-->
# Uvod u neuronske mreÅ¾e. ViÅ¡eslojni perceptron

U prethodnom dijelu ste nauÄili o najjednostavnijem modelu neuronske mreÅ¾e - jednoslojnom perceptronu, linearnom modelu za klasifikaciju u dvije klase.

U ovom dijelu proÅ¡irit Ä‡emo taj model u fleksibilniji okvir koji nam omoguÄ‡uje:

* izvoÄ‘enje **viÅ¡eklasne klasifikacije** uz dvoklasnu
* rjeÅ¡avanje **regresijskih problema** uz klasifikaciju
* razdvajanje klasa koje nisu linearno razdvojive

TakoÄ‘er Ä‡emo razviti vlastiti modularni okvir u Pythonu koji Ä‡e nam omoguÄ‡iti izgradnju razliÄitih arhitektura neuronskih mreÅ¾a.

## Formalizacija strojnog uÄenja

ZapoÄnimo formalizacijom problema strojnog uÄenja. Pretpostavimo da imamo skup podataka za uÄenje **X** s oznakama **Y**, i trebamo izgraditi model *f* koji Ä‡e davati Å¡to toÄnije predikcije. Kvaliteta predikcija mjeri se pomoÄ‡u **funkcije gubitka** â„’. ÄŒesto se koriste sljedeÄ‡e funkcije gubitka:

* Za regresijski problem, kada trebamo predvidjeti broj, moÅ¾emo koristiti **apsolutnu pogreÅ¡ku** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| ili **kvadratnu pogreÅ¡ku** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Za klasifikaciju koristimo **0-1 gubitak** (Å¡to je u biti isto kao **toÄnost** modela) ili **logistiÄki gubitak**.

Za jednoslojni perceptron, funkcija *f* definirana je kao linearna funkcija *f(x)=wx+b* (ovdje je *w* matrica teÅ¾ina, *x* vektor ulaznih znaÄajki, a *b* vektor pristranosti). Za razliÄite arhitekture neuronskih mreÅ¾a, ta funkcija moÅ¾e imati sloÅ¾eniji oblik.

> U sluÄaju klasifikacije Äesto je poÅ¾eljno dobiti vjerojatnosti pripadajuÄ‡ih klasa kao izlaz mreÅ¾e. Za pretvaranje proizvoljnih brojeva u vjerojatnosti (npr. za normalizaciju izlaza) Äesto koristimo **softmax** funkciju Ïƒ, pa funkcija *f* postaje *f(x)=Ïƒ(wx+b)*

U definiciji *f* gore, *w* i *b* nazivaju se **parametri** Î¸=âŸ¨*w,b*âŸ©. S obzirom na skup podataka âŸ¨**X**,**Y**âŸ©, moÅ¾emo izraÄunati ukupnu pogreÅ¡ku na cijelom skupu kao funkciju parametara Î¸.

> âœ… **Cilj treniranja neuronske mreÅ¾e je minimizirati pogreÅ¡ku mijenjanjem parametara Î¸**

## Optimizacija gradijentnim spustom

Postoji poznata metoda optimizacije funkcija nazvana **gradijentni spust**. Ideja je da moÅ¾emo izraÄunati derivaciju (u viÅ¡edimenzionalnom sluÄaju nazvanu **gradijent**) funkcije gubitka u odnosu na parametre, i mijenjati parametre tako da se pogreÅ¡ka smanjuje. To se moÅ¾e formalizirati na sljedeÄ‡i naÄin:

* Inicijaliziraj parametre nekim sluÄajnim vrijednostima w<sup>(0)</sup>, b<sup>(0)</sup>
* Ponavljaj sljedeÄ‡i korak mnogo puta:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Tijekom treniranja, koraci optimizacije trebaju se raÄunati uzimajuÄ‡i u obzir cijeli skup podataka (sjetite se da se gubitak raÄuna kao suma kroz sve uzorke za uÄenje). MeÄ‘utim, u praksi uzimamo male dijelove skupa podataka nazvane **minibatches** i raÄunamo gradijente na temelju podskupa podataka. BuduÄ‡i da se podskup nasumiÄno bira svaki put, takva metoda naziva se **stohastiÄki gradijentni spust** (SGD).

## ViÅ¡eslojni perceptroni i backpropagation

Jednoslojna mreÅ¾a, kao Å¡to smo vidjeli, moÅ¾e klasificirati linearno razdvojive klase. Za izgradnju bogatijeg modela moÅ¾emo kombinirati viÅ¡e slojeva mreÅ¾e. MatematiÄki to znaÄi da funkcija *f* ima sloÅ¾eniji oblik i raÄuna se u nekoliko koraka:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Ovdje je Î± **nelinearna aktivacijska funkcija**, Ïƒ je softmax funkcija, a parametri Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Algoritam gradijentnog spusta ostaje isti, ali je teÅ¾e izraÄunati gradijente. Primjenom pravila lanÄane derivacije moÅ¾emo izraÄunati derivacije kao:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Pravilo lanÄane derivacije koristi se za izraÄun derivacija funkcije gubitka u odnosu na parametre.

Primijetite da je lijevi dio svih ovih izraza isti, pa moÅ¾emo uÄinkovito raÄunati derivacije poÄevÅ¡i od funkcije gubitka i iduÄ‡i "unatrag" kroz raÄunski graf. Zato se metoda treniranja viÅ¡eslojnog perceptrona naziva **backpropagation**, ili 'backprop'.

> TODO: citat slike

> âœ… Backpropagation Ä‡emo detaljnije obraditi u naÅ¡em primjeru u biljeÅ¾nici.

## ZakljuÄak

U ovoj lekciji izgradili smo vlastitu biblioteku za neuronske mreÅ¾e i koristili je za jednostavan zadatak klasifikacije u dvije dimenzije.

## ğŸš€ Izazov

U prateÄ‡oj biljeÅ¾nici implementirat Ä‡ete vlastiti okvir za izgradnju i treniranje viÅ¡eslojnih perceptrona. MoÄ‡i Ä‡ete detaljno vidjeti kako moderne neuronske mreÅ¾e funkcioniraju.

Nastavite na OwnFramework biljeÅ¾nicu i radite kroz nju.

## Pregled i samostalno uÄenje

Backpropagation je uobiÄajeni algoritam u AI i ML, vrijedi ga prouÄiti detaljnije.

## Zadatak

U ovom laboratoriju trebate koristiti okvir koji ste izgradili u ovoj lekciji za rjeÅ¡avanje problema klasifikacije rukom pisanih znamenki MNIST.

* Upute
* BiljeÅ¾nica

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden koriÅ¡tenjem AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako teÅ¾imo toÄnosti, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati sluÅ¾benim i autoritativnim izvorom. Za vaÅ¾ne informacije preporuÄuje se profesionalni ljudski prijevod. Ne snosimo odgovornost za bilo kakva nesporazuma ili pogreÅ¡na tumaÄenja koja proizlaze iz koriÅ¡tenja ovog prijevoda.