<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "807f0d9fc1747e796433534e1be6a98a",
  "translation_date": "2025-10-18T01:34:32+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "hr"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.hr.png)](https://youtu.be/6UAwhL9Q-TQ?si=5jJd8yeQsCfJ97em)

# Fino pode코avanje va코eg LLM-a

Kori코tenje velikih jezi캜nih modela za izgradnju generativnih AI aplikacija donosi nove izazove. Klju캜no pitanje je osigurati kvalitetu odgovora (to캜nost i relevantnost) u sadr쬬ju koji model generira za odre캠eni korisni캜ki zahtjev. U prethodnim lekcijama raspravljali smo o tehnikama poput oblikovanja upita (prompt engineering) i generacije uz pro코ireno pretra쬴vanje, koje poku코avaju rije코iti problem _modificiranjem ulaznog upita_ postoje캖em modelu.

U dana코njoj lekciji raspravljamo o tre캖oj tehnici, **fino pode코avanje**, koja poku코ava rije코iti izazov _ponovnim treniranjem samog modela_ s dodatnim podacima. Zaronimo u detalje.

## Ciljevi u캜enja

Ova lekcija uvodi koncept finog pode코avanja unaprijed treniranih jezi캜nih modela, istra쬿je prednosti i izazove ovog pristupa te pru쬬 smjernice o tome kada i kako koristiti fino pode코avanje za pobolj코anje performansi va코ih generativnih AI modela.

Na kraju ove lekcije trebali biste mo캖i odgovoriti na sljede캖a pitanja:

- 맚o je fino pode코avanje jezi캜nih modela?
- Kada i za코to je fino pode코avanje korisno?
- Kako mogu fino podesiti unaprijed trenirani model?
- Koja su ograni캜enja finog pode코avanja?

Spremni? Krenimo.

## Ilustrirani vodi캜

콯elite li dobiti 코iru sliku o tome 코to 캖emo pokriti prije nego 코to zaronimo u detalje? Pogledajte ovaj ilustrirani vodi캜 koji opisuje put u캜enja za ovu lekciju - od u캜enja osnovnih pojmova i motivacije za fino pode코avanje, do razumijevanja procesa i najboljih praksi za izvr코avanje zadatka finog pode코avanja. Ovo je fascinantna tema za istra쬴vanje, stoga ne zaboravite provjeriti [Resurse](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) za dodatne poveznice koje 캖e podr쬬ti va코e samostalno u캜enje!

![Ilustrirani vodi캜 za fino pode코avanje jezi캜nih modela](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.hr.png)

## 맚o je fino pode코avanje jezi캜nih modela?

Po definiciji, veliki jezi캜ni modeli su _unaprijed trenirani_ na velikim koli캜inama teksta prikupljenog iz raznih izvora, uklju캜uju캖i internet. Kao 코to smo nau캜ili u prethodnim lekcijama, potrebne su nam tehnike poput _oblikovanja upita_ i _generacije uz pro코ireno pretra쬴vanje_ kako bismo pobolj코ali kvalitetu odgovora modela na korisni캜ka pitanja ("upite").

Popularna tehnika oblikovanja upita uklju캜uje davanje modelu vi코e smjernica o tome 코to se o캜ekuje u odgovoru, bilo pru쬬njem _uputa_ (eksplicitne smjernice) ili _davanjem nekoliko primjera_ (implicitne smjernice). To se naziva _u캜enje s nekoliko primjera_ (few-shot learning), ali ima dva ograni캜enja:

- Ograni캜enja broja tokena u modelu mogu ograni캜iti broj primjera koje mo쬰te dati i smanjiti u캜inkovitost.
- Tro코kovi tokena mogu u캜initi skupo dodavanje primjera svakom upitu i ograni캜iti fleksibilnost.

Fino pode코avanje je uobi캜ajena praksa u sustavima strojnog u캜enja gdje uzimamo unaprijed trenirani model i ponovno ga treniramo s novim podacima kako bismo pobolj코ali njegovu izvedbu za odre캠eni zadatak. U kontekstu jezi캜nih modela, mo쬰mo fino podesiti unaprijed trenirani model _s pa쬷jivo odabranim skupom primjera za odre캠eni zadatak ili domenu primjene_ kako bismo stvorili **prilago캠eni model** koji mo쬰 biti precizniji i relevantniji za taj specifi캜ni zadatak ili domenu. Dodatna prednost finog pode코avanja je da mo쬰 smanjiti broj potrebnih primjera za u캜enje s nekoliko primjera - smanjuju캖i kori코tenje tokena i povezane tro코kove.

## Kada i za코to bismo trebali fino pode코avati modele?

U _ovom_ kontekstu, kada govorimo o finom pode코avanju, mislimo na **supervizirano** fino pode코avanje gdje se ponovno treniranje provodi **dodavanjem novih podataka** koji nisu bili dio originalnog skupa podataka za treniranje. To se razlikuje od pristupa nesuperviziranog finog pode코avanja gdje se model ponovno trenira na originalnim podacima, ali s razli캜itim hiperparametrima.

Klju캜no je zapamtiti da je fino pode코avanje napredna tehnika koja zahtijeva odre캠enu razinu stru캜nosti kako bi se postigli 쬰ljeni rezultati. Ako se ne provede ispravno, mo쬯a ne캖e pru쬴ti o캜ekivana pobolj코anja, pa 캜ak mo쬰 i pogor코ati performanse modela za va코u ciljanu domenu.

Dakle, prije nego 코to nau캜ite "kako" fino podesiti jezi캜ne modele, trebate znati "za코to" biste trebali krenuti tim putem i "kada" zapo캜eti proces finog pode코avanja. Po캜nite postavljanjem ovih pitanja:

- **Slu캜aj upotrebe**: Koji je va코 _slu캜aj upotrebe_ za fino pode코avanje? Koji aspekt trenutnog unaprijed treniranog modela 쬰lite pobolj코ati?
- **Alternative**: Jeste li poku코ali _druge tehnike_ za postizanje 쬰ljenih rezultata? Koristite ih za stvaranje osnovne usporedbe.
  - Oblikovanje upita: Poku코ajte s tehnikama poput u캜enja s nekoliko primjera koriste캖i primjere relevantnih odgovora na upite. Procijenite kvalitetu odgovora.
  - Generacija uz pro코ireno pretra쬴vanje: Poku코ajte pro코iriti upite rezultatima pretra쬴vanja va코ih podataka. Procijenite kvalitetu odgovora.
- **Tro코kovi**: Jeste li identificirali tro코kove finog pode코avanja?
  - Mogu캖nost pode코avanja - je li unaprijed trenirani model dostupan za fino pode코avanje?
  - Napor - za pripremu podataka za treniranje, evaluaciju i pobolj코anje modela.
  - Ra캜unalni resursi - za pokretanje poslova finog pode코avanja i implementaciju fino pode코enog modela.
  - Podaci - pristup dovoljnim kvalitetnim primjerima za utjecaj finog pode코avanja.
- **Prednosti**: Jeste li potvrdili prednosti finog pode코avanja?
  - Kvaliteta - je li fino pode코eni model nadma코io osnovnu usporedbu?
  - Tro코ak - smanjuje li kori코tenje tokena pojednostavljivanjem upita?
  - Pro코irivost - mo쬰te li ponovno koristiti osnovni model za nove domene?

Odgovaranjem na ova pitanja trebali biste mo캖i odlu캜iti je li fino pode코avanje pravi pristup za va코 slu캜aj upotrebe. Idealno, pristup je valjan samo ako prednosti nadma코uju tro코kove. Kada odlu캜ite nastaviti, vrijeme je da razmislite o tome _kako_ mo쬰te fino podesiti unaprijed trenirani model.

콯elite li dobiti vi코e uvida u proces dono코enja odluka? Pogledajte [Fino pode코avanje ili ne fino pode코avanje](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Kako mo쬰mo fino podesiti unaprijed trenirani model?

Za fino pode코avanje unaprijed treniranog modela, trebate imati:

- unaprijed trenirani model za fino pode코avanje
- skup podataka za fino pode코avanje
- okru쬰nje za treniranje za pokretanje posla finog pode코avanja
- okru쬰nje za hosting za implementaciju fino pode코enog modela

## Fino pode코avanje u praksi

Sljede캖i resursi pru쬬ju korak-po-korak vodi캜e koji vas vode kroz stvarni primjer kori코tenja odabranog modela s pa쬷jivo odabranim skupom podataka. Za rad kroz ove vodi캜e, trebate ra캜un na odre캠enom pru쬬telju usluga, zajedno s pristupom relevantnom modelu i skupovima podataka.

| Pru쬬telj     | Vodi캜                                                                                                                                                                       | Opis                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI        | [Kako fino podesiti chat modele](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)           | Nau캜ite fino podesiti `gpt-35-turbo` za odre캠enu domenu ("asistent za recepte") pripremom podataka za treniranje, pokretanjem posla finog pode코avanja i kori코tenjem fino pode코enog modela za zaklju캜ivanje.                                                                                                                                                                                                                     |
| Azure OpenAI  | [GPT 3.5 Turbo vodi캜 za fino pode코avanje](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | Nau캜ite fino podesiti model `gpt-35-turbo-0613` **na Azureu** poduzimanjem koraka za kreiranje i u캜itavanje podataka za treniranje, pokretanje posla finog pode코avanja. Implementirajte i koristite novi model.                                                                                                                                                                                                                                                        |
| Hugging Face  | [Fino pode코avanje LLM-a s Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                         | Ovaj blog post vodi vas kroz fino pode코avanje _otvorenog LLM-a_ (npr. `CodeLlama 7B`) koriste캖i [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteku i [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) s otvorenim [skupovima podataka](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) na Hugging Face. |
|               |                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 游뱅 AutoTrain  | [Fino pode코avanje LLM-a s AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                   | AutoTrain (ili AutoTrain Advanced) je python biblioteka koju je razvio Hugging Face i omogu캖uje fino pode코avanje za mnoge razli캜ite zadatke, uklju캜uju캖i fino pode코avanje LLM-a. AutoTrain je rje코enje bez koda, a fino pode코avanje mo쬰 se obaviti u va코em vlastitom oblaku, na Hugging Face Spaces ili lokalno. Podr쬬va i web-bazirani GUI, CLI i treniranje putem yaml konfiguracijskih datoteka.                                           |
|               |                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Zadatak

Odaberite jedan od gore navedenih vodi캜a i pro캠ite kroz njega. _Mo쬯a 캖emo replicirati verziju ovih vodi캜a u Jupyter Notebooks u ovom repozitoriju samo za referencu. Molimo koristite izvorne izvore izravno kako biste dobili najnovije verzije_.

## Odli캜no! Nastavite s u캜enjem.

Nakon 코to zavr코ite ovu lekciju, pogledajte na코u [Generative AI Learning kolekciju](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) kako biste nastavili unapre캠ivati svoje znanje o generativnoj umjetnoj inteligenciji!

캛estitamo!! Zavr코ili ste posljednju lekciju iz v2 serije ovog te캜aja! Nemojte prestati u캜iti i stvarati. \*\*Pogledajte [RESURSE](RESOURCES.md?WT.mc_id=academic-105485-koreyst) stranicu za popis dodatnih prijedloga samo za ovu temu.

Na코a v1 serija lekcija tako캠er je a쬿rirana s vi코e zadataka i koncepata. Stoga odvojite trenutak da osvje쬴te svoje znanje - i molimo vas [podijelite svoja pitanja i povratne informacije](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) kako bismo pobolj코ali ove lekcije za zajednicu.

---

**Izjava o odricanju odgovornosti**:  
Ovaj dokument je preveden pomo캖u AI usluge za prevo캠enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati to캜nost, imajte na umu da automatski prijevodi mogu sadr쬬vati pogre코ke ili neto캜nosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za klju캜ne informacije preporu캜uje se profesionalni prijevod od strane 캜ovjeka. Ne preuzimamo odgovornost za nesporazume ili pogre코na tuma캜enja koja proizlaze iz kori코tenja ovog prijevoda.