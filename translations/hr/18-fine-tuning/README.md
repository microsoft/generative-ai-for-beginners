<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-07-09T17:50:53+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "hr"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.hr.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Fino podeÅ¡avanje vaÅ¡eg LLM-a

KoriÅ¡tenje velikih jeziÄnih modela za izgradnju generativnih AI aplikacija donosi nove izazove. KljuÄno pitanje je osigurati kvalitetu odgovora (toÄnost i relevantnost) u sadrÅ¾aju koji model generira za odreÄ‘eni korisniÄki zahtjev. U prethodnim lekcijama razgovarali smo o tehnikama poput prompt inÅ¾enjeringa i generacije potpomognute dohvatom koje pokuÅ¡avaju rijeÅ¡iti problem _izmjenom ulaza prompta_ postojeÄ‡em modelu.

U danaÅ¡njoj lekciji raspravljamo o treÄ‡oj tehnici, **fino podeÅ¡avanju**, koja pokuÅ¡ava rijeÅ¡iti izazov _ponovnim treniranjem samog modela_ s dodatnim podacima. Krenimo u detalje.

## Ciljevi uÄenja

Ova lekcija uvodi pojam fino podeÅ¡avanja za unaprijed trenirane jeziÄne modele, istraÅ¾uje prednosti i izazove ovog pristupa te pruÅ¾a smjernice kada i kako koristiti fino podeÅ¡avanje za poboljÅ¡anje performansi vaÅ¡ih generativnih AI modela.

Na kraju ove lekcije trebali biste moÄ‡i odgovoriti na sljedeÄ‡a pitanja:

- Å to je fino podeÅ¡avanje jeziÄnih modela?
- Kada i zaÅ¡to je fino podeÅ¡avanje korisno?
- Kako mogu fino podesiti unaprijed trenirani model?
- Koja su ograniÄenja fino podeÅ¡avanja?

Spremni? Krenimo.

## Ilustrirani vodiÄ

Å½elite li dobiti Å¡iru sliku onoga Å¡to Ä‡emo pokriti prije nego Å¡to zaronimo u detalje? Pogledajte ovaj ilustrirani vodiÄ koji opisuje put uÄenja za ovu lekciju â€“ od upoznavanja s osnovnim pojmovima i motivacijom za fino podeÅ¡avanje, do razumijevanja procesa i najboljih praksi za izvoÄ‘enje zadatka fino podeÅ¡avanja. Ovo je fascinantna tema za istraÅ¾ivanje, stoga ne zaboravite posjetiti [Resurse](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) za dodatne poveznice koje Ä‡e podrÅ¾ati vaÅ¡e samostalno uÄenje!

![Ilustrirani vodiÄ za fino podeÅ¡avanje jeziÄnih modela](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.hr.png)

## Å to je fino podeÅ¡avanje jeziÄnih modela?

Prema definiciji, veliki jeziÄni modeli su _unaprijed trenirani_ na velikim koliÄinama teksta prikupljenog iz razliÄitih izvora, ukljuÄujuÄ‡i internet. Kao Å¡to smo nauÄili u prethodnim lekcijama, potrebne su nam tehnike poput _prompt inÅ¾enjeringa_ i _generacije potpomognute dohvatom_ kako bismo poboljÅ¡ali kvalitetu odgovora modela na korisniÄka pitanja ("prompte").

Popularna tehnika prompt inÅ¾enjeringa ukljuÄuje davanje modelu viÅ¡e uputa o tome Å¡to se oÄekuje u odgovoru, bilo pruÅ¾anjem _instrukcija_ (izravne upute) ili _davanjem nekoliko primjera_ (neizravne upute). To se naziva _few-shot learning_, ali ima dva ograniÄenja:

- OgraniÄenja broja tokena modela mogu ograniÄiti broj primjera koje moÅ¾ete dati i smanjiti uÄinkovitost.
- TroÅ¡kovi tokena modela mogu uÄiniti dodavanje primjera u svaki prompt skupim i ograniÄiti fleksibilnost.

Fino podeÅ¡avanje je uobiÄajena praksa u sustavima strojnog uÄenja gdje uzimamo unaprijed trenirani model i ponovno ga treniramo s novim podacima kako bismo poboljÅ¡ali njegove performanse na odreÄ‘enom zadatku. U kontekstu jeziÄnih modela, moÅ¾emo fino podesiti unaprijed trenirani model _s paÅ¾ljivo odabranim skupom primjera za odreÄ‘eni zadatak ili domenu primjene_ kako bismo stvorili **prilagoÄ‘eni model** koji moÅ¾e biti toÄniji i relevantniji za taj specifiÄni zadatak ili domenu. Dodatna prednost fino podeÅ¡avanja je Å¡to moÅ¾e smanjiti broj primjera potrebnih za few-shot uÄenje â€“ smanjujuÄ‡i koriÅ¡tenje tokena i povezane troÅ¡kove.

## Kada i zaÅ¡to bismo trebali fino podeÅ¡avati modele?

U _ovom_ kontekstu, kada govorimo o fino podeÅ¡avanju, mislimo na **nadzorovano** fino podeÅ¡avanje gdje se ponovno treniranje vrÅ¡i **dodavanjem novih podataka** koji nisu bili dio izvornog skupa podataka za treniranje. Ovo se razlikuje od nenadzorovanog pristupa fino podeÅ¡avanja gdje se model ponovno trenira na izvornim podacima, ali s razliÄitim hiperparametrima.

KljuÄna stvar koju treba zapamtiti je da je fino podeÅ¡avanje napredna tehnika koja zahtijeva odreÄ‘enu razinu struÄnosti da bi se postigli Å¾eljeni rezultati. Ako se ne izvede ispravno, moÅ¾da neÄ‡e donijeti oÄekivana poboljÅ¡anja, a moÅ¾e Äak i pogorÅ¡ati performanse modela za vaÅ¡u ciljanu domenu.

Stoga, prije nego Å¡to nauÄite "kako" fino podesiti jeziÄne modele, trebate znati "zaÅ¡to" biste trebali krenuti tim putem i "kada" zapoÄeti proces fino podeÅ¡avanja. PoÄnite tako da si postavite ova pitanja:

- **SluÄaj upotrebe**: Koji je vaÅ¡ _sluÄaj upotrebe_ za fino podeÅ¡avanje? Koji aspekt trenutnog unaprijed treniranog modela Å¾elite poboljÅ¡ati?
- **Alternativa**: Jeste li isprobali _druge tehnike_ za postizanje Å¾eljenih rezultata? Koristite ih za stvaranje osnovne usporedbe.
  - Prompt inÅ¾enjering: Isprobajte tehnike poput few-shot promptinga s primjerima relevantnih odgovora. Procijenite kvalitetu odgovora.
  - Generacija potpomognuta dohvatom: PokuÅ¡ajte obogatiti prompte rezultatima pretraÅ¾ivanja vaÅ¡ih podataka. Procijenite kvalitetu odgovora.
- **TroÅ¡kovi**: Jeste li identificirali troÅ¡kove fino podeÅ¡avanja?
  - MoguÄ‡nost podeÅ¡avanja â€“ je li unaprijed trenirani model dostupan za fino podeÅ¡avanje?
  - UloÅ¾eni trud â€“ za pripremu podataka za treniranje, evaluaciju i doradu modela.
  - RaÄunalni resursi â€“ za izvoÄ‘enje poslova fino podeÅ¡avanja i implementaciju fino podeÅ¡enog modela.
  - Podaci â€“ pristup dovoljnim kvalitetnim primjerima za znaÄajan utjecaj fino podeÅ¡avanja.
- **Prednosti**: Jeste li potvrdili prednosti fino podeÅ¡avanja?
  - Kvaliteta â€“ je li fino podeÅ¡eni model nadmaÅ¡io osnovni model?
  - TroÅ¡kovi â€“ smanjuje li koriÅ¡tenje tokena pojednostavljivanjem prompta?
  - ProÅ¡irivost â€“ moÅ¾ete li osnovni model iskoristiti za nove domene?

OdgovarajuÄ‡i na ova pitanja, trebali biste moÄ‡i odluÄiti je li fino podeÅ¡avanje pravi pristup za vaÅ¡ sluÄaj upotrebe. Idealno, pristup je opravdan samo ako prednosti nadmaÅ¡uju troÅ¡kove. Kad odluÄite nastaviti, vrijeme je da razmislite _kako_ moÅ¾ete fino podesiti unaprijed trenirani model.

Å½elite li dodatne uvide u proces donoÅ¡enja odluka? Pogledajte [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Kako moÅ¾emo fino podesiti unaprijed trenirani model?

Za fino podeÅ¡avanje unaprijed treniranog modela trebate imati:

- unaprijed trenirani model za fino podeÅ¡avanje
- skup podataka za fino podeÅ¡avanje
- okruÅ¾enje za treniranje za izvoÄ‘enje posla fino podeÅ¡avanja
- okruÅ¾enje za implementaciju fino podeÅ¡enog modela

## Fino podeÅ¡avanje u praksi

SljedeÄ‡i resursi pruÅ¾aju vodiÄe korak po korak koji vas vode kroz stvarni primjer koriÅ¡tenja odabranog modela s paÅ¾ljivo odabranim skupom podataka. Za rad s ovim vodiÄima trebate raÄun kod odreÄ‘enog pruÅ¾atelja usluga, kao i pristup relevantnim modelima i skupovima podataka.

| PruÅ¾atelj usluge | VodiÄ                                                                                                                                                                         | Opis                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI           | [Kako fino podesiti chat modele](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)            | NauÄite kako fino podesiti `gpt-35-turbo` za specifiÄnu domenu ("pomoÄ‡nik za recepte") pripremom podataka za treniranje, izvoÄ‘enjem posla fino podeÅ¡avanja i koriÅ¡tenjem fino podeÅ¡enog modela za izvoÄ‘enje.                                                                                                                                                                                                                     |
| Azure OpenAI     | [GPT 3.5 Turbo tutorial za fino podeÅ¡avanje](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | NauÄite kako fino podesiti `gpt-35-turbo-0613` model **na Azureu** kroz korake stvaranja i uÄitavanja podataka za treniranje, izvoÄ‘enje posla fino podeÅ¡avanja, implementaciju i koriÅ¡tenje novog modela.                                                                                                                                                                                                                          |
| Hugging Face     | [Fino podeÅ¡avanje LLM-a s Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                         | Ovaj blog vodi vas kroz fino podeÅ¡avanje _otvorenog LLM-a_ (npr. `CodeLlama 7B`) koristeÄ‡i [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteku i [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) s otvorenim [skupovima podataka](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) na Hugging Face. |
|                  |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ğŸ¤— AutoTrain     | [Fino podeÅ¡avanje LLM-a s AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                     | AutoTrain (ili AutoTrain Advanced) je Python biblioteka koju je razvio Hugging Face, a koja omoguÄ‡uje fino podeÅ¡avanje za mnoge razliÄite zadatke, ukljuÄujuÄ‡i fino podeÅ¡avanje LLM-a. AutoTrain je rjeÅ¡enje bez kodiranja i fino podeÅ¡avanje se moÅ¾e izvesti u vaÅ¡em oblaku, na Hugging Face Spaces ili lokalno. PodrÅ¾ava web suÄelje, CLI i treniranje putem yaml konfiguracijskih datoteka.                                                                                 |
|                  |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                  |

## Zadatak

Odaberite jedan od gore navedenih vodiÄa i proÄ‘ite kroz njega. _MoÅ¾emo replicirati verziju ovih vodiÄa u Jupyter Notebookovima u ovom repozitoriju samo za referencu. Molimo koristite izvorne izvore izravno kako biste dobili najnovije verzije_.

## OdliÄan posao! Nastavite s uÄenjem.

Nakon Å¡to zavrÅ¡ite ovu lekciju, pogledajte naÅ¡u [kolekciju za uÄenje generativne AI](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) kako biste nastavili podizati svoje znanje o generativnoj AI!

ÄŒestitamo!! ZavrÅ¡ili ste zavrÅ¡nu lekciju iz v2 serije ovog teÄaja! Nemojte prestati uÄiti i graditi. \*\*Pogledajte [RESURSE](RESOURCES.md?WT.mc_id=academic-105485-koreyst) stranicu za popis dodatnih prijedloga samo za ovu temu.

NaÅ¡a v1 serija lekcija takoÄ‘er je aÅ¾urirana s viÅ¡e zadataka i koncepata. Zato odvojite minutu za osvjeÅ¾avanje znanja â€“ i molimo vas da [podijelite svoja pitanja i povratne informacije](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) kako bismo mogli poboljÅ¡ati ove lekcije za zajednicu.

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden koriÅ¡tenjem AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako teÅ¾imo toÄnosti, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kritiÄne informacije preporuÄuje se profesionalni ljudski prijevod. Ne snosimo odgovornost za bilo kakva nesporazuma ili pogreÅ¡na tumaÄenja koja proizlaze iz koriÅ¡tenja ovog prijevoda.