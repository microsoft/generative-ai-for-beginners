<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-05-20T08:22:54+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "hr"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.8487555c3e3225eefc1dc84e72c8e00bce1ee76db867a080628fb0fbb04aa0d2.hr.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# Fino pode코avanje va코eg LLM-a

Kori코tenje velikih jezi캜nih modela za izgradnju aplikacija generativne umjetne inteligencije donosi nove izazove. Klju캜no pitanje je osiguranje kvalitete odgovora (to캜nosti i relevantnosti) u sadr쬬ju koji model generira za odre캠eni korisni캜ki zahtjev. U prethodnim lekcijama, raspravljali smo o tehnikama kao 코to su in쬰njering upita i generacija potpomognuta pretra쬴vanjem koje poku코avaju rije코iti problem _modificiranjem ulaza upita_ postoje캖em modelu.

U dana코njoj lekciji raspravljamo o tre캖oj tehnici, **fino pode코avanje**, koja poku코ava rije코iti izazov _ponovnim treniranjem samog modela_ s dodatnim podacima. Uronimo u detalje.

## Ciljevi u캜enja

Ova lekcija uvodi koncept finog pode코avanja za unaprijed trenirane jezi캜ne modele, istra쬿je prednosti i izazove ovog pristupa te pru쬬 smjernice o tome kada i kako koristiti fino pode코avanje za pobolj코anje performansi va코ih generativnih AI modela.

Na kraju ove lekcije trebali biste mo캖i odgovoriti na sljede캖a pitanja:

- 맚o je fino pode코avanje za jezi캜ne modele?
- Kada i za코to je fino pode코avanje korisno?
- Kako mogu fino podesiti unaprijed trenirani model?
- Koja su ograni캜enja finog pode코avanja?

Spremni? Krenimo.

## Ilustrirani vodi캜

콯elite li dobiti 코iru sliku o onome 코to 캖emo pokriti prije nego 코to uronimo? Pogledajte ovaj ilustrirani vodi캜 koji opisuje put u캜enja za ovu lekciju - od u캜enja osnovnih koncepata i motivacije za fino pode코avanje, do razumijevanja procesa i najboljih praksi za izvr코avanje zadatka finog pode코avanja. Ovo je fascinantna tema za istra쬴vanje, stoga ne zaboravite provjeriti [Resurse](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) za dodatne poveznice koje podr쬬vaju va코e samostalno u캜enje!

![Ilustrirani vodi캜 za fino pode코avanje jezi캜nih modela](../../../translated_images/18-fine-tuning-sketchnote.92733966235199dd260184b1aae3a84b877c7496bc872d8e63ad6fa2dd96bafc.hr.png)

## 맚o je fino pode코avanje za jezi캜ne modele?

Po definiciji, veliki jezi캜ni modeli su _unaprijed trenirani_ na velikim koli캜inama teksta prikupljenog iz raznih izvora uklju캜uju캖i internet. Kao 코to smo nau캜ili u prethodnim lekcijama, trebamo tehnike kao 코to su _in쬰njering upita_ i _generacija potpomognuta pretra쬴vanjem_ kako bismo pobolj코ali kvalitetu odgovora modela na korisni캜ka pitanja ("upite").

Popularna tehnika in쬰njeringa upita uklju캜uje davanje modelu vi코e smjernica o tome 코to se o캜ekuje u odgovoru bilo pru쬬njem _uputa_ (eksplicitne smjernice) ili _dav코i mu nekoliko primjera_ (implicitne smjernice). To se naziva _u캜enje s malo primjera_ ali ima dva ograni캜enja:

- Ograni캜enja broja tokena modela mogu ograni캜iti broj primjera koje mo쬰te dati, i ograni캜iti u캜inkovitost.
- Tro코kovi tokena modela mogu u캜initi skupo dodavanje primjera svakom upitu, i ograni캜iti fleksibilnost.

Fino pode코avanje je uobi캜ajena praksa u sustavima strojnog u캜enja gdje uzimamo unaprijed trenirani model i ponovno ga treniramo s novim podacima kako bismo pobolj코ali njegovu izvedbu na specifi캜nom zadatku. U kontekstu jezi캜nih modela, mo쬰mo fino podesiti unaprijed trenirani model _s pa쬷jivo odabranim skupom primjera za odre캠eni zadatak ili domenu primjene_ kako bismo stvorili **prilago캠eni model** koji mo쬰 biti to캜niji i relevantniji za taj specifi캜ni zadatak ili domenu. Sporedna korist finog pode코avanja je da mo쬰 tako캠er smanjiti broj primjera potrebnih za u캜enje s malo primjera - smanjuju캖i kori코tenje tokena i povezane tro코kove.

## Kada i za코to bismo trebali fino pode코avati modele?

U _ovom_ kontekstu, kada govorimo o finom pode코avanju, mislimo na **supervizirano** fino pode코avanje gdje se ponovno treniranje obavlja **dodavanjem novih podataka** koji nisu bili dio originalnog skupa podataka za treniranje. Ovo se razlikuje od nesuperviziranog pristupa finog pode코avanja gdje se model ponovno trenira na originalnim podacima, ali s razli캜itim hiperparametrima.

Klju캜na stvar koju treba zapamtiti je da je fino pode코avanje napredna tehnika koja zahtijeva odre캠enu razinu stru캜nosti kako bi se postigli 쬰ljeni rezultati. Ako se u캜ini pogre코no, mo쬯a ne캖e pru쬴ti o캜ekivana pobolj코anja, pa 캜ak mo쬰 i degradirati performanse modela za va코u ciljanu domenu.

Dakle, prije nego 코to nau캜ite "kako" fino pode코avati jezi캜ne modele, trebate znati "za코to" biste trebali krenuti tim putem, i "kada" zapo캜eti proces finog pode코avanja. Po캜nite postavljanjem ovih pitanja:

- **Upotreba**: Koji je va코 _slu캜aj upotrebe_ za fino pode코avanje? Koji aspekt trenutnog unaprijed treniranog modela 쬰lite pobolj코ati?
- **Alternative**: Jeste li poku코ali _druge tehnike_ za postizanje 쬰ljenih rezultata? Iskoristite ih za stvaranje osnovne linije za usporedbu.
  - In쬰njering upita: Isprobajte tehnike poput upita s malo primjera s primjerima relevantnih odgovora na upite. Procijenite kvalitetu odgovora.
  - Generacija potpomognuta pretra쬴vanjem: Isprobajte pro코irivanje upita s rezultatima pretra쬴vanja va코ih podataka. Procijenite kvalitetu odgovora.
- **Tro코kovi**: Jeste li identificirali tro코kove za fino pode코avanje?
  - Mogu캖nost pode코avanja - je li unaprijed trenirani model dostupan za fino pode코avanje?
  - Napor - za pripremu podataka za treniranje, evaluaciju i rafiniranje modela.
  - Ra캜unanje - za pokretanje poslova finog pode코avanja i implementaciju fino pode코enog modela
  - Podaci - pristup dovoljnim kvalitetnim primjerima za utjecaj finog pode코avanja
- **Prednosti**: Jeste li potvrdili prednosti finog pode코avanja?
  - Kvaliteta - je li fino pode코eni model nadma코io osnovnu liniju?
  - Tro코ak - smanjuje li kori코tenje tokena pojednostavljivanjem upita?
  - Pro코irivost - mo쬰te li ponovno koristiti osnovni model za nove domene?

Odgovaraju캖i na ova pitanja, trebali biste biti u mogu캖nosti odlu캜iti je li fino pode코avanje pravi pristup za va코 slu캜aj upotrebe. Idealno, pristup je valjan samo ako prednosti nadma코uju tro코kove. Kada odlu캜ite nastaviti, vrijeme je da razmislite o tome _kako_ mo쬰te fino podesiti unaprijed trenirani model.

콯elite li dobiti vi코e uvida u proces dono코enja odluka? Pogledajte [Da fino podesite ili ne fino podesite](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## Kako mo쬰mo fino podesiti unaprijed trenirani model?

Za fino pode코avanje unaprijed treniranog modela trebate imati:

- unaprijed trenirani model za fino pode코avanje
- skup podataka za kori코tenje u finom pode코avanju
- okru쬰nje za treniranje za pokretanje posla finog pode코avanja
- okru쬰nje za hosting za implementaciju fino pode코enog modela

## Fino pode코avanje u praksi

Sljede캖i resursi pru쬬ju detaljne tutorijale koji vas vode kroz stvarni primjer kori코tenja odabranog modela s pa쬷jivo odabranim skupom podataka. Za rad kroz ove tutorijale, trebate ra캜un na specifi캜nom pru쬬telju, zajedno s pristupom relevantnom modelu i skupovima podataka.

| Pru쬬telj     | Tutorial                                                                                                                                                                       | Opis                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [Kako fino podesiti chat modele](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                | Nau캜ite fino podesiti `gpt-35-turbo` za specifi캜nu domenu ("asistent za recepte") pripremom podataka za treniranje, pokretanjem posla finog pode코avanja i kori코tenjem fino pode코enog modela za inferenciju.                                                                                                                                                                                                                                              |
| Azure OpenAI | [GPT 3.5 Turbo tutorial za fino pode코avanje](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | Nau캜ite fino podesiti `gpt-35-turbo-0613` model **na Azureu** poduzimanjem koraka za kreiranje i u캜itavanje podataka za treniranje, pokretanje posla finog pode코avanja. Implementirajte i koristite novi model.                                                                                                                                                                                                                                                                 |
| Hugging Face | [Fino pode코avanje LLM-ova s Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | Ovaj blog post vas vodi kroz fino pode코avanje _otvorenog LLM-a_ (npr. `CodeLlama 7B`) kori코tenjem [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) biblioteke i [Reinforcement Learning za Transformere (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst]) s otvorenim [skupovima podataka](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) na Hugging Face. |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 游뱅 AutoTrain | [Fino pode코avanje LLM-ova s AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                         | AutoTrain (ili AutoTrain Advanced) je python biblioteka koju je razvio Hugging Face i omogu캖uje fino pode코avanje za mnoge razli캜ite zadatke uklju캜uju캖i fino pode코avanje LLM-ova. AutoTrain je rje코enje bez koda i fino pode코avanje se mo쬰 obaviti u va코em vlastitom oblaku, na Hugging Face Spaces ili lokalno. Podr쬬va i web-bazirano GUI, CLI i treniranje putem yaml konfiguracijskih datoteka.                                                                               |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## Zadatak

Odaberite jedan od gornjih tutorijala i pro캠ite kroz njega. _Mo쬯a 캖emo replicirati verziju ovih tutorijala u Jupyter Notebooks u ovom repozitoriju samo za referencu. Molimo koristite originalne izvore izravno za najnovije verzije_.

## Odli캜no obavljeno! Nastavite s u캜enjem.

Nakon zavr코etka ove lekcije, pogledajte na코u [Kolekciju u캜enja o generativnoj umjetnoj inteligenciji](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) kako biste nastavili unapre캠ivati svoje znanje o generativnoj umjetnoj inteligenciji!

캛estitamo!! Zavr코ili ste posljednju lekciju iz v2 serije za ovaj te캜aj! Nemojte prestati u캜iti i graditi. \*\*Provjerite [RESURSE](RESOURCES.md?WT.mc_id=academic-105485-koreyst) za popis dodatnih prijedloga samo za ovu temu.

Na코a v1 serija lekcija tako캠er je a쬿rirana s vi코e zadataka i koncepata. Zato odvojite trenutak da osvje쬴te svoje znanje - i molimo [podijelite svoja pitanja i povratne informacije](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) kako biste nam pomogli pobolj코ati ove lekcije za zajednicu.

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomo캖u AI usluge prevo캠enja [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati to캜nost, imajte na umu da automatski prijevodi mogu sadr쬬vati pogre코ke ili neto캜nosti. Izvorni dokument na njegovom izvornom jeziku treba smatrati mjerodavnim izvorom. Za kriti캜ne informacije preporu캜uje se profesionalni ljudski prijevod. Ne odgovaramo za nesporazume ili pogre코na tuma캜enja koja proizlaze iz kori코tenja ovog prijevoda.