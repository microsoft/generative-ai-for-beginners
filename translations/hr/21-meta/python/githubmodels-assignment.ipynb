{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rad s Meta obitelji modela\n",
    "\n",
    "## Uvod\n",
    "\n",
    "Ova lekcija pokriva:\n",
    "\n",
    "- Istraživanje dva glavna modela iz Meta obitelji - Llama 3.1 i Llama 3.2\n",
    "- Razumijevanje primjene i scenarija za svaki model\n",
    "- Primjer koda koji prikazuje jedinstvene značajke svakog modela\n",
    "\n",
    "## Meta obitelj modela\n",
    "\n",
    "U ovoj lekciji istražit ćemo 2 modela iz Meta obitelji ili \"Llama stada\" - Llama 3.1 i Llama 3.2\n",
    "\n",
    "Ovi modeli dolaze u različitim varijantama i dostupni su na Github Model marketplaceu. Više informacija o korištenju Github Modela za [prototipiranje s AI modelima](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Varijante modela:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Napomena: Llama 3 je također dostupna na Github Modelima, ali neće biti obrađena u ovoj lekciji*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Sa 405 milijardi parametara, Llama 3.1 spada u kategoriju otvorenih LLM modela.\n",
    "\n",
    "Ova verzija je nadogradnja na prethodnu Llama 3 i donosi:\n",
    "\n",
    "- Veći kontekstni prozor – 128k tokena naspram 8k tokena\n",
    "- Veći maksimalni broj izlaznih tokena – 4096 naspram 2048\n",
    "- Bolja podrška za više jezika – zahvaljujući većem broju tokena u treningu\n",
    "\n",
    "Ove značajke omogućuju Llama 3.1 da se nosi s kompleksnijim slučajevima upotrebe pri izradi GenAI aplikacija, uključujući:\n",
    "- Izvorno pozivanje funkcija – mogućnost pozivanja vanjskih alata i funkcija izvan LLM tijeka rada\n",
    "- Bolje RAG performanse – zahvaljujući većem kontekstnom prozoru\n",
    "- Generiranje sintetičkih podataka – mogućnost stvaranja učinkovitih podataka za zadatke poput fine-tuninga\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pozivanje izvornih funkcija\n",
    "\n",
    "Llama 3.1 je dodatno prilagođena kako bi bila učinkovitija u pozivanju funkcija ili alata. Također ima dva ugrađena alata koje model može prepoznati kao potrebne za korištenje na temelju korisničkog upita. Ti alati su:\n",
    "\n",
    "- **Brave Search** - Može se koristiti za dobivanje najnovijih informacija poput vremenske prognoze putem pretraživanja interneta\n",
    "- **Wolfram Alpha** - Može se koristiti za složenije matematičke izračune pa nije potrebno pisati vlastite funkcije.\n",
    "\n",
    "Možete također kreirati vlastite prilagođene alate koje LLM može pozivati.\n",
    "\n",
    "U donjem primjeru koda:\n",
    "\n",
    "- Definiramo dostupne alate (brave_search, wolfram_alpha) u sistemskom upitu.\n",
    "- Šaljemo korisnički upit koji pita za vremensku prognozu u određenom gradu.\n",
    "- LLM će odgovoriti pozivom alata Brave Search koji će izgledati ovako `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Napomena: Ovaj primjer samo poziva alat, a ako želite dobiti rezultate, potrebno je otvoriti besplatan račun na Brave API stranici i definirati samu funkciju*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Iako je Llama 3.1 veliki jezični model, jedno od njegovih ograničenja je multimodalnost. To znači mogućnost korištenja različitih vrsta ulaza, poput slika, kao upita i davanja odgovora. Ova mogućnost je jedna od glavnih značajki Llama 3.2. Te značajke uključuju:\n",
    "\n",
    "- Multimodalnost - ima mogućnost obrade i tekstualnih i slikovnih upita\n",
    "- Varijante male do srednje veličine (11B i 90B) - to omogućuje fleksibilne opcije implementacije,\n",
    "- Varijante samo za tekst (1B i 3B) - to omogućuje modelu da se koristi na rubnim / mobilnim uređajima i pruža nisku latenciju\n",
    "\n",
    "Podrška za multimodalnost predstavlja veliki korak u svijetu open source modela. Primjer koda ispod koristi i sliku i tekstualni upit kako bi dobio analizu slike od Llama 3.2 90B.\n",
    "\n",
    "### Multimodalna podrška s Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Učenje ne prestaje ovdje, nastavi svoje putovanje\n",
    "\n",
    "Nakon što završiš ovu lekciju, pogledaj našu [Generative AI Learning kolekciju](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) i nastavi usavršavati svoje znanje o generativnoj umjetnoj inteligenciji!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Odricanje od odgovornosti**:  \nOvaj dokument je preveden pomoću AI usluge prevođenja [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za ključne informacije preporučuje se profesionalni ljudski prijevod. Ne snosimo odgovornost za bilo kakva nesporazume ili pogrešna tumačenja koja proizlaze iz korištenja ovog prijevoda.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:49:42+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "hr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}