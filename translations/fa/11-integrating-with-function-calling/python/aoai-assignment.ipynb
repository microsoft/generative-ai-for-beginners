{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مقدمه\n",
    "\n",
    "در این درس با موارد زیر آشنا می‌شوید:\n",
    "- تابع فراخوانی چیست و چه کاربردهایی دارد\n",
    "- چطور با استفاده از Azure OpenAI یک فراخوانی تابع بسازیم\n",
    "- چگونه یک فراخوانی تابع را در یک برنامه ادغام کنیم\n",
    "\n",
    "## اهداف یادگیری\n",
    "\n",
    "پس از پایان این درس، می‌دانید که چگونه:\n",
    "- هدف استفاده از فراخوانی تابع را درک کنید\n",
    "- فراخوانی تابع را با سرویس Azure Open AI راه‌اندازی کنید\n",
    "- برای کاربرد برنامه خود، فراخوانی‌های تابع مؤثر طراحی کنید\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## درک فراخوانی توابع\n",
    "\n",
    "در این درس، می‌خواهیم یک قابلیت برای استارتاپ آموزشی خود بسازیم که به کاربران اجازه می‌دهد با استفاده از یک چت‌بات، دوره‌های فنی را پیدا کنند. ما دوره‌هایی را پیشنهاد می‌دهیم که با سطح مهارت، نقش فعلی و تکنولوژی مورد علاقه آن‌ها مطابقت داشته باشد.\n",
    "\n",
    "برای انجام این کار، از ترکیبی از موارد زیر استفاده می‌کنیم:\n",
    " - `Azure Open AI` برای ایجاد یک تجربه چت برای کاربر\n",
    " - `Microsoft Learn Catalog API` برای کمک به کاربران در پیدا کردن دوره‌ها بر اساس درخواست آن‌ها\n",
    " - `Function Calling` برای گرفتن پرسش کاربر و ارسال آن به یک تابع جهت انجام درخواست به API\n",
    "\n",
    "برای شروع، بیایید ببینیم اصلاً چرا باید از فراخوانی توابع استفاده کنیم:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # دریافت یک پاسخ جدید از GPT که می‌تواند پاسخ تابع را ببیند\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### چرا فراخوانی تابع\n",
    "\n",
    "اگر هر درس دیگری از این دوره را گذرانده باشید، احتمالاً با قدرت استفاده از مدل‌های زبانی بزرگ (LLM) آشنا شده‌اید. امیدوارم همچنین برخی از محدودیت‌های آن‌ها را هم دیده باشید.\n",
    "\n",
    "فراخوانی تابع قابلیتی در سرویس Azure Open AI است که برای رفع محدودیت‌های زیر ارائه شده است:\n",
    "1) فرمت پاسخ‌دهی یکسان و منظم\n",
    "2) امکان استفاده از داده‌های منابع دیگر یک برنامه در یک گفت‌وگو\n",
    "\n",
    "قبل از فراخوانی تابع، پاسخ‌های یک LLM ساختار مشخص و ثابتی نداشتند. توسعه‌دهندگان مجبور بودند کدهای اعتبارسنجی پیچیده‌ای بنویسند تا بتوانند هر نوع پاسخ را مدیریت کنند.\n",
    "\n",
    "کاربران نمی‌توانستند به سوالاتی مثل «الان هوای استکهلم چطور است؟» پاسخ بگیرند. دلیلش این بود که مدل‌ها فقط تا زمانی که داده‌ها آموزش دیده بودند، اطلاعات داشتند.\n",
    "\n",
    "بیایید به مثال زیر که این مشکل را نشان می‌دهد نگاه کنیم:\n",
    "\n",
    "فرض کنید می‌خواهیم یک پایگاه داده از اطلاعات دانش‌آموزان بسازیم تا بتوانیم دوره مناسب را به آن‌ها پیشنهاد دهیم. در زیر دو توصیف از دانش‌آموزان داریم که از نظر داده‌های موجود بسیار شبیه به هم هستند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ما می‌خواهیم این را برای یک مدل زبانی ارسال کنیم تا داده‌ها را تجزیه کند. بعداً می‌توانیم از این داده‌ها در برنامه‌مان استفاده کنیم تا آن را به یک API ارسال کنیم یا در پایگاه داده ذخیره کنیم.\n",
    "\n",
    "بیایید دو درخواست مشابه بسازیم که به مدل زبانی توضیح می‌دهد چه اطلاعاتی برای ما مهم است:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ما می‌خواهیم این را به یک مدل زبانی بزرگ ارسال کنیم تا بخش‌هایی که برای محصول ما مهم هستند را تجزیه و تحلیل کند. بنابراین می‌توانیم دو درخواست یکسان ایجاد کنیم تا به مدل زبانی بزرگ دستور دهیم:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "پس از ایجاد این دو پرامپت، آن‌ها را با استفاده از `openai.ChatCompletion` به LLM ارسال می‌کنیم. پرامپت را در متغیر `messages` ذخیره می‌کنیم و نقش را به `user` اختصاص می‌دهیم. این کار برای شبیه‌سازی پیامی از طرف کاربر است که به یک چت‌بات نوشته می‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اکنون می‌توانیم هر دو درخواست را به مدل زبان بزرگ ارسال کنیم و پاسخ دریافتی را بررسی کنیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "حتی اگر دستورات یکسان باشند و توضیحات مشابهی داشته باشند، ممکن است فرمت‌های متفاوتی از ویژگی `Grades` دریافت کنیم.\n",
    "\n",
    "اگر سلول بالا را چند بار اجرا کنید، فرمت می‌تواند به صورت `3.7` یا `3.7 GPA` باشد.\n",
    "\n",
    "دلیل این موضوع این است که مدل زبانی داده‌های بدون ساختار را به صورت متن دریافت می‌کند و خروجی هم به صورت داده‌های بدون ساختار است. ما نیاز داریم که یک فرمت ساختاریافته داشته باشیم تا بدانیم هنگام ذخیره یا استفاده از این داده‌ها چه انتظاری باید داشته باشیم.\n",
    "\n",
    "با استفاده از فراخوانی تابعی (functional calling)، می‌توانیم مطمئن شویم که داده‌های ساختاریافته دریافت می‌کنیم. هنگام استفاده از فراخوانی تابعی، مدل زبانی در واقع هیچ تابعی را اجرا یا فراخوانی نمی‌کند. بلکه ما یک ساختار برای پاسخ‌های مدل تعریف می‌کنیم تا مدل طبق آن پاسخ دهد. سپس از این پاسخ‌های ساختاریافته استفاده می‌کنیم تا بدانیم در برنامه‌هایمان باید کدام تابع را اجرا کنیم.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![نمودار جریان فراخوانی تابع](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.fa.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### موارد استفاده از فراخوانی توابع\n",
    "\n",
    "**فراخوانی ابزارهای خارجی**  \n",
    "چت‌بات‌ها در پاسخ دادن به سوالات کاربران بسیار خوب عمل می‌کنند. با استفاده از فراخوانی توابع، چت‌بات‌ها می‌توانند از پیام‌های کاربران برای انجام برخی کارها استفاده کنند. برای مثال، یک دانش‌آموز می‌تواند از چت‌بات بخواهد: «یک ایمیل به استاد من بفرست و بگو که من به کمک بیشتری در این موضوع نیاز دارم». این درخواست می‌تواند تابعی مثل `send_email(to: string, body: string)` را فراخوانی کند.\n",
    "\n",
    "**ایجاد کوئری‌های API یا پایگاه داده**  \n",
    "کاربران می‌توانند با زبان طبیعی اطلاعات مورد نیاز خود را پیدا کنند که این درخواست به یک کوئری یا درخواست API فرمت‌شده تبدیل می‌شود. برای مثال، یک معلم می‌پرسد: «کدام دانش‌آموزان آخرین تکلیف را انجام داده‌اند؟» که می‌تواند تابعی به نام `get_completed(student_name: string, assignment: int, current_status: string)` را فراخوانی کند.\n",
    "\n",
    "**ایجاد داده‌های ساختاریافته**  \n",
    "کاربران می‌توانند یک بلوک متن یا فایل CSV را وارد کنند و با کمک مدل زبانی بزرگ، اطلاعات مهم را از آن استخراج کنند. برای مثال، یک دانش‌آموز می‌تواند یک مقاله ویکی‌پدیا درباره توافق‌نامه‌های صلح را به فلش‌کارت‌های هوش مصنوعی تبدیل کند. این کار با استفاده از تابعی به نام `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)` انجام می‌شود.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ۲. ساخت اولین فراخوانی تابع\n",
    "\n",
    "فرآیند ساخت یک فراخوانی تابع شامل ۳ مرحله اصلی است:\n",
    "۱. فراخوانی API تکمیل چت با لیستی از توابع خود و یک پیام کاربر\n",
    "۲. خواندن پاسخ مدل برای انجام یک عمل، مثلاً اجرای یک تابع یا فراخوانی API\n",
    "۳. انجام یک فراخوانی دیگر به API تکمیل چت با پاسخ دریافتی از تابع خود تا از آن اطلاعات برای ساخت پاسخ به کاربر استفاده کنید.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![جریان یک فراخوانی تابع](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.fa.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### اجزای یک فراخوانی تابع\n",
    "\n",
    "#### ورودی کاربر\n",
    "\n",
    "اولین قدم ایجاد یک پیام کاربر است. این پیام می‌تواند به صورت پویا از مقدار ورودی یک فیلد متنی گرفته شود یا می‌توانید مستقیماً مقداری به آن اختصاص دهید. اگر اولین بار است که با API تکمیل چت کار می‌کنید، باید `role` و `content` پیام را مشخص کنیم.\n",
    "\n",
    "`role` می‌تواند یکی از این سه مقدار باشد: `system` (ایجاد قوانین)، `assistant` (مدل) یا `user` (کاربر نهایی). برای فراخوانی تابع، این مقدار را به عنوان `user` و یک سؤال نمونه قرار می‌دهیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ساختن توابع\n",
    "\n",
    "در ادامه یک تابع و پارامترهای آن را تعریف می‌کنیم. در اینجا فقط از یک تابع به نام `search_courses` استفاده می‌کنیم اما شما می‌توانید چندین تابع بسازید.\n",
    "\n",
    "**مهم**: توابع در پیام سیستمی به LLM اضافه می‌شوند و در میزان توکن‌های قابل استفاده شما محاسبه خواهند شد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**تعاریف**\n",
    "\n",
    "`name` - نام تابعی که می‌خواهیم فراخوانی شود.\n",
    "\n",
    "`description` - این توضیحی است درباره نحوه عملکرد تابع. در اینجا مهم است که توضیح دقیق و واضح باشد.\n",
    "\n",
    "`parameters` - فهرستی از مقادیر و قالبی که می‌خواهید مدل در پاسخ خود تولید کند.\n",
    "\n",
    "`type` - نوع داده‌ای که ویژگی‌ها در آن ذخیره خواهند شد.\n",
    "\n",
    "`properties` - فهرستی از مقادیر مشخصی که مدل برای پاسخ خود استفاده خواهد کرد.\n",
    "\n",
    "`name` - نام ویژگی که مدل در پاسخ قالب‌بندی‌شده خود استفاده می‌کند.\n",
    "\n",
    "`type` - نوع داده‌ای این ویژگی.\n",
    "\n",
    "`description` - توضیح درباره ویژگی خاص.\n",
    "\n",
    "**اختیاری**\n",
    "\n",
    "`required` - ویژگی مورد نیاز برای اینکه فراخوانی تابع کامل شود.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### فراخوانی تابع\n",
    "بعد از تعریف یک تابع، حالا باید آن را در فراخوانی به API تکمیل چت قرار دهیم. این کار را با اضافه کردن `functions` به درخواست انجام می‌دهیم. در این مثال، `functions=functions` است.\n",
    "\n",
    "همچنین گزینه‌ای وجود دارد که مقدار `function_call` را روی `auto` قرار دهید. این یعنی اجازه می‌دهیم مدل زبانی تصمیم بگیرد که کدام تابع باید بر اساس پیام کاربر فراخوانی شود، به جای اینکه خودمان آن را تعیین کنیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "حالا بیایید به پاسخ نگاه کنیم و ببینیم چطور قالب‌بندی شده است:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "می‌بینید که نام تابع فراخوانی شده و مدل زبانی از پیام کاربر توانسته داده‌ها را برای قرار دادن در آرگومان‌های تابع پیدا کند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ۳. ادغام فراخوانی توابع در یک برنامه\n",
    "\n",
    "بعد از اینکه پاسخ قالب‌بندی‌شده از LLM را تست کردیم، حالا می‌توانیم آن را در یک برنامه ادغام کنیم.\n",
    "\n",
    "### مدیریت جریان\n",
    "\n",
    "برای ادغام این موضوع در برنامه‌مان، بیایید مراحل زیر را دنبال کنیم:\n",
    "\n",
    "ابتدا، فراخوانی سرویس‌های Open AI را انجام می‌دهیم و پیام را در متغیری به نام `response_message` ذخیره می‌کنیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اکنون تابعی را تعریف خواهیم کرد که API مایکروسافت لرن را فراخوانی می‌کند تا فهرستی از دوره‌ها را دریافت کند:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "به عنوان یک روش پیشنهادی، ابتدا بررسی می‌کنیم که آیا مدل می‌خواهد تابعی را فراخوانی کند یا نه. پس از آن، یکی از توابع موجود را ایجاد می‌کنیم و آن را با تابعی که فراخوانی شده مطابقت می‌دهیم.\n",
    "سپس آرگومان‌های تابع را می‌گیریم و آن‌ها را به آرگومان‌هایی که از LLM آمده‌اند، نگاشت می‌کنیم.\n",
    "\n",
    "در نهایت، پیام فراخوانی تابع و مقادیری که توسط پیام `search_courses` بازگردانده شده‌اند را اضافه می‌کنیم. این کار تمام اطلاعات لازم را در اختیار LLM قرار می‌دهد تا بتواند با زبان طبیعی به کاربر پاسخ دهد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## چالش کدنویسی\n",
    "\n",
    "کارت عالی بود! برای ادامه یادگیری درباره Azure Open AI Function Calling می‌تونی این پروژه رو بسازی: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    "- پارامترهای بیشتری به تابع اضافه کن تا به یادگیرنده‌ها کمک کنه دوره‌های بیشتری پیدا کنن. می‌تونی پارامترهای موجود API رو اینجا ببینی:  \n",
    "- یک فراخوانی تابع دیگه بساز که اطلاعات بیشتری مثل زبان مادری یادگیرنده رو دریافت کنه  \n",
    "- مدیریت خطا اضافه کن تا اگر فراخوانی تابع یا API هیچ دوره مناسبی برنگردوند، پیام مناسبی نمایش داده بشه\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**سلب مسئولیت**:  \nاین سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطا یا نادقتی باشند. نسخه اصلی سند به زبان مادری آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای توسط انسان توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم یا تفسیر نادرست ناشی از استفاده از این ترجمه نداریم.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-08-25T19:56:31+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "fa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}