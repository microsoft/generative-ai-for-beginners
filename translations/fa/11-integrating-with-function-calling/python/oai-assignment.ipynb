{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مقدمه\n",
    "\n",
    "در این درس به موارد زیر پرداخته می‌شود:\n",
    "- تابع فراخوانی چیست و چه کاربردهایی دارد\n",
    "- چطور با استفاده از OpenAI یک تابع فراخوانی بسازیم\n",
    "- چطور یک تابع فراخوانی را در یک برنامه ادغام کنیم\n",
    "\n",
    "## اهداف یادگیری\n",
    "\n",
    "پس از پایان این درس، شما خواهید دانست و درک خواهید کرد که:\n",
    "\n",
    "- هدف استفاده از تابع فراخوانی چیست\n",
    "- راه‌اندازی تابع فراخوانی با استفاده از سرویس OpenAI\n",
    "- طراحی فراخوانی‌های مؤثر تابع برای کاربردهای برنامه خود\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## درک فراخوانی تابع\n",
    "\n",
    "در این درس، می‌خواهیم قابلیتی برای استارتاپ آموزشی خود بسازیم که کاربران بتوانند با استفاده از یک چت‌بات، دوره‌های فنی مناسب خود را پیدا کنند. ما دوره‌هایی را پیشنهاد می‌دهیم که با سطح مهارت، نقش فعلی و فناوری مورد علاقه‌ی کاربر هماهنگ باشند.\n",
    "\n",
    "برای انجام این کار، از ترکیبی از موارد زیر استفاده می‌کنیم:\n",
    " - `OpenAI` برای ایجاد تجربه چت برای کاربر\n",
    " - `Microsoft Learn Catalog API` برای کمک به کاربران در پیدا کردن دوره‌ها بر اساس درخواستشان\n",
    " - `Function Calling` برای گرفتن پرسش کاربر و ارسال آن به یک تابع جهت انجام درخواست به API\n",
    "\n",
    "برای شروع، بیایید ببینیم اصلاً چرا باید از فراخوانی تابع استفاده کنیم:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # دریافت پاسخ جدید از GPT که می‌تواند پاسخ تابع را مشاهده کند\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### چرا فراخوانی تابع\n",
    "\n",
    "اگر هر درس دیگری از این دوره را گذرانده باشید، احتمالاً با قدرت استفاده از مدل‌های زبانی بزرگ (LLMs) آشنا شده‌اید. امیدوارم همچنین برخی از محدودیت‌های آن‌ها را هم دیده باشید.\n",
    "\n",
    "فراخوانی تابع قابلیتی در سرویس OpenAI است که برای حل چالش‌های زیر طراحی شده است:\n",
    "\n",
    "فرمت‌دهی ناسازگار پاسخ‌ها:\n",
    "- قبل از فراخوانی تابع، پاسخ‌های مدل‌های زبانی بزرگ ساختار مشخص و ثابتی نداشتند. توسعه‌دهندگان مجبور بودند کدهای اعتبارسنجی پیچیده‌ای بنویسند تا هر نوع خروجی متفاوت را مدیریت کنند.\n",
    "\n",
    "ادغام محدود با داده‌های خارجی:\n",
    "- پیش از این قابلیت، وارد کردن داده از بخش‌های دیگر یک برنامه به زمینه گفتگو کار دشواری بود.\n",
    "\n",
    "با استانداردسازی فرمت پاسخ‌ها و فراهم کردن امکان ادغام آسان با داده‌های خارجی، فراخوانی تابع توسعه را ساده‌تر می‌کند و نیاز به منطق اعتبارسنجی اضافی را کاهش می‌دهد.\n",
    "\n",
    "کاربران نمی‌توانستند به سوالاتی مثل «آب و هوای فعلی استکهلم چطور است؟» پاسخ بگیرند. دلیلش این بود که مدل‌ها فقط تا زمانی که داده‌ها آموزش دیده بودند، اطلاعات داشتند.\n",
    "\n",
    "بیایید به مثال زیر نگاه کنیم که این مشکل را نشان می‌دهد:\n",
    "\n",
    "فرض کنید می‌خواهیم یک پایگاه داده از اطلاعات دانش‌آموزان بسازیم تا بتوانیم دوره مناسب را به آن‌ها پیشنهاد دهیم. در زیر دو توصیف از دانش‌آموزان داریم که از نظر داده‌های موجود بسیار شبیه به هم هستند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ما می‌خواهیم این را به یک مدل زبانی بزرگ (LLM) ارسال کنیم تا داده‌ها را تجزیه و تحلیل کند. بعداً می‌توان از این داده‌ها در برنامه ما برای ارسال به یک API یا ذخیره در پایگاه داده استفاده کرد.\n",
    "\n",
    "بیایید دو درخواست یکسان ایجاد کنیم که در آن به مدل زبانی توضیح می‌دهیم چه اطلاعاتی برای ما مهم است:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ما می‌خواهیم این را به یک مدل زبانی بزرگ ارسال کنیم تا بخش‌هایی که برای محصول ما مهم هستند را تجزیه و تحلیل کند. بنابراین می‌توانیم دو درخواست یکسان ایجاد کنیم تا به مدل زبانی بزرگ دستور دهیم:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "پس از ایجاد این دو پرامپت، آن‌ها را با استفاده از `openai.ChatCompletion` به LLM ارسال می‌کنیم. پرامپت را در متغیر `messages` ذخیره می‌کنیم و نقش را به `user` اختصاص می‌دهیم. این کار برای شبیه‌سازی پیامی از طرف کاربر است که به یک چت‌بات نوشته می‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اکنون می‌توانیم هر دو درخواست را به مدل زبان بزرگ ارسال کنیم و پاسخی که دریافت می‌کنیم را بررسی کنیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "حتی اگر دستورات یکسان باشند و توضیحات مشابهی داشته باشند، ممکن است فرمت‌های متفاوتی از ویژگی `Grades` دریافت کنیم.\n",
    "\n",
    "اگر سلول بالا را چندین بار اجرا کنید، فرمت می‌تواند به صورت `3.7` یا `3.7 GPA` باشد.\n",
    "\n",
    "دلیل این موضوع این است که مدل زبانی داده‌های بدون ساختار را به صورت متن دریافت می‌کند و خروجی هم به صورت داده‌های بدون ساختار است. ما نیاز داریم که یک فرمت ساختاریافته داشته باشیم تا بدانیم هنگام ذخیره یا استفاده از این داده‌ها باید چه انتظاری داشته باشیم.\n",
    "\n",
    "با استفاده از فراخوانی تابعی (functional calling)، می‌توانیم مطمئن شویم که داده‌های ساختاریافته دریافت می‌کنیم. هنگام استفاده از فراخوانی تابعی، مدل زبانی در واقع هیچ تابعی را اجرا یا فراخوانی نمی‌کند. در عوض، ما یک ساختار برای پاسخ‌های مدل تعیین می‌کنیم تا طبق آن پاسخ دهد. سپس از این پاسخ‌های ساختاریافته استفاده می‌کنیم تا بدانیم در برنامه‌هایمان باید کدام تابع را اجرا کنیم.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![نمودار جریان فراخوانی تابع](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.fa.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### موارد استفاده از فراخوانی توابع\n",
    "\n",
    "**فراخوانی ابزارهای خارجی**  \n",
    "چت‌بات‌ها در پاسخ دادن به سوالات کاربران بسیار خوب عمل می‌کنند. با استفاده از فراخوانی توابع، چت‌بات‌ها می‌توانند از پیام‌های کاربران برای انجام کارهای خاصی استفاده کنند. برای مثال، یک دانش‌آموز می‌تواند از چت‌بات بخواهد: «یک ایمیل به استاد من بفرست و بگو که در این موضوع به کمک بیشتری نیاز دارم». این درخواست می‌تواند تابعی مثل `send_email(to: string, body: string)` را فراخوانی کند.\n",
    "\n",
    "**ایجاد کوئری‌های API یا پایگاه داده**  \n",
    "کاربران می‌توانند با زبان طبیعی اطلاعات مورد نیاز خود را پیدا کنند و این درخواست به یک کوئری یا درخواست API فرمت‌شده تبدیل شود. مثلاً یک معلم می‌پرسد: «کدام دانش‌آموزان آخرین تکلیف را انجام داده‌اند؟» که می‌تواند تابعی به نام `get_completed(student_name: string, assignment: int, current_status: string)` را فراخوانی کند.\n",
    "\n",
    "**ایجاد داده‌های ساختاریافته**  \n",
    "کاربران می‌توانند یک بلوک متن یا فایل CSV را وارد کنند و با کمک مدل زبانی بزرگ، اطلاعات مهم را از آن استخراج کنند. برای مثال، یک دانش‌آموز می‌تواند یک مقاله ویکی‌پدیا درباره توافق‌نامه‌های صلح را به فلش‌کارت‌های هوش مصنوعی تبدیل کند. این کار با استفاده از تابعی به نام `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)` انجام می‌شود.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ۲. ساخت اولین فراخوانی تابع\n",
    "\n",
    "فرآیند ساخت یک فراخوانی تابع شامل سه مرحله اصلی است:\n",
    "۱. فراخوانی API تکمیل چت با لیستی از توابع خود و یک پیام از کاربر\n",
    "۲. خواندن پاسخ مدل برای انجام یک عمل، یعنی اجرای یک تابع یا فراخوانی API\n",
    "۳. انجام یک فراخوانی دیگر به API تکمیل چت با پاسخ تابع خود تا از آن اطلاعات برای ساخت پاسخ به کاربر استفاده کنید.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![جریان یک فراخوانی تابع](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.fa.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### اجزای یک فراخوانی تابع\n",
    "\n",
    "#### ورودی کاربر\n",
    "\n",
    "اولین قدم ایجاد یک پیام کاربر است. این پیام می‌تواند به صورت پویا از مقدار ورودی یک فیلد متنی گرفته شود یا می‌توانید مستقیماً مقداری به آن اختصاص دهید. اگر اولین بار است که با API تکمیل چت کار می‌کنید، باید `role` و `content` پیام را مشخص کنیم.\n",
    "\n",
    "`role` می‌تواند یکی از این سه مقدار باشد: `system` (ایجاد قوانین)، `assistant` (مدل) یا `user` (کاربر نهایی). برای فراخوانی تابع، این مقدار را به عنوان `user` و یک سؤال نمونه قرار می‌دهیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ساختن توابع\n",
    "\n",
    "در ادامه یک تابع و پارامترهای آن را تعریف می‌کنیم. در اینجا فقط از یک تابع به نام `search_courses` استفاده می‌کنیم اما شما می‌توانید چندین تابع بسازید.\n",
    "\n",
    "**مهم**: توابع در پیام سیستمی به LLM اضافه می‌شوند و در میزان توکن‌های قابل استفاده شما محاسبه خواهند شد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**تعاریف**\n",
    "\n",
    "ساختار تعریف تابع چندین سطح دارد که هر کدام ویژگی‌های خاص خود را دارند. در اینجا یک توضیح از ساختار تو در تو آورده شده است:\n",
    "\n",
    "**ویژگی‌های سطح بالای تابع:**\n",
    "\n",
    "`name` - نام تابعی که می‌خواهیم فراخوانی شود.\n",
    "\n",
    "`description` - توضیحی درباره نحوه کارکرد تابع. در این بخش مهم است که توضیح دقیق و واضح باشد.\n",
    "\n",
    "`parameters` - فهرستی از مقادیر و قالبی که می‌خواهید مدل در پاسخ خود تولید کند.\n",
    "\n",
    "**ویژگی‌های شیء پارامترها:**\n",
    "\n",
    "`type` - نوع داده شیء پارامترها (معمولاً \"object\")\n",
    "\n",
    "`properties` - فهرستی از مقادیر مشخصی که مدل برای پاسخ خود استفاده خواهد کرد.\n",
    "\n",
    "**ویژگی‌های هر پارامتر به صورت جداگانه:**\n",
    "\n",
    "`name` - به طور ضمنی توسط کلید ویژگی تعریف می‌شود (مثلاً \"role\"، \"product\"، \"level\")\n",
    "\n",
    "`type` - نوع داده این پارامتر خاص (مثلاً \"string\"، \"number\"، \"boolean\")\n",
    "\n",
    "`description` - توضیحی درباره این پارامتر خاص\n",
    "\n",
    "**ویژگی‌های اختیاری:**\n",
    "\n",
    "`required` - آرایه‌ای که مشخص می‌کند کدام پارامترها برای تکمیل فراخوانی تابع ضروری هستند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### فراخوانی تابع\n",
    "بعد از تعریف یک تابع، حالا باید آن را در فراخوانی به API تکمیل چت قرار دهیم. این کار را با اضافه کردن `functions` به درخواست انجام می‌دهیم. در اینجا `functions=functions` قرار می‌گیرد.\n",
    "\n",
    "همچنین گزینه‌ای وجود دارد که می‌توان `function_call` را روی `auto` تنظیم کرد. این یعنی اجازه می‌دهیم مدل زبانی بزرگ (LLM) بر اساس پیام کاربر خودش تصمیم بگیرد کدام تابع فراخوانی شود، به جای اینکه خودمان آن را مشخص کنیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "حالا بیایید به پاسخ نگاه کنیم و ببینیم چطور قالب‌بندی شده است:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "می‌توانید ببینید که نام تابع فراخوانی شده و مدل زبانی از پیام کاربر توانسته داده‌ها را برای قرار دادن در آرگومان‌های تابع پیدا کند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ۳. ادغام فراخوانی توابع در یک برنامه\n",
    "\n",
    "بعد از اینکه پاسخ قالب‌بندی‌شده از LLM را تست کردیم، حالا می‌توانیم آن را در یک برنامه ادغام کنیم.\n",
    "\n",
    "### مدیریت جریان\n",
    "\n",
    "برای ادغام این مورد در برنامه‌مان، بیایید مراحل زیر را دنبال کنیم:\n",
    "\n",
    "ابتدا، فراخوانی سرویس‌های OpenAI را انجام می‌دهیم و پیام را در متغیری به نام `response_message` ذخیره می‌کنیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اکنون تابعی را تعریف خواهیم کرد که API مایکروسافت لرن را فراخوانی می‌کند تا فهرستی از دوره‌ها را دریافت کند:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "به عنوان یک روش پیشنهادی، ابتدا بررسی می‌کنیم که آیا مدل می‌خواهد تابعی را فراخوانی کند یا نه. بعد از آن، یکی از توابع موجود را ایجاد می‌کنیم و آن را با تابعی که فراخوانی شده مطابقت می‌دهیم.\n",
    "سپس آرگومان‌های تابع را گرفته و آن‌ها را با آرگومان‌هایی که از LLM دریافت شده‌اند، تطبیق می‌دهیم.\n",
    "\n",
    "در نهایت، پیام فراخوانی تابع و مقادیری که توسط پیام `search_courses` بازگردانده شده‌اند را اضافه می‌کنیم. این کار تمام اطلاعات لازم را در اختیار LLM قرار می‌دهد تا بتواند با زبان طبیعی به کاربر پاسخ دهد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## چالش کدنویسی\n",
    "\n",
    "عالی بود! برای ادامه یادگیری خود درباره OpenAI Function Calling می‌توانید این پروژه را بسازید: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - پارامترهای بیشتری به تابع اضافه کنید تا به یادگیرندگان کمک کند دوره‌های بیشتری پیدا کنند. می‌توانید پارامترهای موجود API را اینجا ببینید:\n",
    " - یک فراخوانی تابع دیگر بسازید که اطلاعات بیشتری مثل زبان مادری یادگیرنده را دریافت کند\n",
    " - مدیریت خطا اضافه کنید تا اگر فراخوانی تابع یا API هیچ دوره مناسبی برنگرداند، پیام مناسبی نمایش داده شود\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**سلب مسئولیت**:  \nاین سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطا یا نادقتی باشند. نسخه اصلی سند به زبان مادری آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم یا تفسیر نادرست ناشی از استفاده از این ترجمه نداریم.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T20:45:04+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "fa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}