{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مقدمه\n",
    "\n",
    "این درس شامل موارد زیر خواهد بود:  \n",
    "- فراخوانی تابع چیست و کاربردهای آن  \n",
    "- چگونه یک فراخوانی تابع با استفاده از OpenAI ایجاد کنیم  \n",
    "- چگونه یک فراخوانی تابع را در یک برنامه ادغام کنیم  \n",
    "\n",
    "## اهداف یادگیری\n",
    "\n",
    "پس از اتمام این درس، شما خواهید دانست و درک خواهید کرد:  \n",
    "\n",
    "- هدف استفاده از فراخوانی تابع  \n",
    "- راه‌اندازی فراخوانی تابع با استفاده از سرویس OpenAI  \n",
    "- طراحی فراخوانی‌های تابع مؤثر برای کاربرد برنامه‌های خود\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## درک فراخوانی توابع\n",
    "\n",
    "برای این درس، می‌خواهیم ویژگی‌ای برای استارتاپ آموزشی خود بسازیم که به کاربران اجازه می‌دهد از یک چت‌بات برای پیدا کردن دوره‌های فنی استفاده کنند. ما دوره‌هایی را پیشنهاد خواهیم داد که با سطح مهارت، نقش فعلی و فناوری مورد علاقه آن‌ها مطابقت دارد.\n",
    "\n",
    "برای تکمیل این کار از ترکیبی از موارد زیر استفاده خواهیم کرد:\n",
    " - `OpenAI` برای ایجاد تجربه چت برای کاربر\n",
    " - `Microsoft Learn Catalog API` برای کمک به کاربران در پیدا کردن دوره‌ها بر اساس درخواست کاربر\n",
    " - `Function Calling` برای گرفتن پرسش کاربر و ارسال آن به یک تابع برای انجام درخواست API.\n",
    "\n",
    "برای شروع، بیایید ببینیم چرا در وهله اول می‌خواهیم از فراخوانی تابع استفاده کنیم:\n",
    "\n",
    "print(\"پیام‌ها در درخواست بعدی:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # دریافت پاسخ جدید از GPT که می‌تواند پاسخ تابع را ببیند\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### چرا فراخوانی تابع\n",
    "\n",
    "اگر هر درس دیگری را در این دوره گذرانده باشید، احتمالاً قدرت استفاده از مدل‌های زبان بزرگ (LLMها) را درک کرده‌اید. امیدواریم بتوانید برخی از محدودیت‌های آن‌ها را نیز ببینید.\n",
    "\n",
    "فراخوانی تابع ویژگی‌ای از سرویس OpenAI است که برای حل چالش‌های زیر طراحی شده است:\n",
    "\n",
    "قالب‌بندی نامنظم پاسخ‌ها:\n",
    "- قبل از فراخوانی تابع، پاسخ‌های مدل زبان بزرگ ساختارمند و منسجم نبودند. توسعه‌دهندگان مجبور بودند کدهای اعتبارسنجی پیچیده‌ای بنویسند تا هر نوع تغییر در خروجی را مدیریت کنند.\n",
    "\n",
    "ادغام محدود با داده‌های خارجی:\n",
    "- پیش از این ویژگی، وارد کردن داده‌ها از بخش‌های دیگر یک برنامه به زمینه چت دشوار بود.\n",
    "\n",
    "با استانداردسازی قالب‌های پاسخ و امکان ادغام بی‌وقفه با داده‌های خارجی، فراخوانی تابع توسعه را ساده‌تر کرده و نیاز به منطق اعتبارسنجی اضافی را کاهش می‌دهد.\n",
    "\n",
    "کاربران نمی‌توانستند پاسخ‌هایی مانند «وضعیت آب و هوای فعلی در استکهلم چیست؟» دریافت کنند. این به این دلیل بود که مدل‌ها محدود به زمانی بودند که داده‌ها آموزش دیده بودند.\n",
    "\n",
    "بیایید به مثال زیر نگاه کنیم که این مشکل را نشان می‌دهد:\n",
    "\n",
    "فرض کنیم می‌خواهیم یک پایگاه داده از اطلاعات دانش‌آموزان ایجاد کنیم تا بتوانیم دوره مناسب را به آن‌ها پیشنهاد دهیم. در زیر دو توصیف از دانش‌آموزان داریم که در داده‌هایشان بسیار مشابه هستند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ما می‌خواهیم این را به یک مدل زبان بزرگ (LLM) ارسال کنیم تا داده‌ها را تجزیه کند. این بعداً می‌تواند در برنامه ما برای ارسال به یک API یا ذخیره در یک پایگاه داده استفاده شود.\n",
    "\n",
    "بیایید دو درخواست یکسان ایجاد کنیم که به مدل زبان بزرگ دستور می‌دهیم به چه اطلاعاتی علاقه‌مند هستیم:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ما می‌خواهیم این را به یک مدل زبان بزرگ (LLM) ارسال کنیم تا بخش‌های مهم برای محصول ما را تجزیه و تحلیل کند. بنابراین می‌توانیم دو درخواست یکسان برای راهنمایی مدل زبان بزرگ ایجاد کنیم:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "پس از ایجاد این دو پرامپت، آن‌ها را با استفاده از `openai.ChatCompletion` به مدل زبان بزرگ ارسال می‌کنیم. ما پرامپت را در متغیر `messages` ذخیره می‌کنیم و نقش را به `user` اختصاص می‌دهیم. این کار برای شبیه‌سازی پیامی است که توسط یک کاربر به یک چت‌بات نوشته می‌شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اکنون می‌توانیم هر دو درخواست را به مدل زبان بزرگ ارسال کنیم و پاسخ دریافتی را بررسی کنیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "با اینکه پرامپت‌ها یکسان هستند و توضیحات مشابه‌اند، می‌توانیم فرمت‌های مختلفی از ویژگی `Grades` دریافت کنیم.\n",
    "\n",
    "اگر سلول بالا را چندین بار اجرا کنید، فرمت می‌تواند به صورت `3.7` یا `3.7 GPA` باشد.\n",
    "\n",
    "این به این دلیل است که مدل زبان بزرگ (LLM) داده‌های بدون ساختار را به صورت پرامپت نوشته شده دریافت می‌کند و همچنین داده‌های بدون ساختار بازمی‌گرداند. ما نیاز داریم فرمت ساختاری داشته باشیم تا بدانیم هنگام ذخیره یا استفاده از این داده‌ها چه انتظاری باید داشته باشیم.\n",
    "\n",
    "با استفاده از فراخوانی تابع، می‌توانیم مطمئن شویم که داده‌های ساختاریافته دریافت می‌کنیم. هنگام استفاده از فراخوانی تابع، مدل زبان بزرگ در واقع هیچ تابعی را فراخوانی یا اجرا نمی‌کند. در عوض، ما ساختاری برای مدل ایجاد می‌کنیم تا در پاسخ‌های خود از آن پیروی کند. سپس از آن پاسخ‌های ساختاریافته استفاده می‌کنیم تا بدانیم در برنامه‌های خود چه تابعی را اجرا کنیم.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![نمودار جریان فراخوانی تابع](../../../../translated_images/fa/Function-Flow.083875364af4f4bb.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "سپس می‌توانیم آنچه از تابع بازگردانده شده است را گرفته و به مدل زبان بزرگ (LLM) ارسال کنیم. مدل زبان بزرگ سپس با استفاده از زبان طبیعی به پرسش کاربر پاسخ خواهد داد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### موارد استفاده از فراخوانی توابع\n",
    "\n",
    "**فراخوانی ابزارهای خارجی**  \n",
    "چت‌بات‌ها در ارائه پاسخ به سوالات کاربران بسیار خوب عمل می‌کنند. با استفاده از فراخوانی توابع، چت‌بات‌ها می‌توانند از پیام‌های کاربران برای انجام وظایف خاص استفاده کنند. به عنوان مثال، یک دانش‌آموز می‌تواند از چت‌بات بخواهد «ارسال ایمیل به استاد من با این مضمون که به کمک بیشتری در این موضوع نیاز دارم». این می‌تواند یک فراخوانی تابع به `send_email(to: string, body: string)` باشد.\n",
    "\n",
    "**ایجاد پرس‌وجوهای API یا پایگاه داده**  \n",
    "کاربران می‌توانند با استفاده از زبان طبیعی اطلاعاتی را پیدا کنند که به یک پرس‌وجوی قالب‌بندی شده یا درخواست API تبدیل می‌شود. مثالی از این مورد می‌تواند معلمی باشد که درخواست می‌کند «دانش‌آموزانی که آخرین تکلیف را انجام داده‌اند چه کسانی هستند» که می‌تواند تابعی به نام `get_completed(student_name: string, assignment: int, current_status: string)` را فراخوانی کند.\n",
    "\n",
    "**ایجاد داده‌های ساختاریافته**  \n",
    "کاربران می‌توانند یک بلوک متن یا CSV را گرفته و از LLM برای استخراج اطلاعات مهم از آن استفاده کنند. به عنوان مثال، یک دانش‌آموز می‌تواند یک مقاله ویکی‌پدیا درباره توافقات صلح را به کارت‌های فلش هوش مصنوعی تبدیل کند. این کار می‌تواند با استفاده از تابعی به نام `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)` انجام شود.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ایجاد اولین فراخوانی تابع شما\n",
    "\n",
    "فرآیند ایجاد یک فراخوانی تابع شامل ۳ مرحله اصلی است:  \n",
    "1. فراخوانی API تکمیل چت با فهرستی از توابع شما و یک پیام کاربر  \n",
    "2. خواندن پاسخ مدل برای انجام یک عمل، یعنی اجرای یک تابع یا فراخوانی API  \n",
    "3. انجام یک فراخوانی دیگر به API تکمیل چت با پاسخ از تابع شما برای استفاده از آن اطلاعات جهت ایجاد پاسخ به کاربر.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![جریان یک فراخوانی تابع](../../../../translated_images/fa/LLM-Flow.3285ed8caf4796d7.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### عناصر یک فراخوانی تابع\n",
    "\n",
    "#### ورودی کاربران\n",
    "\n",
    "اولین مرحله ایجاد یک پیام کاربر است. این می‌تواند به صورت پویا با گرفتن مقدار یک ورودی متنی اختصاص داده شود یا می‌توانید یک مقدار در اینجا اختصاص دهید. اگر این اولین بار است که با API تکمیل چت کار می‌کنید، باید `role` و `content` پیام را تعریف کنیم.\n",
    "\n",
    "`role` می‌تواند یکی از `system` (ایجاد قوانین)، `assistant` (مدل) یا `user` (کاربر نهایی) باشد. برای فراخوانی تابع، ما این را به عنوان `user` و یک سوال نمونه اختصاص خواهیم داد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ایجاد توابع.\n",
    "\n",
    "در ادامه یک تابع و پارامترهای آن تابع را تعریف خواهیم کرد. ما در اینجا فقط از یک تابع به نام `search_courses` استفاده خواهیم کرد اما شما می‌توانید چندین تابع ایجاد کنید.\n",
    "\n",
    "**مهم**: توابع در پیام سیستمی به LLM گنجانده می‌شوند و در تعداد توکن‌های موجود شما محاسبه می‌شوند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**تعاریف**\n",
    "\n",
    "ساختار تعریف تابع چندین سطح دارد که هر کدام ویژگی‌های خاص خود را دارند. در اینجا یک تجزیه و تحلیل از ساختار تو در تو ارائه شده است:\n",
    "\n",
    "**ویژگی‌های تابع در سطح بالا:**\n",
    "\n",
    "`name` - نام تابعی که می‌خواهیم فراخوانی شود.\n",
    "\n",
    "`description` - این توضیحی است درباره نحوه عملکرد تابع. در اینجا مهم است که دقیق و واضح باشید.\n",
    "\n",
    "`parameters` - فهرستی از مقادیر و قالبی که می‌خواهید مدل در پاسخ خود تولید کند.\n",
    "\n",
    "**ویژگی‌های شیء پارامترها:**\n",
    "\n",
    "`type` - نوع داده شیء پارامترها (معمولاً \"object\")\n",
    "\n",
    "`properties` - فهرستی از مقادیر خاصی که مدل برای پاسخ خود استفاده خواهد کرد.\n",
    "\n",
    "**ویژگی‌های هر پارامتر جداگانه:**\n",
    "\n",
    "`name` - به طور ضمنی توسط کلید ویژگی تعریف شده است (مثلاً \"role\"، \"product\"، \"level\")\n",
    "\n",
    "`type` - نوع داده این پارامتر خاص (مثلاً \"string\"، \"number\"، \"boolean\")\n",
    "\n",
    "`description` - توضیح پارامتر خاص.\n",
    "\n",
    "**ویژگی‌های اختیاری:**\n",
    "\n",
    "`required` - آرایه‌ای که پارامترهای مورد نیاز برای تکمیل فراخوانی تابع را فهرست می‌کند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### فراخوانی تابع  \n",
    "پس از تعریف یک تابع، اکنون باید آن را در فراخوانی API تکمیل چت وارد کنیم. این کار را با افزودن `functions` به درخواست انجام می‌دهیم. در این حالت `functions=functions` است.  \n",
    "\n",
    "همچنین گزینه‌ای برای تنظیم `function_call` به `auto` وجود دارد. این بدان معناست که اجازه می‌دهیم مدل زبان بزرگ تصمیم بگیرد کدام تابع باید بر اساس پیام کاربر فراخوانی شود، به جای اینکه خودمان آن را تعیین کنیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "حالا بیایید به پاسخ نگاه کنیم و ببینیم چگونه قالب‌بندی شده است:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "می‌توانید ببینید که نام تابع فراخوانی شده است و از پیام کاربر، مدل زبان بزرگ توانسته داده‌ها را برای مطابقت با آرگومان‌های تابع پیدا کند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ادغام فراخوانی‌های تابع در یک برنامه.\n",
    "\n",
    "پس از اینکه پاسخ قالب‌بندی شده از LLM را آزمایش کردیم، اکنون می‌توانیم این را در یک برنامه ادغام کنیم.\n",
    "\n",
    "### مدیریت جریان\n",
    "\n",
    "برای ادغام این در برنامه خود، بیایید مراحل زیر را انجام دهیم:\n",
    "\n",
    "ابتدا، بیایید فراخوانی به سرویس‌های OpenAI را انجام داده و پیام را در متغیری به نام `response_message` ذخیره کنیم.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اکنون تابعی را تعریف خواهیم کرد که API مایکروسافت لرن را برای دریافت فهرستی از دوره‌ها فراخوانی می‌کند:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "به عنوان یک بهترین روش، سپس بررسی می‌کنیم که آیا مدل می‌خواهد یک تابع را فراخوانی کند یا خیر. پس از آن، یکی از توابع موجود را ایجاد کرده و آن را با تابعی که فراخوانی می‌شود مطابقت می‌دهیم.  \n",
    "سپس آرگومان‌های تابع را گرفته و آن‌ها را به آرگومان‌های LLM نگاشت می‌کنیم.\n",
    "\n",
    "در نهایت، پیام فراخوانی تابع و مقادیری که توسط پیام `search_courses` بازگردانده شده‌اند را اضافه می‌کنیم. این به LLM تمام اطلاعات لازم را می‌دهد تا به کاربر با استفاده از زبان طبیعی پاسخ دهد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اکنون پیام به‌روزشده را به مدل زبان بزرگ (LLM) ارسال می‌کنیم تا بتوانیم پاسخ به زبان طبیعی دریافت کنیم به جای پاسخ قالب‌بندی شده JSON از API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## چالش کد\n",
    "\n",
    "کار عالی! برای ادامه یادگیری خود در مورد فراخوانی تابع OpenAI می‌توانید بسازید: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    "- پارامترهای بیشتری از تابع که ممکن است به یادگیرندگان کمک کند دوره‌های بیشتری پیدا کنند. می‌توانید پارامترهای API موجود را اینجا پیدا کنید:  \n",
    "- ایجاد یک فراخوانی تابع دیگر که اطلاعات بیشتری از یادگیرنده مانند زبان مادری آن‌ها بگیرد  \n",
    "- ایجاد مدیریت خطا زمانی که فراخوانی تابع و/یا فراخوانی API هیچ دوره مناسبی را باز نمی‌گرداند\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**سلب مسئولیت**:  \nاین سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نواقصی باشند. سند اصلی به زبان بومی خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ گونه سوءتفاهم یا تفسیر نادرستی که از استفاده این ترجمه ناشی شود، نیستیم.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T08:58:50+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "fa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}