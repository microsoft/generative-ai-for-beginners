{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ساخت با مدل‌های خانواده متا\n",
    "\n",
    "## مقدمه\n",
    "\n",
    "در این درس به موارد زیر می‌پردازیم:\n",
    "\n",
    "- بررسی دو مدل اصلی خانواده متا - Llama 3.1 و Llama 3.2\n",
    "- درک کاربردها و سناریوهای هر مدل\n",
    "- نمونه کد برای نمایش ویژگی‌های منحصربه‌فرد هر مدل\n",
    "\n",
    "## خانواده مدل‌های متا\n",
    "\n",
    "در این درس، دو مدل از خانواده متا یا \"گله لاما\" را بررسی می‌کنیم - Llama 3.1 و Llama 3.2\n",
    "\n",
    "این مدل‌ها در نسخه‌های مختلف ارائه شده‌اند و در بازار مدل‌های گیت‌هاب در دسترس هستند. برای اطلاعات بیشتر درباره استفاده از مدل‌های گیت‌هاب جهت [نمونه‌سازی با مدل‌های هوش مصنوعی](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) این لینک را ببینید.\n",
    "\n",
    "انواع مدل‌ها:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*توجه: Llama 3 نیز در مدل‌های گیت‌هاب موجود است اما در این درس پوشش داده نمی‌شود*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## لاما ۳.۱\n",
    "\n",
    "با داشتن ۴۰۵ میلیارد پارامتر، لاما ۳.۱ در دسته مدل‌های متن‌باز LLM قرار می‌گیرد.\n",
    "\n",
    "این مدل نسبت به نسخه قبلی یعنی لاما ۳ ارتقا یافته و امکانات زیر را ارائه می‌دهد:\n",
    "\n",
    "- پنجره متنی بزرگ‌تر – ۱۲۸ هزار توکن در مقابل ۸ هزار توکن\n",
    "- حداکثر توکن خروجی بیشتر – ۴۰۹۶ در مقابل ۲۰۴۸\n",
    "- پشتیبانی بهتر از چندزبانگی – به دلیل افزایش تعداد توکن‌های آموزشی\n",
    "\n",
    "این ویژگی‌ها باعث می‌شوند لاما ۳.۱ بتواند در ساخت برنامه‌های GenAI موارد پیچیده‌تری را مدیریت کند، از جمله:\n",
    "- فراخوانی توابع به صورت بومی – امکان فراخوانی ابزارها و توابع خارجی خارج از جریان کاری LLM\n",
    "- عملکرد بهتر RAG – به دلیل پنجره متنی بزرگ‌تر\n",
    "- تولید داده مصنوعی – توانایی ساخت داده‌های مؤثر برای کارهایی مانند ریزتنظیم\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### فراخوانی توابع بومی\n",
    "\n",
    "Llama 3.1 به گونه‌ای بهینه‌سازی شده که در فراخوانی توابع یا ابزارها عملکرد بهتری داشته باشد. همچنین دو ابزار داخلی دارد که مدل می‌تواند بر اساس درخواست کاربر تشخیص دهد که باید از آن‌ها استفاده کند. این ابزارها عبارتند از:\n",
    "\n",
    "- **Brave Search** - می‌تواند برای دریافت اطلاعات به‌روز مانند وضعیت آب‌وهوا با جستجوی وب استفاده شود\n",
    "- **Wolfram Alpha** - می‌تواند برای انجام محاسبات ریاضی پیچیده‌تر به کار رود تا نیازی به نوشتن توابع اختصاصی نباشد.\n",
    "\n",
    "همچنین می‌توانید ابزارهای سفارشی خود را بسازید تا LLM بتواند آن‌ها را فراخوانی کند.\n",
    "\n",
    "در مثال کد زیر:\n",
    "\n",
    "- ابزارهای موجود (brave_search, wolfram_alpha) را در پیام سیستم تعریف می‌کنیم.\n",
    "- یک پیام کاربر ارسال می‌شود که درباره وضعیت آب‌وهوای یک شهر خاص سؤال می‌کند.\n",
    "- LLM با یک فراخوانی ابزار به Brave Search پاسخ می‌دهد که به این صورت خواهد بود: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*توجه: این مثال فقط فراخوانی ابزار را انجام می‌دهد. اگر می‌خواهید نتایج را دریافت کنید، باید یک حساب رایگان در صفحه API بریو بسازید و خود تابع را تعریف کنید.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### لاما ۳.۲\n",
    "\n",
    "با وجود اینکه لاما ۳.۱ یک مدل زبانی بزرگ است، اما یکی از محدودیت‌های آن نبود قابلیت چندرسانه‌ای است. یعنی نمی‌تواند از ورودی‌های مختلف مثل تصویر به عنوان پرامپت استفاده کند و پاسخ بدهد. این قابلیت یکی از ویژگی‌های اصلی لاما ۳.۲ به حساب می‌آید. این ویژگی‌ها شامل موارد زیر هستند:\n",
    "\n",
    "- چندرسانه‌ای بودن – توانایی ارزیابی همزمان پرامپت‌های متنی و تصویری را دارد\n",
    "- نسخه‌های کوچک تا متوسط (۱۱ میلیارد و ۹۰ میلیارد) – این موضوع گزینه‌های استقرار منعطفی را فراهم می‌کند،\n",
    "- نسخه‌های فقط متنی (۱ میلیارد و ۳ میلیارد) – این امکان را می‌دهد که مدل روی دستگاه‌های لبه یا موبایل اجرا شود و تأخیر کمی داشته باشد\n",
    "\n",
    "پشتیبانی از چندرسانه‌ای بودن، یک گام بزرگ در دنیای مدل‌های متن‌باز به شمار می‌رود. نمونه کد زیر همزمان یک تصویر و یک پرامپت متنی را دریافت می‌کند تا تحلیلی از تصویر توسط لاما ۳.۲ نسخه ۹۰ میلیارد ارائه شود.\n",
    "\n",
    "### پشتیبانی چندرسانه‌ای با لاما ۳.۲\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## یادگیری اینجا متوقف نمی‌شود، مسیر را ادامه دهید\n",
    "\n",
    "بعد از اتمام این درس، مجموعه [یادگیری هوش مصنوعی مولد](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) را ببینید تا دانش خود را در زمینه هوش مصنوعی مولد بیشتر کنید!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**سلب مسئولیت**:  \nاین سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطا یا نادقتی باشند. نسخه اصلی سند به زبان مادری آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم یا تفسیر نادرست ناشی از استفاده از این ترجمه نداریم.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:37:18+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "fa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}