{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ساخت با مدل‌های Mistral\n",
    "\n",
    "## مقدمه\n",
    "\n",
    "در این درس به موارد زیر پرداخته می‌شود:\n",
    "- بررسی مدل‌های مختلف Mistral\n",
    "- آشنایی با کاربردها و سناریوهای هر مدل\n",
    "- نمونه کدهایی که ویژگی‌های منحصربه‌فرد هر مدل را نشان می‌دهند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مدل‌های میسترال\n",
    "\n",
    "در این درس، با سه مدل مختلف میسترال آشنا می‌شویم: **Mistral Large**، **Mistral Small** و **Mistral Nemo**.\n",
    "\n",
    "هر یک از این مدل‌ها به صورت رایگان در بازار مدل‌های گیت‌هاب در دسترس هستند. کد این دفترچه از این مدل‌ها برای اجرای کد استفاده می‌کند. برای اطلاعات بیشتر درباره استفاده از مدل‌های گیت‌هاب جهت [نمونه‌سازی با مدل‌های هوش مصنوعی](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) اینجا را ببینید.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## میسترال لارج ۲ (۲۴۰۷)\n",
    "میسترال لارج ۲ در حال حاضر مدل پرچم‌دار شرکت میسترال است و برای استفاده سازمانی طراحی شده است.\n",
    "\n",
    "این مدل نسبت به نسخه اصلی میسترال لارج ارتقا یافته و ویژگی‌های زیر را ارائه می‌دهد:\n",
    "- پنجره متنی بزرگ‌تر - ۱۲۸ هزار در مقابل ۳۲ هزار\n",
    "- عملکرد بهتر در وظایف ریاضی و برنامه‌نویسی - دقت متوسط ۷۶.۹٪ در مقابل ۶۰.۴٪\n",
    "- بهبود عملکرد چندزبانه - زبان‌ها شامل: انگلیسی، فرانسوی، آلمانی، اسپانیایی، ایتالیایی، پرتغالی، هلندی، روسی، چینی، ژاپنی، کره‌ای، عربی و هندی.\n",
    "\n",
    "با این قابلیت‌ها، میسترال لارج در موارد زیر بسیار موفق عمل می‌کند:\n",
    "- *تولید تقویت‌شده با بازیابی (RAG)* - به دلیل پنجره متنی بزرگ‌تر\n",
    "- *فراخوانی تابع* - این مدل به طور بومی از فراخوانی تابع پشتیبانی می‌کند و امکان ادغام با ابزارها و APIهای خارجی را فراهم می‌سازد. این فراخوانی‌ها می‌توانند به صورت موازی یا پشت سر هم انجام شوند.\n",
    "- *تولید کد* - این مدل در تولید کد برای زبان‌های پایتون، جاوا، تایپ‌اسکریپت و ++C بسیار قدرتمند است.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### نمونه RAG با استفاده از Mistral Large 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در این مثال، ما از مدل Mistral Large 2 برای اجرای الگوی RAG روی یک سند متنی استفاده می‌کنیم. سؤال به زبان کره‌ای نوشته شده و درباره فعالیت‌های نویسنده قبل از ورود به دانشگاه پرسیده است.\n",
    "\n",
    "در اینجا از مدل Cohere Embeddings برای ساختن embeddingهای سند متنی و همچنین سؤال استفاده شده است. برای این نمونه، بسته پایتون faiss به عنوان فروشگاه برداری به کار رفته است.\n",
    "\n",
    "در پرامپتی که به مدل Mistral ارسال می‌شود، هم سؤال و هم بخش‌هایی از متن که بیشترین شباهت را به سؤال دارند، قرار داده می‌شوند. سپس مدل یک پاسخ طبیعی به زبان ارائه می‌دهد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /home/codespace/.python/current/lib/python3.12/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author primarily engaged in two activities before college: writing and programming. In terms of writing, they wrote short stories, albeit not very good ones, with minimal plot and characters expressing strong feelings. For programming, they started writing programs on the IBM 1401 used for data processing during their 9th grade, at the age of 13 or 14. They used an early version of Fortran and typed programs on punch cards, later loading them into the card reader to run the program.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference import EmbeddingsClient\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-large\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text\n",
    "\n",
    "chunk_size = 2048\n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "len(chunks)\n",
    "\n",
    "embed_model_name = \"cohere-embed-v3-multilingual\" \n",
    "\n",
    "embed_client = EmbeddingsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(token)\n",
    ")\n",
    "\n",
    "embed_response = embed_client.embed(\n",
    "    input=chunks,\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "text_embeddings = []\n",
    "for item in embed_response.data:\n",
    "    length = len(item.embedding)\n",
    "    text_embeddings.append(item.embedding)\n",
    "text_embeddings = np.array(text_embeddings)\n",
    "\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)\n",
    "\n",
    "question = \"저자가 대학에 오기 전에 주로 했던 두 가지 일은 무엇이었나요?？\"\n",
    "\n",
    "question_embedding = embed_client.embed(\n",
    "    input=[question],\n",
    "    model=embed_model_name\n",
    ")\n",
    "\n",
    "question_embeddings = np.array(question_embedding.data[0].embedding)\n",
    "\n",
    "\n",
    "D, I = index.search(question_embeddings.reshape(1, -1), k=2) # distance, index\n",
    "retrieved_chunks = [chunks[i] for i in I.tolist()[0]]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunks}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=prompt),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## میسترال اسمال\n",
    "میسترال اسمال یکی دیگر از مدل‌های خانواده میسترال است که در دسته مدل‌های پریمیر/سازمانی قرار می‌گیرد. همانطور که از نامش پیداست، این مدل یک مدل زبانی کوچک (SLM) است. مزایای استفاده از میسترال اسمال عبارتند از:\n",
    "- صرفه‌جویی در هزینه نسبت به مدل‌های زبانی بزرگ میسترال مثل Mistral Large و NeMo - کاهش قیمت تا ۸۰٪\n",
    "- تأخیر پایین - پاسخ‌دهی سریع‌تر نسبت به مدل‌های زبانی بزرگ میسترال\n",
    "- انعطاف‌پذیر - قابلیت اجرا در محیط‌های مختلف با محدودیت کمتر در منابع مورد نیاز\n",
    "\n",
    "میسترال اسمال برای این موارد عالی است:\n",
    "- کارهای متنی مثل خلاصه‌سازی، تحلیل احساسات و ترجمه\n",
    "- اپلیکیشن‌هایی که به دلیل مقرون‌به‌صرفه بودن، درخواست‌های مکرر دارند\n",
    "- کارهای کدنویسی با تأخیر پایین مثل بازبینی و پیشنهاد کد\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مقایسه Mistral Small و Mistral Large\n",
    "\n",
    "برای مشاهده تفاوت‌های تأخیر بین Mistral Small و Large، سلول‌های زیر را اجرا کنید.\n",
    "\n",
    "باید تفاوتی در زمان پاسخ‌دهی بین ۳ تا ۵ ثانیه مشاهده کنید. همچنین به طول و سبک پاسخ‌ها در برابر یک پرامپت یکسان توجه کنید.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-small\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Mistral-large\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "        UserMessage(content=\"Can you write a Python function to the fizz buzz test?\"),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## میسترال نِمو\n",
    "\n",
    "در مقایسه با دو مدل دیگری که در این درس بررسی شدند، میسترال نِمو تنها مدلی است که به صورت رایگان و با مجوز Apache2 ارائه می‌شود.\n",
    "\n",
    "این مدل به عنوان نسخه ارتقاء یافته مدل متن‌باز قبلی میسترال، یعنی Mistral 7B شناخته می‌شود.\n",
    "\n",
    "برخی ویژگی‌های دیگر مدل نِمو عبارتند از:\n",
    "\n",
    "- *توکن‌سازی کارآمدتر:* این مدل به جای tiktoken که رایج‌تر است، از توکنایزر Tekken استفاده می‌کند. این موضوع باعث می‌شود عملکرد بهتری در زبان‌ها و کدهای مختلف داشته باشد.\n",
    "\n",
    "- *قابلیت فاین‌تیونینگ:* مدل پایه برای فاین‌تیونینگ در دسترس است. این ویژگی انعطاف‌پذیری بیشتری برای کاربردهایی که نیاز به فاین‌تیونینگ دارند فراهم می‌کند.\n",
    "\n",
    "- *پشتیبانی بومی از فراخوانی توابع* - مانند Mistral Large، این مدل نیز برای فراخوانی توابع آموزش دیده است. این ویژگی باعث می‌شود یکی از اولین مدل‌های متن‌باز با این قابلیت باشد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## میسترال نِمو\n",
    "\n",
    "در مقایسه با دو مدل دیگری که در این درس بررسی شدند، میسترال نِمو تنها مدلی است که به صورت رایگان و با مجوز Apache2 ارائه می‌شود.\n",
    "\n",
    "این مدل به عنوان نسخه ارتقاء یافته مدل متن‌باز قبلی میسترال، یعنی Mistral 7B شناخته می‌شود.\n",
    "\n",
    "برخی ویژگی‌های دیگر مدل نِمو عبارتند از:\n",
    "\n",
    "- *توکن‌سازی کارآمدتر:* این مدل به جای tiktoken که رایج‌تر است، از توکنایزر Tekken استفاده می‌کند. این موضوع باعث می‌شود عملکرد بهتری در زبان‌ها و کدهای مختلف داشته باشد.\n",
    "\n",
    "- *قابلیت فاین‌تیونینگ:* مدل پایه برای فاین‌تیونینگ در دسترس است. این ویژگی انعطاف‌پذیری بیشتری برای کاربردهایی که نیاز به فاین‌تیونینگ دارند فراهم می‌کند.\n",
    "\n",
    "- *پشتیبانی بومی از فراخوانی توابع* - مانند Mistral Large، این مدل نیز روی فراخوانی توابع آموزش دیده است. این ویژگی باعث می‌شود یکی از اولین مدل‌های متن‌باز با این قابلیت باشد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### مقایسه توکنایزرها\n",
    "\n",
    "در این نمونه، بررسی می‌کنیم که Mistral NeMo چگونه توکنیزه کردن را نسبت به Mistral Large انجام می‌دهد.\n",
    "\n",
    "هر دو نمونه از یک پرامپت یکسان استفاده می‌کنند، اما باید متوجه شوید که NeMo تعداد توکن کمتری نسبت به Mistral Large برمی‌گرداند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistral-common\n",
      "  Downloading mistral_common-1.4.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (4.23.0)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (2.1.1)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (10.4.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from mistral-common) (2.9.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from mistral-common) (2.32.3)\n",
      "Collecting sentencepiece==0.2.0 (from mistral-common)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from mistral-common)\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from mistral-common) (4.12.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common) (0.20.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.1->mistral-common) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.6.1->mistral-common) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->mistral-common) (2024.8.30)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<0.8.0,>=0.7.0->mistral-common)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Downloading mistral_common-1.4.4-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, regex, tiktoken, mistral-common\n",
      "Successfully installed mistral-common-1.4.4 regex-2024.9.11 sentencepiece-0.2.0 tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mistral-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"open-mistral-nemo\t\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages:\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load Mistral tokenizer\n",
    "\n",
    "model_name = \"mistral-large-latest\"\n",
    "\n",
    "tokenizer = MistralTokenizer.from_model(model_name)\n",
    "\n",
    "# Tokenize a list of messages\n",
    "tokenized = tokenizer.encode_chat_completion(\n",
    "    ChatCompletionRequest(\n",
    "        tools=[\n",
    "            Tool(\n",
    "                function=Function(\n",
    "                    name=\"get_current_weather\",\n",
    "                    description=\"Get the current weather\",\n",
    "                    parameters={\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        messages=[\n",
    "            UserMessage(content=\"What's the weather like today in Paris\"),\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    ")\n",
    "tokens, text = tokenized.tokens, tokenized.text\n",
    "\n",
    "# Count the number of tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## یادگیری اینجا متوقف نمی‌شود، مسیر را ادامه دهید\n",
    "\n",
    "بعد از اتمام این درس، مجموعه [یادگیری هوش مصنوعی مولد](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) را ببینید تا دانش خود را در زمینه هوش مصنوعی مولد بیشتر کنید!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**سلب مسئولیت**:  \nاین سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطا یا نادقتی باشند. نسخه اصلی سند به زبان مادری آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم یا تفسیر نادرست ناشی از استفاده از این ترجمه نداریم.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "coopTranslator": {
   "original_hash": "03bc2561665a8b411acfc24a8aa0d442",
   "translation_date": "2025-08-25T22:12:59+00:00",
   "source_file": "20-mistral/python/githubmodels-assignment.ipynb",
   "language_code": "fa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}