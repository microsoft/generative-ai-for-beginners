{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# मेटा फॅमिली मॉडेल्ससह बांधणी\n",
    "\n",
    "## परिचय\n",
    "\n",
    "या धड्यात आपण पाहणार आहोत:\n",
    "\n",
    "- मेटा फॅमिलीमधील दोन मुख्य मॉडेल्स - Llama 3.1 आणि Llama 3.2 यांचा अभ्यास\n",
    "- प्रत्येक मॉडेलसाठी वापराचे प्रकार आणि परिस्थिती समजून घेणे\n",
    "- प्रत्येक मॉडेलच्या खास वैशिष्ट्ये दाखवणारा कोड नमुना\n",
    "\n",
    "## मेटा फॅमिलीचे मॉडेल्स\n",
    "\n",
    "या धड्यात आपण मेटा फॅमिली किंवा \"Llama Herd\" मधील २ मॉडेल्स - Llama 3.1 आणि Llama 3.2 यांचा अभ्यास करणार आहोत.\n",
    "\n",
    "ही मॉडेल्स वेगवेगळ्या प्रकारांमध्ये उपलब्ध आहेत आणि Github Model marketplace वर मिळू शकतात. Github Models वापरून [AI मॉडेल्ससह प्रोटोटायपिंग](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) कसे करायचे याची अधिक माहिती येथे आहे.\n",
    "\n",
    "मॉडेल प्रकार:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*टीप: Llama 3 हे देखील Github Models वर उपलब्ध आहे, पण या धड्यात त्याचा समावेश नाही*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "४०५ अब्ज पॅरामीटर्ससह, Llama 3.1 हा ओपन सोर्स LLM श्रेणीत येतो.\n",
    "\n",
    "हा मोड पूर्वीच्या Llama 3 च्या तुलनेत पुढील सुधारणा देतो:\n",
    "\n",
    "- मोठे कॉन्टेक्स्ट विंडो - १२८k टोकन्स विरुद्ध ८k टोकन्स\n",
    "- जास्तीत जास्त आउटपुट टोकन्स - ४०९६ विरुद्ध २०४८\n",
    "- अधिक चांगला बहुभाषिक सपोर्ट - प्रशिक्षण टोकन्समध्ये वाढ झाल्यामुळे\n",
    "\n",
    "या सुधारणा Llama 3.1 ला GenAI अ‍ॅप्लिकेशन्स तयार करताना अधिक क्लिष्ट वापरासाठी सक्षम करतात, जसे की:\n",
    "- नेटिव फंक्शन कॉलिंग - LLM वर्कफ्लोच्या बाहेर बाह्य टूल्स आणि फंक्शन्स कॉल करण्याची क्षमता\n",
    "- अधिक चांगली RAG कार्यक्षमता - मोठ्या कॉन्टेक्स्ट विंडोमुळे\n",
    "- सिंथेटिक डेटा जनरेशन - फाइन-ट्यूनिंगसारख्या कार्यांसाठी प्रभावी डेटा तयार करण्याची क्षमता\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### नेटिव फंक्शन कॉलिंग\n",
    "\n",
    "Llama 3.1 ला फंक्शन किंवा टूल कॉल्स करण्यासाठी अधिक प्रभावीपणे फाइन-ट्यून करण्यात आले आहे. यामध्ये दोन अंगभूत टूल्स आहेत, ज्या मॉडेलला वापरकर्त्याच्या प्रॉम्प्टनुसार वापरण्याची गरज आहे का हे ओळखता येते. ही टूल्स म्हणजे:\n",
    "\n",
    "- **Brave Search** - वेब सर्च करून हवामानासारखी ताजी माहिती मिळवण्यासाठी वापरता येते\n",
    "- **Wolfram Alpha** - अधिक क्लिष्ट गणिती गणनांसाठी वापरता येते, त्यामुळे स्वतःचे फंक्शन्स लिहिण्याची गरज नाही\n",
    "\n",
    "तुम्ही स्वतःची कस्टम टूल्स देखील तयार करू शकता, जी LLM कॉल करू शकते.\n",
    "\n",
    "खालील कोड उदाहरणात:\n",
    "\n",
    "- आपण उपलब्ध टूल्स (brave_search, wolfram_alpha) सिस्टीम प्रॉम्प्टमध्ये परिभाषित करतो.\n",
    "- वापरकर्त्याचा प्रॉम्प्ट पाठवतो, जो एखाद्या शहरातील हवामानाबद्दल विचारतो.\n",
    "- LLM Brave Search टूलला कॉल करून प्रतिसाद देईल, जो अशा प्रकारे दिसेल `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*टीप: या उदाहरणात फक्त टूल कॉल केले जाते, तुम्हाला प्रत्यक्ष निकाल हवे असतील तर, तुम्हाला Brave API पेजवर मोफत खाते तयार करावे लागेल आणि फंक्शन स्वतः परिभाषित करावे लागेल*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "LLM असूनही, Llama 3.1 मध्ये एक मर्यादा म्हणजे मल्टिमोडॅलिटी. म्हणजेच, वेगवेगळ्या प्रकारच्या इनपुट्स जसे की प्रतिमा प्रॉम्प्ट म्हणून वापरणे आणि त्यावर उत्तर देणे. ही क्षमता Llama 3.2 ची एक मुख्य वैशिष्ट्य आहे. या वैशिष्ट्यांमध्ये पुढील गोष्टींचा समावेश आहे:\n",
    "\n",
    "- मल्टिमोडॅलिटी - मजकूर आणि प्रतिमा या दोन्ही प्रकारच्या प्रॉम्प्ट्सचे मूल्यांकन करण्याची क्षमता आहे\n",
    "- लहान ते मध्यम आकाराचे व्हेरिएंट्स (11B आणि 90B) - हे लवचिक डिप्लॉयमेंट पर्याय उपलब्ध करतात,\n",
    "- फक्त मजकूर व्हेरिएंट्स (1B आणि 3B) - हे मॉडेलला एज / मोबाइल डिव्हाइसेसवर डिप्लॉय करता येते आणि कमी विलंब प्रदान करते\n",
    "\n",
    "मल्टिमोडल सपोर्ट हे ओपन सोर्स मॉडेल्सच्या जगात एक मोठे पाऊल आहे. खालील कोड उदाहरणात प्रतिमा आणि मजकूर प्रॉम्प्ट दोन्ही वापरून Llama 3.2 90B कडून प्रतिमेचे विश्लेषण मिळवले आहे.\n",
    "\n",
    "### Llama 3.2 सह मल्टिमोडल सपोर्ट\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## शिक्षण इथेच थांबत नाही, प्रवास सुरू ठेवा\n",
    "\n",
    "हे धडा पूर्ण केल्यानंतर, आमच्या [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) ला भेट द्या आणि तुमचे Generative AI ज्ञान आणखी वाढवा!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nहे दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून भाषांतरित केले आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील दस्तऐवज हा अधिकृत स्रोत मानावा. अत्यावश्यक माहितीसाठी व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर केल्यामुळे उद्भवणाऱ्या कोणत्याही गैरसमजुती किंवा चुकीच्या अर्थ लावण्यास आम्ही जबाबदार राहणार नाही.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:40:37+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "mr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}