{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Ailesi Modelleriyle Çalışmak\n",
    "\n",
    "## Giriş\n",
    "\n",
    "Bu derste şunları öğreneceksiniz:\n",
    "\n",
    "- Meta ailesinin iki ana modelini keşfetmek - Llama 3.1 ve Llama 3.2\n",
    "- Her modelin kullanım alanlarını ve senaryolarını anlamak\n",
    "- Her modelin kendine özgü özelliklerini gösteren bir kod örneği\n",
    "\n",
    "## Meta Model Ailesi\n",
    "\n",
    "Bu derste, Meta ailesinden veya \"Llama Sürüsü\"nden 2 modeli inceleyeceğiz - Llama 3.1 ve Llama 3.2\n",
    "\n",
    "Bu modeller farklı varyantlarda gelir ve Github Model pazarında bulunabilir. Github Modelleri kullanarak [AI modelleriyle prototipleme](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst) hakkında daha fazla bilgiye buradan ulaşabilirsiniz.\n",
    "\n",
    "Model Varyantları:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Not: Llama 3 de Github Modellerde mevcut, ancak bu derste ele alınmayacak*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "405 Milyar Parametre ile Llama 3.1, açık kaynak LLM kategorisine giriyor.\n",
    "\n",
    "Bu model, önceki Llama 3 sürümüne göre şu geliştirmeleri sunuyor:\n",
    "\n",
    "- Daha büyük bağlam penceresi - 128k token, önceki 8k tokene kıyasla\n",
    "- Daha yüksek Maksimum Çıktı Tokenı - 4096, önceki 2048'e kıyasla\n",
    "- Daha iyi Çok Dilli Destek - eğitim tokenlarının artması sayesinde\n",
    "\n",
    "Bunlar, Llama 3.1’in GenAI uygulamaları geliştirirken daha karmaşık kullanım senaryolarını yönetmesini sağlıyor. Bunlar arasında:\n",
    "- Yerel Fonksiyon Çağırma - LLM iş akışının dışında harici araç ve fonksiyonları çağırabilme yeteneği\n",
    "- Daha İyi RAG Performansı - daha yüksek bağlam penceresi sayesinde\n",
    "- Sentetik Veri Üretimi - ince ayar gibi görevler için etkili veri oluşturabilme yeteneği\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yerel Fonksiyon Çağrısı\n",
    "\n",
    "Llama 3.1, fonksiyon veya araç çağrıları yapmada daha etkili olacak şekilde ince ayarlandı. Ayrıca, modelin kullanıcıdan gelen isteğe göre kullanılması gerektiğini anlayabileceği iki yerleşik aracı bulunuyor. Bu araçlar:\n",
    "\n",
    "- **Brave Search** - Web araması yaparak hava durumu gibi güncel bilgilere ulaşmak için kullanılabilir\n",
    "- **Wolfram Alpha** - Daha karmaşık matematiksel hesaplamalar için kullanılabilir, böylece kendi fonksiyonlarınızı yazmanıza gerek kalmaz.\n",
    "\n",
    "Ayrıca, LLM'nin çağırabileceği kendi özel araçlarınızı da oluşturabilirsiniz.\n",
    "\n",
    "Aşağıdaki kod örneğinde:\n",
    "\n",
    "- Kullanılabilir araçları (brave_search, wolfram_alpha) sistem isteminde tanımlıyoruz.\n",
    "- Belirli bir şehirdeki hava durumu hakkında soru soran bir kullanıcı istemi gönderiyoruz.\n",
    "- LLM, Brave Search aracına bir araç çağrısı ile yanıt verecek ve bu şekilde görünecek: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Not: Bu örnek yalnızca araç çağrısı yapar, sonuçları almak isterseniz Brave API sayfasında ücretsiz bir hesap oluşturmanız ve fonksiyonu kendiniz tanımlamanız gerekir.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Llama 3.1 bir LLM olmasına rağmen, sahip olduğu sınırlamalardan biri çok modluluk özelliğinin olmamasıdır. Yani, farklı türde girdileri (örneğin görselleri) istem olarak kullanıp yanıt verebilme yeteneği eksiktir. Bu yetenek, Llama 3.2'nin en önemli özelliklerinden biridir. Bu özellikler ayrıca şunları da içerir:\n",
    "\n",
    "- Çok modluluk - hem metin hem de görsel istemleri değerlendirme yeteneğine sahiptir\n",
    "- Küçük ve Orta boyutlu varyasyonlar (11B ve 90B) - bu, esnek dağıtım seçenekleri sunar,\n",
    "- Sadece metin varyasyonları (1B ve 3B) - bu, modelin uç / mobil cihazlarda dağıtılmasına ve düşük gecikme sağlamasına olanak tanır\n",
    "\n",
    "Çok modlu destek, açık kaynak modeller dünyasında büyük bir adımı temsil ediyor. Aşağıdaki kod örneği, hem bir görsel hem de metin istemi alarak Llama 3.2 90B'den görselin analizini elde ediyor.\n",
    "\n",
    "### Llama 3.2 ile Çok Modlu Destek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Öğrenme burada bitmiyor, yolculuğa devam et\n",
    "\n",
    "Bu dersi tamamladıktan sonra, Generative AI bilginizi geliştirmeye devam etmek için [Generative AI Öğrenme koleksiyonumuza](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) göz atmayı unutmayın!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Feragatname**:  \nBu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerde hata veya yanlışlıklar bulunabileceğini lütfen unutmayın. Belgenin orijinal diliyle hazırlanmış hali esas alınmalıdır. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından doğabilecek yanlış anlama veya yanlış yorumlamalardan sorumlu değiliz.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:42:54+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "tr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}