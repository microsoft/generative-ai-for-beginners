<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68664f7e754a892ae1d8d5e2b7bd2081",
  "translation_date": "2025-07-09T17:42:43+00:00",
  "source_file": "18-fine-tuning/README.md",
  "language_code": "tr"
}
-->
[![Open Source Models](../../../translated_images/18-lesson-banner.f30176815b1a5074fce9cceba317720586caa99e24001231a92fd04eeb54a121.tr.png)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# LLMâ€™inizi Ä°nce Ayarlama

BÃ¼yÃ¼k dil modellerini kullanarak Ã¼retken yapay zeka uygulamalarÄ± geliÅŸtirmek yeni zorluklarÄ± beraberinde getirir. Temel sorunlardan biri, modelin belirli bir kullanÄ±cÄ± isteÄŸi iÃ§in oluÅŸturduÄŸu iÃ§eriÄŸin yanÄ±t kalitesini (doÄŸruluk ve alaka) saÄŸlamaktÄ±r. Ã–nceki derslerde, mevcut modele _girdi istemini deÄŸiÅŸtirerek_ bu sorunu Ã§Ã¶zmeye Ã§alÄ±ÅŸan prompt mÃ¼hendisliÄŸi ve retrieval-augmented generation gibi teknikleri ele aldÄ±k.

BugÃ¼nkÃ¼ derste, bu zorluÄŸu _modelin kendisini ek verilerle yeniden eÄŸiterek_ Ã§Ã¶zmeyi amaÃ§layan Ã¼Ã§Ã¼ncÃ¼ bir teknik olan **ince ayarlama**yÄ± tartÄ±ÅŸacaÄŸÄ±z. Detaylara geÃ§elim.

## Ã–ÄŸrenme Hedefleri

Bu ders, Ã¶nceden eÄŸitilmiÅŸ dil modelleri iÃ§in ince ayarlama kavramÄ±nÄ± tanÄ±tÄ±r, bu yaklaÅŸÄ±mÄ±n faydalarÄ±nÄ± ve zorluklarÄ±nÄ± inceler ve Ã¼retken yapay zeka modellerinizin performansÄ±nÄ± artÄ±rmak iÃ§in ince ayarlamanÄ±n ne zaman ve nasÄ±l kullanÄ±lacaÄŸÄ±na dair rehberlik saÄŸlar.

Dersin sonunda aÅŸaÄŸÄ±daki sorularÄ± yanÄ±tlayabilmelisiniz:

- Dil modelleri iÃ§in ince ayarlama nedir?
- Ä°nce ayarlama ne zaman ve neden faydalÄ±dÄ±r?
- Ã–nceden eÄŸitilmiÅŸ bir modeli nasÄ±l ince ayarlayabilirim?
- Ä°nce ayarlamanÄ±n sÄ±nÄ±rlamalarÄ± nelerdir?

HazÄ±r mÄ±sÄ±nÄ±z? BaÅŸlayalÄ±m.

## GÃ¶rselleÅŸtirilmiÅŸ Rehber

Ä°Ã§eriÄŸe dalmadan Ã¶nce genel resmi gÃ¶rmek ister misiniz? Bu dersin Ã¶ÄŸrenme yolculuÄŸunu anlatan gÃ¶rselleÅŸtirilmiÅŸ rehbere gÃ¶z atÄ±n â€” ince ayarlamanÄ±n temel kavramlarÄ± ve motivasyonundan baÅŸlayarak, ince ayarlama sÃ¼recini ve en iyi uygulamalarÄ± anlamaya kadar. Bu keÅŸif iÃ§in bÃ¼yÃ¼leyici bir konu, bu yÃ¼zden kendi kendinize Ã¶ÄŸrenme yolculuÄŸunuzu destekleyecek ek baÄŸlantÄ±lar iÃ§in [Kaynaklar](./RESOURCES.md?WT.mc_id=academic-105485-koreyst) sayfasÄ±nÄ± unutmayÄ±n!

![Dil Modellerini Ä°nce Ayarlamaya GÃ¶rselleÅŸtirilmiÅŸ Rehber](../../../translated_images/18-fine-tuning-sketchnote.11b21f9ec8a703467a120cb79a28b5ac1effc8d8d9d5b31bbbac6b8640432e14.tr.png)

## Dil Modelleri iÃ§in Ä°nce Ayarlama Nedir?

TanÄ±m olarak, bÃ¼yÃ¼k dil modelleri internet dahil Ã§eÅŸitli kaynaklardan toplanan bÃ¼yÃ¼k miktarda metin Ã¼zerinde _Ã¶nceden eÄŸitilmiÅŸtir_. Ã–nceki derslerde Ã¶ÄŸrendiÄŸimiz gibi, modelin kullanÄ±cÄ± sorularÄ±na ("promptlara") verdiÄŸi yanÄ±tlarÄ±n kalitesini artÄ±rmak iÃ§in _prompt mÃ¼hendisliÄŸi_ ve _retrieval-augmented generation_ gibi tekniklere ihtiyacÄ±mÄ±z var.

PopÃ¼ler bir prompt mÃ¼hendisliÄŸi tekniÄŸi, modele yanÄ±tÄ±nda ne beklendiÄŸine dair daha fazla rehberlik vermektir; bu ya _talimatlar_ (aÃ§Ä±k rehberlik) vererek ya da _birkaÃ§ Ã¶rnek sunarak_ (Ã¶rtÃ¼k rehberlik) yapÄ±lÄ±r. Buna _few-shot learning_ denir ancak iki sÄ±nÄ±rlamasÄ± vardÄ±r:

- Modelin token sÄ±nÄ±rlarÄ±, verebileceÄŸiniz Ã¶rnek sayÄ±sÄ±nÄ± kÄ±sÄ±tlayabilir ve etkinliÄŸi azaltabilir.
- Model token maliyetleri, her prompta Ã¶rnek eklemeyi pahalÄ± hale getirebilir ve esnekliÄŸi sÄ±nÄ±rlar.

Ä°nce ayarlama, Ã¶nceden eÄŸitilmiÅŸ bir modeli alÄ±p belirli bir gÃ¶revde performansÄ±nÄ± artÄ±rmak iÃ§in yeni verilerle yeniden eÄŸitme uygulamasÄ±dÄ±r. Dil modelleri baÄŸlamÄ±nda, Ã¶nceden eÄŸitilmiÅŸ modeli _belirli bir gÃ¶rev veya uygulama alanÄ± iÃ§in Ã¶zenle seÃ§ilmiÅŸ Ã¶rneklerle_ ince ayarlayarak, o gÃ¶rev veya alan iÃ§in daha doÄŸru ve alakalÄ± olabilecek **Ã¶zel bir model** oluÅŸturabiliriz. Ä°nce ayarlamanÄ±n yan faydalarÄ±ndan biri, few-shot learning iÃ§in gereken Ã¶rnek sayÄ±sÄ±nÄ± azaltarak token kullanÄ±mÄ±nÄ± ve ilgili maliyetleri dÃ¼ÅŸÃ¼rmesidir.

## Ne Zaman ve Neden Modelleri Ä°nce AyarlamalÄ±yÄ±z?

_Bu_ baÄŸlamda ince ayarlamadan bahsederken, orijinal eÄŸitim veri setinde olmayan **yeni veriler ekleyerek** yapÄ±lan **denetimli** ince ayarlamadan sÃ¶z ediyoruz. Bu, modelin orijinal veriler Ã¼zerinde farklÄ± hiperparametrelerle yeniden eÄŸitildiÄŸi denetimsiz ince ayarlamadan farklÄ±dÄ±r.

UnutulmamasÄ± gereken Ã¶nemli nokta, ince ayarlamanÄ±n istenen sonuÃ§larÄ± elde etmek iÃ§in belirli bir uzmanlÄ±k gerektiren geliÅŸmiÅŸ bir teknik olduÄŸudur. YanlÄ±ÅŸ yapÄ±ldÄ±ÄŸÄ±nda beklenen iyileÅŸtirmeleri saÄŸlamayabilir, hatta hedeflenen alan iÃ§in model performansÄ±nÄ± dÃ¼ÅŸÃ¼rebilir.

Bu yÃ¼zden "dil modellerini nasÄ±l ince ayarlayacaÄŸÄ±nÄ±zÄ±" Ã¶ÄŸrenmeden Ã¶nce, "neden" bu yolu seÃ§meniz gerektiÄŸini ve "ne zaman" ince ayarlama sÃ¼recine baÅŸlamanÄ±z gerektiÄŸini bilmelisiniz. Kendinize ÅŸu sorularÄ± sorun:

- **KullanÄ±m Durumu**: Ä°nce ayarlama iÃ§in kullanÄ±m durumunuz nedir? Mevcut Ã¶nceden eÄŸitilmiÅŸ modelin hangi yÃ¶nÃ¼nÃ¼ geliÅŸtirmek istiyorsunuz?
- **Alternatifler**: Ä°stenen sonuÃ§larÄ± elde etmek iÃ§in _baÅŸka teknikler_ denediniz mi? BunlarÄ± karÅŸÄ±laÅŸtÄ±rma iÃ§in temel olarak kullanÄ±n.
  - Prompt mÃ¼hendisliÄŸi: Ä°lgili prompt yanÄ±tlarÄ± Ã¶rnekleriyle few-shot prompting gibi teknikleri deneyin. YanÄ±tlarÄ±n kalitesini deÄŸerlendirin.
  - Retrieval Augmented Generation: Verilerinizi arayarak elde edilen sorgu sonuÃ§larÄ±yla promptlarÄ± zenginleÅŸtirmeyi deneyin. YanÄ±tlarÄ±n kalitesini deÄŸerlendirin.
- **Maliyetler**: Ä°nce ayarlama maliyetlerini belirlediniz mi?
  - Ä°ncelenebilirlik - Ã¶nceden eÄŸitilmiÅŸ model ince ayarlamaya uygun mu?
  - Ã‡aba - eÄŸitim verisi hazÄ±rlama, modeli deÄŸerlendirme ve iyileÅŸtirme iÃ§in gereken emek
  - Hesaplama - ince ayarlama iÅŸlemlerini Ã§alÄ±ÅŸtÄ±rma ve ince ayarlanmÄ±ÅŸ modeli daÄŸÄ±tma iÃ§in gereken kaynaklar
  - Veri - ince ayarlama etkisi iÃ§in yeterli kalitede Ã¶rneklere eriÅŸim
- **Faydalar**: Ä°nce ayarlamanÄ±n faydalarÄ±nÄ± doÄŸruladÄ±nÄ±z mÄ±?
  - Kalite - ince ayarlanmÄ±ÅŸ model temel modeli geride bÄ±raktÄ± mÄ±?
  - Maliyet - promptlarÄ± basitleÅŸtirerek token kullanÄ±mÄ±nÄ± azaltÄ±yor mu?
  - GeniÅŸletilebilirlik - temel modeli yeni alanlar iÃ§in yeniden kullanabilir misiniz?

Bu sorularÄ± yanÄ±tlayarak, ince ayarlamanÄ±n kullanÄ±m durumunuz iÃ§in doÄŸru yaklaÅŸÄ±m olup olmadÄ±ÄŸÄ±na karar verebilirsiniz. Ä°deal olarak, faydalar maliyetlerden fazla olmalÄ±dÄ±r. Devam etmeye karar verdiÄŸinizde, Ã¶nceden eÄŸitilmiÅŸ modeli _nasÄ±l_ ince ayarlayabileceÄŸinizi dÃ¼ÅŸÃ¼nmenin zamanÄ± gelmiÅŸtir.

Karar verme sÃ¼reci hakkÄ±nda daha fazla bilgi edinmek ister misiniz? [To fine-tune or not to fine-tune](https://www.youtube.com/watch?v=0Jo-z-MFxJs) videosunu izleyin.

## Ã–nceden EÄŸitilmiÅŸ Bir Model NasÄ±l Ä°nce AyarlanÄ±r?

Ã–nceden eÄŸitilmiÅŸ bir modeli ince ayarlamak iÃ§in ÅŸunlara ihtiyacÄ±nÄ±z vardÄ±r:

- ince ayar yapÄ±lacak Ã¶nceden eÄŸitilmiÅŸ model
- ince ayarlama iÃ§in kullanÄ±lacak veri seti
- ince ayarlama iÅŸlemini Ã§alÄ±ÅŸtÄ±racak eÄŸitim ortamÄ±
- ince ayarlanmÄ±ÅŸ modeli daÄŸÄ±tmak iÃ§in barÄ±ndÄ±rma ortamÄ±

## Ä°nce Ayarlama UygulamasÄ±

AÅŸaÄŸÄ±daki kaynaklar, seÃ§ilmiÅŸ bir model ve Ã¶zenle hazÄ±rlanmÄ±ÅŸ bir veri seti kullanarak gerÃ§ek bir Ã¶rnek Ã¼zerinden adÄ±m adÄ±m rehberlik saÄŸlar. Bu eÄŸitimleri uygulamak iÃ§in ilgili saÄŸlayÄ±cÄ±da bir hesabÄ±nÄ±zÄ±n olmasÄ± ve ilgili model ile veri setlerine eriÅŸiminizin bulunmasÄ± gerekir.

| SaÄŸlayÄ±cÄ±    | EÄŸitim                                                                                                                                                                       | AÃ§Ä±klama                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [Chat modellerini nasÄ±l ince ayarlarsÄ±nÄ±z](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst) | `gpt-35-turbo` modelini belirli bir alan ("tarif asistanÄ±") iÃ§in nasÄ±l ince ayarlayacaÄŸÄ±nÄ±zÄ± Ã¶ÄŸrenin; eÄŸitim verisi hazÄ±rlama, ince ayarlama iÅŸlemini Ã§alÄ±ÅŸtÄ±rma ve ince ayarlanmÄ±ÅŸ modeli Ã§Ä±karÄ±m iÃ§in kullanma adÄ±mlarÄ±nÄ± iÃ§erir.                                                                                                                                                                                             |
| Azure OpenAI | [GPT 3.5 Turbo ince ayarlama eÄŸitimi](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | `gpt-35-turbo-0613` modelini **Azure Ã¼zerinde** nasÄ±l ince ayarlayacaÄŸÄ±nÄ±zÄ± Ã¶ÄŸrenin; eÄŸitim verisi oluÅŸturma ve yÃ¼kleme, ince ayarlama iÅŸlemini Ã§alÄ±ÅŸtÄ±rma, yeni modeli daÄŸÄ±tma ve kullanma adÄ±mlarÄ±nÄ± iÃ§erir.                                                                                                                                                                                                                      |
| Hugging Face | [Hugging Face ile LLM ince ayarlama](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                             | Bu blog yazÄ±sÄ±, [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) kÃ¼tÃ¼phanesi ve [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) kullanarak aÃ§Ä±k bir LLMâ€™yi (Ã¶rneÄŸin `CodeLlama 7B`) nasÄ±l ince ayarlayacaÄŸÄ±nÄ±zÄ± anlatÄ±r. AyrÄ±ca Hugging Face Ã¼zerindeki aÃ§Ä±k [veri setleri](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) kullanÄ±lÄ±r. |
|              |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ğŸ¤— AutoTrain | [AutoTrain ile LLM ince ayarlama](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                       | AutoTrain (veya AutoTrain Advanced), Hugging Face tarafÄ±ndan geliÅŸtirilen ve birÃ§ok farklÄ± gÃ¶rev iÃ§in, LLM ince ayarlama dahil, ince ayarlama yapmanÄ±zÄ± saÄŸlayan bir Python kÃ¼tÃ¼phanesidir. AutoTrain, kod yazmadan kullanÄ±labilen bir Ã§Ã¶zÃ¼mdÃ¼r ve ince ayarlama iÅŸlemi kendi bulutunuzda, Hugging Face Spaces Ã¼zerinde veya yerel olarak yapÄ±labilir. Web tabanlÄ± GUI, CLI ve yaml konfigÃ¼rasyon dosyalarÄ± ile eÄŸitim desteÄŸi sunar.                                   |
|              |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                  |

## Ã–dev

YukarÄ±daki eÄŸitimlerden birini seÃ§in ve adÄ±m adÄ±m uygulayÄ±n. _Bu eÄŸitimlerin bir versiyonunu referans amaÃ§lÄ± olarak bu repoda Jupyter Notebookâ€™larda Ã§oÄŸaltabiliriz. En gÃ¼ncel sÃ¼rÃ¼mler iÃ§in lÃ¼tfen orijinal kaynaklarÄ± doÄŸrudan kullanÄ±n_.

## Harika Ä°ÅŸ! Ã–ÄŸrenmeye Devam Edin.

Bu dersi tamamladÄ±ktan sonra, Ã¼retken yapay zeka bilginizi geliÅŸtirmeye devam etmek iÃ§in [Generative AI Learning koleksiyonumuza](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) gÃ¶z atÄ±n!

Tebrikler!! Bu kursun v2 serisindeki son dersi tamamladÄ±nÄ±z! Ã–ÄŸrenmeyi ve geliÅŸtirmeyi bÄ±rakmayÄ±n. \*\*Sadece bu konu iÃ§in ek Ã¶neriler iÃ§eren listeyi gÃ¶rmek iÃ§in [KAYNAKLAR](RESOURCES.md?WT.mc_id=academic-105485-koreyst) sayfasÄ±nÄ± inceleyin.

v1 ders serimiz de daha fazla Ã¶dev ve kavramla gÃ¼ncellendi. Bilginizi tazelemek iÃ§in bir dakikanÄ±zÄ± ayÄ±rÄ±n ve lÃ¼tfen [sorularÄ±nÄ±zÄ± ve geri bildirimlerinizi paylaÅŸÄ±n](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) â€” bÃ¶ylece bu dersleri topluluk iÃ§in daha iyi hale getirebiliriz.

**Feragatname**:  
Bu belge, AI Ã§eviri servisi [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ± sonucu ortaya Ã§Ä±kabilecek yanlÄ±ÅŸ anlamalar veya yorum hatalarÄ±ndan sorumlu deÄŸiliz.