<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:26:06+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "tr"
}
-->
# Ãœretken Yapay Zeka UygulamalarÄ±nÄ±zÄ± GÃ¼vence AltÄ±na Alma

[![Ãœretken Yapay Zeka UygulamalarÄ±nÄ±zÄ± GÃ¼vence AltÄ±na Alma](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.tr.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## GiriÅŸ

Bu derste ÅŸunlar ele alÄ±nacaktÄ±r:

- Yapay zeka sistemleri baÄŸlamÄ±nda gÃ¼venlik.
- Yapay zeka sistemlerine yÃ¶nelik yaygÄ±n riskler ve tehditler.
- Yapay zeka sistemlerini gÃ¼vence altÄ±na alma yÃ¶ntemleri ve dikkate alÄ±nmasÄ± gerekenler.

## Ã–ÄŸrenme Hedefleri

Bu dersi tamamladÄ±ktan sonra ÅŸunlarÄ± anlayabileceksiniz:

- Yapay zeka sistemlerine yÃ¶nelik tehditler ve riskler.
- Yapay zeka sistemlerini gÃ¼vence altÄ±na almak iÃ§in yaygÄ±n yÃ¶ntemler ve uygulamalar.
- GÃ¼venlik testlerinin uygulanmasÄ±nÄ±n beklenmedik sonuÃ§larÄ± ve kullanÄ±cÄ± gÃ¼veninin zedelenmesini nasÄ±l Ã¶nleyebileceÄŸi.

## Ãœretken Yapay Zeka baÄŸlamÄ±nda gÃ¼venlik ne anlama gelir?

Yapay Zeka (AI) ve Makine Ã–ÄŸrenimi (ML) teknolojileri hayatÄ±mÄ±zÄ± giderek daha fazla ÅŸekillendirirken, sadece mÃ¼ÅŸteri verilerini deÄŸil, aynÄ± zamanda yapay zeka sistemlerini de korumak kritik hale gelmiÅŸtir. AI/ML, yanlÄ±ÅŸ kararÄ±n ciddi sonuÃ§lar doÄŸurabileceÄŸi sektÃ¶rlerde yÃ¼ksek deÄŸerli karar alma sÃ¼reÃ§lerini desteklemek iÃ§in giderek daha fazla kullanÄ±lmaktadÄ±r.

Dikkate alÄ±nmasÄ± gereken Ã¶nemli noktalar ÅŸunlardÄ±r:

- **AI/MLâ€™nin Etkisi**: AI/ML gÃ¼nlÃ¼k yaÅŸam Ã¼zerinde Ã¶nemli etkilere sahiptir ve bu nedenle korunmalarÄ± zorunlu hale gelmiÅŸtir.
- **GÃ¼venlik ZorluklarÄ±**: AI/MLâ€™nin bu etkisi, troller veya organize gruplar tarafÄ±ndan yapÄ±lan sofistike saldÄ±rÄ±lara karÅŸÄ± AI tabanlÄ± Ã¼rÃ¼nlerin korunmasÄ± ihtiyacÄ±nÄ± doÄŸru ÅŸekilde ele almayÄ± gerektirir.
- **Stratejik Sorunlar**: Teknoloji sektÃ¶rÃ¼, uzun vadeli mÃ¼ÅŸteri gÃ¼venliÄŸi ve veri gÃ¼venliÄŸini saÄŸlamak iÃ§in stratejik zorluklarÄ± proaktif olarak ele almalÄ±dÄ±r.

AyrÄ±ca, Makine Ã–ÄŸrenimi modelleri kÃ¶tÃ¼ niyetli girdiler ile zararsÄ±z anormal veriler arasÄ±nda ayrÄ±m yapma konusunda bÃ¼yÃ¼k Ã¶lÃ§Ã¼de yetersizdir. EÄŸitim verilerinin Ã¶nemli bir kÄ±smÄ±, Ã¼Ã§Ã¼ncÃ¼ taraf katkÄ±larÄ±na aÃ§Ä±k, denetlenmemiÅŸ ve dÃ¼zenlenmemiÅŸ kamuya aÃ§Ä±k veri kÃ¼melerinden elde edilir. SaldÄ±rganlarÄ±n veri kÃ¼melerini ele geÃ§irmesine gerek yoktur; katkÄ±da bulunmalarÄ± yeterlidir. Zamanla, dÃ¼ÅŸÃ¼k gÃ¼venilirlikteki kÃ¶tÃ¼ niyetli veriler, veri yapÄ±sÄ±/formatÄ± doÄŸru kaldÄ±ÄŸÄ± sÃ¼rece yÃ¼ksek gÃ¼venilirlikte gÃ¼venilen verilere dÃ¶nÃ¼ÅŸÃ¼r.

Bu nedenle, modellerinizin karar verirken kullandÄ±ÄŸÄ± veri depolarÄ±nÄ±n bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ ve korunmasÄ±nÄ± saÄŸlamak kritik Ã¶nemdedir.

## Yapay Zeka tehditleri ve risklerini anlamak

Yapay zeka ve ilgili sistemler aÃ§Ä±sÄ±ndan, veri zehirlenmesi gÃ¼nÃ¼mÃ¼zde en Ã¶nemli gÃ¼venlik tehdidi olarak Ã¶ne Ã§Ä±kmaktadÄ±r. Veri zehirlenmesi, birinin yapay zekayÄ± eÄŸitmek iÃ§in kullanÄ±lan bilgileri kasÄ±tlÄ± olarak deÄŸiÅŸtirmesi ve bÃ¶ylece yapay zekanÄ±n hata yapmasÄ±na neden olmasÄ±dÄ±r. Bu durum, standartlaÅŸtÄ±rÄ±lmÄ±ÅŸ tespit ve hafifletme yÃ¶ntemlerinin olmamasÄ± ve eÄŸitim iÃ§in gÃ¼venilmeyen ya da denetlenmemiÅŸ kamu veri kÃ¼melerine baÄŸÄ±mlÄ±lÄ±ÄŸÄ±mÄ±z nedeniyle ortaya Ã§Ä±kar. Veri bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ korumak ve hatalÄ± bir eÄŸitim sÃ¼recini Ã¶nlemek iÃ§in verinizin kaynaÄŸÄ±nÄ± ve kÃ¶kenini takip etmek Ã§ok Ã¶nemlidir. Aksi takdirde, â€œÃ§Ã¶p girer, Ã§Ã¶p Ã§Ä±karâ€ sÃ¶zÃ¼ geÃ§erli olur ve model performansÄ± zarar gÃ¶rÃ¼r.

Veri zehirlenmesinin modellerinizi nasÄ±l etkileyebileceÄŸine dair Ã¶rnekler:

1. **Etiket DeÄŸiÅŸtirme**: Ä°kili sÄ±nÄ±flandÄ±rma gÃ¶revinde, bir saldÄ±rgan eÄŸitim verisinin kÃ¼Ã§Ã¼k bir alt kÃ¼mesinin etiketlerini kasÄ±tlÄ± olarak deÄŸiÅŸtirir. Ã–rneÄŸin, zararsÄ±z Ã¶rnekler kÃ¶tÃ¼ niyetli olarak etiketlenir ve model yanlÄ±ÅŸ iliÅŸkiler Ã¶ÄŸrenir.\
   **Ã–rnek**: Bir spam filtresi, manipÃ¼le edilmiÅŸ etiketler nedeniyle gerÃ§ek e-postalarÄ± spam olarak yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rÄ±r.
2. **Ã–zellik Zehirlenmesi**: Bir saldÄ±rgan, modeli yanÄ±ltmak veya Ã¶nyargÄ± oluÅŸturmak iÃ§in eÄŸitim verisindeki Ã¶zellikleri ince ince deÄŸiÅŸtirir.\
   **Ã–rnek**: Tavsiye sistemlerini manipÃ¼le etmek iÃ§in Ã¼rÃ¼n aÃ§Ä±klamalarÄ±na alakasÄ±z anahtar kelimeler eklemek.
3. **Veri Enjeksiyonu**: Modelin davranÄ±ÅŸÄ±nÄ± etkilemek iÃ§in eÄŸitim setine kÃ¶tÃ¼ niyetli veri eklemek.\
   **Ã–rnek**: Duygu analizi sonuÃ§larÄ±nÄ± Ã§arpÄ±tmak iÃ§in sahte kullanÄ±cÄ± yorumlarÄ± eklemek.
4. **Arka KapÄ± SaldÄ±rÄ±larÄ±**: Bir saldÄ±rgan, eÄŸitim verisine gizli bir desen (arka kapÄ±) ekler. Model bu deseni tanÄ±mayÄ± Ã¶ÄŸrenir ve tetiklendiÄŸinde kÃ¶tÃ¼ niyetli davranÄ±r.\
   **Ã–rnek**: Arka kapÄ±lÄ± gÃ¶rÃ¼ntÃ¼lerle eÄŸitilmiÅŸ bir yÃ¼z tanÄ±ma sistemi, belirli bir kiÅŸiyi yanlÄ±ÅŸ tanÄ±mlar.

MITRE Corporation, AI sistemlerine yÃ¶nelik gerÃ§ek dÃ¼nya saldÄ±rÄ±larÄ±nda kullanÄ±lan taktik ve tekniklerin bilgi tabanÄ± olan [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) adlÄ± bir kaynak oluÅŸturmuÅŸtur.

> AI destekli sistemlerde, AIâ€™nÄ±n entegrasyonu mevcut sistemlerin saldÄ±rÄ± yÃ¼zeyini geleneksel siber saldÄ±rÄ±larÄ±n Ã¶tesinde artÄ±rdÄ±ÄŸÄ±ndan, bu sistemlerde artan sayÄ±da gÃ¼venlik aÃ§Ä±ÄŸÄ± bulunmaktadÄ±r. KÃ¼resel topluluk AIâ€™yÄ± Ã§eÅŸitli sistemlere giderek daha fazla entegre ettikÃ§e, bu benzersiz ve geliÅŸen gÃ¼venlik aÃ§Ä±klarÄ±na dikkat Ã§ekmek iÃ§in ATLASâ€™Ä± geliÅŸtirdik. ATLAS, MITRE ATT&CKÂ® Ã§erÃ§evesi temel alÄ±narak modellenmiÅŸ olup, taktikleri, teknikleri ve prosedÃ¼rleri (TTPâ€™ler) ATT&CK ile tamamlayÄ±cÄ±dÄ±r.

MITRE ATT&CKÂ® Ã§erÃ§evesi gibi, geleneksel siber gÃ¼venlikte geliÅŸmiÅŸ tehdit simÃ¼lasyonlarÄ± planlamak iÃ§in yaygÄ±n olarak kullanÄ±lan ATLAS, ortaya Ã§Ä±kan saldÄ±rÄ±lara karÅŸÄ± savunmayÄ± daha iyi anlamak ve hazÄ±rlamak iÃ§in kolayca aranabilir bir TTP seti sunar.

AyrÄ±ca, Open Web Application Security Project (OWASP), LLM kullanan uygulamalarda bulunan en kritik gÃ¼venlik aÃ§Ä±klarÄ±nÄ±n yer aldÄ±ÄŸÄ± bir "[En Ä°yi 10 liste](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" oluÅŸturmuÅŸtur. Bu liste, yukarÄ±da bahsedilen veri zehirlenmesi gibi tehditlerin yanÄ± sÄ±ra ÅŸu riskleri de vurgular:

- **Prompt Injection**: SaldÄ±rganlarÄ±n, bÃ¼yÃ¼k dil modellerini (LLM) dikkatlice hazÄ±rlanmÄ±ÅŸ girdilerle manipÃ¼le ederek modelin amaÃ§lanan davranÄ±ÅŸÄ±nÄ±n dÄ±ÅŸÄ±na Ã§Ä±kmasÄ±na neden olduÄŸu bir teknik.
- **Tedarik Zinciri GÃ¼venlik AÃ§Ä±klarÄ±**: LLM tarafÄ±ndan kullanÄ±lan uygulamalarÄ± oluÅŸturan bileÅŸenler ve yazÄ±lÄ±mlar, Ã¶rneÄŸin Python modÃ¼lleri veya dÄ±ÅŸ veri kÃ¼meleri, kendileri ele geÃ§irilebilir ve bu da beklenmedik sonuÃ§lara, Ã¶nyargÄ±lara ve altyapÄ±da gÃ¼venlik aÃ§Ä±klarÄ±na yol aÃ§abilir.
- **AÅŸÄ±rÄ± GÃ¼ven**: LLMâ€™ler hataya meyillidir ve yanlÄ±ÅŸ veya gÃ¼vensiz sonuÃ§lar Ã¼retebilir. BelgelendirilmiÅŸ birÃ§ok durumda, insanlar bu sonuÃ§larÄ± olduÄŸu gibi kabul etmiÅŸ ve istenmeyen gerÃ§ek dÃ¼nya olumsuz sonuÃ§larÄ±na yol aÃ§mÄ±ÅŸtÄ±r.

Microsoft Cloud Advocate Rod Trent, bu ve diÄŸer ortaya Ã§Ä±kan AI tehditlerine derinlemesine deÄŸinen ve bu senaryolarla baÅŸa Ã§Ä±kmak iÃ§in kapsamlÄ± rehberlik sunan Ã¼cretsiz bir ebook yazmÄ±ÅŸtÄ±r: [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst).

## AI Sistemleri ve LLMâ€™ler iÃ§in GÃ¼venlik Testi

Yapay zeka (AI), Ã§eÅŸitli alanlarÄ± ve endÃ¼strileri dÃ¶nÃ¼ÅŸtÃ¼rerek topluma yeni olanaklar ve faydalar sunmaktadÄ±r. Ancak AI, veri gizliliÄŸi, Ã¶nyargÄ±, aÃ§Ä±klanabilirlik eksikliÄŸi ve kÃ¶tÃ¼ye kullanÄ±m potansiyeli gibi Ã¶nemli zorluklar ve riskler de taÅŸÄ±r. Bu nedenle, AI sistemlerinin gÃ¼venli ve sorumlu olmasÄ±, yani etik ve yasal standartlara uymasÄ± ve kullanÄ±cÄ±lar ile paydaÅŸlar tarafÄ±ndan gÃ¼venilir olmasÄ± Ã§ok Ã¶nemlidir.

GÃ¼venlik testi, bir AI sistemi veya LLMâ€™nin gÃ¼venliÄŸini deÄŸerlendirme sÃ¼recidir; bu sÃ¼reÃ§te zayÄ±f noktalar tespit edilir ve istismar edilir. Test, amacÄ±na ve kapsamÄ±na baÄŸlÄ± olarak geliÅŸtiriciler, kullanÄ±cÄ±lar veya Ã¼Ã§Ã¼ncÃ¼ taraf denetÃ§iler tarafÄ±ndan yapÄ±labilir. AI sistemleri ve LLMâ€™ler iÃ§in en yaygÄ±n gÃ¼venlik testi yÃ¶ntemlerinden bazÄ±larÄ± ÅŸunlardÄ±r:

- **Veri temizleme**: AI sistemi veya LLMâ€™nin eÄŸitim verilerinden veya girdilerinden hassas ya da Ã¶zel bilgilerin kaldÄ±rÄ±lmasÄ± veya anonimleÅŸtirilmesi sÃ¼recidir. Veri temizleme, gizli veya kiÅŸisel verilerin aÃ§Ä±ÄŸa Ã§Ä±kmasÄ±nÄ± ve kÃ¶tÃ¼ niyetli manipÃ¼lasyonlarÄ± Ã¶nlemeye yardÄ±mcÄ± olur.
- **DÃ¼ÅŸmanca testler (Adversarial testing)**: AI sistemi veya LLMâ€™nin girdisine veya Ã§Ä±ktÄ±sÄ±na dÃ¼ÅŸmanca Ã¶rnekler Ã¼reterek uygulanmasÄ±dÄ±r; bÃ¶ylece sistemin dÃ¼ÅŸmanca saldÄ±rÄ±lara karÅŸÄ± dayanÄ±klÄ±lÄ±ÄŸÄ± ve saÄŸlamlÄ±ÄŸÄ± deÄŸerlendirilir. Bu testler, saldÄ±rganlar tarafÄ±ndan kullanÄ±labilecek zayÄ±flÄ±klarÄ± ve aÃ§Ä±klarÄ± tespit etmeye ve hafifletmeye yardÄ±mcÄ± olur.
- **Model doÄŸrulama**: AI sistemi veya LLMâ€™nin model parametrelerinin veya mimarisinin doÄŸruluÄŸunu ve eksiksizliÄŸini kontrol etme sÃ¼recidir. Model doÄŸrulama, modelin korunmasÄ±nÄ± ve doÄŸrulanmasÄ±nÄ± saÄŸlayarak model hÄ±rsÄ±zlÄ±ÄŸÄ±nÄ± tespit etmeye ve Ã¶nlemeye yardÄ±mcÄ± olur.
- **Ã‡Ä±ktÄ± doÄŸrulama**: AI sistemi veya LLMâ€™nin Ã§Ä±ktÄ±sÄ±nÄ±n kalitesini ve gÃ¼venilirliÄŸini doÄŸrulama sÃ¼recidir. Ã‡Ä±ktÄ± doÄŸrulama, kÃ¶tÃ¼ niyetli manipÃ¼lasyonlarÄ± tespit etmeye ve dÃ¼zeltmeye yardÄ±mcÄ± olur; bÃ¶ylece Ã§Ä±ktÄ± tutarlÄ± ve doÄŸru olur.

AI sistemlerinde lider olan OpenAI, AI gÃ¼venliÄŸine katkÄ±da bulunmak amacÄ±yla Ã§Ä±ktÄ±larÄ±nÄ± test etmeye yÃ¶nelik bir red teaming aÄŸÄ± kapsamÄ±nda bir dizi _gÃ¼venlik deÄŸerlendirmesi_ oluÅŸturmuÅŸtur.

> DeÄŸerlendirmeler basit soru-cevap testlerinden daha karmaÅŸÄ±k simÃ¼lasyonlara kadar deÄŸiÅŸebilir. Somut Ã¶rnekler olarak, OpenAI tarafÄ±ndan AI davranÄ±ÅŸlarÄ±nÄ± Ã§eÅŸitli aÃ§Ä±lardan deÄŸerlendirmek iÃ§in geliÅŸtirilen Ã¶rnek deÄŸerlendirmeler ÅŸunlardÄ±r:

#### Ä°kna

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Bir AI sistemi, baÅŸka bir AI sistemini gizli bir kelime sÃ¶ylemesi iÃ§in ne kadar iyi kandÄ±rabilir?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Bir AI sistemi, baÅŸka bir AI sistemini para baÄŸÄ±ÅŸlamaya ne kadar iyi ikna edebilir?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Bir AI sistemi, baÅŸka bir AI sisteminin siyasi bir Ã¶neriyi desteklemesini ne kadar etkileyebilir?

#### Steganografi (gizli mesajlaÅŸma)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Bir AI sistemi, baÅŸka bir AI sistemi tarafÄ±ndan yakalanmadan gizli mesajlarÄ± ne kadar iyi iletebilir?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Bir AI sistemi, gizli mesajlarÄ± saklamak iÃ§in mesajlarÄ± ne kadar iyi sÄ±kÄ±ÅŸtÄ±rÄ±p aÃ§abilir?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Bir AI sistemi, baÅŸka bir AI sistemiyle doÄŸrudan iletiÅŸim olmadan ne kadar iyi koordinasyon saÄŸlayabilir?

### AI GÃ¼venliÄŸi

AI sistemlerini kÃ¶tÃ¼ niyetli saldÄ±rÄ±lardan, kÃ¶tÃ¼ye kullanÄ±mdan veya istenmeyen sonuÃ§lardan korumak zorundayÄ±z. Bu, AI sistemlerinin gÃ¼venliÄŸi, gÃ¼venilirliÄŸi ve kullanÄ±cÄ±lar ile paydaÅŸlar tarafÄ±ndan gÃ¼venilir olmasÄ± iÃ§in ÅŸu adÄ±mlarÄ± iÃ§erir:

- AI modellerini eÄŸitmek ve Ã§alÄ±ÅŸtÄ±rmak iÃ§in kullanÄ±lan veri ve algoritmalarÄ±n gÃ¼vence altÄ±na alÄ±nmasÄ±
- AI sistemlerine yetkisiz eriÅŸim, manipÃ¼lasyon veya sabotajÄ±n Ã¶nlenmesi
- AI sistemlerinde Ã¶nyargÄ±, ayrÄ±mcÄ±lÄ±k veya etik sorunlarÄ±n tespiti ve hafifletilmesi
- AI kararlarÄ±nÄ±n ve eylemlerinin hesap verebilirliÄŸinin, ÅŸeffaflÄ±ÄŸÄ±nÄ±n ve aÃ§Ä±klanabilirliÄŸinin saÄŸlanmasÄ±
- AI sistemlerinin hedef ve deÄŸerlerinin insan ve toplum deÄŸerleriyle uyumlu hale getirilmesi

AI gÃ¼venliÄŸi, AI sistemleri ve verilerinin bÃ¼tÃ¼nlÃ¼ÄŸÃ¼, eriÅŸilebilirliÄŸi ve gizliliÄŸinin saÄŸlanmasÄ± iÃ§in Ã¶nemlidir. AI gÃ¼venliÄŸinin bazÄ± zorluklarÄ± ve fÄ±rsatlarÄ± ÅŸunlardÄ±r:

- FÄ±rsat: AI, tehditleri tanÄ±mlamada ve mÃ¼dahale sÃ¼relerini iyileÅŸtirmede kritik rol oynayabileceÄŸinden, siber gÃ¼venlik stratejilerine AI entegrasyonu yapÄ±labilir. AI, kimlik avÄ±, kÃ¶tÃ¼ amaÃ§lÄ± yazÄ±lÄ±m veya fidye yazÄ±lÄ±mÄ± gibi siber saldÄ±rÄ±larÄ±n tespiti ve hafifletilmesini otomatikleÅŸtirmeye ve desteklemeye yardÄ±mcÄ± olabilir.
- Zorluk: AI, saldÄ±rganlar tarafÄ±ndan sahte veya yanÄ±ltÄ±cÄ± iÃ§erik Ã¼retmek, kullanÄ±cÄ±larÄ± taklit etmek veya AI sistemlerindeki gÃ¼venlik aÃ§Ä±klarÄ±nÄ± kullanmak iÃ§in de kullanÄ±labilir. Bu nedenle, AI geliÅŸtiricilerinin, sistemleri kÃ¶tÃ¼ye kullanÄ±ma karÅŸÄ± dayanÄ±klÄ± ve saÄŸlam tasarlama konusunda Ã¶zel bir sorumluluÄŸu vardÄ±r.

### Veri Koruma

LLMâ€™ler, kullandÄ±klarÄ± verilerin gizliliÄŸi ve gÃ¼venliÄŸi aÃ§Ä±sÄ±ndan riskler taÅŸÄ±yabilir. Ã–rneÄŸin, LLMâ€™ler eÄŸitim verilerinden kiÅŸisel isimler, adresler, ÅŸifreler veya kredi kartÄ± numaralarÄ± gibi hassas bilgileri ezberleyip sÄ±zdÄ±rabilir. AyrÄ±ca, kÃ¶tÃ¼ niyetli aktÃ¶rler tarafÄ±ndan zayÄ±f noktalarÄ± veya Ã¶nyargÄ±larÄ± kullanmak iÃ§in manipÃ¼le edilebilir veya saldÄ±rÄ±ya uÄŸrayabilirler. Bu nedenle, bu risklerin farkÄ±nda olmak ve LLMâ€™lerle kullanÄ±lan verileri korumak iÃ§in uygun Ã¶nlemler almak Ã¶nemlidir. LLMâ€™lerle kullanÄ±lan verileri korumak iÃ§in atÄ±labilecek bazÄ± adÄ±mlar ÅŸunlardÄ±r:

- **LLMâ€™lerle paylaÅŸÄ±lan veri miktarÄ±nÄ± ve tÃ¼rÃ¼nÃ¼ sÄ±nÄ±rlamak**: YalnÄ±zca amaÃ§lanan kullanÄ±m iÃ§in gerekli ve ilgili verileri paylaÅŸÄ±n; hassas, gizli veya kiÅŸisel verileri paylaÅŸmaktan kaÃ§Ä±nÄ±n. KullanÄ±cÄ±lar ayrÄ±ca, paylaÅŸtÄ±klarÄ± verileri anonimleÅŸtirmeli veya ÅŸifrelemeli; Ã¶rneÄŸin, tanÄ±mlayÄ±cÄ± bilgileri kaldÄ±rarak veya maskeleyerek ya da gÃ¼venli iletiÅŸim kanallarÄ± kullanarak.
- **LLMâ€™lerin Ã¼rettiÄŸi verileri doÄŸrulamak**: LLMâ€™lerin Ã¼rettiÄŸi Ã§Ä±ktÄ±nÄ±n doÄŸruluÄŸunu ve kalitesini her zaman kontrol edin; istenmeyen veya uygunsuz bilgi iÃ§ermediÄŸinden emin olun.
- **Herhangi bir veri ihlali veya olayÄ± bildirmek ve uyarÄ±da bulunmak**: LLMâ€™lerden gelen ÅŸÃ¼pheli veya anormal faaliyetlere karÅŸÄ± dikkatli olun; Ã¶rneÄŸin, alakasÄ±z, yanlÄ±ÅŸ, saldÄ±rgan veya zararlÄ± metinler Ã¼retmesi. Bu, bir veri ihlali veya gÃ¼venlik olayÄ± belirtisi olabilir.

Veri gÃ¼venliÄŸi, yÃ¶netiÅŸim ve uyumluluk, Ã§oklu bulut ortamÄ±nda veri ve AI gÃ¼cÃ¼nden yararlanmak isteyen her kuruluÅŸ iÃ§in kritik Ã¶neme sahiptir. TÃ¼m verilerinizi gÃ¼vence altÄ±na almak ve yÃ¶netmek karmaÅŸÄ±k ve Ã§ok yÃ¶nlÃ¼ bir iÅŸtir. FarklÄ± tÃ¼rde verileri (yapÄ±landÄ±rÄ±lmÄ±ÅŸ, yapÄ±landÄ±rÄ±lmamÄ±ÅŸ ve AI tarafÄ±ndan Ã¼retilen veriler) farklÄ± konumlarda ve birden fazla bulutta gÃ¼vence altÄ±na almalÄ± ve yÃ¶netmelisiniz; ayrÄ±ca mevcut ve gelecekteki veri gÃ¼venliÄŸi, yÃ¶netiÅŸim ve AI dÃ¼zenlemelerini dikkate almalÄ±sÄ±nÄ±z. Verilerinizi korumak iÃ§in bazÄ± en iyi uygulamalarÄ± ve Ã¶nlemleri benimsemelisiniz, Ã¶rneÄŸin:

- Veri koruma ve gizlilik Ã¶zellikleri sunan bulut hizmetleri veya platformlarÄ± kullanmak.
- Verilerinizi hatalar, tutarsÄ±zlÄ±klar veya anormallikler aÃ§Ä±sÄ±ndan kontrol etmek iÃ§in veri kalitesi ve doÄŸrulama araÃ§larÄ± kullanmak.
- Verilerinizin sorumlu ve ÅŸeffaf bir ÅŸekilde kullanÄ±ldÄ±ÄŸÄ±ndan emin olmak iÃ§in veri yÃ¶netiÅŸimi ve etik Ã§erÃ§eveleri uygulamak.

### GerÃ§ek dÃ¼nya tehditlerini taklit etmek - AI red teaming

GerÃ§ek dÃ¼nya tehditlerini taklit etmek, sistemlere yÃ¶nelik riskleri belirlemek ve savunucularÄ±n tepkisini test etmek iÃ§in benzer araÃ§lar, taktikler ve prosedÃ¼rler kullanarak dayanÄ±klÄ± AI sistemleri oluÅŸturmanÄ±n standart bir uygulamasÄ± olarak kabul edilmektedir.
> AI red teaming uygulamasÄ± daha geniÅŸ bir anlam kazanacak ÅŸekilde evrildi: sadece gÃ¼venlik aÃ§Ä±klarÄ±nÄ± araÅŸtÄ±rmakla kalmayÄ±p, aynÄ± zamanda potansiyel olarak zararlÄ± iÃ§erik Ã¼retimi gibi diÄŸer sistem hatalarÄ±nÄ± da incelemeyi kapsÄ±yor. AI sistemleri yeni risklerle birlikte gelir ve red teaming, prompt injection ve temelsiz iÃ§erik Ã¼retimi gibi bu yeni riskleri anlamanÄ±n temelidir. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![KÄ±rmÄ±zÄ± takÄ±m Ã§alÄ±ÅŸmasÄ± iÃ§in rehberlik ve kaynaklar](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.tr.png)]()

AÅŸaÄŸÄ±da Microsoftâ€™un AI Red Team programÄ±nÄ± ÅŸekillendiren temel iÃ§gÃ¶rÃ¼ler yer almaktadÄ±r.

1. **AI Red Teamingâ€™in GeniÅŸ KapsamÄ±:**  
   AI red teaming artÄ±k hem gÃ¼venlik hem de Sorumlu AI (RAI) sonuÃ§larÄ±nÄ± kapsÄ±yor. Geleneksel olarak, red teaming gÃ¼venlik yÃ¶nlerine odaklanÄ±r ve modeli bir vektÃ¶r olarak ele alÄ±r (Ã¶rneÄŸin, temel modelin Ã§alÄ±nmasÄ±). Ancak, AI sistemleri yeni gÃ¼venlik aÃ§Ä±klarÄ± (Ã¶rneÄŸin, prompt enjeksiyonu, zehirleme) ortaya Ã§Ä±karÄ±r ve Ã¶zel dikkat gerektirir. GÃ¼venliÄŸin Ã¶tesinde, AI red teaming aynÄ± zamanda adalet sorunlarÄ±nÄ± (Ã¶rneÄŸin, stereotipler) ve zararlÄ± iÃ§erikleri (Ã¶rneÄŸin, ÅŸiddetin yÃ¼celtilmesi) de inceler. Bu sorunlarÄ±n erken tespiti, savunma yatÄ±rÄ±mlarÄ±nÄ±n Ã¶nceliklendirilmesini saÄŸlar.  
2. **KÃ¶tÃ¼ Niyetli ve Ä°yi Niyetli Hatalar:**  
   AI red teaming, hatalarÄ± hem kÃ¶tÃ¼ niyetli hem de iyi niyetli perspektiflerden ele alÄ±r. Ã–rneÄŸin, yeni Bing Ã¼zerinde red teaming yaparken, sadece kÃ¶tÃ¼ niyetli saldÄ±rganlarÄ±n sistemi nasÄ±l alt edebileceÄŸini deÄŸil, aynÄ± zamanda sÄ±radan kullanÄ±cÄ±larÄ±n nasÄ±l sorunlu veya zararlÄ± iÃ§eriklerle karÅŸÄ±laÅŸabileceÄŸini de araÅŸtÄ±rÄ±yoruz. Geleneksel gÃ¼venlik red teamingâ€™in aksine, ki o genellikle kÃ¶tÃ¼ niyetli aktÃ¶rlere odaklanÄ±r, AI red teaming daha geniÅŸ bir kullanÄ±cÄ± ve hata yelpazesini dikkate alÄ±r.  
3. **AI Sistemlerinin Dinamik DoÄŸasÄ±:**  
   AI uygulamalarÄ± sÃ¼rekli evrilir. BÃ¼yÃ¼k dil modeli uygulamalarÄ±nda geliÅŸtiriciler deÄŸiÅŸen gereksinimlere uyum saÄŸlar. SÃ¼rekli red teaming, geliÅŸen risklere karÅŸÄ± sÃ¼rekli dikkat ve uyum saÄŸlar.

AI red teaming her ÅŸeyi kapsayan bir yÃ¶ntem deÄŸildir ve [rol tabanlÄ± eriÅŸim kontrolÃ¼ (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) ve kapsamlÄ± veri yÃ¶netimi Ã§Ã¶zÃ¼mleri gibi ek kontrollerle tamamlayÄ±cÄ± bir hareket olarak dÃ¼ÅŸÃ¼nÃ¼lmelidir. Bu, gizlilik ve gÃ¼venliÄŸi dikkate alarak gÃ¼venli ve sorumlu AI Ã§Ã¶zÃ¼mleri kullanmaya odaklanan bir gÃ¼venlik stratejisini desteklemek, Ã¶nyargÄ±larÄ±, zararlÄ± iÃ§erikleri ve kullanÄ±cÄ± gÃ¼venini zedeleyebilecek yanlÄ±ÅŸ bilgileri en aza indirmeyi amaÃ§lar.

AI sistemlerinizde riskleri tanÄ±mlamanÄ±za ve azaltmanÄ±za yardÄ±mcÄ± olabilecek red teaming hakkÄ±nda daha iyi anlamanÄ±zÄ± saÄŸlayacak ek okumalar:

- [BÃ¼yÃ¼k dil modelleri (LLMâ€™ler) ve uygulamalarÄ± iÃ§in red teaming planlama](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [OpenAI Red Teaming AÄŸÄ± nedir?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming - Daha GÃ¼venli ve Sorumlu AI Ã‡Ã¶zÃ¼mleri OluÅŸturmak Ä°Ã§in Temel Bir Uygulama](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Yapay Zeka Sistemleri iÃ§in DÃ¼ÅŸmanca Tehdit ManzarasÄ±)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), AI sistemlerine yÃ¶nelik gerÃ§ek dÃ¼nya saldÄ±rÄ±larÄ±nda dÃ¼ÅŸmanlarÄ±n kullandÄ±ÄŸÄ± taktik ve tekniklerin bilgi tabanÄ±.

## Bilgi KontrolÃ¼

Veri bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ korumak ve kÃ¶tÃ¼ye kullanÄ±mÄ± Ã¶nlemek iÃ§in iyi bir yaklaÅŸÄ±m ne olabilir?

1. Veri eriÅŸimi ve veri yÃ¶netimi iÃ§in gÃ¼Ã§lÃ¼ rol tabanlÄ± kontroller uygulamak  
1. Veri yanlÄ±ÅŸ temsilini veya kÃ¶tÃ¼ye kullanÄ±mÄ± Ã¶nlemek iÃ§in veri etiketlemeyi uygulamak ve denetlemek  
1. AI altyapÄ±nÄ±zÄ±n iÃ§erik filtrelemeyi desteklediÄŸinden emin olmak

Cevap: 1, ÃœÃ§Ã¼ de harika Ã¶neriler olsa da, kullanÄ±cÄ±lara doÄŸru veri eriÅŸim ayrÄ±calÄ±klarÄ±nÄ± atamak, LLMâ€™lerin kullandÄ±ÄŸÄ± verilerin manipÃ¼lasyonunu ve yanlÄ±ÅŸ temsilini Ã¶nlemede bÃ¼yÃ¼k fark yaratÄ±r.

## ğŸš€ Meydan Okuma

AI Ã§aÄŸÄ±nda [hassas bilgileri nasÄ±l yÃ¶neteceÄŸiniz ve koruyacaÄŸÄ±nÄ±z](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) hakkÄ±nda daha fazla bilgi edinin.

## Harika Ä°ÅŸ, Ã–ÄŸrenmeye Devam Et

Bu dersi tamamladÄ±ktan sonra, Generative AI bilginizi geliÅŸtirmeye devam etmek iÃ§in [Generative AI Ã–ÄŸrenme koleksiyonumuza](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) gÃ¶z atÄ±n!

Bir sonraki derse, [Generative AI Uygulama YaÅŸam DÃ¶ngÃ¼sÃ¼](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst) konusuna geÃ§elim!

**Feragatname**:  
Bu belge, AI Ã§eviri servisi [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±nÄ±z. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ± sonucu ortaya Ã§Ä±kabilecek yanlÄ±ÅŸ anlamalar veya yorum hatalarÄ±ndan sorumlu deÄŸiliz.