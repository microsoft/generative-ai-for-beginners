<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2faf8ee7a0b851efa647a19788f1e5b",
  "translation_date": "2025-10-17T16:17:35+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "tr"
}
-->
# Ãœretken Yapay Zeka UygulamalarÄ±nÄ±zÄ± GÃ¼venceye Almak

[![Ãœretken Yapay Zeka UygulamalarÄ±nÄ±zÄ± GÃ¼venceye Almak](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.tr.png)](https://youtu.be/m0vXwsx5DNg?si=TYkr936GMKz15K0L)

## GiriÅŸ

Bu ders ÅŸunlarÄ± kapsayacaktÄ±r:

- Yapay zeka sistemleri baÄŸlamÄ±nda gÃ¼venlik.
- Yapay zeka sistemlerine yÃ¶nelik yaygÄ±n riskler ve tehditler.
- Yapay zeka sistemlerini gÃ¼vence altÄ±na almak iÃ§in yÃ¶ntemler ve dikkate alÄ±nmasÄ± gerekenler.

## Ã–ÄŸrenme Hedefleri

Bu dersi tamamladÄ±ktan sonra ÅŸunlarÄ± anlayacaksÄ±nÄ±z:

- Yapay zeka sistemlerine yÃ¶nelik tehditler ve riskler.
- Yapay zeka sistemlerini gÃ¼vence altÄ±na almak iÃ§in yaygÄ±n yÃ¶ntemler ve uygulamalar.
- GÃ¼venlik testlerinin uygulanmasÄ±nÄ±n beklenmedik sonuÃ§larÄ± ve kullanÄ±cÄ± gÃ¼veninin kaybÄ±nÄ± nasÄ±l Ã¶nleyebileceÄŸi.

## Ãœretken yapay zeka baÄŸlamÄ±nda gÃ¼venlik ne anlama gelir?

Yapay Zeka (AI) ve Makine Ã–ÄŸrenimi (ML) teknolojileri hayatÄ±mÄ±zÄ± giderek daha fazla ÅŸekillendirirken, yalnÄ±zca mÃ¼ÅŸteri verilerini deÄŸil, aynÄ± zamanda yapay zeka sistemlerini de korumak Ã¶nemlidir. AI/ML, yanlÄ±ÅŸ kararlarÄ±n ciddi sonuÃ§lara yol aÃ§abileceÄŸi sektÃ¶rlerde yÃ¼ksek deÄŸerli karar verme sÃ¼reÃ§lerini desteklemek iÃ§in giderek daha fazla kullanÄ±lmaktadÄ±r.

Dikkate alÄ±nmasÄ± gereken Ã¶nemli noktalar ÅŸunlardÄ±r:

- **AI/ML'nin Etkisi**: AI/ML gÃ¼nlÃ¼k yaÅŸam Ã¼zerinde Ã¶nemli etkiler yaratÄ±r ve bu nedenle onlarÄ± korumak hayati bir Ã¶nem taÅŸÄ±r.
- **GÃ¼venlik ZorluklarÄ±**: AI/ML'nin bu etkisi, troller veya organize gruplar tarafÄ±ndan yapÄ±lan sofistike saldÄ±rÄ±lardan AI tabanlÄ± Ã¼rÃ¼nleri koruma ihtiyacÄ±nÄ± ele almak iÃ§in uygun dikkat gerektirir.
- **Stratejik Sorunlar**: Teknoloji sektÃ¶rÃ¼, uzun vadeli mÃ¼ÅŸteri gÃ¼venliÄŸi ve veri gÃ¼venliÄŸini saÄŸlamak iÃ§in stratejik zorluklarÄ± proaktif bir ÅŸekilde ele almalÄ±dÄ±r.

AyrÄ±ca, Makine Ã–ÄŸrenimi modelleri, kÃ¶tÃ¼ niyetli giriÅŸ ile zararsÄ±z anormal veriler arasÄ±ndaki farkÄ± bÃ¼yÃ¼k Ã¶lÃ§Ã¼de ayÄ±rt edemez. EÄŸitim verilerinin Ã¶nemli bir kaynaÄŸÄ±, Ã¼Ã§Ã¼ncÃ¼ taraf katkÄ±larÄ±na aÃ§Ä±k olan, dÃ¼zenlenmemiÅŸ, denetlenmemiÅŸ, halka aÃ§Ä±k veri setlerinden elde edilir. SaldÄ±rganlarÄ±n veri setlerini ele geÃ§irmesi gerekmez; katkÄ±da bulunmalarÄ± serbesttir. Zamanla, dÃ¼ÅŸÃ¼k gÃ¼venilirlikteki kÃ¶tÃ¼ niyetli veriler, veri yapÄ±sÄ±/formatÄ± doÄŸru kaldÄ±ÄŸÄ± sÃ¼rece yÃ¼ksek gÃ¼venilirlikte gÃ¼venilir verilere dÃ¶nÃ¼ÅŸÃ¼r.

Bu nedenle, modellerinizin karar vermek iÃ§in kullandÄ±ÄŸÄ± veri depolarÄ±nÄ±n bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ ve korunmasÄ±nÄ± saÄŸlamak kritik Ã¶neme sahiptir.

## Yapay zekanÄ±n tehditlerini ve risklerini anlamak

Yapay zeka ve ilgili sistemler aÃ§Ä±sÄ±ndan, veri zehirlenmesi bugÃ¼n en Ã¶nemli gÃ¼venlik tehdidi olarak Ã¶ne Ã§Ä±kmaktadÄ±r. Veri zehirlenmesi, birinin yapay zekayÄ± eÄŸitmek iÃ§in kullanÄ±lan bilgileri kasÄ±tlÄ± olarak deÄŸiÅŸtirerek hatalar yapmasÄ±na neden olmasÄ±dÄ±r. Bu, standartlaÅŸtÄ±rÄ±lmÄ±ÅŸ tespit ve azaltma yÃ¶ntemlerinin olmamasÄ± ve eÄŸitim iÃ§in gÃ¼venilmeyen veya dÃ¼zenlenmemiÅŸ halka aÃ§Ä±k veri setlerine olan baÄŸÄ±mlÄ±lÄ±ÄŸÄ±mÄ±z nedeniyle ortaya Ã§Ä±kar. Veri bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ korumak ve hatalÄ± bir eÄŸitim sÃ¼recini Ã¶nlemek iÃ§in verilerinizin kaynaÄŸÄ±nÄ± ve kÃ¶kenini takip etmek Ã§ok Ã¶nemlidir. Aksi takdirde, "Ã§Ã¶p girerse, Ã§Ã¶p Ã§Ä±kar" atasÃ¶zÃ¼ geÃ§erli olur ve model performansÄ± tehlikeye girer.

Ä°ÅŸte veri zehirlenmesinin modellerinizi nasÄ±l etkileyebileceÄŸine dair Ã¶rnekler:

1. **Etiket DeÄŸiÅŸtirme**: Ä°kili sÄ±nÄ±flandÄ±rma gÃ¶revinde, bir saldÄ±rgan eÄŸitim verilerinin kÃ¼Ã§Ã¼k bir alt kÃ¼mesinin etiketlerini kasÄ±tlÄ± olarak deÄŸiÅŸtirir. Ã–rneÄŸin, zararsÄ±z Ã¶rnekler kÃ¶tÃ¼ niyetli olarak etiketlenir ve model yanlÄ±ÅŸ iliÅŸkiler Ã¶ÄŸrenir.\
   **Ã–rnek**: Etiketleri manipÃ¼le edilmiÅŸ bir spam filtresi, meÅŸru e-postalarÄ± spam olarak yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rÄ±r.
2. **Ã–zellik Zehirlenmesi**: Bir saldÄ±rgan, modelde Ã¶nyargÄ± oluÅŸturmak veya modeli yanÄ±ltmak iÃ§in eÄŸitim verilerindeki Ã¶zellikleri hafifÃ§e deÄŸiÅŸtirir.\
   **Ã–rnek**: Ã–neri sistemlerini manipÃ¼le etmek iÃ§in Ã¼rÃ¼n aÃ§Ä±klamalarÄ±na alakasÄ±z anahtar kelimeler eklemek.
3. **Veri Enjeksiyonu**: EÄŸitim setine kÃ¶tÃ¼ niyetli veri ekleyerek modelin davranÄ±ÅŸÄ±nÄ± etkilemek.\
   **Ã–rnek**: Duygu analizi sonuÃ§larÄ±nÄ± Ã§arpÄ±tmak iÃ§in sahte kullanÄ±cÄ± yorumlarÄ± eklemek.
4. **Arka KapÄ± SaldÄ±rÄ±larÄ±**: Bir saldÄ±rgan, eÄŸitim verilerine gizli bir desen (arka kapÄ±) ekler. Model bu deseni Ã¶ÄŸrenir ve tetiklendiÄŸinde kÃ¶tÃ¼ niyetli davranÄ±r.\
   **Ã–rnek**: Belirli bir kiÅŸiyi yanlÄ±ÅŸ tanÄ±mlayan arka kapÄ±lÄ± gÃ¶rÃ¼ntÃ¼lerle eÄŸitilmiÅŸ bir yÃ¼z tanÄ±ma sistemi.

MITRE Corporation, yapay zeka sistemlerine yÃ¶nelik gerÃ§ek dÃ¼nya saldÄ±rÄ±larÄ±nda kullanÄ±lan taktik ve tekniklerin bir bilgi tabanÄ± olan [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst) oluÅŸturmuÅŸtur.

> Yapay zeka destekli sistemlerdeki gÃ¼venlik aÃ§Ä±klarÄ±nÄ±n sayÄ±sÄ± artÄ±yor, Ã§Ã¼nkÃ¼ yapay zekanÄ±n entegrasyonu, mevcut sistemlerin saldÄ±rÄ± yÃ¼zeyini geleneksel siber saldÄ±rÄ±larÄ±n Ã¶tesine taÅŸÄ±yor. ATLAS'Ä±, bu benzersiz ve geliÅŸen gÃ¼venlik aÃ§Ä±klarÄ±na yÃ¶nelik farkÄ±ndalÄ±ÄŸÄ± artÄ±rmak iÃ§in geliÅŸtirdik, Ã§Ã¼nkÃ¼ kÃ¼resel topluluk giderek yapay zekayÄ± Ã§eÅŸitli sistemlere entegre ediyor. ATLAS, MITRE ATT&CKÂ® Ã§erÃ§evesi Ã¼zerine modellenmiÅŸtir ve taktikleri, teknikleri ve prosedÃ¼rleri (TTP'ler) ATT&CK'teki olanlarla tamamlayÄ±cÄ±dÄ±r.

Geleneksel siber gÃ¼venlikte geliÅŸmiÅŸ tehdit simÃ¼lasyon senaryolarÄ±nÄ± planlamak iÃ§in yaygÄ±n olarak kullanÄ±lan MITRE ATT&CKÂ® Ã§erÃ§evesine benzer ÅŸekilde, ATLAS, ortaya Ã§Ä±kan saldÄ±rÄ±lara karÅŸÄ± savunma hazÄ±rlÄ±ÄŸÄ± iÃ§in daha iyi anlamaya yardÄ±mcÄ± olabilecek kolayca aranabilir bir TTP seti saÄŸlar.

AyrÄ±ca, Open Web Application Security Project (OWASP), LLM'leri kullanan uygulamalarda bulunan en kritik gÃ¼venlik aÃ§Ä±klarÄ±nÄ±n "[En Ä°yi 10 listesi](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)"ni oluÅŸturmuÅŸtur. Liste, yukarÄ±da bahsedilen veri zehirlenmesi gibi tehditlerin yanÄ± sÄ±ra ÅŸunlar gibi diÄŸer riskleri vurgular:

- **Komut Enjeksiyonu**: SaldÄ±rganlarÄ±n dikkatlice hazÄ±rlanmÄ±ÅŸ girdilerle BÃ¼yÃ¼k Dil Modelini (LLM) manipÃ¼le ederek modelin tasarlanan davranÄ±ÅŸÄ±nÄ±n dÄ±ÅŸÄ±na Ã§Ä±kmasÄ±na neden olduÄŸu bir teknik.
- **Tedarik Zinciri GÃ¼venlik AÃ§Ä±klarÄ±**: LLM tarafÄ±ndan kullanÄ±lan uygulamalarÄ± oluÅŸturan bileÅŸenler ve yazÄ±lÄ±mlar, Ã¶rneÄŸin Python modÃ¼lleri veya harici veri setleri, kendileri de tehlikeye dÃ¼ÅŸebilir ve beklenmedik sonuÃ§lara, Ã¶nyargÄ±lara ve hatta altyapÄ±da gÃ¼venlik aÃ§Ä±klarÄ±na yol aÃ§abilir.
- **AÅŸÄ±rÄ± GÃ¼ven**: LLM'ler yanÄ±labilir ve yanlÄ±ÅŸ veya gÃ¼vensiz sonuÃ§lar Ã¼retmeye eÄŸilimlidir. BelgelenmiÅŸ birÃ§ok durumda, insanlar sonuÃ§larÄ± olduÄŸu gibi kabul etmiÅŸ ve istenmeyen gerÃ§ek dÃ¼nya olumsuz sonuÃ§larÄ±na yol aÃ§mÄ±ÅŸtÄ±r.

Microsoft Cloud Advocate Rod Trent, bu ve diÄŸer ortaya Ã§Ä±kan yapay zeka tehditlerini derinlemesine inceleyen ve bu senaryolarÄ± en iyi ÅŸekilde ele almak iÃ§in kapsamlÄ± rehberlik saÄŸlayan Ã¼cretsiz bir e-kitap yazmÄ±ÅŸtÄ±r: [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst).

## Yapay Zeka Sistemleri ve LLM'ler iÃ§in GÃ¼venlik Testi

Yapay zeka (AI), Ã§eÅŸitli alanlarÄ± ve endÃ¼strileri dÃ¶nÃ¼ÅŸtÃ¼rerek toplum iÃ§in yeni olanaklar ve faydalar sunuyor. Ancak, yapay zeka veri gizliliÄŸi, Ã¶nyargÄ±, aÃ§Ä±klanabilirlik eksikliÄŸi ve potansiyel kÃ¶tÃ¼ye kullanÄ±m gibi Ã¶nemli zorluklar ve riskler de taÅŸÄ±yor. Bu nedenle, yapay zeka sistemlerinin gÃ¼venli ve sorumlu olduÄŸundan emin olmak, yani etik ve yasal standartlara uygun olmalarÄ± ve kullanÄ±cÄ±lar ile paydaÅŸlar tarafÄ±ndan gÃ¼venilebilir olmalarÄ± Ã§ok Ã¶nemlidir.

GÃ¼venlik testi, bir yapay zeka sistemi veya LLM'nin gÃ¼venliÄŸini deÄŸerlendirerek gÃ¼venlik aÃ§Ä±klarÄ±nÄ± belirleme ve bunlardan yararlanma sÃ¼recidir. Bu, geliÅŸtiriciler, kullanÄ±cÄ±lar veya Ã¼Ã§Ã¼ncÃ¼ taraf denetÃ§iler tarafÄ±ndan, testin amacÄ± ve kapsamÄ±na baÄŸlÄ± olarak gerÃ§ekleÅŸtirilebilir. Yapay zeka sistemleri ve LLM'ler iÃ§in en yaygÄ±n gÃ¼venlik testi yÃ¶ntemlerinden bazÄ±larÄ± ÅŸunlardÄ±r:

- **Veri temizleme**: Bu, bir yapay zeka sistemi veya LLM'nin eÄŸitim verilerinden veya girdisinden hassas veya Ã¶zel bilgileri kaldÄ±rma veya anonimleÅŸtirme sÃ¼recidir. Veri temizleme, gizli veya kiÅŸisel verilerin maruz kalmasÄ±nÄ± azaltarak veri sÄ±zÄ±ntÄ±sÄ±nÄ± ve kÃ¶tÃ¼ niyetli manipÃ¼lasyonu Ã¶nlemeye yardÄ±mcÄ± olabilir.
- **Adversaryal test**: Bu, bir yapay zeka sistemi veya LLM'nin giriÅŸ veya Ã§Ä±kÄ±ÅŸÄ±na karÅŸÄ±t Ã¶rnekler oluÅŸturma ve uygulama sÃ¼recidir. Adversaryal test, bir yapay zeka sistemi veya LLM'nin saldÄ±rganlar tarafÄ±ndan istismar edilebilecek gÃ¼venlik aÃ§Ä±klarÄ±nÄ± ve zayÄ±flÄ±klarÄ±nÄ± belirlemeye ve azaltmaya yardÄ±mcÄ± olabilir.
- **Model doÄŸrulama**: Bu, bir yapay zeka sistemi veya LLM'nin model parametrelerini veya mimarisini doÄŸrulama sÃ¼recidir. Model doÄŸrulama, modelin korunduÄŸundan ve kimlik doÄŸrulamasÄ±nÄ±n yapÄ±ldÄ±ÄŸÄ±ndan emin olarak model hÄ±rsÄ±zlÄ±ÄŸÄ±nÄ± tespit etmeye ve Ã¶nlemeye yardÄ±mcÄ± olabilir.
- **Ã‡Ä±ktÄ± doÄŸrulama**: Bu, bir yapay zeka sistemi veya LLM'nin Ã§Ä±ktÄ±sÄ±nÄ±n kalitesini ve gÃ¼venilirliÄŸini doÄŸrulama sÃ¼recidir. Ã‡Ä±ktÄ± doÄŸrulama, Ã§Ä±ktÄ±nÄ±n tutarlÄ± ve doÄŸru olmasÄ±nÄ± saÄŸlayarak kÃ¶tÃ¼ niyetli manipÃ¼lasyonu tespit etmeye ve dÃ¼zeltmeye yardÄ±mcÄ± olabilir.

OpenAI, yapay zeka gÃ¼venliÄŸine katkÄ±da bulunma umuduyla yapay zeka sistemlerinin Ã§Ä±ktÄ±sÄ±nÄ± test etmeyi amaÃ§layan bir _kÄ±rmÄ±zÄ± ekip oluÅŸturma aÄŸÄ± giriÅŸimi_ kapsamÄ±nda bir dizi _gÃ¼venlik deÄŸerlendirmesi_ kurmuÅŸtur.

> DeÄŸerlendirmeler, basit Soru-Cevap testlerinden daha karmaÅŸÄ±k simÃ¼lasyonlara kadar deÄŸiÅŸebilir. Somut Ã¶rnekler olarak, OpenAI tarafÄ±ndan yapay zeka davranÄ±ÅŸlarÄ±nÄ± Ã§eÅŸitli aÃ§Ä±lardan deÄŸerlendirmek iÃ§in geliÅŸtirilen Ã¶rnek deÄŸerlendirmeler ÅŸunlardÄ±r:

#### Ä°kna

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Bir yapay zeka sistemi baÅŸka bir yapay zeka sistemini gizli bir kelime sÃ¶ylemeye ne kadar iyi ikna edebilir?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Bir yapay zeka sistemi baÅŸka bir yapay zeka sistemini baÄŸÄ±ÅŸ yapmaya ne kadar iyi ikna edebilir?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Bir yapay zeka sistemi baÅŸka bir yapay zeka sisteminin politik bir Ã¶neriyi desteklemesini ne kadar iyi etkileyebilir?

#### Steganografi (gizli mesajlaÅŸma)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Bir yapay zeka sistemi baÅŸka bir yapay zeka sistemi tarafÄ±ndan yakalanmadan gizli mesajlarÄ± ne kadar iyi iletebilir?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Bir yapay zeka sistemi mesajlarÄ± sÄ±kÄ±ÅŸtÄ±rÄ±p aÃ§arak gizli mesajlarÄ± iletmek iÃ§in ne kadar iyi Ã§alÄ±ÅŸabilir?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Bir yapay zeka sistemi, doÄŸrudan iletiÅŸim olmadan baÅŸka bir yapay zeka sistemiyle ne kadar iyi koordinasyon saÄŸlayabilir?

### Yapay Zeka GÃ¼venliÄŸi

Yapay zeka sistemlerini kÃ¶tÃ¼ niyetli saldÄ±rÄ±lardan, kÃ¶tÃ¼ye kullanÄ±mdan veya istenmeyen sonuÃ§lardan korumayÄ± hedeflemek Ã§ok Ã¶nemlidir. Bu, yapay zeka sistemlerinin gÃ¼venliÄŸi, gÃ¼venilirliÄŸi ve gÃ¼venilirliÄŸini saÄŸlamak iÃ§in aÅŸaÄŸÄ±daki adÄ±mlarÄ± iÃ§ermelidir:

- Yapay zeka modellerini eÄŸitmek ve Ã§alÄ±ÅŸtÄ±rmak iÃ§in kullanÄ±lan veri ve algoritmalarÄ± gÃ¼vence altÄ±na almak
- Yapay zeka sistemlerine yetkisiz eriÅŸimi, manipÃ¼lasyonu veya sabotajÄ± Ã¶nlemek
- Yapay zeka sistemlerindeki Ã¶nyargÄ±, ayrÄ±mcÄ±lÄ±k veya etik sorunlarÄ± tespit etmek ve azaltmak
- Yapay zeka kararlarÄ±nÄ±n ve eylemlerinin hesap verebilirliÄŸini, ÅŸeffaflÄ±ÄŸÄ±nÄ± ve aÃ§Ä±klanabilirliÄŸini saÄŸlamak
- Yapay zeka sistemlerinin hedeflerini ve deÄŸerlerini insan ve toplum deÄŸerleriyle uyumlu hale getirmek

Yapay zeka gÃ¼venliÄŸi, yapay zeka sistemlerinin ve verilerin bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼, kullanÄ±labilirliÄŸini ve gizliliÄŸini saÄŸlamak iÃ§in Ã¶nemlidir. Yapay zeka gÃ¼venliÄŸinin bazÄ± zorluklarÄ± ve fÄ±rsatlarÄ± ÅŸunlardÄ±r:

- **FÄ±rsat**: Yapay zekayÄ± siber gÃ¼venlik stratejilerine dahil etmek, tehditleri belirlemede ve yanÄ±t sÃ¼relerini iyileÅŸtirmede Ã¶nemli bir rol oynayabilir. Yapay zeka, kimlik avÄ±, kÃ¶tÃ¼ amaÃ§lÄ± yazÄ±lÄ±m veya fidye yazÄ±lÄ±mÄ± gibi siber saldÄ±rÄ±larÄ±n tespitini ve azaltÄ±lmasÄ±nÄ± otomatikleÅŸtirmeye ve artÄ±rmaya yardÄ±mcÄ± olabilir.
- **Zorluk**: Yapay zeka, saldÄ±rganlar tarafÄ±ndan sahte veya yanÄ±ltÄ±cÄ± iÃ§erik oluÅŸturmak, kullanÄ±cÄ±larÄ± taklit etmek veya yapay zeka sistemlerindeki gÃ¼venlik aÃ§Ä±klarÄ±nÄ± istismar etmek gibi sofistike saldÄ±rÄ±lar baÅŸlatmak iÃ§in de kullanÄ±labilir. Bu nedenle, yapay zeka geliÅŸtiricileri, kÃ¶tÃ¼ye kullanÄ±ma karÅŸÄ± saÄŸlam ve dayanÄ±klÄ± sistemler tasarlama konusunda benzersiz bir sorumluluÄŸa sahiptir.

### Veri Koruma

LLM'ler, kullandÄ±klarÄ± verilerin gizliliÄŸi ve gÃ¼venliÄŸi aÃ§Ä±sÄ±ndan riskler oluÅŸturabilir. Ã–rneÄŸin, LLM'ler eÄŸitim verilerinden kiÅŸisel isimler, adresler, ÅŸifreler veya kredi kartÄ± numaralarÄ± gibi hassas bilgileri hatÄ±rlayÄ±p sÄ±zdÄ±rabilir. AyrÄ±ca, kÃ¶tÃ¼ niyetli aktÃ¶rler tarafÄ±ndan manipÃ¼le edilebilir veya saldÄ±rÄ±ya uÄŸrayabilirler. Bu nedenle, bu risklerin farkÄ±nda olmak ve LLM'lerle kullanÄ±lan verileri korumak iÃ§in uygun Ã¶nlemleri almak Ã¶nemlidir. LLM'lerle kullanÄ±lan verileri korumak iÃ§in alabileceÄŸiniz birkaÃ§ adÄ±m ÅŸunlardÄ±r:

- **LLM'lerle paylaÅŸÄ±lan veri miktarÄ±nÄ± ve tÃ¼rÃ¼nÃ¼ sÄ±nÄ±rlamak**: YalnÄ±zca gerekli ve ilgili verileri paylaÅŸÄ±n ve hassas, gizli veya kiÅŸisel verileri paylaÅŸmaktan kaÃ§Ä±nÄ±n. KullanÄ±cÄ±lar ayrÄ±ca LLM'lerle paylaÅŸtÄ±klarÄ± verileri anonimleÅŸtirmeli veya ÅŸifrelemelidir, Ã¶rneÄŸin herhangi bir tanÄ±mlayÄ±cÄ± bilgiyi kaldÄ±rarak veya gizleyerek ya da gÃ¼venli iletiÅŸim kanallarÄ± kullanarak.
- **LLM'lerin Ã¼rettiÄŸi verileri doÄŸrulamak**: LLM'ler tarafÄ±ndan Ã¼retilen Ã§Ä±ktÄ±nÄ±n doÄŸruluÄŸunu ve kalitesini her zaman kontrol edin, istenmeyen veya uygunsuz bilgiler iÃ§ermediÄŸinden emin olun.
- **Veri ihlallerini veya olaylarÄ±nÄ± bildirmek ve uyarÄ±da bulunmak**: LLM'lerden gelen metinlerin alakasÄ±z, yanlÄ±ÅŸ, saldÄ±rgan veya zararlÄ± olmasÄ± gibi ÅŸÃ¼pheli veya anormal etkinliklere veya davranÄ±ÅŸlara karÅŸÄ± dikkatli olun. Bu, bir veri ihlali veya gÃ¼venlik olayÄ± gÃ¶stergesi olabilir.

Veri gÃ¼venliÄŸi, yÃ¶netimi ve uyumluluÄŸu, Ã§oklu bulut ortamÄ±nda veri ve yapay zekanÄ±n gÃ¼cÃ¼nden yararlanmak isteyen herhangi bir kuruluÅŸ iÃ§in kritik Ã¶neme sahiptir. TÃ¼m verilerinizi gÃ¼vence altÄ±na almak ve yÃ¶netmek karmaÅŸÄ±k ve Ã§ok yÃ¶nlÃ¼ bir iÅŸtir. FarklÄ± bulutlarda farklÄ± konumlarda farklÄ± tÃ¼rdeki verileri (yapÄ±sal, yapÄ±sal olmayan ve yapay zeka tarafÄ±ndan Ã¼retilen veriler) gÃ¼vence altÄ±na almanÄ±z ve mevcut ve gelecekteki veri gÃ¼venliÄŸi, yÃ¶netimi ve yapay zeka dÃ¼zenlemelerini dikkate almanÄ±z gerekir. Verilerinizi korumak iÃ§in aÅŸaÄŸÄ±daki gibi en iyi uygulamalarÄ± ve Ã¶nlemleri benimsemelisiniz:

- Veri koruma ve gizlilik Ã¶zellikleri sunan bulut hizmetlerini veya platformlarÄ±nÄ± kullanÄ±n.
- Verilerinizi hatalar, tutarsÄ±zlÄ±klar veya anormallikler aÃ§Ä±sÄ±ndan kontrol etmek iÃ§in veri kalitesi ve doÄŸrulama araÃ§larÄ±nÄ± kullanÄ±n.
- Verilerinizin sorumlu ve ÅŸeffaf bir ÅŸekilde kullanÄ±lmasÄ±nÄ± saÄŸlamak iÃ§in veri yÃ¶netimi ve etik Ã§erÃ§evelerini kullanÄ±n.

### GerÃ§ek DÃ¼nya Tehditlerini SimÃ¼le Etmek - Yapay Zeka KÄ±rmÄ±zÄ± Ekip OluÅŸturma
GerÃ§ek dÃ¼nya tehditlerini taklit etmek, sistemlerin risklerini belirlemek ve savunucularÄ±n tepkilerini test etmek iÃ§in benzer araÃ§lar, taktikler ve prosedÃ¼rler kullanarak dayanÄ±klÄ± yapay zeka sistemleri oluÅŸturmakta artÄ±k standart bir uygulama olarak kabul edilmektedir.

> Yapay zeka red teaming uygulamasÄ±, daha geniÅŸ bir anlam kazanacak ÅŸekilde evrimleÅŸmiÅŸtir: yalnÄ±zca gÃ¼venlik aÃ§Ä±klarÄ±nÄ± araÅŸtÄ±rmayÄ± deÄŸil, aynÄ± zamanda potansiyel olarak zararlÄ± iÃ§erik Ã¼retimi gibi diÄŸer sistem hatalarÄ±nÄ± da araÅŸtÄ±rmayÄ± iÃ§erir. Yapay zeka sistemleri yeni riskler taÅŸÄ±r ve red teaming, bu yeni riskleri anlamanÄ±n temelidir; Ã¶rneÄŸin, prompt enjeksiyonu ve temelsiz iÃ§erik Ã¼retimi gibi. - [Microsoft AI Red Team daha gÃ¼venli yapay zeka geleceÄŸi inÅŸa ediyor](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![Red teaming iÃ§in rehberlik ve kaynaklar](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.tr.png)]()

AÅŸaÄŸÄ±da Microsoftâ€™un AI Red Team programÄ±nÄ± ÅŸekillendiren Ã¶nemli bilgiler yer almaktadÄ±r.

1. **Yapay Zeka Red Teamingâ€™in GeniÅŸ KapsamÄ±:**
   Yapay zeka red teaming artÄ±k hem gÃ¼venlik hem de Sorumlu Yapay Zeka (RAI) sonuÃ§larÄ±nÄ± kapsÄ±yor. Geleneksel olarak, red teaming gÃ¼venlik yÃ¶nlerine odaklanÄ±r ve modeli bir vektÃ¶r olarak ele alÄ±rdÄ± (Ã¶rneÄŸin, temel modeli Ã§almak). Ancak yapay zeka sistemleri, prompt enjeksiyonu ve zehirleme gibi yeni gÃ¼venlik aÃ§Ä±klarÄ± getirir ve Ã¶zel dikkat gerektirir. GÃ¼venliÄŸin Ã¶tesinde, yapay zeka red teaming aynÄ± zamanda adalet sorunlarÄ±nÄ± (Ã¶rneÄŸin, stereotipler) ve zararlÄ± iÃ§erikleri (Ã¶rneÄŸin, ÅŸiddeti yÃ¼celtme) araÅŸtÄ±rÄ±r. Bu sorunlarÄ±n erken tespiti, savunma yatÄ±rÄ±mlarÄ±nÄ±n Ã¶nceliklendirilmesini saÄŸlar.
2. **KÃ¶tÃ¼ AmaÃ§lÄ± ve ZararsÄ±z Hatalar:**
   Yapay zeka red teaming, hem kÃ¶tÃ¼ amaÃ§lÄ± hem de zararsÄ±z perspektiflerden kaynaklanan hatalarÄ± dikkate alÄ±r. Ã–rneÄŸin, yeni Bingâ€™i red teaming yaparken, yalnÄ±zca kÃ¶tÃ¼ niyetli saldÄ±rganlarÄ±n sistemi nasÄ±l alt edebileceÄŸini deÄŸil, aynÄ± zamanda sÄ±radan kullanÄ±cÄ±larÄ±n problemli veya zararlÄ± iÃ§eriklerle nasÄ±l karÅŸÄ±laÅŸabileceÄŸini de inceliyoruz. Geleneksel gÃ¼venlik red teaming, genellikle kÃ¶tÃ¼ niyetli aktÃ¶rlere odaklanÄ±rken, yapay zeka red teaming daha geniÅŸ bir persona ve potansiyel hata yelpazesini hesaba katar.
3. **Yapay Zeka Sistemlerinin Dinamik DoÄŸasÄ±:**
   Yapay zeka uygulamalarÄ± sÃ¼rekli olarak evrim geÃ§irir. BÃ¼yÃ¼k dil modeli uygulamalarÄ±nda, geliÅŸtiriciler deÄŸiÅŸen gereksinimlere uyum saÄŸlar. SÃ¼rekli red teaming, geliÅŸen risklere karÅŸÄ± sÃ¼rekli bir uyanÄ±klÄ±k ve uyum saÄŸlar.

Yapay zeka red teaming her ÅŸeyi kapsamaz ve [rol tabanlÄ± eriÅŸim kontrolÃ¼ (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) ve kapsamlÄ± veri yÃ¶netimi Ã§Ã¶zÃ¼mleri gibi ek kontrolleri tamamlayÄ±cÄ± bir hareket olarak dÃ¼ÅŸÃ¼nÃ¼lmelidir. Gizlilik ve gÃ¼venliÄŸi hesaba katan, Ã¶nyargÄ±larÄ±, zararlÄ± iÃ§erikleri ve yanlÄ±ÅŸ bilgileri en aza indirmeyi hedefleyen gÃ¼venli ve sorumlu yapay zeka Ã§Ã¶zÃ¼mleri kullanmaya odaklanan bir gÃ¼venlik stratejisini desteklemek iÃ§in tasarlanmÄ±ÅŸtÄ±r.

Yapay zeka sistemlerinizdeki riskleri belirlemek ve azaltmak iÃ§in red teamingâ€™in nasÄ±l yardÄ±mcÄ± olabileceÄŸini daha iyi anlamanÄ±za yardÄ±mcÄ± olacak ek okuma listesi:

- [BÃ¼yÃ¼k dil modelleri (LLM'ler) ve uygulamalarÄ± iÃ§in red teaming planlama](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [OpenAI Red Teaming Network nedir?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - Daha GÃ¼venli ve Daha Sorumlu Yapay Zeka Ã‡Ã¶zÃ¼mleri OluÅŸturmak Ä°Ã§in Temel Bir Uygulama](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Yapay Zeka Sistemleri iÃ§in DÃ¼ÅŸmanca Tehdit ManzarasÄ±)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), yapay zeka sistemlerine yÃ¶nelik gerÃ§ek dÃ¼nya saldÄ±rÄ±larÄ±nda kullanÄ±lan taktikler ve teknikler hakkÄ±nda bir bilgi tabanÄ±.

## Bilgi KontrolÃ¼

Veri bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ korumak ve kÃ¶tÃ¼ye kullanÄ±mÄ± Ã¶nlemek iÃ§in iyi bir yaklaÅŸÄ±m ne olabilir?

1. Veri eriÅŸimi ve veri yÃ¶netimi iÃ§in gÃ¼Ã§lÃ¼ rol tabanlÄ± kontroller uygulayÄ±n  
1. Veri yanlÄ±ÅŸ temsilini veya kÃ¶tÃ¼ye kullanÄ±mÄ±nÄ± Ã¶nlemek iÃ§in veri etiketlemeyi uygulayÄ±n ve denetleyin  
1. Yapay zeka altyapÄ±nÄ±zÄ±n iÃ§erik filtrelemeyi desteklediÄŸinden emin olun  

A:1, ÃœÃ§ Ã¶neri de harika olsa da, kullanÄ±cÄ±larÄ±n doÄŸru veri eriÅŸim ayrÄ±calÄ±klarÄ±nÄ± almasÄ±nÄ± saÄŸlamak, LLM'ler tarafÄ±ndan kullanÄ±lan verilerin manipÃ¼lasyonunu ve yanlÄ±ÅŸ temsilini Ã¶nlemek iÃ§in uzun bir yol kat edecektir.

## ğŸš€ Zorluk

Yapay zeka Ã§aÄŸÄ±nda [hassas bilgileri yÃ¶netme ve koruma](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) hakkÄ±nda daha fazla bilgi edinin.

## Harika Ä°ÅŸ, Ã–ÄŸrenmeye Devam Edin

Bu dersi tamamladÄ±ktan sonra, [Ãœretken Yapay Zeka Ã–ÄŸrenme koleksiyonumuza](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) gÃ¶z atarak Ãœretken Yapay Zeka bilginizi geliÅŸtirmeye devam edin!

14. Derse geÃ§in, burada [Ãœretken Yapay Zeka Uygulama YaÅŸam DÃ¶ngÃ¼sÃ¼ne](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst) bakacaÄŸÄ±z!

---

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.