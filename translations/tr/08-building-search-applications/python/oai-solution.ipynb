{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki not defterlerini çalıştırmak için, henüz yapmadıysanız, .env dosyasının içine `OPENAI_API_KEY` olarak openai anahtarını ayarlamanız gerekir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\")\n",
    "assert API_KEY, \"ERROR: OpenAI Key is missing\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY\n",
    "    )\n",
    "\n",
    "model = 'text-embedding-ada-002'\n",
    "\n",
    "SIMILARITIES_RESULTS_THRESHOLD = 0.75\n",
    "DATASET_NAME = \"../embedding_index_3m.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonraki adımda, Embedding İndeksini bir Pandas Dataframe'e yükleyeceğiz. Embedding İndeksi, `embedding_index_3m.json` adlı bir JSON dosyasında saklanmaktadır. Embedding İndeksi, Ekim 2023 sonuna kadar olan her YouTube transkripti için Embedding'leri içermektedir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(source: str) -> pd.core.frame.DataFrame:\n",
    "    # Load the video session index\n",
    "    pd_vectors = pd.read_json(source)\n",
    "    return pd_vectors.drop(columns=[\"text\"], errors=\"ignore\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonraki adımda, sorgu için Embedding İndeksinde arama yapacak `get_videos` adlı bir fonksiyon oluşturacağız. Fonksiyon, sorguya en benzer ilk 5 videoyu döndürecektir. Fonksiyon şu şekilde çalışır:\n",
    "\n",
    "1. Öncelikle, Embedding İndeksinin bir kopyası oluşturulur.\n",
    "2. Daha sonra, OpenAI Embedding API kullanılarak sorgunun Embedding'i hesaplanır.\n",
    "3. Ardından, Embedding İndeksinde `similarity` adlı yeni bir sütun oluşturulur. `similarity` sütunu, sorgu Embedding'i ile her video segmentinin Embedding'i arasındaki kosinüs benzerliğini içerir.\n",
    "4. Sonra, Embedding İndeksi `similarity` sütununa göre filtrelenir. Embedding İndeksi, kosinüs benzerliği 0.75 veya daha yüksek olan videoları içerecek şekilde filtrelenir.\n",
    "5. Son olarak, Embedding İndeksi `similarity` sütununa göre sıralanır ve en benzer ilk 5 video döndürülür.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_videos(\n",
    "    query: str, dataset: pd.core.frame.DataFrame, rows: int\n",
    ") -> pd.core.frame.DataFrame:\n",
    "    # create a copy of the dataset\n",
    "    video_vectors = dataset.copy()\n",
    "\n",
    "    # get the embeddings for the query    \n",
    "    query_embeddings = client.embeddings.create(input=query, model=model).data[0].embedding\n",
    "\n",
    "    # create a new column with the calculated similarity for each row\n",
    "    video_vectors[\"similarity\"] = video_vectors[\"ada_v2\"].apply(\n",
    "        lambda x: cosine_similarity(np.array(query_embeddings), np.array(x))\n",
    "    )\n",
    "\n",
    "    # filter the videos by similarity\n",
    "    mask = video_vectors[\"similarity\"] >= SIMILARITIES_RESULTS_THRESHOLD\n",
    "    video_vectors = video_vectors[mask].copy()\n",
    "\n",
    "    # sort the videos by similarity\n",
    "    video_vectors = video_vectors.sort_values(by=\"similarity\", ascending=False).head(\n",
    "        rows\n",
    "    )\n",
    "\n",
    "    # return the top rows\n",
    "    return video_vectors.head(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu fonksiyon çok basittir, sadece arama sorgusunun sonuçlarını yazdırır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(videos: pd.core.frame.DataFrame, query: str):\n",
    "    def _gen_yt_url(video_id: str, seconds: int) -> str:\n",
    "        \"\"\"convert time in format 00:00:00 to seconds\"\"\"\n",
    "        return f\"https://youtu.be/{video_id}?t={seconds}\"\n",
    "\n",
    "    print(f\"\\nVideos similar to '{query}':\")\n",
    "    for _, row in videos.iterrows():\n",
    "        youtube_url = _gen_yt_url(row[\"videoId\"], row[\"seconds\"])\n",
    "        print(f\" - {row['title']}\")\n",
    "        print(f\"   Summary: {' '.join(row['summary'].split()[:15])}...\")\n",
    "        print(f\"   YouTube: {youtube_url}\")\n",
    "        print(f\"   Similarity: {row['similarity']}\")\n",
    "        print(f\"   Speakers: {row['speaker']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. İlk olarak, Gömülü Dizin bir Pandas Dataframe'e yüklenir.\n",
    "2. Sonra, kullanıcıdan bir sorgu girmesi istenir.\n",
    "3. Ardından, sorgu için Gömülü Dizin'de arama yapmak üzere `get_videos` fonksiyonu çağrılır.\n",
    "4. Son olarak, sonuçları kullanıcıya göstermek için `display_results` fonksiyonu çağrılır.\n",
    "5. Daha sonra kullanıcıdan başka bir sorgu girmesi istenir. Bu işlem kullanıcı `exit` girene kadar devam eder.\n",
    "\n",
    "![](../../../../translated_images/notebook-search.1e320b9c7fcbb0bc.tr.png)\n",
    "\n",
    "Bir sorgu girmeniz istenecek. Bir sorgu girin ve enter tuşuna basın. Uygulama, sorguyla ilgili videoların bir listesini döndürecektir. Uygulama ayrıca sorunun cevabının bulunduğu videodaki yere bir bağlantı da döndürecektir.\n",
    "\n",
    "Denemek için bazı sorgular şunlardır:\n",
    "\n",
    "- Azure Machine Learning nedir?\n",
    "- Konvolüsyonel sinir ağları nasıl çalışır?\n",
    "- Sinir ağı nedir?\n",
    "- Azure Machine Learning ile Jupyter Notebooks kullanabilir miyim?\n",
    "- ONNX nedir?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_vectors = load_dataset(DATASET_NAME)\n",
    "\n",
    "# get user query from input\n",
    "while True:\n",
    "    query = input(\"Enter a query: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    videos = get_videos(query, pd_vectors, 5)\n",
    "    display_results(videos, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Feragatname**:  \nBu belge, AI çeviri servisi [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba gösterilse de, otomatik çevirilerin hatalar veya yanlışlıklar içerebileceğini lütfen unutmayınız. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımı sonucu oluşabilecek yanlış anlamalar veya yorum hatalarından sorumlu değiliz.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "afb84920098ad1e6e4ca63ee9a61d9b8",
   "translation_date": "2025-12-19T10:14:29+00:00",
   "source_file": "08-building-search-applications/python/oai-solution.ipynb",
   "language_code": "tr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}