<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:46:16+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "tr"
}
-->
# Sinir AÄŸlarÄ±na GiriÅŸ. Ã‡ok KatmanlÄ± Perceptron

Ã–nceki bÃ¶lÃ¼mde, en basit sinir aÄŸÄ± modeli olan tek katmanlÄ± perceptronâ€™u, yani doÄŸrusal iki sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma modelini Ã¶ÄŸrendiniz.

Bu bÃ¶lÃ¼mde, bu modeli daha esnek bir yapÄ±ya geniÅŸleteceÄŸiz ve ÅŸunlarÄ± yapmamÄ±za olanak tanÄ±yacaÄŸÄ±z:

* Ä°ki sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rmanÄ±n yanÄ± sÄ±ra **Ã§ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma** yapmak
* SÄ±nÄ±flandÄ±rmanÄ±n yanÄ± sÄ±ra **regresyon problemlerini** Ã§Ã¶zmek
* DoÄŸrusal olarak ayrÄ±lamayan sÄ±nÄ±flarÄ± ayÄ±rmak

AyrÄ±ca, farklÄ± sinir aÄŸÄ± mimarileri oluÅŸturabilmemizi saÄŸlayacak kendi modÃ¼ler Python Ã§erÃ§evemizi geliÅŸtireceÄŸiz.

## Makine Ã–ÄŸrenmesinin Formalizasyonu

Makine Ã–ÄŸrenmesi problemini formalize ederek baÅŸlayalÄ±m. Diyelim ki etiketleri **Y** olan bir eÄŸitim veri setimiz **X** var ve en doÄŸru tahminleri yapacak bir model *f* inÅŸa etmemiz gerekiyor. Tahminlerin kalitesi **KayÄ±p fonksiyonu** â„’ ile Ã¶lÃ§Ã¼lÃ¼r. AÅŸaÄŸÄ±daki kayÄ±p fonksiyonlarÄ± sÄ±kÃ§a kullanÄ±lÄ±r:

* Regresyon problemi iÃ§in, yani bir sayÄ± tahmin etmemiz gerektiÄŸinde, **mutlak hata** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>| veya **kare hata** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup> kullanÄ±labilir
* SÄ±nÄ±flandÄ±rma iÃ§in, **0-1 kaybÄ±** (temelde modelin **doÄŸruluÄŸu** ile aynÄ±) veya **lojistik kayÄ±p** kullanÄ±lÄ±r.

Tek katmanlÄ± perceptron iÃ§in, *f* fonksiyonu doÄŸrusal bir fonksiyon olarak tanÄ±mlanmÄ±ÅŸtÄ±: *f(x)=wx+b* (burada *w* aÄŸÄ±rlÄ±k matrisi, *x* giriÅŸ Ã¶zellikleri vektÃ¶rÃ¼, *b* ise bias vektÃ¶rÃ¼dÃ¼r). FarklÄ± sinir aÄŸÄ± mimarileri iÃ§in bu fonksiyon daha karmaÅŸÄ±k bir biÃ§im alabilir.

> SÄ±nÄ±flandÄ±rma durumunda, aÄŸ Ã§Ä±ktÄ±sÄ± olarak ilgili sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±nÄ± almak genellikle tercih edilir. Rastgele sayÄ±larÄ± olasÄ±lÄ±klara dÃ¶nÃ¼ÅŸtÃ¼rmek (Ã¶rneÄŸin Ã§Ä±ktÄ±yÄ± normalize etmek) iÃ§in sÄ±klÄ±kla **softmax** fonksiyonu Ïƒ kullanÄ±lÄ±r ve fonksiyon *f* ÅŸu hale gelir: *f(x)=Ïƒ(wx+b)*

YukarÄ±daki *f* tanÄ±mÄ±nda, *w* ve *b* **parametreler** olarak adlandÄ±rÄ±lÄ±r ve Î¸=âŸ¨*w,b*âŸ© ile gÃ¶sterilir. Veri seti âŸ¨**X**,**Y**âŸ© verildiÄŸinde, parametrelerin bir fonksiyonu olarak tÃ¼m veri seti Ã¼zerindeki toplam hatayÄ± hesaplayabiliriz.

> âœ… **Sinir aÄŸÄ± eÄŸitiminin amacÄ±, parametreler Î¸â€™yi deÄŸiÅŸtirerek hatayÄ± minimize etmektir**

## Gradyan Ä°niÅŸi Optimizasyonu

Fonksiyon optimizasyonunda iyi bilinen bir yÃ¶ntem olan **gradyan iniÅŸi** vardÄ±r. Fikir, kayÄ±p fonksiyonunun parametrelere gÃ¶re tÃ¼revini (Ã§ok boyutlu durumda buna **gradyan** denir) hesaplayÄ±p, parametreleri hatayÄ± azaltacak ÅŸekilde deÄŸiÅŸtirmektir. Bu ÅŸu ÅŸekilde formalize edilebilir:

* Parametreleri rastgele deÄŸerlerle baÅŸlat w<sup>(0)</sup>, b<sup>(0)</sup>
* AÅŸaÄŸÄ±daki adÄ±mÄ± birÃ§ok kez tekrarla:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

EÄŸitim sÄ±rasÄ±nda, optimizasyon adÄ±mlarÄ± tÃ¼m veri seti gÃ¶z Ã¶nÃ¼nde bulundurularak hesaplanmalÄ±dÄ±r (kayÄ±p tÃ¼m eÄŸitim Ã¶rnekleri Ã¼zerinden toplanarak hesaplanÄ±r). Ancak pratikte, veri setinden kÃ¼Ã§Ã¼k parÃ§alar olan **minibatch**â€™ler alÄ±nÄ±r ve gradyanlar bu alt kÃ¼me Ã¼zerinden hesaplanÄ±r. Her seferinde rastgele alt kÃ¼me seÃ§ildiÄŸi iÃ§in bu yÃ¶nteme **stokastik gradyan iniÅŸi** (SGD) denir.

## Ã‡ok KatmanlÄ± Perceptronlar ve Geri YayÄ±lÄ±m

YukarÄ±da gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z tek katmanlÄ± aÄŸ, doÄŸrusal olarak ayrÄ±labilen sÄ±nÄ±flarÄ± sÄ±nÄ±flandÄ±rabilir. Daha zengin bir model oluÅŸturmak iÃ§in aÄŸÄ±n birkaÃ§ katmanÄ±nÄ± birleÅŸtirebiliriz. Matematiksel olarak bu, *f* fonksiyonunun daha karmaÅŸÄ±k bir biÃ§im almasÄ± ve birkaÃ§ adÄ±mda hesaplanmasÄ± anlamÄ±na gelir:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

Burada, Î± **doÄŸrusal olmayan aktivasyon fonksiyonu**, Ïƒ softmax fonksiyonu ve parametreler Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Gradyan iniÅŸi algoritmasÄ± aynÄ± kalÄ±r, ancak gradyanlarÄ± hesaplamak daha zorlaÅŸÄ±r. Zincir tÃ¼rev kuralÄ± kullanÄ±larak tÃ¼revler ÅŸu ÅŸekilde hesaplanabilir:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… KayÄ±p fonksiyonunun parametrelere gÃ¶re tÃ¼revlerini hesaplamak iÃ§in zincir tÃ¼rev kuralÄ± kullanÄ±lÄ±r.

Dikkat edin, tÃ¼m bu ifadelerin en solundaki kÄ±sÄ±m aynÄ±dÄ±r, bu yÃ¼zden tÃ¼revleri kayÄ±p fonksiyonundan baÅŸlayarak hesaplama grafiÄŸinde "geriye doÄŸru" etkili bir ÅŸekilde hesaplayabiliriz. Bu nedenle Ã§ok katmanlÄ± perceptron eÄŸitme yÃ¶ntemi **geri yayÄ±lÄ±m** veya 'backprop' olarak adlandÄ±rÄ±lÄ±r.

> TODO: resim atÄ±fÄ±

> âœ… Geri yayÄ±lÄ±mÄ± not defteri Ã¶rneÄŸimizde Ã§ok daha detaylÄ± inceleyeceÄŸiz.

## SonuÃ§

Bu derste, kendi sinir aÄŸÄ± kÃ¼tÃ¼phanemizi oluÅŸturduk ve bunu basit iki boyutlu bir sÄ±nÄ±flandÄ±rma gÃ¶revi iÃ§in kullandÄ±k.

## ğŸš€ Meydan Okuma

YanÄ±ndaki not defterinde, Ã§ok katmanlÄ± perceptronlar oluÅŸturup eÄŸitmek iÃ§in kendi Ã§erÃ§evenizi uygulayacaksÄ±nÄ±z. Modern sinir aÄŸlarÄ±nÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± ayrÄ±ntÄ±lÄ± olarak gÃ¶rebileceksiniz.

OwnFramework not defterine geÃ§in ve Ã¼zerinde Ã§alÄ±ÅŸÄ±n.

## GÃ¶zden GeÃ§irme & Kendi Kendine Ã‡alÄ±ÅŸma

Geri yayÄ±lÄ±m, yapay zeka ve makine Ã¶ÄŸrenmesinde yaygÄ±n kullanÄ±lan bir algoritmadÄ±r ve daha detaylÄ± incelenmeye deÄŸerdir.

## Ã–dev

Bu laboratuvarda, bu derste oluÅŸturduÄŸunuz Ã§erÃ§eveyi kullanarak MNIST el yazÄ±sÄ± rakam sÄ±nÄ±flandÄ±rma problemini Ã§Ã¶zmeniz isteniyor.

* Talimatlar
* Not defteri

**Feragatname**:  
Bu belge, AI Ã§eviri servisi [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±nÄ±z. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ± sonucu ortaya Ã§Ä±kabilecek yanlÄ±ÅŸ anlamalar veya yorum hatalarÄ±ndan sorumlu deÄŸiliz.