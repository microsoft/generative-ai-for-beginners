{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giriş\n",
    "\n",
    "Bu derste şunlar ele alınacaktır:\n",
    "- Fonksiyon çağrısı nedir ve hangi durumlarda kullanılır\n",
    "- Azure OpenAI kullanarak nasıl fonksiyon çağrısı oluşturulur\n",
    "- Bir uygulamaya fonksiyon çağrısının nasıl entegre edileceği\n",
    "\n",
    "## Öğrenme Hedefleri\n",
    "\n",
    "Bu dersi tamamladıktan sonra şunları bilecek ve anlayacaksınız:\n",
    "\n",
    "- Fonksiyon çağrısı kullanmanın amacı\n",
    "- Azure Open AI Servisi ile Fonksiyon Çağrısı Kurulumu\n",
    "- Uygulamanızın kullanım senaryosu için etkili fonksiyon çağrıları tasarlama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonksiyon Çağrılarını Anlamak\n",
    "\n",
    "Bu derste, eğitim girişimimiz için kullanıcıların teknik kursları bulmak amacıyla bir sohbet botu kullanmasını sağlayan bir özellik geliştirmek istiyoruz. Kullanıcıların beceri seviyelerine, mevcut rollerine ve ilgi duydukları teknolojiye uygun kurslar önereceğiz.\n",
    "\n",
    "Bunu tamamlamak için şu araçların bir kombinasyonunu kullanacağız:\n",
    " - Kullanıcıya sohbet deneyimi sunmak için `Azure Open AI`\n",
    " - Kullanıcının isteğine göre kurs bulmasına yardımcı olmak için `Microsoft Learn Catalog API`\n",
    " - Kullanıcının sorgusunu alıp API isteği yapmak için bir fonksiyona göndermek amacıyla `Function Calling`\n",
    "\n",
    "Başlamak için, ilk etapta neden fonksiyon çağrısı kullanmak isteyebileceğimize bakalım:\n",
    "\n",
    "print(\"Sonraki istekteki mesajlar:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # GPT'den, fonksiyonun yanıtını görebileceği yeni bir yanıt al\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neden Function Calling\n",
    "\n",
    "Bu kurstaki başka bir dersi tamamladıysanız, Büyük Dil Modelleri'nin (LLM'ler) gücünü muhtemelen anlamışsınızdır. Umarım aynı zamanda bazı sınırlamalarını da görmüşsünüzdür.\n",
    "\n",
    "Function Calling, Azure Open AI Service'in şu sınırlamaları aşmak için sunduğu bir özelliktir:\n",
    "1) Tutarlı yanıt formatı\n",
    "2) Bir sohbet bağlamında bir uygulamanın diğer kaynaklarından veri kullanabilme yeteneği\n",
    "\n",
    "Function calling öncesinde, bir LLM'den gelen yanıtlar yapılandırılmamış ve tutarsız oluyordu. Geliştiriciler, her yanıt varyasyonunu işleyebilmek için karmaşık doğrulama kodları yazmak zorundaydı.\n",
    "\n",
    "Kullanıcılar \"Stockholm'de şu an hava nasıl?\" gibi sorulara yanıt alamıyordu. Bunun nedeni, modellerin yalnızca eğitildikleri zamana kadar olan verilerle sınırlı olmasıydı.\n",
    "\n",
    "Aşağıda bu sorunu gösteren bir örneğe bakalım:\n",
    "\n",
    "Diyelim ki öğrenci verilerinden oluşan bir veritabanı oluşturmak ve onlara en uygun kursu önermek istiyoruz. Aşağıda, içerdiği veriler açısından birbirine çok benzeyen iki öğrenci tanımı var.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu veriyi ayrıştırması için bunu bir LLM'ye göndermek istiyoruz. Bu daha sonra uygulamamızda bir API'ye göndermek veya bir veritabanında saklamak için kullanılabilir.\n",
    "\n",
    "İlgilendiğimiz bilgileri LLM'ye anlatan iki özdeş istem oluşturalım:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bunu ürünümüz için önemli olan bölümleri ayrıştırması için bir LLM'ye göndermek istiyoruz. Böylece LLM'ye talimat vermek için iki özdeş istem oluşturabiliriz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu iki istemi oluşturduktan sonra, onları `openai.ChatCompletion` kullanarak LLM'ye göndereceğiz. İstemi `messages` değişkeninde saklıyoruz ve role olarak `user` atıyoruz. Bu, bir kullanıcının bir sohbet botuna yazdığı bir mesajı taklit etmek içindir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi her iki isteği de LLM'ye gönderebilir ve aldığımız yanıtı inceleyebiliriz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aynı istemler kullanılsa ve açıklamalar benzer olsa da, `Grades` özelliğinin farklı formatlarını alabiliriz.\n",
    "\n",
    "Yukarıdaki hücreyi birkaç kez çalıştırırsanız, format `3.7` veya `3.7 GPA` olabilir.\n",
    "\n",
    "Bunun nedeni, LLM'in yazılı istem şeklinde yapılandırılmamış veriler alması ve yine yapılandırılmamış veriler döndürmesidir. Veriyi saklarken veya kullanırken neyle karşılaşacağımızı bilmek için yapılandırılmış bir formata ihtiyacımız var.\n",
    "\n",
    "Fonksiyonel çağrı kullanarak, bize yapılandırılmış veri döneceğinden emin olabiliriz. Fonksiyon çağrısı kullanıldığında, LLM aslında herhangi bir fonksiyonu çağırmaz veya çalıştırmaz. Bunun yerine, LLM'in yanıtları için takip etmesi gereken bir yapı oluştururuz. Daha sonra bu yapılandırılmış yanıtları, uygulamalarımızda hangi fonksiyonun çalıştırılacağını bilmek için kullanırız.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fonksiyon Çağırma Akış Diyagramı](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.tr.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonksiyon çağrılarının kullanım alanları\n",
    "\n",
    "**Harici Araçları Çağırmak**  \n",
    "Sohbet botları, kullanıcılardan gelen sorulara yanıt vermede oldukça iyidir. Fonksiyon çağrısı kullanılarak, sohbet botları kullanıcı mesajlarını belirli görevleri tamamlamak için kullanabilir. Örneğin, bir öğrenci sohbet botuna \"Eğitmenime bu konuda daha fazla yardıma ihtiyacım olduğunu belirten bir e-posta gönder\" diyebilir. Bu durumda `send_email(to: string, body: string)` fonksiyonu çağrılabilir.\n",
    "\n",
    "**API veya Veritabanı Sorguları Oluşturmak**  \n",
    "Kullanıcılar, doğal dilde sordukları soruların biçimlendirilmiş bir sorguya veya API isteğine dönüştürülmesiyle bilgi bulabilirler. Buna örnek olarak, bir öğretmenin \"Son ödevi tamamlayan öğrenciler kimler?\" diye sorması verilebilir. Bu durumda `get_completed(student_name: string, assignment: int, current_status: string)` adında bir fonksiyon çağrılabilir.\n",
    "\n",
    "**Yapılandırılmış Veri Oluşturmak**  \n",
    "Kullanıcılar, bir metin bloğu veya CSV dosyasını kullanarak LLM’den önemli bilgileri çıkarmasını isteyebilir. Örneğin, bir öğrenci barış anlaşmalarıyla ilgili bir Wikipedia makalesini yapay zeka ile hazırlanmış bilgi kartlarına dönüştürebilir. Bu, `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)` adlı bir fonksiyon kullanılarak yapılabilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. İlk Fonksiyon Çağrınızı Oluşturma\n",
    "\n",
    "Bir fonksiyon çağrısı oluşturma süreci 3 ana adımdan oluşur:\n",
    "1. Fonksiyonlarınızın bir listesini ve bir kullanıcı mesajını kullanarak Chat Completions API'ını çağırmak\n",
    "2. Modelin yanıtını okuyup bir işlem yapmak, yani bir fonksiyon veya API çağrısı gerçekleştirmek\n",
    "3. Fonksiyonunuzdan gelen yanıtla birlikte Chat Completions API'ına tekrar çağrı yapmak ve bu bilgiyi kullanarak kullanıcıya bir yanıt oluşturmak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bir Fonksiyon Çağrısının Akışı](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.tr.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bir fonksiyon çağrısının öğeleri\n",
    "\n",
    "#### Kullanıcı Girdisi\n",
    "\n",
    "İlk adım, bir kullanıcı mesajı oluşturmaktır. Bu, bir metin girişinden alınan değeri dinamik olarak atayarak yapılabilir veya burada bir değer atayabilirsiniz. Eğer Chat Completions API ile ilk kez çalışıyorsanız, mesajın `role` (rol) ve `content` (içerik) kısımlarını tanımlamamız gerekir.\n",
    "\n",
    "`role` değeri `system` (kuralları belirleyen), `assistant` (model) veya `user` (son kullanıcı) olabilir. Fonksiyon çağrısı için bunu `user` olarak ve örnek bir soru ile atayacağız.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonksiyon oluşturma.\n",
    "\n",
    "Şimdi bir fonksiyon ve bu fonksiyonun parametrelerini tanımlayacağız. Burada sadece `search_courses` adlı bir fonksiyon kullanacağız ama birden fazla fonksiyon da oluşturabilirsiniz.\n",
    "\n",
    "**Önemli** : Fonksiyonlar, sistem mesajına LLM'ye dahil edilir ve kullanılabilir token miktarınıza dahil edilir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tanımlar**\n",
    "\n",
    "`name` - Çağrılmasını istediğimiz fonksiyonun adı.\n",
    "\n",
    "`description` - Fonksiyonun nasıl çalıştığının açıklaması. Burada açık ve net olmak önemli.\n",
    "\n",
    "`parameters` - Modelin yanıtında üretmesini istediğiniz değerlerin ve formatın listesi.\n",
    "\n",
    "`type` - Özelliklerin saklanacağı veri tipi.\n",
    "\n",
    "`properties` - Modelin yanıtı için kullanacağı belirli değerlerin listesi.\n",
    "\n",
    "`name` - Modelin biçimlendirilmiş yanıtında kullanacağı özelliğin adı.\n",
    "\n",
    "`type` - Bu özelliğin veri tipi.\n",
    "\n",
    "`description` - Belirli özelliğin açıklaması.\n",
    "\n",
    "**Opsiyonel**\n",
    "\n",
    "`required` - Fonksiyon çağrısının tamamlanması için gerekli olan özellik.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonksiyon çağrısı yapmak\n",
    "Bir fonksiyon tanımladıktan sonra, şimdi onu Chat Completion API çağrısına dahil etmemiz gerekiyor. Bunu, isteğe `functions` ekleyerek yapıyoruz. Bu durumda `functions=functions` olarak ekleniyor.\n",
    "\n",
    "Ayrıca `function_call` seçeneğini `auto` olarak ayarlama imkanı da var. Bu, fonksiyonun hangisinin çağrılacağına bizim karar vermemiz yerine, LLM’in kullanıcı mesajına göre uygun fonksiyonu seçmesine izin vereceğimiz anlamına gelir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi yanıtı inceleyelim ve nasıl biçimlendirildiğine bakalım:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Burada fonksiyonun adının çağrıldığını ve kullanıcı mesajından, LLM'nin fonksiyonun argümanlarına uygun verileri bulabildiğini görebilirsiniz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bir Uygulamaya Fonksiyon Çağrılarını Entegre Etmek\n",
    "\n",
    "LLM'den gelen biçimlendirilmiş yanıtı test ettikten sonra, artık bunu bir uygulamaya entegre edebiliriz.\n",
    "\n",
    "### Akışı Yönetmek\n",
    "\n",
    "Bunu uygulamamıza entegre etmek için şu adımları izleyelim:\n",
    "\n",
    "Öncelikle, Open AI servislerine çağrı yapalım ve mesajı `response_message` adlı bir değişkende saklayalım.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi, Microsoft Learn API'sini çağırarak bir kurs listesi alacak olan fonksiyonu tanımlayacağız:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En iyi uygulama olarak, modelin bir fonksiyon çağırmak isteyip istemediğine bakacağız. Ardından, mevcut fonksiyonlardan birini oluşturup çağrılan fonksiyonla eşleştireceğiz.\n",
    "Sonrasında, fonksiyonun argümanlarını alıp bunları LLM'den gelen argümanlarla eşleştireceğiz.\n",
    "\n",
    "Son olarak, fonksiyon çağrısı mesajını ve `search_courses` mesajından dönen değerleri ekleyeceğiz. Bu, LLM'nin kullanıcıya doğal bir dille yanıt vermesi için ihtiyaç duyduğu tüm bilgileri sağlar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kod Yarışması\n",
    "\n",
    "Harika iş çıkardınız! Azure Open AI Fonksiyon Çağrısı hakkında öğrenmeye devam etmek için şunları yapabilirsiniz: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - Öğrencilerin daha fazla kurs bulmasına yardımcı olabilecek fonksiyonun daha fazla parametresini ekleyin. Kullanılabilir API parametrelerini burada bulabilirsiniz: \n",
    " - Öğrenciden ana dili gibi daha fazla bilgi alan başka bir fonksiyon çağrısı oluşturun \n",
    " - Fonksiyon çağrısı ve/veya API çağrısı uygun bir kurs döndürmediğinde hata yönetimi ekleyin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Feragatname**:\nBu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerde hata veya yanlışlıklar bulunabileceğini lütfen unutmayın. Belgenin orijinal diliyle hazırlanmış hali esas alınmalıdır. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından doğabilecek yanlış anlama veya yanlış yorumlamalardan sorumlu değiliz.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-08-25T20:13:59+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "tr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}