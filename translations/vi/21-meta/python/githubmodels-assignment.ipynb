{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xây Dựng Với Các Mô Hình Gia Đình Meta\n",
    "\n",
    "## Giới Thiệu\n",
    "\n",
    "Bài học này sẽ đề cập đến:\n",
    "\n",
    "- Khám phá hai mô hình chính của gia đình Meta - Llama 3.1 và Llama 3.2\n",
    "- Hiểu các trường hợp sử dụng và tình huống phù hợp cho từng mô hình\n",
    "- Mẫu mã nguồn minh họa các tính năng nổi bật của từng mô hình\n",
    "\n",
    "## Gia Đình Các Mô Hình Meta\n",
    "\n",
    "Trong bài học này, chúng ta sẽ tìm hiểu 2 mô hình thuộc gia đình Meta hay còn gọi là \"Llama Herd\" - Llama 3.1 và Llama 3.2\n",
    "\n",
    "Những mô hình này có nhiều biến thể khác nhau và có sẵn trên chợ mô hình Github. Dưới đây là thông tin chi tiết hơn về cách sử dụng Github Models để [tạo mẫu với các mô hình AI](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Các biến thể mô hình:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Lưu ý: Llama 3 cũng có trên Github Models nhưng sẽ không được đề cập trong bài học này*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Với 405 tỷ tham số, Llama 3.1 thuộc nhóm LLM mã nguồn mở.\n",
    "\n",
    "Phiên bản này là bản nâng cấp so với Llama 3 trước đó với các điểm nổi bật:\n",
    "\n",
    "- Cửa sổ ngữ cảnh lớn hơn - 128k tokens so với 8k tokens\n",
    "- Số lượng token đầu ra tối đa lớn hơn - 4096 so với 2048\n",
    "- Hỗ trợ đa ngôn ngữ tốt hơn - nhờ tăng số lượng token huấn luyện\n",
    "\n",
    "Những cải tiến này giúp Llama 3.1 xử lý các trường hợp sử dụng phức tạp hơn khi xây dựng ứng dụng GenAI, bao gồm:\n",
    "- Gọi hàm gốc - khả năng gọi các công cụ và hàm bên ngoài quy trình LLM\n",
    "- Hiệu suất RAG tốt hơn - nhờ cửa sổ ngữ cảnh lớn hơn\n",
    "- Tạo dữ liệu tổng hợp - khả năng tạo dữ liệu hiệu quả cho các tác vụ như tinh chỉnh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gọi Hàm Gốc\n",
    "\n",
    "Llama 3.1 đã được tinh chỉnh để hiệu quả hơn trong việc gọi hàm hoặc sử dụng công cụ. Nó cũng có hai công cụ tích hợp sẵn mà mô hình có thể nhận biết khi nào cần sử dụng dựa trên yêu cầu của người dùng. Hai công cụ này là:\n",
    "\n",
    "- **Brave Search** - Có thể dùng để lấy thông tin mới nhất như thời tiết bằng cách tìm kiếm trên web\n",
    "- **Wolfram Alpha** - Có thể dùng cho các phép tính toán học phức tạp hơn nên bạn không cần tự viết hàm\n",
    "\n",
    "Bạn cũng có thể tạo các công cụ tùy chỉnh để LLM có thể gọi.\n",
    "\n",
    "Trong ví dụ mã dưới đây:\n",
    "\n",
    "- Chúng ta định nghĩa các công cụ có sẵn (brave_search, wolfram_alpha) trong prompt hệ thống.\n",
    "- Gửi một prompt từ người dùng hỏi về thời tiết ở một thành phố cụ thể.\n",
    "- LLM sẽ phản hồi bằng một lệnh gọi công cụ tới Brave Search, ví dụ như sau `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Lưu ý: Ví dụ này chỉ thực hiện lệnh gọi công cụ, nếu bạn muốn lấy kết quả, bạn cần tạo một tài khoản miễn phí trên trang API của Brave và tự định nghĩa hàm*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Mặc dù là một mô hình ngôn ngữ lớn, Llama 3.1 vẫn còn hạn chế về khả năng đa phương thức. Tức là, chưa thể sử dụng các loại đầu vào khác nhau như hình ảnh để làm prompt và trả về phản hồi. Đây chính là một trong những điểm nổi bật của Llama 3.2. Những tính năng này còn bao gồm:\n",
    "\n",
    "- Đa phương thức - có thể xử lý cả prompt văn bản lẫn hình ảnh\n",
    "- Các phiên bản kích thước nhỏ đến vừa (11B và 90B) - giúp linh hoạt trong việc triển khai,\n",
    "- Các phiên bản chỉ văn bản (1B và 3B) - cho phép triển khai trên thiết bị biên / di động và mang lại độ trễ thấp\n",
    "\n",
    "Việc hỗ trợ đa phương thức là một bước tiến lớn trong thế giới các mô hình mã nguồn mở. Ví dụ mã dưới đây nhận cả hình ảnh và prompt văn bản để phân tích hình ảnh bằng Llama 3.2 90B.\n",
    "\n",
    "### Hỗ trợ đa phương thức với Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Việc học không dừng lại ở đây, hãy tiếp tục hành trình\n",
    "\n",
    "Sau khi hoàn thành bài học này, hãy khám phá [Bộ sưu tập học về AI sinh sinh](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) của chúng tôi để tiếp tục nâng cao kiến thức về AI sinh sinh!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Tuyên bố miễn trừ trách nhiệm**:  \nTài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn tham khảo chính thức. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm đối với bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:46:02+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "vi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}