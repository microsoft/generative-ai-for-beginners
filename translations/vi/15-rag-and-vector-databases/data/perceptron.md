<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "59021c5f419d3feda19075910a74280a",
  "translation_date": "2025-07-09T16:59:30+00:00",
  "source_file": "15-rag-and-vector-databases/data/perceptron.md",
  "language_code": "vi"
}
-->
# Giá»›i thiá»‡u vá» Máº¡ng NÆ¡-ron: Perceptron

Má»™t trong nhá»¯ng ná»— lá»±c Ä‘áº§u tiÃªn Ä‘á»ƒ triá»ƒn khai má»™t thá»© gÃ¬ Ä‘Ã³ tÆ°Æ¡ng tá»± nhÆ° máº¡ng nÆ¡-ron hiá»‡n Ä‘áº¡i Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi Frank Rosenblatt tá»« PhÃ²ng thÃ­ nghiá»‡m HÃ ng khÃ´ng Cornell vÃ o nÄƒm 1957. ÄÃ³ lÃ  má»™t thiáº¿t bá»‹ pháº§n cá»©ng gá»i lÃ  "Mark-1", Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ nháº­n dáº¡ng cÃ¡c hÃ¬nh há»c sÆ¡ khai, nhÆ° tam giÃ¡c, hÃ¬nh vuÃ´ng vÃ  hÃ¬nh trÃ²n.

|      |      |
|--------------|-----------|
|<img src='images/Rosenblatt-wikipedia.jpg' alt='Frank Rosenblatt'/> | <img src='images/Mark_I_perceptron_wikipedia.jpg' alt='The Mark 1 Perceptron' />|

> HÃ¬nh áº£nh tá»« Wikipedia

Má»™t hÃ¬nh áº£nh Ä‘áº§u vÃ o Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng máº£ng 20x20 táº¿ bÃ o quang Ä‘iá»‡n, vÃ¬ váº­y máº¡ng nÆ¡-ron cÃ³ 400 Ä‘áº§u vÃ o vÃ  má»™t Ä‘áº§u ra nhá»‹ phÃ¢n. Máº¡ng Ä‘Æ¡n giáº£n chá»‰ chá»©a má»™t neuron, cÃ²n gá»i lÃ  **Ä‘Æ¡n vá»‹ logic ngÆ°á»¡ng**. Trá»ng sá»‘ cá»§a máº¡ng nÆ¡-ron hoáº¡t Ä‘á»™ng giá»‘ng nhÆ° cÃ¡c biáº¿n trá»Ÿ cáº§n Ä‘Æ°á»£c Ä‘iá»u chá»‰nh thá»§ cÃ´ng trong giai Ä‘oáº¡n huáº¥n luyá»‡n.

> âœ… Biáº¿n trá»Ÿ lÃ  má»™t thiáº¿t bá»‹ cho phÃ©p ngÆ°á»i dÃ¹ng Ä‘iá»u chá»‰nh Ä‘iá»‡n trá»Ÿ cá»§a máº¡ch.

> The New York Times Ä‘Ã£ viáº¿t vá» perceptron vÃ o thá»i Ä‘iá»ƒm Ä‘Ã³: *phÃ´i thai cá»§a má»™t mÃ¡y tÃ­nh Ä‘iá»‡n tá»­ mÃ  [Háº£i quÃ¢n] ká»³ vá»ng cÃ³ thá»ƒ Ä‘i, nÃ³i, nhÃ¬n, viáº¿t, tá»± nhÃ¢n báº£n vÃ  nháº­n thá»©c Ä‘Æ°á»£c sá»± tá»“n táº¡i cá»§a nÃ³.*

## MÃ´ hÃ¬nh Perceptron

Giáº£ sá»­ chÃºng ta cÃ³ N Ä‘áº·c trÆ°ng trong mÃ´ hÃ¬nh, trong trÆ°á»ng há»£p nÃ y vector Ä‘áº§u vÃ o sáº½ lÃ  má»™t vector kÃ­ch thÆ°á»›c N. Perceptron lÃ  má»™t mÃ´ hÃ¬nh **phÃ¢n loáº¡i nhá»‹ phÃ¢n**, tá»©c lÃ  nÃ³ cÃ³ thá»ƒ phÃ¢n biá»‡t giá»¯a hai lá»›p dá»¯ liá»‡u Ä‘áº§u vÃ o. ChÃºng ta sáº½ giáº£ Ä‘á»‹nh ráº±ng vá»›i má»—i vector Ä‘áº§u vÃ o x, Ä‘áº§u ra cá»§a perceptron sáº½ lÃ  +1 hoáº·c -1, tÃ¹y thuá»™c vÃ o lá»›p. Äáº§u ra Ä‘Æ°á»£c tÃ­nh theo cÃ´ng thá»©c:

y(x) = f(w<sup>T</sup>x)

trong Ä‘Ã³ f lÃ  hÃ m kÃ­ch hoáº¡t bÆ°á»›c

## Huáº¥n luyá»‡n Perceptron

Äá»ƒ huáº¥n luyá»‡n perceptron, chÃºng ta cáº§n tÃ¬m vector trá»ng sá»‘ w sao cho phÃ¢n loáº¡i Ä‘Ãºng pháº§n lá»›n cÃ¡c giÃ¡ trá»‹, tá»©c lÃ  táº¡o ra **lá»—i** nhá» nháº¥t. Lá»—i nÃ y Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi **tiÃªu chÃ­ perceptron** nhÆ° sau:

E(w) = -âˆ‘w<sup>T</sup>x<sub>i</sub>t<sub>i</sub>

trong Ä‘Ã³:

* tá»•ng Ä‘Æ°á»£c láº¥y trÃªn cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u huáº¥n luyá»‡n i mÃ  bá»‹ phÃ¢n loáº¡i sai
* x<sub>i</sub> lÃ  dá»¯ liá»‡u Ä‘áº§u vÃ o, vÃ  t<sub>i</sub> lÃ  -1 hoáº·c +1 tÆ°Æ¡ng á»©ng vá»›i cÃ¡c vÃ­ dá»¥ Ã¢m vÃ  dÆ°Æ¡ng.

TiÃªu chÃ­ nÃ y Ä‘Æ°á»£c xem nhÆ° má»™t hÃ m cá»§a trá»ng sá»‘ w, vÃ  chÃºng ta cáº§n tá»‘i thiá»ƒu hÃ³a nÃ³. ThÆ°á»ng thÃ¬ má»™t phÆ°Æ¡ng phÃ¡p gá»i lÃ  **gradient descent** Ä‘Æ°á»£c sá»­ dá»¥ng, trong Ä‘Ã³ ta báº¯t Ä‘áº§u vá»›i má»™t vector trá»ng sá»‘ khá»Ÿi táº¡o w<sup>(0)</sup>, rá»“i táº¡i má»—i bÆ°á»›c cáº­p nháº­t trá»ng sá»‘ theo cÃ´ng thá»©c:

w<sup>(t+1)</sup> = w<sup>(t)</sup> - Î·âˆ‡E(w)

á» Ä‘Ã¢y Î· lÃ  cÃ¡i gá»i lÃ  **tá»‘c Ä‘á»™ há»c**, vÃ  âˆ‡E(w) lÃ  **Ä‘áº¡o hÃ m** cá»§a E. Sau khi tÃ­nh Ä‘áº¡o hÃ m, ta cÃ³:

w<sup>(t+1)</sup> = w<sup>(t)</sup> + âˆ‘Î·x<sub>i</sub>t<sub>i</sub>

Thuáº­t toÃ¡n trong Python trÃ´ng nhÆ° sau:

```python
def train(positive_examples, negative_examples, num_iterations = 100, eta = 1):

    weights = [0,0,0] # Initialize weights (almost randomly :)
        
    for i in range(num_iterations):
        pos = random.choice(positive_examples)
        neg = random.choice(negative_examples)

        z = np.dot(pos, weights) # compute perceptron output
        if z < 0: # positive example classified as negative
            weights = weights + eta*weights.shape

        z  = np.dot(neg, weights)
        if z >= 0: # negative example classified as positive
            weights = weights - eta*weights.shape

    return weights
```

## Káº¿t luáº­n

Trong bÃ i há»c nÃ y, báº¡n Ä‘Ã£ tÃ¬m hiá»ƒu vá» perceptron, má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i nhá»‹ phÃ¢n, vÃ  cÃ¡ch huáº¥n luyá»‡n nÃ³ báº±ng cÃ¡ch sá»­ dá»¥ng vector trá»ng sá»‘.

## ğŸš€ Thá»­ thÃ¡ch

Náº¿u báº¡n muá»‘n thá»­ xÃ¢y dá»±ng perceptron cá»§a riÃªng mÃ¬nh, hÃ£y thá»­ lÃ m bÃ i lab nÃ y trÃªn Microsoft Learn sá»­ dá»¥ng Azure ML designer


## Ã”n táº­p & Tá»± há»c

Äá»ƒ xem cÃ¡ch chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng perceptron Ä‘á»ƒ giáº£i quyáº¿t má»™t bÃ i toÃ¡n Ä‘Æ¡n giáº£n cÅ©ng nhÆ° cÃ¡c bÃ i toÃ¡n thá»±c táº¿, vÃ  Ä‘á»ƒ tiáº¿p tá»¥c há»c táº­p - hÃ£y truy cáº­p vÃ o sá»• tay Perceptron.

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t bÃ i viáº¿t thÃº vá»‹ vá» perceptron.

## BÃ i táº­p

Trong bÃ i há»c nÃ y, chÃºng ta Ä‘Ã£ triá»ƒn khai perceptron cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n, vÃ  Ä‘Ã£ sá»­ dá»¥ng nÃ³ Ä‘á»ƒ phÃ¢n loáº¡i giá»¯a hai chá»¯ sá»‘ viáº¿t tay. Trong bÃ i lab nÃ y, báº¡n Ä‘Æ°á»£c yÃªu cáº§u giáº£i quyáº¿t hoÃ n toÃ n bÃ i toÃ¡n phÃ¢n loáº¡i chá»¯ sá»‘, tá»©c lÃ  xÃ¡c Ä‘á»‹nh chá»¯ sá»‘ nÃ o cÃ³ kháº£ nÄƒng tÆ°Æ¡ng á»©ng nháº¥t vá»›i má»™t hÃ¬nh áº£nh cho trÆ°á»›c.

* HÆ°á»›ng dáº«n
* Sá»• tay

**TuyÃªn bá»‘ tá»« chá»‘i trÃ¡ch nhiá»‡m**:  
TÃ i liá»‡u nÃ y Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch báº±ng dá»‹ch vá»¥ dá»‹ch thuáº­t AI [Co-op Translator](https://github.com/Azure/co-op-translator). Máº·c dÃ¹ chÃºng tÃ´i cá»‘ gáº¯ng Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c, xin lÆ°u Ã½ ráº±ng cÃ¡c báº£n dá»‹ch tá»± Ä‘á»™ng cÃ³ thá»ƒ chá»©a lá»—i hoáº·c khÃ´ng chÃ­nh xÃ¡c. TÃ i liá»‡u gá»‘c báº±ng ngÃ´n ngá»¯ gá»‘c cá»§a nÃ³ nÃªn Ä‘Æ°á»£c coi lÃ  nguá»“n chÃ­nh xÃ¡c vÃ  Ä‘Ã¡ng tin cáº­y. Äá»‘i vá»›i cÃ¡c thÃ´ng tin quan trá»ng, nÃªn sá»­ dá»¥ng dá»‹ch vá»¥ dá»‹ch thuáº­t chuyÃªn nghiá»‡p do con ngÆ°á»i thá»±c hiá»‡n. ChÃºng tÃ´i khÃ´ng chá»‹u trÃ¡ch nhiá»‡m vá» báº¥t ká»³ sá»± hiá»ƒu láº§m hoáº·c giáº£i thÃ­ch sai nÃ o phÃ¡t sinh tá»« viá»‡c sá»­ dá»¥ng báº£n dá»‹ch nÃ y.