<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:48:56+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "vi"
}
-->
# Giá»›i thiá»‡u vá» Máº¡ng NÆ¡-ron. Perceptron Ä‘a lá»›p

Trong pháº§n trÆ°á»›c, báº¡n Ä‘Ã£ tÃ¬m hiá»ƒu vá» mÃ´ hÃ¬nh máº¡ng nÆ¡-ron Ä‘Æ¡n giáº£n nháº¥t - perceptron má»™t lá»›p, má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i hai lá»›p tuyáº¿n tÃ­nh.

Trong pháº§n nÃ y, chÃºng ta sáº½ má»Ÿ rá»™ng mÃ´ hÃ¬nh nÃ y thÃ nh má»™t khuÃ´n khá»• linh hoáº¡t hÆ¡n, cho phÃ©p chÃºng ta:

* thá»±c hiá»‡n **phÃ¢n loáº¡i Ä‘a lá»›p** ngoÃ i phÃ¢n loáº¡i hai lá»›p
* giáº£i quyáº¿t **bÃ i toÃ¡n há»“i quy** ngoÃ i phÃ¢n loáº¡i
* tÃ¡ch cÃ¡c lá»›p khÃ´ng thá»ƒ phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh

ChÃºng ta cÅ©ng sáº½ phÃ¡t triá»ƒn má»™t khuÃ´n khá»• mÃ´-Ä‘un riÃªng báº±ng Python, giÃºp xÃ¢y dá»±ng cÃ¡c kiáº¿n trÃºc máº¡ng nÆ¡-ron khÃ¡c nhau.

## HÃ¬nh thá»©c hÃ³a bÃ i toÃ¡n MÃ¡y há»c

HÃ£y báº¯t Ä‘áº§u vá»›i viá»‡c hÃ¬nh thá»©c hÃ³a bÃ i toÃ¡n MÃ¡y há»c. Giáº£ sá»­ chÃºng ta cÃ³ táº­p dá»¯ liá»‡u huáº¥n luyá»‡n **X** vá»›i nhÃ£n **Y**, vÃ  cáº§n xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh *f* Ä‘á»ƒ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nháº¥t cÃ³ thá»ƒ. Cháº¥t lÆ°á»£ng dá»± Ä‘oÃ¡n Ä‘Æ°á»£c Ä‘o báº±ng **hÃ m máº¥t mÃ¡t** â„’. CÃ¡c hÃ m máº¥t mÃ¡t thÆ°á»ng dÃ¹ng bao gá»“m:

* Vá»›i bÃ i toÃ¡n há»“i quy, khi cáº§n dá»± Ä‘oÃ¡n má»™t sá»‘, ta cÃ³ thá»ƒ dÃ¹ng **lá»—i tuyá»‡t Ä‘á»‘i** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, hoáº·c **lá»—i bÃ¬nh phÆ°Æ¡ng** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Vá»›i bÃ i toÃ¡n phÃ¢n loáº¡i, ta dÃ¹ng **0-1 loss** (vá» cÆ¡ báº£n tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i **Ä‘á»™ chÃ­nh xÃ¡c** cá»§a mÃ´ hÃ¬nh), hoáº·c **logistic loss**.

Vá»›i perceptron má»™t lá»›p, hÃ m *f* Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  hÃ m tuyáº¿n tÃ­nh *f(x)=wx+b* (á»Ÿ Ä‘Ã¢y *w* lÃ  ma tráº­n trá»ng sá»‘, *x* lÃ  vector Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o, vÃ  *b* lÃ  vector bias). Vá»›i cÃ¡c kiáº¿n trÃºc máº¡ng nÆ¡-ron khÃ¡c, hÃ m nÃ y cÃ³ thá»ƒ phá»©c táº¡p hÆ¡n.

> Trong trÆ°á»ng há»£p phÃ¢n loáº¡i, thÆ°á»ng ta muá»‘n Ä‘áº§u ra cá»§a máº¡ng lÃ  xÃ¡c suáº¥t cá»§a cÃ¡c lá»›p tÆ°Æ¡ng á»©ng. Äá»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c sá»‘ báº¥t ká»³ thÃ nh xÃ¡c suáº¥t (vÃ­ dá»¥ Ä‘á»ƒ chuáº©n hÃ³a Ä‘áº§u ra), ta thÆ°á»ng dÃ¹ng hÃ m **softmax** Ïƒ, vÃ  hÃ m *f* trá»Ÿ thÃ nh *f(x)=Ïƒ(wx+b)*

Trong Ä‘á»‹nh nghÄ©a *f* á»Ÿ trÃªn, *w* vÃ  *b* Ä‘Æ°á»£c gá»i lÃ  **tham sá»‘** Î¸=âŸ¨*w,b*âŸ©. Vá»›i táº­p dá»¯ liá»‡u âŸ¨**X**,**Y**âŸ©, ta cÃ³ thá»ƒ tÃ­nh tá»•ng lá»—i trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u nhÆ° má»™t hÃ m cá»§a tham sá»‘ Î¸.

> âœ… **Má»¥c tiÃªu cá»§a viá»‡c huáº¥n luyá»‡n máº¡ng nÆ¡-ron lÃ  giáº£m thiá»ƒu lá»—i báº±ng cÃ¡ch Ä‘iá»u chá»‰nh tham sá»‘ Î¸**

## Tá»‘i Æ°u hÃ³a báº±ng Gradient Descent

CÃ³ má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ m ráº¥t phá»• biáº¿n gá»i lÃ  **gradient descent**. Ã tÆ°á»Ÿng lÃ  ta cÃ³ thá»ƒ tÃ­nh Ä‘áº¡o hÃ m (trong trÆ°á»ng há»£p Ä‘a chiá»u gá»i lÃ  **gradient**) cá»§a hÃ m máº¥t mÃ¡t theo tham sá»‘, vÃ  Ä‘iá»u chá»‰nh tham sá»‘ sao cho lá»—i giáº£m xuá»‘ng. CÃ¡ch lÃ m Ä‘Æ°á»£c hÃ¬nh thá»©c hÃ³a nhÆ° sau:

* Khá»Ÿi táº¡o tham sá»‘ vá»›i giÃ¡ trá»‹ ngáº«u nhiÃªn w<sup>(0)</sup>, b<sup>(0)</sup>
* Láº·p láº¡i nhiá»u láº§n bÆ°á»›c sau:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, cÃ¡c bÆ°á»›c tá»‘i Æ°u Ä‘Æ°á»£c tÃ­nh dá»±a trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u (nhá»› ráº±ng lá»—i Ä‘Æ°á»£c tÃ­nh báº±ng tá»•ng qua táº¥t cáº£ máº«u huáº¥n luyá»‡n). Tuy nhiÃªn, trong thá»±c táº¿ ta láº¥y cÃ¡c pháº§n nhá» cá»§a táº­p dá»¯ liá»‡u gá»i lÃ  **minibatches**, vÃ  tÃ­nh gradient dá»±a trÃªn má»™t pháº§n dá»¯ liá»‡u. VÃ¬ pháº§n dá»¯ liá»‡u nÃ y Ä‘Æ°á»£c láº¥y ngáº«u nhiÃªn má»—i láº§n, phÆ°Æ¡ng phÃ¡p nÃ y gá»i lÃ  **stochastic gradient descent** (SGD).

## Perceptron Ä‘a lá»›p vÃ  Thuáº­t toÃ¡n lan truyá»n ngÆ°á»£c

Máº¡ng má»™t lá»›p, nhÆ° ta Ä‘Ã£ tháº¥y á»Ÿ trÃªn, chá»‰ cÃ³ thá»ƒ phÃ¢n loáº¡i cÃ¡c lá»›p tÃ¡ch biá»‡t tuyáº¿n tÃ­nh. Äá»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh phong phÃº hÆ¡n, ta cÃ³ thá»ƒ káº¿t há»£p nhiá»u lá»›p máº¡ng. Vá» máº·t toÃ¡n há»c, Ä‘iá»u nÃ y cÃ³ nghÄ©a hÃ m *f* sáº½ phá»©c táº¡p hÆ¡n vÃ  Ä‘Æ°á»£c tÃ­nh qua nhiá»u bÆ°á»›c:

* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

á» Ä‘Ã¢y, Î± lÃ  **hÃ m kÃ­ch hoáº¡t phi tuyáº¿n**, Ïƒ lÃ  hÃ m softmax, vÃ  tham sá»‘ Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Thuáº­t toÃ¡n gradient descent váº«n giá»¯ nguyÃªn, nhÆ°ng viá»‡c tÃ­nh gradient trá»Ÿ nÃªn phá»©c táº¡p hÆ¡n. Dá»±a trÃªn quy táº¯c Ä‘áº¡o hÃ m theo chuá»—i, ta cÃ³ thá»ƒ tÃ­nh cÃ¡c Ä‘áº¡o hÃ m nhÆ° sau:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Quy táº¯c Ä‘áº¡o hÃ m theo chuá»—i Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t theo tham sá»‘.

LÆ°u Ã½ pháº§n bÃªn trÃ¡i cá»§a táº¥t cáº£ cÃ¡c biá»ƒu thá»©c trÃªn lÃ  giá»‘ng nhau, do Ä‘Ã³ ta cÃ³ thá»ƒ tÃ­nh Ä‘áº¡o hÃ m hiá»‡u quáº£ báº¯t Ä‘áº§u tá»« hÃ m máº¥t mÃ¡t vÃ  Ä‘i "ngÆ°á»£c láº¡i" qua Ä‘á»“ thá»‹ tÃ­nh toÃ¡n. PhÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n perceptron Ä‘a lá»›p nÃ y gá»i lÃ  **backpropagation**, hay 'backprop'.

> TODO: trÃ­ch dáº«n hÃ¬nh áº£nh

> âœ… ChÃºng ta sáº½ tÃ¬m hiá»ƒu chi tiáº¿t hÆ¡n vá» backprop trong vÃ­ dá»¥ notebook kÃ¨m theo.

## Káº¿t luáº­n

Trong bÃ i há»c nÃ y, chÃºng ta Ä‘Ã£ xÃ¢y dá»±ng thÆ° viá»‡n máº¡ng nÆ¡-ron riÃªng, vÃ  sá»­ dá»¥ng nÃ³ cho bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘Æ¡n giáº£n hai chiá»u.

## ğŸš€ Thá»­ thÃ¡ch

Trong notebook kÃ¨m theo, báº¡n sáº½ tá»± triá»ƒn khai khuÃ´n khá»• xÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n perceptron Ä‘a lá»›p. Báº¡n sáº½ cÃ³ cÆ¡ há»™i quan sÃ¡t chi tiáº¿t cÃ¡ch cÃ¡c máº¡ng nÆ¡-ron hiá»‡n Ä‘áº¡i hoáº¡t Ä‘á»™ng.

HÃ£y chuyá»ƒn sang notebook OwnFramework vÃ  lÃ m theo hÆ°á»›ng dáº«n.

## Ã”n táº­p & Tá»± há»c

Backpropagation lÃ  thuáº­t toÃ¡n phá»• biáº¿n trong AI vÃ  ML, ráº¥t Ä‘Ã¡ng Ä‘á»ƒ nghiÃªn cá»©u ká»¹ hÆ¡n.

## BÃ i táº­p

Trong bÃ i lab nÃ y, báº¡n Ä‘Æ°á»£c yÃªu cáº§u sá»­ dá»¥ng khuÃ´n khá»• Ä‘Ã£ xÃ¢y dá»±ng trong bÃ i há»c Ä‘á»ƒ giáº£i bÃ i toÃ¡n phÃ¢n loáº¡i chá»¯ sá»‘ viáº¿t tay MNIST.

* HÆ°á»›ng dáº«n
* Notebook

**TuyÃªn bá»‘ tá»« chá»‘i trÃ¡ch nhiá»‡m**:  
TÃ i liá»‡u nÃ y Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch báº±ng dá»‹ch vá»¥ dá»‹ch thuáº­t AI [Co-op Translator](https://github.com/Azure/co-op-translator). Máº·c dÃ¹ chÃºng tÃ´i cá»‘ gáº¯ng Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c, xin lÆ°u Ã½ ráº±ng cÃ¡c báº£n dá»‹ch tá»± Ä‘á»™ng cÃ³ thá»ƒ chá»©a lá»—i hoáº·c khÃ´ng chÃ­nh xÃ¡c. TÃ i liá»‡u gá»‘c báº±ng ngÃ´n ngá»¯ gá»‘c cá»§a nÃ³ nÃªn Ä‘Æ°á»£c coi lÃ  nguá»“n chÃ­nh xÃ¡c vÃ  Ä‘Ã¡ng tin cáº­y. Äá»‘i vá»›i cÃ¡c thÃ´ng tin quan trá»ng, nÃªn sá»­ dá»¥ng dá»‹ch vá»¥ dá»‹ch thuáº­t chuyÃªn nghiá»‡p do con ngÆ°á»i thá»±c hiá»‡n. ChÃºng tÃ´i khÃ´ng chá»‹u trÃ¡ch nhiá»‡m vá» báº¥t ká»³ sá»± hiá»ƒu láº§m hoáº·c giáº£i thÃ­ch sai nÃ o phÃ¡t sinh tá»« viá»‡c sá»­ dá»¥ng báº£n dá»‹ch nÃ y.