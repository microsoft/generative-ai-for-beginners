<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-05-20T02:23:24+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "vi"
}
-->
# Giá»›i thiá»‡u vá» Máº¡ng NÆ¡-ron. Perceptron Äa Táº§ng

Trong pháº§n trÆ°á»›c, báº¡n Ä‘Ã£ tÃ¬m hiá»ƒu vá» mÃ´ hÃ¬nh máº¡ng nÆ¡-ron Ä‘Æ¡n giáº£n nháº¥t - perceptron má»™t táº§ng, má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i tuyáº¿n tÃ­nh hai lá»›p.

Trong pháº§n nÃ y, chÃºng ta sáº½ má»Ÿ rá»™ng mÃ´ hÃ¬nh nÃ y thÃ nh má»™t khung linh hoáº¡t hÆ¡n, cho phÃ©p chÃºng ta:

* thá»±c hiá»‡n **phÃ¢n loáº¡i Ä‘a lá»›p** ngoÃ i phÃ¢n loáº¡i hai lá»›p
* giáº£i quyáº¿t **cÃ¡c váº¥n Ä‘á» há»“i quy** ngoÃ i phÃ¢n loáº¡i
* phÃ¢n tÃ¡ch cÃ¡c lá»›p khÃ´ng thá»ƒ phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh

ChÃºng ta cÅ©ng sáº½ phÃ¡t triá»ƒn khung mÃ´-Ä‘un cá»§a riÃªng mÃ¬nh trong Python Ä‘á»ƒ cho phÃ©p xÃ¢y dá»±ng cÃ¡c kiáº¿n trÃºc máº¡ng nÆ¡-ron khÃ¡c nhau.

## Äá»‹nh hÃ¬nh Há»c MÃ¡y

HÃ£y báº¯t Ä‘áº§u vá»›i viá»‡c Ä‘á»‹nh hÃ¬nh váº¥n Ä‘á» Há»c MÃ¡y. Giáº£ sá»­ chÃºng ta cÃ³ má»™t táº­p dá»¯ liá»‡u huáº¥n luyá»‡n **X** vá»›i nhÃ£n **Y**, vÃ  chÃºng ta cáº§n xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh *f* Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nháº¥t. Cháº¥t lÆ°á»£ng dá»± Ä‘oÃ¡n Ä‘Æ°á»£c Ä‘o báº±ng **hÃ m máº¥t mÃ¡t** â„’. CÃ¡c hÃ m máº¥t mÃ¡t sau Ä‘Ã¢y thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng:

* Äá»‘i vá»›i váº¥n Ä‘á» há»“i quy, khi cáº§n dá»± Ä‘oÃ¡n má»™t sá»‘, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng **lá»—i tuyá»‡t Ä‘á»‘i** âˆ‘<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, hoáº·c **lá»—i bÃ¬nh phÆ°Æ¡ng** âˆ‘<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Äá»‘i vá»›i phÃ¢n loáº¡i, chÃºng ta sá»­ dá»¥ng **máº¥t mÃ¡t 0-1** (vá» cÆ¡ báº£n lÃ  giá»‘ng nhÆ° **Ä‘á»™ chÃ­nh xÃ¡c** cá»§a mÃ´ hÃ¬nh), hoáº·c **máº¥t mÃ¡t logistic**.

Äá»‘i vá»›i perceptron má»™t táº§ng, hÃ m *f* Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  má»™t hÃ m tuyáº¿n tÃ­nh *f(x)=wx+b* (á»Ÿ Ä‘Ã¢y *w* lÃ  ma tráº­n trá»ng sá»‘, *x* lÃ  vector cá»§a cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o, vÃ  *b* lÃ  vector Ä‘á»™ lá»‡ch). Äá»‘i vá»›i cÃ¡c kiáº¿n trÃºc máº¡ng nÆ¡-ron khÃ¡c nhau, hÃ m nÃ y cÃ³ thá»ƒ cÃ³ dáº¡ng phá»©c táº¡p hÆ¡n.

> Trong trÆ°á»ng há»£p phÃ¢n loáº¡i, thÆ°á»ng mong muá»‘n cÃ³ Ä‘Æ°á»£c xÃ¡c suáº¥t cá»§a cÃ¡c lá»›p tÆ°Æ¡ng á»©ng nhÆ° Ä‘áº§u ra cá»§a máº¡ng. Äá»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c sá»‘ báº¥t ká»³ thÃ nh xÃ¡c suáº¥t (vÃ­ dá»¥ Ä‘á»ƒ chuáº©n hÃ³a Ä‘áº§u ra), chÃºng ta thÆ°á»ng sá»­ dá»¥ng hÃ m **softmax** Ïƒ, vÃ  hÃ m *f* trá»Ÿ thÃ nh *f(x)=Ïƒ(wx+b)*

Trong Ä‘á»‹nh nghÄ©a cá»§a *f* á»Ÿ trÃªn, *w* vÃ  *b* Ä‘Æ°á»£c gá»i lÃ  **tham sá»‘** Î¸=âŸ¨*w,b*âŸ©. Cho táº­p dá»¯ liá»‡u âŸ¨**X**,**Y**âŸ©, chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n lá»—i tá»•ng thá»ƒ trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u nhÆ° má»™t hÃ m cá»§a cÃ¡c tham sá»‘ Î¸.

> âœ… **Má»¥c tiÃªu cá»§a viá»‡c huáº¥n luyá»‡n máº¡ng nÆ¡-ron lÃ  giáº£m thiá»ƒu lá»—i báº±ng cÃ¡ch thay Ä‘á»•i cÃ¡c tham sá»‘ Î¸**

## Tá»‘i Æ°u hÃ³a Gradient Descent

CÃ³ má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a hÃ m Ä‘Æ°á»£c biáº¿t Ä‘áº¿n rá»™ng rÃ£i gá»i lÃ  **gradient descent**. Ã tÆ°á»Ÿng lÃ  chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n Ä‘áº¡o hÃ m (trong trÆ°á»ng há»£p Ä‘a chiá»u gá»i lÃ  **gradient**) cá»§a hÃ m máº¥t mÃ¡t Ä‘á»‘i vá»›i cÃ¡c tham sá»‘, vÃ  thay Ä‘á»•i cÃ¡c tham sá»‘ theo cÃ¡ch mÃ  lá»—i sáº½ giáº£m. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh hÃ¬nh nhÆ° sau:

* Khá»Ÿi táº¡o cÃ¡c tham sá»‘ báº±ng má»™t sá»‘ giÃ¡ trá»‹ ngáº«u nhiÃªn w<sup>(0)</sup>, b<sup>(0)</sup>
* Láº·p láº¡i bÆ°á»›c sau nhiá»u láº§n:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚w
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Î·âˆ‚â„’/âˆ‚b

Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, cÃ¡c bÆ°á»›c tá»‘i Æ°u hÃ³a Ä‘Æ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u (nhá»› ráº±ng máº¥t mÃ¡t Ä‘Æ°á»£c tÃ­nh nhÆ° tá»•ng qua táº¥t cáº£ cÃ¡c máº«u huáº¥n luyá»‡n). Tuy nhiÃªn, trong thá»±c táº¿, chÃºng ta láº¥y cÃ¡c pháº§n nhá» cá»§a táº­p dá»¯ liá»‡u gá»i lÃ  **minibatches**, vÃ  tÃ­nh toÃ¡n gradient dá»±a trÃªn má»™t pháº§n dá»¯ liá»‡u. VÃ¬ pháº§n dá»¯ liá»‡u Ä‘Æ°á»£c láº¥y ngáº«u nhiÃªn má»—i láº§n, phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c gá»i lÃ  **stochastic gradient descent** (SGD).

## Perceptron Äa Táº§ng vÃ  Backpropagation

Máº¡ng má»™t táº§ng, nhÆ° chÃºng ta Ä‘Ã£ tháº¥y á»Ÿ trÃªn, cÃ³ kháº£ nÄƒng phÃ¢n loáº¡i cÃ¡c lá»›p cÃ³ thá»ƒ phÃ¢n tÃ¡ch tuyáº¿n tÃ­nh. Äá»ƒ xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh phong phÃº hÆ¡n, chÃºng ta cÃ³ thá»ƒ káº¿t há»£p nhiá»u táº§ng cá»§a máº¡ng. Vá» máº·t toÃ¡n há»c, Ä‘iá»u nÃ y cÃ³ nghÄ©a lÃ  hÃ m *f* sáº½ cÃ³ dáº¡ng phá»©c táº¡p hÆ¡n, vÃ  sáº½ Ä‘Æ°á»£c tÃ­nh toÃ¡n trong nhiá»u bÆ°á»›c:
* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Î±(z<sub>1</sub>)+b<sub>2</sub>
* f = Ïƒ(z<sub>2</sub>)

á» Ä‘Ã¢y, Î± lÃ  má»™t **hÃ m kÃ­ch hoáº¡t phi tuyáº¿n**, Ïƒ lÃ  hÃ m softmax, vÃ  cÃ¡c tham sá»‘ Î¸=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

Thuáº­t toÃ¡n gradient descent sáº½ váº«n giá»¯ nguyÃªn, nhÆ°ng sáº½ khÃ³ khÄƒn hÆ¡n Ä‘á»ƒ tÃ­nh toÃ¡n gradient. Theo quy táº¯c chuá»—i Ä‘áº¡o hÃ m, chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n Ä‘áº¡o hÃ m nhÆ° sau:

* âˆ‚â„’/âˆ‚w<sub>2</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚w<sub>2</sub>)
* âˆ‚â„’/âˆ‚w<sub>1</sub> = (âˆ‚â„’/âˆ‚Ïƒ)(âˆ‚Ïƒ/âˆ‚z<sub>2</sub>)(âˆ‚z<sub>2</sub>/âˆ‚Î±)(âˆ‚Î±/âˆ‚z<sub>1</sub>)(âˆ‚z<sub>1</sub>/âˆ‚w<sub>1</sub>)

> âœ… Quy táº¯c chuá»—i Ä‘áº¡o hÃ m Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t Ä‘á»‘i vá»›i cÃ¡c tham sá»‘.

LÆ°u Ã½ ráº±ng pháº§n bÃªn trÃ¡i cá»§a táº¥t cáº£ cÃ¡c biá»ƒu thá»©c nÃ y lÃ  giá»‘ng nhau, vÃ  do Ä‘Ã³ chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n hiá»‡u quáº£ cÃ¡c Ä‘áº¡o hÃ m báº¯t Ä‘áº§u tá»« hÃ m máº¥t mÃ¡t vÃ  Ä‘i "ngÆ°á»£c láº¡i" qua biá»ƒu Ä‘á»“ tÃ­nh toÃ¡n. Do Ä‘Ã³, phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n perceptron Ä‘a táº§ng Ä‘Æ°á»£c gá»i lÃ  **backpropagation**, hoáº·c 'backprop'.

> TODO: trÃ­ch dáº«n hÃ¬nh áº£nh

> âœ… ChÃºng ta sáº½ nghiÃªn cá»©u backprop chi tiáº¿t hÆ¡n trong vÃ­ dá»¥ notebook cá»§a mÃ¬nh.

## Káº¿t luáº­n

Trong bÃ i há»c nÃ y, chÃºng ta Ä‘Ã£ xÃ¢y dá»±ng thÆ° viá»‡n máº¡ng nÆ¡-ron cá»§a riÃªng mÃ¬nh, vÃ  chÃºng ta Ä‘Ã£ sá»­ dá»¥ng nÃ³ cho má»™t nhiá»‡m vá»¥ phÃ¢n loáº¡i hai chiá»u Ä‘Æ¡n giáº£n.

## ğŸš€ Thá»­ thÃ¡ch

Trong notebook Ä‘i kÃ¨m, báº¡n sáº½ triá»ƒn khai khung cá»§a riÃªng mÃ¬nh Ä‘á»ƒ xÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n perceptron Ä‘a táº§ng. Báº¡n sáº½ cÃ³ thá»ƒ tháº¥y chi tiáº¿t cÃ¡ch cÃ¡c máº¡ng nÆ¡-ron hiá»‡n Ä‘áº¡i hoáº¡t Ä‘á»™ng.

Tiáº¿n hÃ nh Ä‘áº¿n notebook OwnFramework vÃ  thá»±c hiá»‡n nÃ³.

## ÄÃ¡nh giÃ¡ & Tá»± há»c

Backpropagation lÃ  má»™t thuáº­t toÃ¡n phá»• biáº¿n Ä‘Æ°á»£c sá»­ dá»¥ng trong AI vÃ  ML, Ä‘Ã¡ng Ä‘á»ƒ nghiÃªn cá»©u chi tiáº¿t hÆ¡n.

## BÃ i táº­p

Trong phÃ²ng thÃ­ nghiá»‡m nÃ y, báº¡n Ä‘Æ°á»£c yÃªu cáº§u sá»­ dá»¥ng khung mÃ  báº¡n Ä‘Ã£ xÃ¢y dá»±ng trong bÃ i há»c nÃ y Ä‘á»ƒ giáº£i quyáº¿t phÃ¢n loáº¡i chá»¯ sá»‘ viáº¿t tay MNIST.

* HÆ°á»›ng dáº«n
* Notebook

**TuyÃªn bá»‘ miá»…n trá»« trÃ¡ch nhiá»‡m**:  
TÃ i liá»‡u nÃ y Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch báº±ng dá»‹ch vá»¥ dá»‹ch thuáº­t AI [Co-op Translator](https://github.com/Azure/co-op-translator). Máº·c dÃ¹ chÃºng tÃ´i cá»‘ gáº¯ng Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c, xin lÆ°u Ã½ ráº±ng cÃ¡c báº£n dá»‹ch tá»± Ä‘á»™ng cÃ³ thá»ƒ chá»©a lá»—i hoáº·c khÃ´ng chÃ­nh xÃ¡c. TÃ i liá»‡u gá»‘c báº±ng ngÃ´n ngá»¯ báº£n Ä‘á»‹a nÃªn Ä‘Æ°á»£c coi lÃ  nguá»“n thÃ´ng tin chÃ­nh thá»©c. Äá»‘i vá»›i thÃ´ng tin quan trá»ng, nÃªn sá»­ dá»¥ng dá»‹ch vá»¥ dá»‹ch thuáº­t chuyÃªn nghiá»‡p tá»« con ngÆ°á»i. ChÃºng tÃ´i khÃ´ng chá»‹u trÃ¡ch nhiá»‡m cho báº¥t ká»³ sá»± hiá»ƒu láº§m hoáº·c diá»…n giáº£i sai nÃ o phÃ¡t sinh tá»« viá»‡c sá»­ dá»¥ng báº£n dá»‹ch nÃ y.