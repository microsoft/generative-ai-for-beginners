{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giới thiệu\n",
    "\n",
    "Bài học này sẽ đề cập đến:\n",
    "- Gọi hàm là gì và các trường hợp sử dụng của nó\n",
    "- Cách tạo một lệnh gọi hàm bằng OpenAI\n",
    "- Cách tích hợp lệnh gọi hàm vào một ứng dụng\n",
    "\n",
    "## Mục tiêu học tập\n",
    "\n",
    "Sau khi hoàn thành bài học này, bạn sẽ biết cách và hiểu được:\n",
    "\n",
    "- Mục đích của việc sử dụng gọi hàm\n",
    "- Thiết lập Gọi Hàm bằng Dịch vụ OpenAI\n",
    "- Thiết kế các lệnh gọi hàm hiệu quả cho trường hợp sử dụng ứng dụng của bạn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiểu về Gọi Hàm\n",
    "\n",
    "Trong bài học này, chúng ta sẽ xây dựng một tính năng cho startup giáo dục cho phép người dùng sử dụng chatbot để tìm các khóa học kỹ thuật. Chúng ta sẽ đề xuất các khóa học phù hợp với trình độ kỹ năng, vai trò hiện tại và công nghệ mà họ quan tâm.\n",
    "\n",
    "Để hoàn thành điều này, chúng ta sẽ kết hợp:\n",
    " - `OpenAI` để tạo trải nghiệm trò chuyện cho người dùng\n",
    " - `Microsoft Learn Catalog API` để giúp người dùng tìm kiếm các khóa học dựa trên yêu cầu của họ\n",
    " - `Function Calling` để lấy truy vấn của người dùng và gửi đến một hàm nhằm thực hiện yêu cầu API.\n",
    "\n",
    "Để bắt đầu, hãy cùng tìm hiểu lý do tại sao chúng ta lại muốn sử dụng gọi hàm ngay từ đầu:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # lấy phản hồi mới từ GPT, nơi nó có thể xem phản hồi của hàm\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tại sao cần Function Calling\n",
    "\n",
    "Nếu bạn đã hoàn thành bất kỳ bài học nào khác trong khóa học này, có lẽ bạn đã hiểu sức mạnh của việc sử dụng các Mô hình Ngôn ngữ Lớn (LLMs). Hy vọng bạn cũng nhận ra một số hạn chế của chúng.\n",
    "\n",
    "Function Calling là một tính năng của OpenAI Service được thiết kế để giải quyết các thách thức sau:\n",
    "\n",
    "Định dạng phản hồi không nhất quán:\n",
    "- Trước khi có function calling, các phản hồi từ mô hình ngôn ngữ lớn thường không có cấu trúc và thiếu nhất quán. Lập trình viên phải viết mã kiểm tra phức tạp để xử lý từng trường hợp khác nhau trong kết quả trả về.\n",
    "\n",
    "Khó tích hợp với dữ liệu bên ngoài:\n",
    "- Trước khi có tính năng này, việc đưa dữ liệu từ các phần khác của ứng dụng vào ngữ cảnh trò chuyện là rất khó khăn.\n",
    "\n",
    "Bằng cách chuẩn hóa định dạng phản hồi và cho phép tích hợp mượt mà với dữ liệu bên ngoài, function calling giúp đơn giản hóa quá trình phát triển và giảm nhu cầu viết thêm các đoạn mã kiểm tra.\n",
    "\n",
    "Người dùng không thể nhận được các câu trả lời như \"Thời tiết hiện tại ở Stockholm như thế nào?\". Nguyên nhân là do các mô hình chỉ giới hạn trong khoảng thời gian mà dữ liệu được huấn luyện.\n",
    "\n",
    "Hãy cùng xem ví dụ dưới đây để minh họa cho vấn đề này:\n",
    "\n",
    "Giả sử chúng ta muốn tạo một cơ sở dữ liệu về thông tin sinh viên để có thể gợi ý khóa học phù hợp cho họ. Dưới đây là hai mô tả về sinh viên có dữ liệu rất giống nhau.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta muốn gửi dữ liệu này đến một LLM để phân tích. Sau đó, dữ liệu này có thể được sử dụng trong ứng dụng của chúng ta để gửi đến một API hoặc lưu trữ trong cơ sở dữ liệu.\n",
    "\n",
    "Hãy tạo hai prompt giống hệt nhau để hướng dẫn LLM về những thông tin mà chúng ta quan tâm:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng tôi muốn gửi điều này đến một LLM để phân tích các phần quan trọng đối với sản phẩm của chúng tôi. Vì vậy, chúng tôi có thể tạo hai lời nhắc giống hệt nhau để hướng dẫn LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi tạo hai lời nhắc này, chúng ta sẽ gửi chúng đến LLM bằng cách sử dụng `openai.ChatCompletion`. Chúng ta lưu lời nhắc vào biến `messages` và gán vai trò là `user`. Điều này nhằm mô phỏng một tin nhắn từ người dùng được gửi đến chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc dù các prompt giống nhau và mô tả cũng tương tự, chúng ta có thể nhận được các định dạng khác nhau của thuộc tính `Grades`.\n",
    "\n",
    "Nếu bạn chạy ô trên nhiều lần, định dạng có thể là `3.7` hoặc `3.7 GPA`.\n",
    "\n",
    "Điều này là do LLM nhận dữ liệu không có cấu trúc dưới dạng prompt được viết và cũng trả về dữ liệu không có cấu trúc. Chúng ta cần có một định dạng có cấu trúc để biết trước sẽ nhận được gì khi lưu trữ hoặc sử dụng dữ liệu này.\n",
    "\n",
    "Bằng cách sử dụng gọi hàm (functional calling), chúng ta có thể đảm bảo nhận lại dữ liệu có cấu trúc. Khi sử dụng gọi hàm, LLM thực tế không gọi hoặc chạy bất kỳ hàm nào. Thay vào đó, chúng ta tạo ra một cấu trúc để LLM tuân theo khi trả lời. Sau đó, chúng ta sử dụng các phản hồi có cấu trúc đó để biết nên chạy hàm nào trong ứng dụng của mình.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sơ đồ luồng gọi hàm](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.vi.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Các trường hợp sử dụng gọi hàm\n",
    "\n",
    "**Gọi các công cụ bên ngoài**  \n",
    "Chatbot rất hữu ích trong việc cung cấp câu trả lời cho các câu hỏi của người dùng. Bằng cách sử dụng gọi hàm, chatbot có thể dùng tin nhắn từ người dùng để thực hiện một số tác vụ nhất định. Ví dụ, một sinh viên có thể yêu cầu chatbot \"Gửi email cho giảng viên của tôi nói rằng tôi cần thêm sự hỗ trợ với môn học này\". Điều này có thể thực hiện bằng cách gọi hàm `send_email(to: string, body: string)`\n",
    "\n",
    "**Tạo truy vấn API hoặc cơ sở dữ liệu**  \n",
    "Người dùng có thể tìm kiếm thông tin bằng ngôn ngữ tự nhiên, sau đó được chuyển đổi thành một truy vấn hoặc yêu cầu API có định dạng. Ví dụ, một giáo viên có thể yêu cầu \"Những học sinh nào đã hoàn thành bài tập cuối cùng\" và điều này có thể gọi một hàm tên là `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Tạo dữ liệu có cấu trúc**  \n",
    "Người dùng có thể lấy một đoạn văn bản hoặc tệp CSV và sử dụng LLM để trích xuất thông tin quan trọng từ đó. Ví dụ, một sinh viên có thể chuyển một bài viết Wikipedia về các hiệp định hòa bình để tạo thẻ ghi nhớ AI. Việc này có thể thực hiện bằng cách sử dụng hàm `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tạo Lệnh Gọi Hàm Đầu Tiên Của Bạn\n",
    "\n",
    "Quy trình tạo một lệnh gọi hàm bao gồm 3 bước chính:\n",
    "1. Gọi API Chat Completions với danh sách các hàm của bạn và một tin nhắn từ người dùng\n",
    "2. Đọc phản hồi của mô hình để thực hiện một hành động, ví dụ như chạy một hàm hoặc gọi API\n",
    "3. Gọi lại API Chat Completions với phản hồi từ hàm của bạn để sử dụng thông tin đó tạo phản hồi cho người dùng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Luồng của một Lời gọi Hàm](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.vi.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Các thành phần của một lần gọi hàm\n",
    "\n",
    "#### Đầu vào của người dùng\n",
    "\n",
    "Bước đầu tiên là tạo một tin nhắn từ người dùng. Bạn có thể gán giá trị này một cách động bằng cách lấy giá trị từ một ô nhập liệu văn bản hoặc có thể gán giá trị trực tiếp tại đây. Nếu đây là lần đầu bạn làm việc với Chat Completions API, chúng ta cần xác định `role` và `content` của tin nhắn.\n",
    "\n",
    "`role` có thể là `system` (tạo quy tắc), `assistant` (mô hình) hoặc `user` (người dùng cuối). Đối với việc gọi hàm, chúng ta sẽ gán là `user` cùng với một câu hỏi ví dụ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tạo hàm\n",
    "\n",
    "Tiếp theo, chúng ta sẽ định nghĩa một hàm và các tham số của hàm đó. Ở đây, chúng ta sẽ chỉ sử dụng một hàm có tên là `search_courses`, nhưng bạn có thể tạo nhiều hàm khác nhau.\n",
    "\n",
    "**Lưu ý quan trọng**: Các hàm sẽ được đưa vào thông điệp hệ thống gửi đến LLM và sẽ tính vào tổng số token mà bạn có thể sử dụng.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Định nghĩa**\n",
    "\n",
    "Cấu trúc định nghĩa hàm có nhiều cấp độ, mỗi cấp có các thuộc tính riêng. Dưới đây là phần giải thích về cấu trúc lồng nhau này:\n",
    "\n",
    "**Thuộc tính Hàm Cấp Cao Nhất:**\n",
    "\n",
    "`name` - Tên của hàm mà bạn muốn được gọi.\n",
    "\n",
    "`description` - Đây là phần mô tả cách hoạt động của hàm. Ở đây, điều quan trọng là phải cụ thể và rõ ràng.\n",
    "\n",
    "`parameters` - Danh sách các giá trị và định dạng mà bạn muốn mô hình tạo ra trong phản hồi của nó.\n",
    "\n",
    "**Thuộc tính Đối tượng Parameters:**\n",
    "\n",
    "`type` - Kiểu dữ liệu của đối tượng parameters (thường là \"object\")\n",
    "\n",
    "`properties` - Danh sách các giá trị cụ thể mà mô hình sẽ sử dụng cho phản hồi\n",
    "\n",
    "**Thuộc tính của Từng Tham Số:**\n",
    "\n",
    "`name` - Được xác định ngầm định bởi khóa thuộc tính (ví dụ: \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Kiểu dữ liệu của tham số cụ thể này (ví dụ: \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - Mô tả về tham số cụ thể\n",
    "\n",
    "**Thuộc tính Tùy chọn:**\n",
    "\n",
    "`required` - Một mảng liệt kê những tham số nào là bắt buộc để hoàn thành việc gọi hàm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gọi hàm\n",
    "Sau khi đã định nghĩa một hàm, bây giờ chúng ta cần đưa nó vào trong lệnh gọi tới Chat Completion API. Chúng ta làm điều này bằng cách thêm `functions` vào yêu cầu. Trong trường hợp này là `functions=functions`.\n",
    "\n",
    "Ngoài ra còn có tùy chọn đặt `function_call` thành `auto`. Điều này có nghĩa là chúng ta sẽ để LLM tự quyết định nên gọi hàm nào dựa trên tin nhắn của người dùng thay vì tự mình chỉ định.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ hãy cùng xem phản hồi và cách nó được định dạng:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Bạn có thể thấy tên của hàm được gọi và từ tin nhắn của người dùng, LLM đã có thể tìm dữ liệu để điền vào các tham số của hàm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tích hợp Gọi Hàm vào Ứng dụng.\n",
    "\n",
    "Sau khi chúng ta đã kiểm tra phản hồi được định dạng từ LLM, bây giờ chúng ta có thể tích hợp nó vào một ứng dụng.\n",
    "\n",
    "### Quản lý luồng xử lý\n",
    "\n",
    "Để tích hợp điều này vào ứng dụng của mình, hãy thực hiện các bước sau:\n",
    "\n",
    "Đầu tiên, hãy gọi tới dịch vụ OpenAI và lưu tin nhắn vào một biến có tên là `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta sẽ định nghĩa hàm sẽ gọi API Microsoft Learn để lấy danh sách các khóa học:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theo thông lệ tốt nhất, chúng ta sẽ kiểm tra xem mô hình có muốn gọi một hàm hay không. Sau đó, chúng ta sẽ tạo một trong các hàm có sẵn và ghép nó với hàm đang được gọi.\n",
    "Tiếp theo, chúng ta sẽ lấy các đối số của hàm và ánh xạ chúng với các đối số từ LLM.\n",
    "\n",
    "Cuối cùng, chúng ta sẽ thêm thông điệp gọi hàm và các giá trị được trả về bởi thông điệp `search_courses`. Điều này cung cấp cho LLM tất cả thông tin cần thiết để\n",
    "phản hồi người dùng bằng ngôn ngữ tự nhiên.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thử thách lập trình\n",
    "\n",
    "Làm tốt lắm! Để tiếp tục học về OpenAI Function Calling, bạn có thể xây dựng: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Thêm nhiều tham số cho hàm để giúp người học tìm được nhiều khóa học hơn. Bạn có thể xem các tham số API có sẵn tại đây:\n",
    " - Tạo một hàm gọi khác nhận thêm thông tin từ người học như ngôn ngữ mẹ đẻ của họ\n",
    " - Xử lý lỗi khi hàm gọi hoặc API không trả về khóa học phù hợp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Tuyên bố miễn trừ trách nhiệm**:  \nTài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn tham khảo chính thức. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm đối với bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T21:11:49+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "vi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}