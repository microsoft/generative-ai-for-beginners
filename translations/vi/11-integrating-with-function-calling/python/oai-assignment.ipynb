{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giới thiệu\n",
    "\n",
    "Bài học này sẽ bao gồm:  \n",
    "- Gọi hàm là gì và các trường hợp sử dụng của nó  \n",
    "- Cách tạo một cuộc gọi hàm sử dụng OpenAI  \n",
    "- Cách tích hợp cuộc gọi hàm vào một ứng dụng  \n",
    "\n",
    "## Mục tiêu học tập\n",
    "\n",
    "Sau khi hoàn thành bài học này, bạn sẽ biết cách và hiểu được:  \n",
    "\n",
    "- Mục đích của việc sử dụng gọi hàm  \n",
    "- Cài đặt Cuộc gọi Hàm sử dụng Dịch vụ OpenAI  \n",
    "- Thiết kế các cuộc gọi hàm hiệu quả cho trường hợp sử dụng ứng dụng của bạn  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiểu về Gọi Hàm\n",
    "\n",
    "Trong bài học này, chúng ta muốn xây dựng một tính năng cho startup giáo dục của mình cho phép người dùng sử dụng chatbot để tìm các khóa học kỹ thuật. Chúng tôi sẽ đề xuất các khóa học phù hợp với trình độ kỹ năng, vai trò hiện tại và công nghệ mà họ quan tâm.\n",
    "\n",
    "Để hoàn thành điều này, chúng ta sẽ sử dụng kết hợp:\n",
    " - `OpenAI` để tạo trải nghiệm trò chuyện cho người dùng\n",
    " - `Microsoft Learn Catalog API` để giúp người dùng tìm khóa học dựa trên yêu cầu của họ\n",
    " - `Function Calling` để lấy truy vấn của người dùng và gửi nó đến một hàm để thực hiện yêu cầu API.\n",
    "\n",
    "Để bắt đầu, hãy xem tại sao chúng ta muốn sử dụng gọi hàm ngay từ đầu:\n",
    "\n",
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # nhận phản hồi mới từ GPT nơi nó có thể xem phản hồi của hàm\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tại sao gọi hàm\n",
    "\n",
    "Nếu bạn đã hoàn thành bất kỳ bài học nào khác trong khóa học này, có lẽ bạn đã hiểu được sức mạnh của việc sử dụng Mô hình Ngôn ngữ Lớn (LLMs). Hy vọng bạn cũng có thể thấy một số hạn chế của chúng.\n",
    "\n",
    "Gọi hàm là một tính năng của Dịch vụ OpenAI được thiết kế để giải quyết các thách thức sau:\n",
    "\n",
    "Định dạng phản hồi không nhất quán:\n",
    "- Trước khi có gọi hàm, các phản hồi từ mô hình ngôn ngữ lớn thường không có cấu trúc và không nhất quán. Các nhà phát triển phải viết mã xác thực phức tạp để xử lý từng biến thể trong đầu ra.\n",
    "\n",
    "Tích hợp hạn chế với dữ liệu bên ngoài:\n",
    "- Trước tính năng này, việc kết hợp dữ liệu từ các phần khác của ứng dụng vào ngữ cảnh trò chuyện rất khó khăn.\n",
    "\n",
    "Bằng cách chuẩn hóa định dạng phản hồi và cho phép tích hợp liền mạch với dữ liệu bên ngoài, gọi hàm đơn giản hóa việc phát triển và giảm nhu cầu về logic xác thực bổ sung.\n",
    "\n",
    "Người dùng không thể nhận được câu trả lời như \"Thời tiết hiện tại ở Stockholm là gì?\". Điều này là do các mô hình bị giới hạn ở thời điểm dữ liệu được huấn luyện.\n",
    "\n",
    "Hãy xem ví dụ dưới đây minh họa cho vấn đề này:\n",
    "\n",
    "Giả sử chúng ta muốn tạo một cơ sở dữ liệu về dữ liệu sinh viên để có thể đề xuất khóa học phù hợp cho họ. Dưới đây là hai mô tả về sinh viên rất giống nhau về dữ liệu mà chúng chứa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng tôi muốn gửi điều này đến một LLM để phân tích dữ liệu. Điều này sau đó có thể được sử dụng trong ứng dụng của chúng tôi để gửi đến một API hoặc lưu trữ trong cơ sở dữ liệu.\n",
    "\n",
    "Hãy tạo hai lời nhắc giống hệt nhau mà chúng tôi hướng dẫn LLM về thông tin mà chúng tôi quan tâm:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng tôi muốn gửi điều này đến một LLM để phân tích các phần quan trọng đối với sản phẩm của chúng tôi. Vì vậy, chúng tôi có thể tạo hai lời nhắc giống hệt nhau để hướng dẫn LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi tạo hai lời nhắc này, chúng tôi sẽ gửi chúng đến LLM bằng cách sử dụng `openai.ChatCompletion`. Chúng tôi lưu lời nhắc trong biến `messages` và gán vai trò là `user`. Điều này nhằm mô phỏng một tin nhắn từ người dùng được viết cho chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta có thể gửi cả hai yêu cầu đến LLM và kiểm tra phản hồi mà chúng ta nhận được.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc dù các lời nhắc giống nhau và các mô tả tương tự, chúng ta có thể nhận được các định dạng khác nhau của thuộc tính `Grades`.\n",
    "\n",
    "Nếu bạn chạy ô trên nhiều lần, định dạng có thể là `3.7` hoặc `3.7 GPA`.\n",
    "\n",
    "Điều này là do LLM lấy dữ liệu không có cấu trúc dưới dạng lời nhắc được viết và cũng trả về dữ liệu không có cấu trúc. Chúng ta cần có một định dạng có cấu trúc để biết được điều gì sẽ xảy ra khi lưu trữ hoặc sử dụng dữ liệu này.\n",
    "\n",
    "Bằng cách sử dụng gọi hàm chức năng, chúng ta có thể đảm bảo nhận lại dữ liệu có cấu trúc. Khi sử dụng gọi hàm chức năng, LLM thực sự không gọi hoặc chạy bất kỳ hàm nào. Thay vào đó, chúng ta tạo ra một cấu trúc để LLM tuân theo trong các phản hồi của nó. Sau đó, chúng ta sử dụng các phản hồi có cấu trúc đó để biết hàm nào cần chạy trong các ứng dụng của mình.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sơ đồ luồng gọi hàm](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.vi.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta sau đó có thể lấy kết quả trả về từ hàm và gửi lại cho LLM. LLM sẽ phản hồi bằng ngôn ngữ tự nhiên để trả lời câu hỏi của người dùng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Các trường hợp sử dụng cho việc gọi hàm\n",
    "\n",
    "**Gọi công cụ bên ngoài**  \n",
    "Chatbot rất giỏi trong việc cung cấp câu trả lời cho các câu hỏi từ người dùng. Bằng cách sử dụng gọi hàm, chatbot có thể dùng các tin nhắn từ người dùng để hoàn thành một số tác vụ nhất định. Ví dụ, một sinh viên có thể yêu cầu chatbot \"Gửi email cho giảng viên của tôi nói rằng tôi cần thêm sự trợ giúp về môn học này\". Điều này có thể thực hiện một cuộc gọi hàm tới `send_email(to: string, body: string)`\n",
    "\n",
    "**Tạo truy vấn API hoặc cơ sở dữ liệu**  \n",
    "Người dùng có thể tìm thông tin bằng ngôn ngữ tự nhiên được chuyển đổi thành truy vấn hoặc yêu cầu API có định dạng. Một ví dụ có thể là một giáo viên yêu cầu \"Ai là những sinh viên đã hoàn thành bài tập cuối cùng\" có thể gọi một hàm tên là `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Tạo dữ liệu có cấu trúc**  \n",
    "Người dùng có thể lấy một đoạn văn bản hoặc CSV và sử dụng LLM để trích xuất thông tin quan trọng từ đó. Ví dụ, một sinh viên có thể chuyển đổi một bài viết Wikipedia về các thỏa thuận hòa bình để tạo thẻ học AI. Việc này có thể được thực hiện bằng cách sử dụng một hàm gọi là `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tạo Lời Gọi Hàm Đầu Tiên Của Bạn\n",
    "\n",
    "Quá trình tạo một lời gọi hàm bao gồm 3 bước chính:\n",
    "1. Gọi API Chat Completions với danh sách các hàm của bạn và một tin nhắn từ người dùng\n",
    "2. Đọc phản hồi của mô hình để thực hiện một hành động, ví dụ như thực thi một hàm hoặc gọi API\n",
    "3. Thực hiện một lần gọi khác tới API Chat Completions với phản hồi từ hàm của bạn để sử dụng thông tin đó tạo ra phản hồi cho người dùng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Luồng của một Cuộc gọi Hàm](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.vi.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Các thành phần của một cuộc gọi hàm\n",
    "\n",
    "#### Đầu vào của người dùng\n",
    "\n",
    "Bước đầu tiên là tạo một tin nhắn người dùng. Điều này có thể được gán động bằng cách lấy giá trị từ một ô nhập văn bản hoặc bạn có thể gán một giá trị tại đây. Nếu đây là lần đầu tiên bạn làm việc với Chat Completions API, chúng ta cần định nghĩa `role` và `content` của tin nhắn.\n",
    "\n",
    "`role` có thể là `system` (tạo quy tắc), `assistant` (mô hình) hoặc `user` (người dùng cuối). Đối với cuộc gọi hàm, chúng ta sẽ gán giá trị này là `user` và một câu hỏi ví dụ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tạo hàm.\n",
    "\n",
    "Tiếp theo chúng ta sẽ định nghĩa một hàm và các tham số của hàm đó. Chúng ta sẽ chỉ sử dụng một hàm ở đây gọi là `search_courses` nhưng bạn có thể tạo nhiều hàm khác nhau.\n",
    "\n",
    "**Quan trọng**: Các hàm được bao gồm trong tin nhắn hệ thống gửi đến LLM và sẽ được tính vào số lượng token khả dụng mà bạn có.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Định nghĩa**\n",
    "\n",
    "Cấu trúc định nghĩa hàm có nhiều cấp độ, mỗi cấp độ có các thuộc tính riêng. Dưới đây là phân tích cấu trúc lồng nhau:\n",
    "\n",
    "**Thuộc tính Hàm Cấp Đầu:**\n",
    "\n",
    "`name` - Tên của hàm mà chúng ta muốn gọi.\n",
    "\n",
    "`description` - Đây là mô tả về cách hàm hoạt động. Ở đây điều quan trọng là phải cụ thể và rõ ràng.\n",
    "\n",
    "`parameters` - Danh sách các giá trị và định dạng mà bạn muốn mô hình tạo ra trong phản hồi của nó.\n",
    "\n",
    "**Thuộc tính Đối tượng Tham số:**\n",
    "\n",
    "`type` - Kiểu dữ liệu của đối tượng tham số (thường là \"object\")\n",
    "\n",
    "`properties` - Danh sách các giá trị cụ thể mà mô hình sẽ sử dụng cho phản hồi của nó.\n",
    "\n",
    "**Thuộc tính Tham số Cá nhân:**\n",
    "\n",
    "`name` - Được định nghĩa ngầm định bởi khóa thuộc tính (ví dụ: \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Kiểu dữ liệu của tham số cụ thể này (ví dụ: \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - Mô tả về tham số cụ thể.\n",
    "\n",
    "**Thuộc tính Tùy chọn:**\n",
    "\n",
    "`required` - Một mảng liệt kê các tham số bắt buộc để cuộc gọi hàm được hoàn thành.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gọi hàm  \n",
    "Sau khi định nghĩa một hàm, bây giờ chúng ta cần đưa nó vào trong lời gọi API Chat Completion. Chúng ta làm điều này bằng cách thêm `functions` vào yêu cầu. Trong trường hợp này là `functions=functions`.  \n",
    "\n",
    "Cũng có một tùy chọn để đặt `function_call` thành `auto`. Điều này có nghĩa là chúng ta sẽ để LLM quyết định hàm nào nên được gọi dựa trên tin nhắn của người dùng thay vì tự gán.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ hãy cùng xem phản hồi và cách nó được định dạng:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Bạn có thể thấy tên của hàm được gọi và từ tin nhắn của người dùng, LLM đã có thể tìm dữ liệu phù hợp với các đối số của hàm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Tích hợp các cuộc gọi hàm vào một ứng dụng. \n",
    "\n",
    "\n",
    "Sau khi chúng ta đã kiểm tra phản hồi được định dạng từ LLM, bây giờ chúng ta có thể tích hợp điều này vào một ứng dụng. \n",
    "\n",
    "### Quản lý luồng \n",
    "\n",
    "Để tích hợp điều này vào ứng dụng của chúng ta, hãy thực hiện các bước sau: \n",
    "\n",
    "Đầu tiên, hãy gọi dịch vụ OpenAI và lưu tin nhắn vào một biến có tên `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta sẽ định nghĩa hàm sẽ gọi API Microsoft Learn để lấy danh sách các khóa học:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như một thực hành tốt nhất, chúng ta sẽ kiểm tra xem mô hình có muốn gọi một hàm hay không. Sau đó, chúng ta sẽ tạo một trong các hàm có sẵn và khớp nó với hàm đang được gọi.  \n",
    "Tiếp theo, chúng ta sẽ lấy các đối số của hàm và ánh xạ chúng với các đối số từ LLM.\n",
    "\n",
    "Cuối cùng, chúng ta sẽ thêm tin nhắn gọi hàm và các giá trị được trả về bởi tin nhắn `search_courses`. Điều này cung cấp cho LLM tất cả thông tin cần thiết để  \n",
    "phản hồi người dùng bằng ngôn ngữ tự nhiên.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta sẽ gửi tin nhắn đã cập nhật đến LLM để chúng ta có thể nhận được phản hồi bằng ngôn ngữ tự nhiên thay vì phản hồi định dạng JSON của API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thử thách mã \n",
    "\n",
    "Tuyệt vời! Để tiếp tục học về Gọi Hàm OpenAI, bạn có thể xây dựng: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - Thêm các tham số của hàm có thể giúp người học tìm nhiều khóa học hơn. Bạn có thể tìm các tham số API có sẵn tại đây: \n",
    " - Tạo một cuộc gọi hàm khác lấy thêm thông tin từ người học như ngôn ngữ mẹ đẻ của họ \n",
    " - Tạo xử lý lỗi khi cuộc gọi hàm và/hoặc cuộc gọi API không trả về khóa học phù hợp nào\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Tuyên bố từ chối trách nhiệm**:  \nTài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ gốc của nó nên được coi là nguồn tham khảo chính thức. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp do con người thực hiện. Chúng tôi không chịu trách nhiệm về bất kỳ sự hiểu lầm hoặc giải thích sai nào phát sinh từ việc sử dụng bản dịch này.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T10:56:31+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "vi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}