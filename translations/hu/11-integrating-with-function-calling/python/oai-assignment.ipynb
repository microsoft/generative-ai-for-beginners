{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bevezetés \n",
    "\n",
    "Ez a lecke a következőket fogja lefedni: \n",
    "- Mi az a függvényhívás és milyen esetekben használható \n",
    "- Hogyan lehet függvényhívást létrehozni az OpenAI segítségével \n",
    "- Hogyan lehet egy függvényhívást integrálni egy alkalmazásba \n",
    "\n",
    "## Tanulási célok \n",
    "\n",
    "A lecke elvégzése után tudni fogod és megérted: \n",
    "\n",
    "- A függvényhívás használatának célját \n",
    "- A függvényhívás beállítását az OpenAI szolgáltatás segítségével \n",
    "- Hatékony függvényhívások tervezését az alkalmazásod használati esetéhez\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Függvényhívások megértése\n",
    "\n",
    "Ehhez a leckéhez egy olyan funkciót szeretnénk létrehozni az oktatási startupunk számára, amely lehetővé teszi a felhasználók számára, hogy egy chatbot segítségével technikai tanfolyamokat találjanak. Olyan tanfolyamokat fogunk ajánlani, amelyek megfelelnek a képességszintjüknek, jelenlegi szerepüknek és az érdeklődési technológiának.\n",
    "\n",
    "Ehhez a következő kombinációt fogjuk használni:\n",
    " - `OpenAI` a felhasználó számára egy csevegési élmény létrehozásához\n",
    " - `Microsoft Learn Catalog API` a felhasználók segítésére a tanfolyamok megtalálásában a felhasználó kérésének alapján\n",
    " - `Function Calling` a felhasználó lekérdezésének átvételéhez és egy függvényhez való elküldéséhez az API kérés végrehajtásához.\n",
    "\n",
    "A kezdéshez nézzük meg, miért szeretnénk egyáltalán függvényhívást használni:\n",
    "\n",
    "print(\"Üzenetek a következő kérésben:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # új választ kapunk a GPT-től, ahol láthatja a függvény válaszát\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miért a függvényhívás\n",
    "\n",
    "Ha elvégeztél bármely más leckét ebben a tanfolyamban, valószínűleg érted a Nagy Nyelvi Modellek (LLM-ek) használatának erejét. Remélhetőleg a korlátaikat is látod.\n",
    "\n",
    "A függvényhívás az OpenAI Szolgáltatás egy olyan funkciója, amely a következő kihívások kezelésére készült:\n",
    "\n",
    "Válaszformátumok következetlensége:\n",
    "- A függvényhívás előtt a nagy nyelvi modell válaszai strukturálatlanok és következetlenek voltak. A fejlesztőknek bonyolult érvényesítő kódot kellett írniuk az egyes kimeneti variációk kezelésére.\n",
    "\n",
    "Korlátozott integráció külső adatokkal:\n",
    "- E funkció előtt nehéz volt az alkalmazás más részeiből származó adatokat beépíteni egy csevegési kontextusba.\n",
    "\n",
    "A válaszformátumok szabványosításával és a külső adatok zökkenőmentes integrációjának lehetővé tételével a függvényhívás egyszerűsíti a fejlesztést, és csökkenti a további érvényesítési logika szükségességét.\n",
    "\n",
    "A felhasználók nem kaphattak olyan válaszokat, mint például „Milyen az aktuális időjárás Stockholmban?”. Ennek oka, hogy a modellek csak a betanításuk idejéig rendelkeztek adatokkal.\n",
    "\n",
    "Nézzük meg az alábbi példát, amely illusztrálja ezt a problémát:\n",
    "\n",
    "Tegyük fel, hogy létre akarunk hozni egy hallgatói adatbázist, hogy a megfelelő kurzust javasolhassuk nekik. Lent két olyan hallgató leírását látjuk, amelyek nagyon hasonló adatokat tartalmaznak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ezt szeretnénk elküldeni egy LLM-nek az adatok elemzéséhez. Ezt később felhasználhatjuk az alkalmazásunkban, hogy elküldjük egy API-nak vagy eltároljuk egy adatbázisban.\n",
    "\n",
    "Hozzunk létre két azonos promptot, amelyekben utasítjuk az LLM-et, hogy milyen információk érdekelnek minket:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ezt egy LLM-nek szeretnénk elküldeni, hogy elemezze a termékünk szempontjából fontos részeket. Így két azonos promptot hozhatunk létre az LLM utasításához:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miután elkészítettük ezt a két promptot, elküldjük őket az LLM-nek az `openai.ChatCompletion` használatával. A promptot a `messages` változóban tároljuk, és a szerepet `user`-re állítjuk. Ez azért van, hogy utánozzuk egy felhasználó üzenetének írását egy chatbot számára.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most már mindkét kérést elküldhetjük az LLM-nek, és megvizsgálhatjuk a kapott választ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bár a promptok ugyanazok és a leírások hasonlóak, a `Grades` tulajdonság különböző formátumokban jelenhet meg.\n",
    "\n",
    "Ha többször lefuttatod a fenti cellát, a formátum lehet `3.7` vagy `3.7 GPA`.\n",
    "\n",
    "Ennek az az oka, hogy az LLM nem strukturált adatokat vesz be a megírt prompt formájában, és szintén nem strukturált adatokat ad vissza. Szükségünk van egy strukturált formátumra, hogy tudjuk, mire számíthatunk az adatok tárolásakor vagy használatakor.\n",
    "\n",
    "Funkcionális hívás használatával biztosíthatjuk, hogy strukturált adatokat kapjunk vissza. Funkcionális hívás esetén az LLM valójában nem hív meg vagy futtat le semmilyen függvényt. Ehelyett létrehozunk egy struktúrát, amelyet az LLM követ a válaszai során. Ezután ezeket a strukturált válaszokat használjuk annak meghatározására, hogy melyik függvényt futtassuk az alkalmazásainkban.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Függvényhívás folyamata](../../../../translated_images/Function-Flow.083875364af4f4bb.hu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ezután elvehetjük, amit a függvény visszaad, és visszaküldhetjük ezt az LLM-nek. Az LLM ezután természetes nyelven válaszol a felhasználó kérdésére.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcióhívások használati esetei\n",
    "\n",
    "**Külső eszközök hívása**  \n",
    "A chatbotok nagyszerűek arra, hogy válaszokat adjanak a felhasználók kérdéseire. A funkcióhívás használatával a chatbotok a felhasználók üzeneteit felhasználva bizonyos feladatokat végezhetnek el. Például egy diák megkérheti a chatbotot, hogy „Küldj e-mailt az oktatómnak, hogy több segítségre van szükségem ebben a témában”. Ez egy `send_email(to: string, body: string)` nevű funkcióhívást eredményezhet.\n",
    "\n",
    "**API vagy adatbázis lekérdezések létrehozása**  \n",
    "A felhasználók természetes nyelv használatával találhatnak információkat, amelyeket formázott lekérdezéssé vagy API-kéréssé alakítanak át. Ennek példája lehet egy tanár, aki megkérdezi: „Kik azok a diákok, akik teljesítették az utolsó feladatot”, ami egy `get_completed(student_name: string, assignment: int, current_status: string)` nevű funkcióhívást indíthat.\n",
    "\n",
    "**Strukturált adatok létrehozása**  \n",
    "A felhasználók egy szövegrészt vagy CSV-t vehetnek, és az LLM segítségével fontos információkat nyerhetnek ki belőle. Például egy diák egy Wikipédia cikket alakíthat át a békemegállapodásokról, hogy AI villámkártyákat készítsen. Ezt egy `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)` nevű funkció használatával lehet megtenni.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Az első függvényhívás létrehozása\n",
    "\n",
    "A függvényhívás létrehozásának folyamata 3 fő lépésből áll:  \n",
    "1. A Chat Completions API hívása a függvényeid listájával és egy felhasználói üzenettel  \n",
    "2. A modell válaszának elolvasása egy művelet végrehajtásához, pl. egy függvény vagy API hívás végrehajtása  \n",
    "3. Egy újabb hívás végrehajtása a Chat Completions API-hoz a függvényed válaszával, hogy ezt az információt felhasználva válaszolj a felhasználónak.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Egy függvényhívás folyamata](../../../../translated_images/LLM-Flow.3285ed8caf4796d7.hu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Egy függvényhívás elemei\n",
    "\n",
    "#### Felhasználói bemenet\n",
    "\n",
    "Az első lépés egy felhasználói üzenet létrehozása. Ezt dinamikusan is hozzárendelhetjük egy szövegbeviteli mező értékének felhasználásával, vagy itt is megadhatunk egy értéket. Ha ez az első alkalom, hogy a Chat Completions API-val dolgozol, meg kell határoznunk az üzenet `role` és `content` értékét.\n",
    "\n",
    "A `role` lehet `system` (szabályok létrehozása), `assistant` (a modell) vagy `user` (a végfelhasználó). A függvényhíváshoz ezt `user`-nek állítjuk be, és egy példa kérdést adunk meg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Függvények létrehozása.\n",
    "\n",
    "Ezután definiálunk egy függvényt és annak paramétereit. Itt csak egy függvényt fogunk használni, amelynek neve `search_courses`, de több függvényt is létrehozhatsz.\n",
    "\n",
    "**Fontos** : A függvények be vannak építve a rendszerüzenetbe az LLM számára, és beleszámítanak a rendelkezésre álló tokenek mennyiségébe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definíciók** \n",
    "\n",
    "A függvénydefiníció szerkezete több szintből áll, mindegyiknek megvannak a saját tulajdonságai. Íme a beágyazott szerkezet bontása:\n",
    "\n",
    "**Felső szintű függvénytulajdonságok:**\n",
    "\n",
    "`name` - A függvény neve, amelyet hívni szeretnénk. \n",
    "\n",
    "`description` - Ez a függvény működésének leírása. Itt fontos, hogy pontos és világos legyen. \n",
    "\n",
    "`parameters` - Egy lista azokról az értékekről és formátumról, amelyeket a modellnek a válaszában elő kell állítania. \n",
    "\n",
    "**Paraméter objektum tulajdonságai:**\n",
    "\n",
    "`type` - A paraméter objektum adattípusa (általában \"object\")\n",
    "\n",
    "`properties` - A konkrét értékek listája, amelyeket a modell a válaszához használni fog. \n",
    "\n",
    "**Egyedi paraméter tulajdonságok:**\n",
    "\n",
    "`name` - Implicit módon a tulajdonság kulcsa határozza meg (pl. \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Ennek a konkrét paraméternek az adattípusa (pl. \"string\", \"number\", \"boolean\") \n",
    "\n",
    "`description` - A konkrét paraméter leírása \n",
    "\n",
    "**Opcionális tulajdonságok:**\n",
    "\n",
    "`required` - Egy tömb, amely felsorolja, hogy mely paraméterek szükségesek a függvényhívás teljesítéséhez. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A függvényhívás elkészítése  \n",
    "Miután definiáltuk a függvényt, most be kell illesztenünk azt a Chat Completion API hívásába. Ezt úgy tesszük meg, hogy hozzáadjuk a `functions`-t a kéréshez. Ebben az esetben `functions=functions`.  \n",
    "\n",
    "Van egy lehetőség arra is, hogy a `function_call` értékét `auto`-ra állítsuk. Ez azt jelenti, hogy az LLM döntheti el, melyik függvényt kell meghívni a felhasználói üzenet alapján, ahelyett, hogy mi rendelnénk hozzá.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most nézzük meg a választ, és lássuk, hogyan van formázva:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Látható, hogy a függvény neve meg van hívva, és a felhasználói üzenet alapján a LLM képes volt megtalálni az adatokat, hogy illeszkedjenek a függvény argumentumaihoz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Függvényhívások integrálása egy alkalmazásba. \n",
    "\n",
    "\n",
    "Miután teszteltük a formázott választ az LLM-től, most integrálhatjuk ezt egy alkalmazásba. \n",
    "\n",
    "### A folyamat kezelése \n",
    "\n",
    "Ahhoz, hogy ezt integráljuk az alkalmazásunkba, tegyük meg a következő lépéseket: \n",
    "\n",
    "Először hívjuk meg az OpenAI szolgáltatásokat, és tároljuk az üzenetet egy `response_message` nevű változóban. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most definiáljuk azt a függvényt, amely meghívja a Microsoft Learn API-t, hogy lekérje a tanfolyamok listáját:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legjobb gyakorlatként ezután megnézzük, hogy a modell szeretne-e egy függvényt hívni. Ezután létrehozunk egy elérhető függvényt, és összepárosítjuk azt a hívott függvénnyel.  \n",
    "Ezután a függvény argumentumait leképezzük az LLM argumentumaira.\n",
    "\n",
    "Végül hozzáfűzzük a függvényhívás üzenetét és azokat az értékeket, amelyeket a `search_courses` üzenet adott vissza. Ez megadja az LLM-nek az összes szükséges információt ahhoz, hogy természetes nyelven válaszoljon a felhasználónak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most elküldjük a frissített üzenetet az LLM-nek, hogy természetes nyelvű választ kapjunk az API JSON formátumú válasza helyett.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kód kihívás\n",
    "\n",
    "Nagyszerű munka! Az OpenAI Function Calling tanulásának folytatásához elkészítheted: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    " - A függvény további paraméterei, amelyek segíthetnek a tanulóknak több tanfolyam megtalálásában. A rendelkezésre álló API paramétereket itt találod:  \n",
    " - Készíts egy másik függvényhívást, amely több információt kér a tanulótól, például az anyanyelvét  \n",
    " - Készíts hibakezelést arra az esetre, ha a függvényhívás és/vagy az API hívás nem ad vissza megfelelő tanfolyamokat  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Jogi nyilatkozat**:\nEzt a dokumentumot az AI fordító szolgáltatás [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével fordítottuk le. Bár a pontosságra törekszünk, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az anyanyelvén tekintendő hiteles forrásnak. Fontos információk esetén szakmai, emberi fordítást javaslunk. Nem vállalunk felelősséget a fordítás használatából eredő félreértésekért vagy téves értelmezésekért.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T11:18:13+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "hu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}