{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bevezetés\n",
    "\n",
    "Ebben a leckében szó lesz:\n",
    "- Mi az a függvényhívás, és mire használható\n",
    "- Hogyan hozhatunk létre függvényhívást az Azure OpenAI segítségével\n",
    "- Hogyan illeszthetünk be függvényhívást egy alkalmazásba\n",
    "\n",
    "## Tanulási célok\n",
    "\n",
    "A lecke elvégzése után tudni fogod és megérted:\n",
    "\n",
    "- Miért érdemes függvényhívást használni\n",
    "- Hogyan állítsd be a Function Call-t az Azure Open AI Szolgáltatásban\n",
    "- Hogyan tervezhetsz hatékony függvényhívásokat az alkalmazásodhoz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A függvényhívások megértése\n",
    "\n",
    "Ebben a leckében egy olyan funkciót szeretnénk fejleszteni oktatási startupunk számára, amely lehetővé teszi a felhasználók számára, hogy egy chatbot segítségével technikai kurzusokat találjanak. Olyan tanfolyamokat fogunk ajánlani, amelyek illeszkednek a felhasználó tudásszintjéhez, jelenlegi szerepköréhez és az őt érdeklő technológiához.\n",
    "\n",
    "Ehhez a következőket fogjuk kombinálni:\n",
    " - `Azure Open AI` a felhasználói chatélmény megteremtéséhez\n",
    " - `Microsoft Learn Catalog API` a felhasználó igényei alapján történő kurzuskereséshez\n",
    " - `Function Calling`, amellyel a felhasználó lekérdezését egy függvényhez továbbítjuk, hogy API-hívást indítsunk\n",
    "\n",
    "Kezdésként nézzük meg, miért is érdemes egyáltalán függvényhívást használni:\n",
    "\n",
    "print(\"Üzenetek a következő kérésben:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # új választ kérünk a GPT-től, ahol már látja a függvény válaszát is\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miért hasznos a Function Calling\n",
    "\n",
    "Ha már elvégeztél bármelyik másik leckét ebben a kurzusban, valószínűleg már érted, milyen erőteljesek a Nagy Nyelvi Modellek (LLM-ek). Remélhetőleg azt is látod, hogy vannak korlátaik.\n",
    "\n",
    "A Function Calling az Azure Open AI Service egyik funkciója, amely a következő korlátokat segít áthidalni:\n",
    "1) Következetes válaszformátum\n",
    "2) Annak lehetősége, hogy egy alkalmazás más forrásaiból származó adatokat is felhasználjunk egy chatben\n",
    "\n",
    "A function calling előtt az LLM-ek válaszai strukturálatlanok és következetlenek voltak. A fejlesztőknek bonyolult ellenőrző kódokat kellett írniuk, hogy minden változatot kezelni tudjanak.\n",
    "\n",
    "A felhasználók nem kaphattak olyan válaszokat, mint például: „Milyen most az időjárás Stockholmban?”. Ennek oka, hogy a modellek csak a tanításuk idejéig ismert adatokkal dolgoztak.\n",
    "\n",
    "Nézzük meg az alábbi példát, amely ezt a problémát szemlélteti:\n",
    "\n",
    "Tegyük fel, hogy szeretnénk egy adatbázist létrehozni a diákok adataival, hogy a megfelelő kurzust tudjuk ajánlani nekik. Az alábbiakban két diák leírását látjuk, amelyek nagyon hasonló adatokat tartalmaznak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ezt el szeretnénk küldeni egy LLM-nek, hogy feldolgozza az adatokat. Később ezt felhasználhatjuk arra, hogy elküldjük egy API-nak vagy eltároljuk egy adatbázisban.\n",
    "\n",
    "Készítsünk két teljesen azonos promptot, amelyekben utasítjuk a LLM-et, hogy milyen információkra vagyunk kíváncsiak:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ezt el szeretnénk küldeni egy LLM-nek, hogy feldolgozza a termékünk szempontjából fontos részeket. Így két azonos promptot tudunk létrehozni, hogy utasítsuk az LLM-et:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miután létrehoztuk ezt a két promptot, elküldjük őket az LLM-nek az `openai.ChatCompletion` használatával. A promptot a `messages` változóban tároljuk, és a szerepet `user`-re állítjuk. Ez azért van, hogy egy felhasználó üzenetét utánozzuk, amelyet egy chatbotnak írnak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    "  api_version = \"2023-07-01-preview\"\n",
    "  )\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annak ellenére, hogy a promptok ugyanazok és a leírások is hasonlóak, a `Grades` tulajdonság különböző formátumokban jelenhet meg.\n",
    "\n",
    "Ha többször futtatod a fenti cellát, a formátum lehet `3.7` vagy `3.7 GPA` is.\n",
    "\n",
    "Ez azért van, mert az LLM a szöveges prompt formájában kapja meg a strukturálatlan adatokat, és ugyanígy strukturálatlan adatokat is ad vissza. Szükségünk van egy strukturált formátumra, hogy tudjuk, mire számíthatunk, amikor tároljuk vagy használjuk ezeket az adatokat.\n",
    "\n",
    "A funkcióhívás használatával biztosíthatjuk, hogy strukturált adatokat kapjunk vissza. A funkcióhívás során az LLM valójában nem hív meg vagy futtat le semmilyen függvényt. Ehelyett létrehozunk egy struktúrát, amelyet az LLM-nek követnie kell a válaszaiban. Ezeket a strukturált válaszokat aztán arra használjuk, hogy eldöntsük, melyik függvényt futtassuk az alkalmazásainkban.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Függvényhívási folyamatábra](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.hu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcióhívások felhasználási esetei\n",
    "\n",
    "**Külső eszközök meghívása**  \n",
    "A csevegőrobotok kiválóan válaszolnak a felhasználók kérdéseire. Funkcióhívások segítségével a chatbotok a felhasználók üzeneteit felhasználva bizonyos feladatokat is el tudnak végezni. Például egy diák megkérheti a chatbotot: „Küldj e-mailt az oktatómnak, hogy több segítségre van szükségem ebben a témában.” Ilyenkor meghívható a `send_email(to: string, body: string)` nevű függvény.\n",
    "\n",
    "**API- vagy adatbázis-lekérdezések létrehozása**  \n",
    "A felhasználók természetes nyelven kereshetnek információkat, amelyeket a rendszer formázott lekérdezéssé vagy API-kéréssé alakít. Például egy tanár megkérdezheti: „Kik azok a diákok, akik befejezték az utolsó feladatot?”, ami meghívhat egy `get_completed(student_name: string, assignment: int, current_status: string)` nevű függvényt.\n",
    "\n",
    "**Strukturált adatok létrehozása**  \n",
    "A felhasználók egy szövegrészletből vagy CSV-ből a LLM segítségével kiemelhetik a legfontosabb információkat. Például egy diák egy Wikipédia-cikket a békemegállapodásokról átalakíthat AI tanulókártyákká. Ezt meg lehet oldani a `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)` nevű függvény használatával.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Első függvényhívás létrehozása\n",
    "\n",
    "A függvényhívás létrehozása 3 fő lépésből áll:\n",
    "1. Meghívod a Chat Completions API-t a függvényeid listájával és egy felhasználói üzenettel\n",
    "2. Elolvasod a modell válaszát, hogy végrehajts egy műveletet, például egy függvény vagy API hívást\n",
    "3. Ismét meghívod a Chat Completions API-t a függvényed válaszával, hogy ezt az információt felhasználva választ adj a felhasználónak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Egy függvényhívás folyamata](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.hu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Egy függvényhívás elemei\n",
    "\n",
    "#### Felhasználói bemenet\n",
    "\n",
    "Az első lépés, hogy létrehozzuk a felhasználói üzenetet. Ezt dinamikusan megadhatjuk például egy szövegmező értékével, vagy itt is beállíthatunk egy értéket. Ha most dolgozol először a Chat Completions API-val, meg kell határoznunk az üzenet `role` (szerep) és `content` (tartalom) mezőit.\n",
    "\n",
    "A `role` lehet `system` (szabályok létrehozása), `assistant` (a modell) vagy `user` (a végfelhasználó). Függvényhívás esetén ezt `user`-re állítjuk, és megadunk egy példa kérdést.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Függvények létrehozása.\n",
    "\n",
    "Most definiálunk egy függvényt és annak paramétereit. Itt csak egyetlen függvényt fogunk használni, amelynek neve `search_courses`, de természetesen több függvényt is létrehozhatsz.\n",
    "\n",
    "**Fontos**: A függvények bekerülnek a rendszerüzenetbe az LLM számára, és beleszámítanak a rendelkezésre álló tokenek mennyiségébe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definíciók**\n",
    "\n",
    "`name` – Annak a függvénynek a neve, amelyet meg szeretnénk hívni.\n",
    "\n",
    "`description` – Ez a leírás arról, hogyan működik a függvény. Itt fontos, hogy pontosak és egyértelműek legyünk.\n",
    "\n",
    "`parameters` – Azoknak az értékeknek és formátumoknak a listája, amelyeket szeretnénk, hogy a modell válaszként előállítson.\n",
    "\n",
    "\n",
    "`type` – Az adattípus, amelyben a tulajdonságok tárolva lesznek.\n",
    "\n",
    "`properties` – Azoknak a konkrét értékeknek a listája, amelyeket a modell a válaszában használni fog.\n",
    "\n",
    "\n",
    "`name` – Annak a tulajdonságnak a neve, amelyet a modell a formázott válaszában használni fog.\n",
    "\n",
    "`type` – Ennek a tulajdonságnak az adattípusa.\n",
    "\n",
    "`description` – Az adott tulajdonság leírása.\n",
    "\n",
    "\n",
    "**Opcionális**\n",
    "\n",
    "`required` – Szükséges tulajdonság ahhoz, hogy a függvényhívás végrehajtható legyen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A függvény meghívása\n",
    "Miután definiáltuk a függvényt, most bele kell foglalnunk azt a Chat Completion API hívásába. Ezt úgy tesszük meg, hogy hozzáadjuk a `functions` paramétert a kéréshez. Ebben az esetben `functions=functions`.\n",
    "\n",
    "Van lehetőség arra is, hogy a `function_call` értékét `auto`-ra állítsuk. Ez azt jelenti, hogy az LLM dönti el, melyik függvényt kell meghívni a felhasználói üzenet alapján, nem pedig mi rendeljük hozzá.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most nézzük meg a választ, és nézzük meg, hogyan van formázva:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Láthatod, hogy a függvény neve meg van adva, és a felhasználói üzenet alapján a LLM képes volt megtalálni az adatokat, hogy kitöltse a függvény argumentumait.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Funkcióhívások integrálása egy alkalmazásba.\n",
    "\n",
    "Miután leteszteltük az LLM által formázott választ, most már beépíthetjük ezt az alkalmazásunkba.\n",
    "\n",
    "### A folyamat kezelése\n",
    "\n",
    "Ahhoz, hogy ezt beépítsük az alkalmazásunkba, kövessük az alábbi lépéseket:\n",
    "\n",
    "Először hívjuk meg az Open AI szolgáltatásait, és tároljuk az üzenetet egy `response_message` nevű változóban.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most definiáljuk azt a függvényt, amely meghívja a Microsoft Learn API-t, hogy lekérje a tanfolyamok listáját:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legjobb gyakorlatként először megnézzük, hogy a modell szeretne-e meghívni egy függvényt. Ezután létrehozunk egy elérhető függvényt, és összepárosítjuk azzal, amelyet a modell hívni szeretne.\n",
    "Ezután a függvény argumentumait hozzárendeljük az LLM-től kapott argumentumokhoz.\n",
    "\n",
    "Végül hozzáfűzzük a függvényhívás üzenetet és azokat az értékeket, amelyeket a `search_courses` üzenet visszaadott. Ez minden szükséges információt megad az LLM számára ahhoz, hogy természetes nyelven válaszoljon a felhasználónak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kód kihívás\n",
    "\n",
    "Szép munka! Ha szeretnéd tovább bővíteni az ismereteidet az Azure Open AI Function Calling témakörében, próbáld ki ezt: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst  \n",
    " - Adj hozzá több paramétert a függvényhez, amelyek segíthetnek a tanulóknak még több tanfolyamot találni. Az elérhető API paramétereket itt találod:  \n",
    " - Hozz létre egy másik függvényhívást, amely több információt kér be a tanulótól, például az anyanyelvét  \n",
    " - Készíts hibakezelést arra az esetre, ha a függvényhívás és/vagy az API hívás nem ad vissza megfelelő tanfolyamokat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Jogi nyilatkozat**:  \nEz a dokumentum AI fordítási szolgáltatás, a [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével készült. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti, eredeti nyelvű dokumentum tekintendő hiteles forrásnak. Kritikus információk esetén javasoljuk a professzionális, emberi fordítást. Nem vállalunk felelősséget a fordítás használatából eredő félreértésekért vagy félremagyarázásokért.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "2277587ff6cb5c40437e18d61fc2e239",
   "translation_date": "2025-08-25T20:27:55+00:00",
   "source_file": "11-integrating-with-function-calling/python/aoai-assignment.ipynb",
   "language_code": "hu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}