<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df98b2c59f87d8543135301e87969f70",
  "translation_date": "2025-07-09T16:50:16+00:00",
  "source_file": "15-rag-and-vector-databases/data/own_framework.md",
  "language_code": "hu"
}
-->
# Bevezet√©s a neur√°lis h√°l√≥zatokba. T√∂bbr√©teg≈± perceptron

Az el≈ëz≈ë r√©szben megismerted a legegyszer≈±bb neur√°lis h√°l√≥zati modellt ‚Äì az egyr√©teg≈± perceptront, ami egy line√°ris k√©toszt√°lyos oszt√°lyoz√≥ modell.

Ebben a r√©szben ezt a modellt egy rugalmasabb keretrendszerr√© b≈ëv√≠tj√ºk, amely lehet≈ëv√© teszi sz√°munkra, hogy:

* v√©gezz√ºnk **t√∂bboszt√°lyos oszt√°lyoz√°st** a k√©toszt√°lyos mellett
* oldjunk meg **regresszi√≥s probl√©m√°kat** az oszt√°lyoz√°s mellett
* sz√©tv√°lasszuk azokat az oszt√°lyokat, amelyek nem line√°risan elv√°laszthat√≥k

Emellett saj√°t modul√°ris keretrendszert is fejleszt√ºnk Pythonban, amely lehet≈ëv√© teszi k√ºl√∂nb√∂z≈ë neur√°lis h√°l√≥zati architekt√∫r√°k fel√©p√≠t√©s√©t.

## A g√©pi tanul√°s formaliz√°l√°sa

Kezdj√ºk a g√©pi tanul√°s probl√©m√°j√°nak formaliz√°l√°s√°val. Tegy√ºk fel, hogy van egy tan√≠t√≥ adatb√°zisunk **X** c√≠mk√©kkel **Y**, √©s egy olyan modellt *f* kell √©p√≠ten√ºnk, amely a lehet≈ë legpontosabb el≈ërejelz√©seket adja. Az el≈ërejelz√©sek min≈ës√©g√©t a **vesztes√©gf√ºggv√©ny** ‚Ñí m√©ri. A k√∂vetkez≈ë vesztes√©gf√ºggv√©nyeket haszn√°ljuk gyakran:

* Regresszi√≥s probl√©m√°n√°l, amikor sz√°mot kell el≈ëre jelezni, haszn√°lhatjuk az **abszol√∫t hib√°t** ‚àë<sub>i</sub>|f(x<sup>(i)</sup>)-y<sup>(i)</sup>|, vagy a **n√©gyzetes hib√°t** ‚àë<sub>i</sub>(f(x<sup>(i)</sup>)-y<sup>(i)</sup>)<sup>2</sup>
* Oszt√°lyoz√°sn√°l haszn√°ljuk a **0-1 vesztes√©get** (ami l√©nyeg√©ben a modell **pontoss√°g√°val** egyezik meg), vagy a **logisztikus vesztes√©get**.

Az egyr√©teg≈± perceptron eset√©n az *f* f√ºggv√©nyt line√°ris f√ºggv√©nyk√©nt defini√°ltuk: *f(x)=wx+b* (itt *w* a s√∫lym√°trix, *x* a bemeneti jellemz≈ëk vektora, √©s *b* az eltol√°svektor). K√ºl√∂nb√∂z≈ë neur√°lis h√°l√≥zati architekt√∫r√°kn√°l ez a f√ºggv√©ny bonyolultabb form√°t √∂lthet.

> Oszt√°lyoz√°s eset√©n gyakran k√≠v√°natos, hogy a h√°l√≥zat kimenete az adott oszt√°lyok val√≥sz√≠n≈±s√©ge legyen. Az √°ltal√°nos sz√°mokat val√≥sz√≠n≈±s√©gekk√© alak√≠t√°shoz (p√©ld√°ul a kimenet normaliz√°l√°s√°hoz) gyakran haszn√°ljuk a **softmax** f√ºggv√©nyt œÉ, √≠gy az *f* f√ºggv√©ny *f(x)=œÉ(wx+b)* lesz.

Az *f* defin√≠ci√≥j√°ban *w* √©s *b* az √∫n. **param√©terek** Œ∏=‚ü®*w,b*‚ü©. Adott az adatb√°zis ‚ü®**X**,**Y**‚ü©, kisz√°m√≠thatjuk az √∂sszes√≠tett hib√°t az eg√©sz adathalmazon a param√©terek Œ∏ f√ºggv√©ny√©ben.

> ‚úÖ **A neur√°lis h√°l√≥zat tan√≠t√°s√°nak c√©lja, hogy a param√©terek v√°ltoztat√°s√°val minimaliz√°ljuk a hib√°t.**

## Gradiens cs√∂kken√©ses optimaliz√°ci√≥

Ismert m√≥dszer a f√ºggv√©nyoptimaliz√°l√°sra a **gradiens cs√∂kken√©s**. Az √∂tlet az, hogy kisz√°moljuk a vesztes√©gf√ºggv√©ny deriv√°ltj√°t (t√∂bbdimenzi√≥s esetben **gradiens√©t**) a param√©terek szerint, √©s √∫gy v√°ltoztatjuk a param√©tereket, hogy a hiba cs√∂kkenjen. Ezt a k√∂vetkez≈ëk√©ppen formaliz√°lhatjuk:

* Inicializ√°ljuk a param√©tereket v√©letlenszer≈± √©rt√©kekkel w<sup>(0)</sup>, b<sup>(0)</sup>
* Ism√©telj√ºk a k√∂vetkez≈ë l√©p√©st sokszor:
    - w<sup>(i+1)</sup> = w<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇw
    - b<sup>(i+1)</sup> = b<sup>(i)</sup>-Œ∑‚àÇ‚Ñí/‚àÇb

A tan√≠t√°s sor√°n az optimaliz√°ci√≥s l√©p√©seket az eg√©sz adathalmaz figyelembev√©tel√©vel kellene kisz√°molni (eml√©kezz√ºnk, hogy a vesztes√©g az √∂sszes tan√≠t√≥ minta √∂sszegz√©se). Azonban a gyakorlatban az adathalmazb√≥l kis r√©szeket, √∫n. **minicsomagokat** (minibatch-eket) vesz√ºnk, √©s ezek alapj√°n sz√°moljuk a gradiens √©rt√©keket. Mivel a r√©szhalmaz minden alkalommal v√©letlenszer≈±en ker√ºl kiv√°laszt√°sra, ezt a m√≥dszert **sztochasztikus gradiens cs√∂kken√©snek** (SGD) nevezz√ºk.

## T√∂bbr√©teg≈± perceptronok √©s visszaterjeszt√©s

Az egyr√©teg≈± h√°l√≥zat, ahogy l√°ttuk, k√©pes line√°risan elv√°laszthat√≥ oszt√°lyokat oszt√°lyozni. Egy gazdagabb modell fel√©p√≠t√©s√©hez t√∂bb r√©teget kapcsolhatunk √∂ssze. Matematikailag ez azt jelenti, hogy az *f* f√ºggv√©ny bonyolultabb form√°t √∂lt, √©s t√∂bb l√©p√©sben sz√°moljuk ki:

* z<sub>1</sub>=w<sub>1</sub>x+b<sub>1</sub>
* z<sub>2</sub>=w<sub>2</sub>Œ±(z<sub>1</sub>)+b<sub>2</sub>
* f = œÉ(z<sub>2</sub>)

Itt Œ± egy **nemline√°ris aktiv√°ci√≥s f√ºggv√©ny**, œÉ a softmax f√ºggv√©ny, √©s a param√©terek Œ∏=<*w<sub>1</sub>,b<sub>1</sub>,w<sub>2</sub>,b<sub>2</sub>*>.

A gradiens cs√∂kken√©s algoritmusa ugyanaz marad, de a gradiens kisz√°m√≠t√°sa bonyolultabb√° v√°lik. A l√°ncszab√°ly seg√≠ts√©g√©vel a deriv√°ltakat √≠gy sz√°molhatjuk:

* ‚àÇ‚Ñí/‚àÇw<sub>2</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇw<sub>2</sub>)
* ‚àÇ‚Ñí/‚àÇw<sub>1</sub> = (‚àÇ‚Ñí/‚àÇœÉ)(‚àÇœÉ/‚àÇz<sub>2</sub>)(‚àÇz<sub>2</sub>/‚àÇŒ±)(‚àÇŒ±/‚àÇz<sub>1</sub>)(‚àÇz<sub>1</sub>/‚àÇw<sub>1</sub>)

> ‚úÖ A l√°ncszab√°lyt haszn√°ljuk a vesztes√©gf√ºggv√©ny param√©terek szerinti deriv√°ltjainak kisz√°m√≠t√°s√°hoz.

Fontos megjegyezni, hogy ezeknek a kifejez√©seknek a bal sz√©ls≈ë r√©sze azonos, √≠gy hat√©konyan sz√°molhatjuk a deriv√°ltakat a vesztes√©gf√ºggv√©nyt≈ël kiindulva, "visszafel√©" haladva a sz√°m√≠t√°si gr√°fon. Ez√©rt a t√∂bbr√©teg≈± perceptron tan√≠t√°si m√≥dszer√©t **visszaterjeszt√©snek** vagy 'backprop'-nak nevezz√ºk.

> TODO: k√©p forr√°sa

> ‚úÖ A visszaterjeszt√©st r√©szletesebben is bemutatjuk a jegyzetf√ºzet√ºnkben.

## √ñsszefoglal√°s

Ebben a leck√©ben saj√°t neur√°lis h√°l√≥zati k√∂nyvt√°rat √©p√≠tett√ºnk, √©s egy egyszer≈± k√©tdimenzi√≥s oszt√°lyoz√°si feladaton haszn√°ltuk.

## üöÄ Kih√≠v√°s

A mell√©kelt jegyzetf√ºzetben megval√≥s√≠tod saj√°t keretrendszeredet t√∂bbr√©teg≈± perceptronok √©p√≠t√©s√©re √©s tan√≠t√°s√°ra. R√©szletesen megismerheted, hogyan m≈±k√∂dnek a modern neur√°lis h√°l√≥zatok.

Folytasd az OwnFramework jegyzetf√ºzettel, √©s dolgozd v√©gig.

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

A visszaterjeszt√©s egy gyakori algoritmus az AI √©s ML ter√ºlet√©n, √©rdemes m√©lyebben tanulm√°nyozni.

## Feladat

Ebben a laborban a leck√©ben fel√©p√≠tett keretrendszert haszn√°lva kell megoldanod az MNIST k√©z√≠r√°sos sz√°mfelismer√©si oszt√°lyoz√°si feladatot.

* √ötmutat√≥
* Jegyzetf√ºzet

**Jogi nyilatkozat**:  
Ez a dokumentum az AI ford√≠t√≥ szolg√°ltat√°s, a [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel k√©sz√ºlt. B√°r a pontoss√°gra t√∂reksz√ºnk, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az anyanyelv√©n tekintend≈ë hiteles forr√°snak. Fontos inform√°ci√≥k eset√©n szakmai, emberi ford√≠t√°st javaslunk. Nem v√°llalunk felel≈ëss√©get a ford√≠t√°s haszn√°lat√°b√≥l ered≈ë f√©lre√©rt√©sek√©rt vagy t√©ves √©rtelmez√©sek√©rt.