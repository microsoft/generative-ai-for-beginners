<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:36:07+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "hu"
}
-->
# Neur√°lis H√°l√≥zat Keretrendszerek

Ahogy m√°r megtanultuk, a neur√°lis h√°l√≥zatok hat√©kony tan√≠t√°s√°hoz k√©t dolgot kell tudnunk:

* M≈±veleteket v√©gezni tenzorokon, p√©ld√°ul szorz√°s, √∂sszead√°s, illetve olyan f√ºggv√©nyek kisz√°m√≠t√°sa, mint a sigmoid vagy softmax
* Kisz√°m√≠tani az √∂sszes kifejez√©s gradiens√©t, hogy elv√©gezhess√ºk a gradiens cs√∂kken√©ses optimaliz√°ci√≥t

M√≠g a `numpy` k√∂nyvt√°r az els≈ë r√©szhez elegend≈ë, sz√ºks√©g√ºnk van egy mechanizmusra a gradienssz√°m√≠t√°shoz. A kor√°bbi szakaszban fejlesztett keretrendszer√ºnkben manu√°lisan kellett meg√≠rnunk az √∂sszes deriv√°lt f√ºggv√©nyt a `backward` met√≥dusban, amely a visszaterjeszt√©st v√©gzi. Ide√°lis esetben egy keretrendszer lehet≈ës√©get kell adjon b√°rmilyen √°ltalunk defini√°lt kifejez√©s gradiens√©nek kisz√°m√≠t√°s√°ra.

Egy m√°sik fontos szempont, hogy k√©pesek legy√ºnk sz√°m√≠t√°sokat v√©gezni GPU-n vagy m√°s speci√°lis sz√°m√≠t√≥egys√©geken, p√©ld√°ul TPU-n. A m√©ly neur√°lis h√°l√≥zatok tan√≠t√°sa *rengeteg* sz√°m√≠t√°st ig√©nyel, √©s nagyon fontos, hogy ezeket a sz√°m√≠t√°sokat p√°rhuzamos√≠tani tudjuk GPU-kon.

> ‚úÖ A 'p√°rhuzamos√≠t√°s' azt jelenti, hogy a sz√°m√≠t√°sokat t√∂bb eszk√∂z k√∂z√∂tt osztjuk meg.

Jelenleg a k√©t legn√©pszer≈±bb neur√°lis keretrendszer a TensorFlow √©s a PyTorch. Mindkett≈ë alacsony szint≈± API-t biztos√≠t tenzorok kezel√©s√©re CPU-n √©s GPU-n egyar√°nt. Az alacsony szint≈± API f√∂l√∂tt magasabb szint≈± API is el√©rhet≈ë, ezek a Keras √©s a PyTorch Lightning.

Alacsony szint≈± API | TensorFlow | PyTorch
-------------------|------------|---------
Magas szint≈± API   | Keras      | PyTorch

**Az alacsony szint≈± API-k** mindk√©t keretrendszerben lehet≈ëv√© teszik √∫gynevezett **sz√°m√≠t√°si gr√°fok** √©p√≠t√©s√©t. Ez a gr√°f meghat√°rozza, hogyan sz√°moljuk ki a kimenetet (√°ltal√°ban a vesztes√©gf√ºggv√©nyt) adott bemeneti param√©terekkel, √©s ha el√©rhet≈ë, GPU-n is futtathat√≥. Vannak f√ºggv√©nyek, amelyek k√©pesek differenci√°lni ezt a sz√°m√≠t√°si gr√°fot √©s kisz√°m√≠tani a gradienseket, amelyeket azt√°n a modellparam√©terek optimaliz√°l√°s√°ra haszn√°lhatunk.

**A magas szint≈± API-k** a neur√°lis h√°l√≥zatokat jellemz≈ëen **r√©tegek sorozatak√©nt** kezelik, √©s megk√∂nny√≠tik a legt√∂bb h√°l√≥zat fel√©p√≠t√©s√©t. A modell tan√≠t√°sa √°ltal√°ban az adatok el≈ëk√©sz√≠t√©s√©t, majd a `fit` f√ºggv√©ny megh√≠v√°s√°t jelenti.

A magas szint≈± API lehet≈ëv√© teszi, hogy tipikus neur√°lis h√°l√≥zatokat nagyon gyorsan √©p√≠ts√ºnk an√©lk√ºl, hogy sok r√©szlettel kellene foglalkoznunk. Ugyanakkor az alacsony szint≈± API sokkal nagyobb kontrollt ad a tan√≠t√°si folyamat felett, ez√©rt kutat√°sokban gyakran haszn√°lj√°k, amikor √∫j neur√°lis h√°l√≥zati architekt√∫r√°kkal dolgozunk.

Fontos meg√©rteni, hogy mindk√©t API-t egy√ºtt is haszn√°lhatjuk, p√©ld√°ul saj√°t h√°l√≥zati r√©tegarchitekt√∫r√°t fejleszthet√ºnk alacsony szint≈± API-val, majd ezt beilleszthetj√ºk egy nagyobb, magas szint≈± API-val √©p√≠tett √©s tan√≠tott h√°l√≥zatba. Vagy defini√°lhatunk egy h√°l√≥zatot magas szint≈± API-val r√©tegek sorozatak√©nt, majd saj√°t alacsony szint≈± tan√≠t√°si ciklust haszn√°lhatunk az optimaliz√°ci√≥hoz. Mindk√©t API ugyanazokon az alapelveken nyugszik, √©s √∫gy tervezt√©k, hogy j√≥l m≈±k√∂djenek egy√ºtt.

## Tanul√°s

Ebben a tanfolyamban a tartalom nagy r√©sz√©t mind PyTorch, mind TensorFlow eset√©n k√≠n√°ljuk. V√°laszthatod a neked tetsz≈ë keretrendszert, √©s csak a hozz√° tartoz√≥ jegyzeteket n√©zheted √°t. Ha nem vagy biztos, melyik keretrendszert v√°laszd, olvass el n√©h√°ny internetes vit√°t a **PyTorch vs. TensorFlow** t√©m√°ban. √ârdemes mindkett≈ët megn√©zni, hogy jobban meg√©rtsd ≈ëket.

Ahol csak lehet, egyszer≈±s√©g miatt magas szint≈± API-kat haszn√°lunk. Ugyanakkor fontosnak tartjuk, hogy az alapokt√≥l √©rtsd meg a neur√°lis h√°l√≥zatok m≈±k√∂d√©s√©t, ez√©rt az elej√©n alacsony szint≈± API-val √©s tenzorokkal dolgozunk. Ha viszont gyorsan szeretn√©l haladni, √©s nem akarsz sok id≈ët t√∂lteni ezeknek a r√©szleteknek a tanul√°s√°val, √°tugorhatod ezt, √©s egyb≈ël a magas szint≈± API-s jegyzetekhez mehetsz.

## ‚úçÔ∏è Gyakorlatok: Keretrendszerek

Folytasd a tanul√°st a k√∂vetkez≈ë jegyzetekben:

Alacsony szint≈± API | TensorFlow+Keras jegyzet | PyTorch
--------------------|----------------------------|---------
Magas szint≈± API    | Keras                      | *PyTorch Lightning*

Miut√°n elsaj√°t√≠tottad a keretrendszereket, tekints√ºk √°t az overfitting fogalm√°t.

# Overfitting (T√∫ltanul√°s)

Az overfitting rendk√≠v√ºl fontos fogalom a g√©pi tanul√°sban, √©s nagyon l√©nyeges, hogy helyesen √©rts√ºk meg!

Vegy√ºk az al√°bbi probl√©m√°t, ahol 5 pontot kell k√∂zel√≠ten√ºnk (a grafikonokon `x` jel√∂li ≈ëket):

!linear | overfit
-------------------------|--------------------------
**Line√°ris modell, 2 param√©ter** | **Nemline√°ris modell, 7 param√©ter**
Tanul√°si hiba = 5.3 | Tanul√°si hiba = 0
Valid√°ci√≥s hiba = 5.1 | Valid√°ci√≥s hiba = 20

* Bal oldalon egy j√≥ egyenes vonal k√∂zel√≠t√©st l√°tunk. Mivel a param√©terek sz√°ma megfelel≈ë, a modell j√≥l megragadja a pontok eloszl√°s√°nak l√©nyeg√©t.
* Jobb oldalon a modell t√∫l er≈ës. Mivel csak 5 pont van, de a modellnek 7 param√©tere, k√©pes √∫gy igazodni, hogy √°tmenjen az √∂sszes ponton, √≠gy a tanul√°si hiba 0 lesz. Ez azonban megakad√°lyozza, hogy a modell meg√©rtse az adatok m√∂g√∂tti helyes mint√°zatot, ez√©rt a valid√°ci√≥s hiba nagyon magas.

Nagyon fontos megtal√°lni a megfelel≈ë egyens√∫lyt a modell komplexit√°sa (param√©terek sz√°ma) √©s a tan√≠t√≥ mint√°k sz√°ma k√∂z√∂tt.

## Mi√©rt alakul ki az overfitting?

  * Nem el√©g tan√≠t√≥ adat √°ll rendelkez√©sre
  * T√∫l er≈ës modell
  * T√∫l sok zaj van a bemeneti adatokban

## Hogyan ismerhet≈ë fel az overfitting?

Ahogy a fenti grafikonon is l√°that√≥, az overfittinget nagyon alacsony tanul√°si hiba √©s magas valid√°ci√≥s hiba jellemzi. √Åltal√°ban a tan√≠t√°s sor√°n mind a tanul√°si, mind a valid√°ci√≥s hiba cs√∂kken, majd egy ponton a valid√°ci√≥s hiba meg√°ll a cs√∂kken√©sben, √©s elkezd n≈ëni. Ez az overfitting jele, √©s arra utal, hogy val√≥sz√≠n≈±leg itt kellene le√°ll√≠tani a tan√≠t√°st (vagy legal√°bb k√©sz√≠teni egy pillanatk√©pet a modellr≈ël).

overfitting

## Hogyan el≈ëzhet≈ë meg az overfitting?

Ha √©szleled az overfittinget, a k√∂vetkez≈ëk egyik√©t teheted:

 * N√∂veld a tan√≠t√≥ adatok mennyis√©g√©t
 * Cs√∂kkentsd a modell komplexit√°s√°t
 * Haszn√°lj valamilyen regulariz√°ci√≥s technik√°t, p√©ld√°ul Dropout-ot, amit k√©s≈ëbb t√°rgyalunk

## Overfitting √©s a Bias-Variance kompromisszum

Az overfitting val√≥j√°ban egy √°ltal√°nosabb statisztikai probl√©ma, az √∫gynevezett Bias-Variance kompromisszum esete. Ha megn√©zz√ºk a modell hib√°inak lehets√©ges forr√°sait, k√©t t√≠pust k√ºl√∂nb√∂ztethet√ºnk meg:

* **Bias hib√°k** abb√≥l ad√≥dnak, hogy az algoritmus nem k√©pes helyesen megragadni a tan√≠t√≥ adatok k√∂z√∂tti √∂sszef√ºgg√©st. Ez abb√≥l fakadhat, hogy a modell nem el√©g er≈ës (**alultanul√°s**).
* **Variance hib√°k** abb√≥l erednek, hogy a modell a bemeneti adatok zaj√°t k√∂zel√≠ti, nem pedig a val√≥di √∂sszef√ºgg√©st (**t√∫ltanul√°s**).

A tan√≠t√°s sor√°n a bias hiba cs√∂kken (ahogy a modell megtanulja az adatokat), m√≠g a variance hiba n≈ë. Fontos, hogy a tan√≠t√°st id≈ëben le√°ll√≠tsuk ‚Äì ak√°r manu√°lisan (amikor √©szlelj√ºk az overfittinget), ak√°r automatikusan (regulariz√°ci√≥ bevezet√©s√©vel) ‚Äì, hogy megel≈ëzz√ºk a t√∫lilleszt√©st.

## √ñsszefoglal√°s

Ebben a leck√©ben megismerted a k√©t legn√©pszer≈±bb AI keretrendszer, a TensorFlow √©s a PyTorch k√ºl√∂nb√∂z≈ë API-jainak k√ºl√∂nbs√©geit. Emellett egy nagyon fontos t√©m√°r√≥l, az overfittingr≈ël is tanult√°l.

## üöÄ Feladat

A mell√©kelt jegyzetek alj√°n tal√°lsz 'feladatokat'; dolgozd v√©gig a jegyzeteket, √©s oldd meg a feladatokat.

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Kutat√°sokat v√©gezhetsz a k√∂vetkez≈ë t√©m√°kban:

- TensorFlow
- PyTorch
- Overfitting

Tedd fel magadnak a k√∂vetkez≈ë k√©rd√©seket:

- Mi a k√ºl√∂nbs√©g a TensorFlow √©s a PyTorch k√∂z√∂tt?
- Mi a k√ºl√∂nbs√©g az overfitting √©s az underfitting k√∂z√∂tt?

## H√°zi feladat

Ebben a laborban k√©t oszt√°lyoz√°si probl√©m√°t kell megoldanod egy- √©s t√∂bbr√©teg≈±, teljesen √∂sszekapcsolt h√°l√≥zatokkal PyTorch vagy TensorFlow haszn√°lat√°val.

**Jogi nyilatkozat**:  
Ez a dokumentum az AI ford√≠t√≥ szolg√°ltat√°s, a [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel k√©sz√ºlt. B√°r a pontoss√°gra t√∂reksz√ºnk, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az anyanyelv√©n tekintend≈ë hiteles forr√°snak. Fontos inform√°ci√≥k eset√©n szakmai, emberi ford√≠t√°st javaslunk. Nem v√°llalunk felel≈ëss√©get a ford√≠t√°s haszn√°lat√°b√≥l ered≈ë f√©lre√©rt√©sek√©rt vagy t√©ves √©rtelmez√©sek√©rt.