{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 7. fejezet: Chat alkalmazások készítése\n",
    "## Github Models API Gyorsindítás\n",
    "\n",
    "Ez a jegyzet az [Azure OpenAI Minták Tárolójából](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst) származik, amely olyan jegyzeteket tartalmaz, amelyek hozzáférnek az [Azure OpenAI](notebook-azure-openai.ipynb) szolgáltatásokhoz.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Áttekintés  \n",
    "„A nagy nyelvi modellek olyan függvények, amelyek szöveget alakítanak át szöveggé. Ha megadunk egy szöveges bemenetet, a nagy nyelvi modell megpróbálja megjósolni, mi lesz a következő szöveg”(1). Ez a „gyors kezdés” jegyzetfüzet bevezetést nyújt a felhasználóknak a LLM-ek főbb fogalmaiba, az AML használatához szükséges alapvető csomagokba, egy könnyed bevezetést az utasítások megtervezésébe, valamint néhány rövid példát különböző felhasználási területekre.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Tartalomjegyzék  \n",
    "\n",
    "[Áttekintés](../../../../07-building-chat-applications/python)  \n",
    "[Az OpenAI szolgáltatás használata](../../../../07-building-chat-applications/python)  \n",
    "[1. Az OpenAI szolgáltatás létrehozása](../../../../07-building-chat-applications/python)  \n",
    "[2. Telepítés](../../../../07-building-chat-applications/python)    \n",
    "[3. Hitelesítő adatok](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Felhasználási esetek](../../../../07-building-chat-applications/python)    \n",
    "[1. Szöveg összefoglalása](../../../../07-building-chat-applications/python)  \n",
    "[2. Szöveg osztályozása](../../../../07-building-chat-applications/python)  \n",
    "[3. Új terméknevek generálása](../../../../07-building-chat-applications/python)  \n",
    "[4. Osztályozó finomhangolása](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Hivatkozások](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Készítsd el az első promptodat  \n",
    "Ez a rövid gyakorlat alapvető bevezetést nyújt ahhoz, hogyan küldhetsz be promptokat egy modellnek a Github Models-ben egy egyszerű feladathoz, például az \"összefoglaláshoz\".\n",
    "\n",
    "**Lépések**:  \n",
    "1. Telepítsd az `azure-ai-inference` könyvtárat a python környezetedbe, ha még nem tetted meg.  \n",
    "2. Töltsd be a szokásos segédkönyvtárakat, és állítsd be a Github Models hitelesítést.  \n",
    "3. Válassz egy modellt a feladatodhoz  \n",
    "4. Készíts egy egyszerű promptot a modell számára  \n",
    "5. Küldd el a kérésedet a modell API-nak!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. Telepítse az `azure-ai-inference` csomagot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. A megfelelő modell kiválasztása  \n",
    "A GPT-3.5-turbo vagy a GPT-4 modellek képesek megérteni és előállítani természetes nyelvet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Prompttervezés  \n",
    "\n",
    "„A nagy nyelvi modellek varázsa abban rejlik, hogy amikor hatalmas mennyiségű szövegen tanulják meg minimalizálni az előrejelzési hibát, a modellek végül olyan fogalmakat sajátítanak el, amelyek hasznosak ezekhez az előrejelzésekhez. Például megtanulják az olyan fogalmakat, mint”(1):\n",
    "\n",
    "* hogyan kell helyesen írni\n",
    "* hogyan működik a nyelvtan\n",
    "* hogyan lehet átfogalmazni\n",
    "* hogyan kell kérdésekre válaszolni\n",
    "* hogyan kell beszélgetést folytatni\n",
    "* hogyan kell több nyelven írni\n",
    "* hogyan kell programozni\n",
    "* stb.\n",
    "\n",
    "#### Hogyan lehet irányítani egy nagy nyelvi modellt  \n",
    "„A nagy nyelvi modellek bemenetei közül messze a legnagyobb hatással a szöveges prompt van”(1).\n",
    "\n",
    "A nagy nyelvi modelleket többféleképpen is lehet promptolni, hogy választ adjanak:\n",
    "\n",
    "Instrukció: Mondd el a modellnek, mit szeretnél\n",
    "Kiegészítés: Késztessük a modellt arra, hogy befejezze azt, amit elkezdtünk\n",
    "Demonstráció: Mutassuk meg a modellnek, mit szeretnénk, akár:\n",
    "Néhány példával a promptban\n",
    "Több száz vagy ezer példával egy finomhangolt tanító adathalmazban\n",
    "\n",
    "\n",
    "\n",
    "#### Három alapvető irányelv van a promptok készítéséhez:\n",
    "\n",
    "**Mutasd meg és mondd el**. Tedd egyértelművé, mit szeretnél, akár utasításokkal, példákkal, vagy ezek kombinációjával. Ha azt szeretnéd, hogy a modell egy listát ábécé sorrendbe rendezzen, vagy egy bekezdést érzelem szerint osztályozzon, mutasd meg neki, hogy ezt várod el.\n",
    "\n",
    "**Adj minőségi adatokat**. Ha osztályozót szeretnél készíteni, vagy azt akarod, hogy a modell kövessen egy mintát, gondoskodj róla, hogy elegendő példa álljon rendelkezésre. Mindig ellenőrizd a példáidat — a modell általában elég okos ahhoz, hogy átlásson az alapvető helyesírási hibákon és választ adjon, de előfordulhat, hogy szándékosnak veszi ezeket, és ez befolyásolhatja a választ.\n",
    "\n",
    "**Ellenőrizd a beállításokat.** A temperature és top_p beállítások szabályozzák, mennyire determinisztikus a modell válasza. Ha olyan választ vársz, amire csak egy helyes megoldás van, érdemes ezeket alacsonyabbra állítani. Ha változatosabb válaszokat szeretnél, akkor magasabbra is állíthatod. A leggyakoribb hiba ezekkel a beállításokkal kapcsolatban, hogy sokan azt hiszik, ezek a „leleményesség” vagy „kreativitás” szabályozói.\n",
    "\n",
    "\n",
    "Forrás: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Szöveg összefoglalása  \n",
    "#### Feladat  \n",
    "Foglalja össze a szöveget úgy, hogy a végére hozzáad egy 'tl;dr:'-t. Figyelje meg, hogy a modell képes számos feladatot elvégezni további utasítások nélkül is. Kísérletezhet részletesebb promptokkal is, hogy módosítsa a modell viselkedését és testre szabja az összefoglalást(3).  \n",
    "\n",
    "A közelmúltban végzett kutatások jelentős előrelépést mutattak számos NLP feladatban és mérőszámban, amikor egy nagy szövegkorpusz előzetes betanítását követően egy adott feladatra finomhangolást végeztek. Bár ezek a módszerek általában feladatfüggetlenek az architektúra szempontjából, mégis szükségük van több ezer vagy akár több tízezer példából álló, feladatspecifikus finomhangolási adathalmazokra. Ezzel szemben az emberek általában már néhány példából vagy egyszerű utasításokból is képesek új nyelvi feladatokat megoldani – ez az, amivel a jelenlegi NLP rendszerek még mindig nehezen birkóznak meg. Itt bemutatjuk, hogy a nyelvi modellek méretének növelése jelentősen javítja a feladatfüggetlen, kevés példán alapuló teljesítményt, néha még a korábbi csúcsteljesítményű finomhangolási megközelítésekkel is versenyképes szintet ér el.\n",
    "\n",
    "\n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Gyakorlatok különböző felhasználási esetekre  \n",
    "1. Szöveg összefoglalása  \n",
    "2. Szöveg osztályozása  \n",
    "3. Új terméknevek generálása\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Szöveg osztályozása  \n",
    "#### Feladat  \n",
    "Oszd be az elemeket olyan kategóriákba, amelyeket a lekérdezés során adunk meg. Az alábbi példában mind a kategóriákat, mind az osztályozandó szöveget megadjuk a promptban (*playground_reference).\n",
    "\n",
    "Ügyfél érdeklődése: Üdvözlöm, az egyik billentyű a laptopom billentyűzetén nemrég eltört, és szükségem lenne egy cserére:\n",
    "\n",
    "Osztályozott kategória:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Új terméknevek generálása\n",
    "#### Kihívás\n",
    "Készíts termékneveket példaszavakból! Az utasításban megadjuk, hogy milyen termékhez kell neveket alkotni. Egy hasonló példát is mellékelünk, hogy lásd, milyen mintát várunk. A temperature értéket magasra állítottuk, hogy a válaszok véletlenszerűbbek és kreatívabbak legyenek.\n",
    "\n",
    "Termékleírás: Otthoni turmixkészítő\n",
    "Kulcsszavak: gyors, egészséges, kompakt.\n",
    "Terméknevek: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Termékleírás: Egy pár cipő, amely bármilyen lábmérethez igazodik.\n",
    "Kulcsszavak: alkalmazkodó, illeszkedő, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Hivatkozások  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [OpenAI Studio példák](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Legjobb gyakorlatok a GPT-3 finomhangolásához szöveg osztályozására](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# További segítségért  \n",
    "[OpenAI Kereskedelmi Csapat](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Közreműködők\n",
    "* [Chew-Yean Yam](https://www.linkedin.com/in/cyyam/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Jogi nyilatkozat**:  \nEz a dokumentum AI fordítási szolgáltatás, a [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével készült. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti, eredeti nyelvű dokumentum tekintendő hiteles forrásnak. Kritikus információk esetén javasoljuk a professzionális, emberi fordítást. Nem vállalunk felelősséget a fordítás használatából eredő félreértésekért vagy téves értelmezésekért.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "3eeb8e5cad61b52a8366f6259a53ed49",
   "translation_date": "2025-08-25T17:47:18+00:00",
   "source_file": "07-building-chat-applications/python/githubmodels-assignment.ipynb",
   "language_code": "hu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}