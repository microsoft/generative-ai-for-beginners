{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Építés a Meta család modelljeivel\n",
    "\n",
    "## Bevezetés\n",
    "\n",
    "Ebben a leckében szó lesz:\n",
    "\n",
    "- A Meta család két fő modelljének, a Llama 3.1-nek és a Llama 3.2-nek a bemutatása\n",
    "- Annak megértése, hogy melyik modellt milyen felhasználási esetekhez és helyzetekhez érdemes választani\n",
    "- Kódrészlet, amely bemutatja az egyes modellek egyedi jellemzőit\n",
    "\n",
    "## A Meta család modelljei\n",
    "\n",
    "Ebben a leckében a Meta család, vagyis a \"Llama csorda\" két modelljét vizsgáljuk meg: a Llama 3.1-et és a Llama 3.2-t.\n",
    "\n",
    "Ezek a modellek többféle változatban érhetők el, és megtalálhatók a Github Model piactéren. További információkat itt találsz arról, hogyan lehet Github Modellekkel [AI modelleket prototípusozni](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Modellváltozatok:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Megjegyzés: A Llama 3 is elérhető a Github Modellek között, de ebben a leckében nem térünk ki rá*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "A 405 milliárd paraméterrel rendelkező Llama 3.1 az open source LLM kategóriába tartozik.\n",
    "\n",
    "Ez a modell a korábbi Llama 3 továbbfejlesztett változata, amely a következőket kínálja:\n",
    "\n",
    "- Nagyobb kontextusablak – 128k token az eddigi 8k helyett\n",
    "- Nagyobb maximális kimeneti token – 4096 az eddigi 2048 helyett\n",
    "- Jobb többnyelvű támogatás – a tanító tokenek számának növekedése miatt\n",
    "\n",
    "Ezek lehetővé teszik, hogy a Llama 3.1 összetettebb felhasználási eseteket is kezeljen GenAI alkalmazások fejlesztésekor, például:\n",
    "- Natív függvényhívás – lehetőség külső eszközök és függvények meghívására az LLM munkafolyamatán kívül\n",
    "- Jobb RAG teljesítmény – a nagyobb kontextusablaknak köszönhetően\n",
    "- Szintetikus adatok generálása – hatékony adatok létrehozása olyan feladatokhoz, mint például a finomhangolás\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natív függvényhívás\n",
    "\n",
    "A Llama 3.1-et finomhangolták, hogy hatékonyabban tudjon függvényeket vagy eszközöket meghívni. Két beépített eszközzel is rendelkezik, amelyeket a modell képes felismerni, ha a felhasználó promptja alapján szükség van rájuk. Ezek az eszközök:\n",
    "\n",
    "- **Brave Search** – Friss információk, például időjárás lekérdezésére használható webes kereséssel\n",
    "- **Wolfram Alpha** – Összetettebb matematikai számításokhoz használható, így nem kell saját függvényeket írni\n",
    "\n",
    "Saját egyedi eszközöket is létrehozhatsz, amelyeket az LLM meghívhat.\n",
    "\n",
    "Az alábbi kódrészletben:\n",
    "\n",
    "- Meghatározzuk az elérhető eszközöket (brave_search, wolfram_alpha) a rendszer promptban.\n",
    "- Egy felhasználói promptot küldünk, amely egy adott város időjárásáról érdeklődik.\n",
    "- Az LLM válaszként egy eszközhívást fog küldeni a Brave Search eszköznek, ami így fog kinézni: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Megjegyzés: Ez a példa csak az eszközhívást mutatja be, ha az eredményeket is szeretnéd megkapni, akkor ingyenes fiókot kell létrehoznod a Brave API oldalán, és magát a függvényt is definiálnod kell.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Annak ellenére, hogy Llama 3.1 egy LLM, van egy korlátja: a multimodalitás. Ez azt jelenti, hogy nem képes különböző típusú bemeneteket, például képeket használni promptként, és ezekre válaszolni. Ez a képesség az egyik fő újdonsága a Llama 3.2-nek. Ezek a funkciók a következőket is tartalmazzák:\n",
    "\n",
    "- Multimodalitás – képes szöveges és képes promptokat is értékelni\n",
    "- Kis és közepes méretű változatok (11B és 90B) – ez rugalmas telepítési lehetőségeket biztosít,\n",
    "- Csak szöveges változatok (1B és 3B) – ez lehetővé teszi, hogy a modellt edge / mobil eszközökön is futtassuk, alacsony késleltetéssel\n",
    "\n",
    "A multimodális támogatás nagy előrelépést jelent a nyílt forráskódú modellek világában. Az alábbi kódrészlet egy képet és egy szöveges promptot is felhasznál, hogy elemzést kapjunk a képről a Llama 3.2 90B-től.\n",
    "\n",
    "### Multimodális támogatás a Llama 3.2-vel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tanulás itt nem ér véget, folytasd az utat\n",
    "\n",
    "Miután befejezted ezt a leckét, nézd meg a [Generatív MI tanulási gyűjteményünket](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), hogy tovább fejleszd a generatív MI-vel kapcsolatos tudásodat!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Jogi nyilatkozat**:  \nEz a dokumentum AI fordítási szolgáltatás, a [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével készült. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti, eredeti nyelvű dokumentum tekintendő hiteles forrásnak. Kritikus információk esetén javasoljuk a professzionális, emberi fordítást. Nem vállalunk felelősséget a fordítás használatából eredő félreértésekért vagy téves értelmezésekért.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:47:40+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "hu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}