{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A következő jegyzetfüzetek futtatásához, ha még nem tetted meg, be kell állítanod az openai kulcsot a .env fájlban `OPENAI_API_KEY` néven.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\")\n",
    "assert API_KEY, \"ERROR: OpenAI Key is missing\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY\n",
    "    )\n",
    "\n",
    "model = 'text-embedding-ada-002'\n",
    "\n",
    "SIMILARITIES_RESULTS_THRESHOLD = 0.75\n",
    "DATASET_NAME = \"../embedding_index_3m.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ezután betöltjük az Embedding Indexet egy Pandas Dataframe-be. Az Embedding Index egy JSON fájlban van tárolva, amelynek neve `embedding_index_3m.json`. Az Embedding Index tartalmazza az egyes YouTube átiratok beágyazásait 2023 októberének végéig.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(source: str) -> pd.core.frame.DataFrame:\n",
    "    # Load the video session index\n",
    "    pd_vectors = pd.read_json(source)\n",
    "    return pd_vectors.drop(columns=[\"text\"], errors=\"ignore\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ezután létrehozunk egy `get_videos` nevű függvényt, amely az Embedding Indexben keres a lekérdezés alapján. A függvény visszaadja az 5 leginkább a lekérdezéshez hasonló videót. A függvény a következőképpen működik:\n",
    "\n",
    "1. Először létrejön egy másolat az Embedding Indexről.\n",
    "2. Ezután az OpenAI Embedding API segítségével kiszámítjuk a lekérdezés Embeddingjét.\n",
    "3. Majd létrehozunk egy új oszlopot az Embedding Indexben `similarity` néven. A `similarity` oszlop a lekérdezés Embeddingje és az egyes videószegmensek Embeddingje közötti koszinusz hasonlóságot tartalmazza.\n",
    "4. Ezután az Embedding Indexet a `similarity` oszlop alapján szűrjük. Csak azok a videók maradnak benne, amelyek koszinusz hasonlósága nagyobb vagy egyenlő 0,75-tel.\n",
    "5. Végül az Embedding Indexet a `similarity` oszlop szerint rendezzük, és visszaadjuk az 5 legjobb videót.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_videos(\n",
    "    query: str, dataset: pd.core.frame.DataFrame, rows: int\n",
    ") -> pd.core.frame.DataFrame:\n",
    "    # create a copy of the dataset\n",
    "    video_vectors = dataset.copy()\n",
    "\n",
    "    # get the embeddings for the query    \n",
    "    query_embeddings = client.embeddings.create(input=query, model=model).data[0].embedding\n",
    "\n",
    "    # create a new column with the calculated similarity for each row\n",
    "    video_vectors[\"similarity\"] = video_vectors[\"ada_v2\"].apply(\n",
    "        lambda x: cosine_similarity(np.array(query_embeddings), np.array(x))\n",
    "    )\n",
    "\n",
    "    # filter the videos by similarity\n",
    "    mask = video_vectors[\"similarity\"] >= SIMILARITIES_RESULTS_THRESHOLD\n",
    "    video_vectors = video_vectors[mask].copy()\n",
    "\n",
    "    # sort the videos by similarity\n",
    "    video_vectors = video_vectors.sort_values(by=\"similarity\", ascending=False).head(\n",
    "        rows\n",
    "    )\n",
    "\n",
    "    # return the top rows\n",
    "    return video_vectors.head(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ez a függvény nagyon egyszerű, csak kiírja a keresési lekérdezés eredményeit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(videos: pd.core.frame.DataFrame, query: str):\n",
    "    def _gen_yt_url(video_id: str, seconds: int) -> str:\n",
    "        \"\"\"convert time in format 00:00:00 to seconds\"\"\"\n",
    "        return f\"https://youtu.be/{video_id}?t={seconds}\"\n",
    "\n",
    "    print(f\"\\nVideos similar to '{query}':\")\n",
    "    for _, row in videos.iterrows():\n",
    "        youtube_url = _gen_yt_url(row[\"videoId\"], row[\"seconds\"])\n",
    "        print(f\" - {row['title']}\")\n",
    "        print(f\"   Summary: {' '.join(row['summary'].split()[:15])}...\")\n",
    "        print(f\"   YouTube: {youtube_url}\")\n",
    "        print(f\"   Similarity: {row['similarity']}\")\n",
    "        print(f\"   Speakers: {row['speaker']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Először az Embedding Index betöltődik egy Pandas Dataframe-be.\n",
    "2. Ezután a felhasználót lekérdezik, hogy adjon meg egy lekérdezést.\n",
    "3. Majd meghívódik a `get_videos` függvény, hogy megkeresse a lekérdezést az Embedding Indexben.\n",
    "4. Végül meghívódik a `display_results` függvény, hogy megjelenítse az eredményeket a felhasználónak.\n",
    "5. Ezután a felhasználót ismét lekérdezik egy újabb lekérdezés megadására. Ez a folyamat addig folytatódik, amíg a felhasználó be nem írja az `exit` parancsot.\n",
    "\n",
    "![](../../../../translated_images/hu/notebook-search.1e320b9c7fcbb0bc.webp)\n",
    "\n",
    "A rendszer lekérdezi, hogy adjon meg egy lekérdezést. Írjon be egy lekérdezést, majd nyomja meg az Entert. Az alkalmazás visszaad egy listát a lekérdezéshez releváns videókról. Az alkalmazás egy linket is visszaad a videó azon helyére, ahol a kérdésre adott válasz található.\n",
    "\n",
    "Íme néhány kipróbálható lekérdezés:\n",
    "\n",
    "- Mi az Azure Machine Learning?\n",
    "- Hogyan működnek a konvolúciós neurális hálózatok?\n",
    "- Mi az a neurális hálózat?\n",
    "- Használhatom a Jupyter Notebookokat az Azure Machine Learninggel?\n",
    "- Mi az az ONNX?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_vectors = load_dataset(DATASET_NAME)\n",
    "\n",
    "# get user query from input\n",
    "while True:\n",
    "    query = input(\"Enter a query: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    videos = get_videos(query, pd_vectors, 5)\n",
    "    display_results(videos, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Jogi nyilatkozat**:\nEzt a dokumentumot az AI fordító szolgáltatás, a [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével fordítottuk le. Bár a pontosságra törekszünk, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az anyanyelvén tekintendő hiteles forrásnak. Fontos információk esetén professzionális emberi fordítást javaslunk. Nem vállalunk felelősséget a fordítás használatából eredő félreértésekért vagy félreértelmezésekért.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "coopTranslator": {
   "original_hash": "afb84920098ad1e6e4ca63ee9a61d9b8",
   "translation_date": "2025-12-19T11:15:58+00:00",
   "source_file": "08-building-search-applications/python/oai-solution.ipynb",
   "language_code": "hu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}