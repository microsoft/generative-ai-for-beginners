<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f3cac698e9eea47dd563633bd82daf8c",
  "translation_date": "2025-07-09T15:23:56+00:00",
  "source_file": "13-securing-ai-applications/README.md",
  "language_code": "br"
}
-->
# Protegendo Suas Aplica√ß√µes de IA Generativa

[![Protegendo Suas Aplica√ß√µes de IA Generativa](../../../translated_images/13-lesson-banner.14103e36b4bbf17398b64ed2b0531f6f2c6549e7f7342f797c40bcae5a11862e.br.png)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## Introdu√ß√£o

Esta li√ß√£o abordar√°:

- Seguran√ßa no contexto de sistemas de IA.
- Riscos e amea√ßas comuns a sistemas de IA.
- M√©todos e considera√ß√µes para proteger sistemas de IA.

## Objetivos de Aprendizagem

Ap√≥s concluir esta li√ß√£o, voc√™ ter√° uma compreens√£o de:

- As amea√ßas e riscos aos sistemas de IA.
- M√©todos e pr√°ticas comuns para proteger sistemas de IA.
- Como a implementa√ß√£o de testes de seguran√ßa pode evitar resultados inesperados e a perda da confian√ßa dos usu√°rios.

## O que significa seguran√ßa no contexto da IA generativa?

√Ä medida que as tecnologias de Intelig√™ncia Artificial (IA) e Aprendizado de M√°quina (ML) moldam cada vez mais nossas vidas, √© fundamental proteger n√£o apenas os dados dos clientes, mas tamb√©m os pr√≥prios sistemas de IA. IA/ML √© cada vez mais usada para apoiar processos decis√≥rios de alto valor em setores onde uma decis√£o errada pode resultar em consequ√™ncias graves.

Aqui est√£o pontos-chave a considerar:

- **Impacto da IA/ML**: IA/ML tem impactos significativos no cotidiano e, por isso, proteg√™-las se tornou essencial.
- **Desafios de Seguran√ßa**: Esse impacto exige aten√ß√£o adequada para proteger produtos baseados em IA contra ataques sofisticados, seja por trolls ou grupos organizados.
- **Problemas Estrat√©gicos**: A ind√∫stria de tecnologia deve abordar proativamente desafios estrat√©gicos para garantir a seguran√ßa a longo prazo dos clientes e dos dados.

Al√©m disso, modelos de Aprendizado de M√°quina t√™m grande dificuldade em distinguir entre entradas maliciosas e dados an√¥malos benignos. Uma parte significativa dos dados de treinamento vem de conjuntos p√∫blicos n√£o curados e n√£o moderados, abertos a contribui√ß√µes de terceiros. Atacantes n√£o precisam comprometer os conjuntos de dados quando podem simplesmente contribuir para eles. Com o tempo, dados maliciosos de baixa confian√ßa podem se tornar dados confi√°veis de alta confian√ßa, desde que a estrutura/formata√ß√£o dos dados permane√ßa correta.

Por isso, √© fundamental garantir a integridade e prote√ß√£o dos reposit√≥rios de dados que seus modelos usam para tomar decis√µes.

## Entendendo as amea√ßas e riscos da IA

No contexto da IA e sistemas relacionados, o envenenamento de dados √© a amea√ßa de seguran√ßa mais significativa atualmente. Envenenamento de dados ocorre quando algu√©m altera intencionalmente as informa√ß√µes usadas para treinar uma IA, fazendo com que ela cometa erros. Isso acontece devido √† aus√™ncia de m√©todos padronizados de detec√ß√£o e mitiga√ß√£o, al√©m da depend√™ncia de conjuntos de dados p√∫blicos n√£o confi√°veis ou n√£o curados para treinamento. Para manter a integridade dos dados e evitar um processo de treinamento falho, √© crucial rastrear a origem e a linhagem dos seus dados. Caso contr√°rio, o velho ditado ‚Äúlixo entra, lixo sai‚Äù se aplica, comprometendo o desempenho do modelo.

Aqui est√£o exemplos de como o envenenamento de dados pode afetar seus modelos:

1. **Invers√£o de R√≥tulos**: Em uma tarefa de classifica√ß√£o bin√°ria, um advers√°rio inverte intencionalmente os r√≥tulos de um pequeno subconjunto dos dados de treinamento. Por exemplo, amostras benignas s√£o rotuladas como maliciosas, levando o modelo a aprender associa√ß√µes incorretas.\
   **Exemplo**: Um filtro de spam que classifica erroneamente e-mails leg√≠timos como spam devido a r√≥tulos manipulados.
2. **Envenenamento de Caracter√≠sticas**: Um atacante modifica sutilmente caracter√≠sticas nos dados de treinamento para introduzir vi√©s ou enganar o modelo.\
   **Exemplo**: Adicionar palavras-chave irrelevantes √†s descri√ß√µes de produtos para manipular sistemas de recomenda√ß√£o.
3. **Inje√ß√£o de Dados**: Inser√ß√£o de dados maliciosos no conjunto de treinamento para influenciar o comportamento do modelo.\
   **Exemplo**: Introduzir avalia√ß√µes falsas de usu√°rios para distorcer resultados de an√°lise de sentimento.
4. **Ataques de Porta dos Fundos (Backdoor)**: Um advers√°rio insere um padr√£o oculto (porta dos fundos) nos dados de treinamento. O modelo aprende a reconhecer esse padr√£o e age de forma maliciosa quando ele √© acionado.\
   **Exemplo**: Um sistema de reconhecimento facial treinado com imagens contendo backdoors que identifica incorretamente uma pessoa espec√≠fica.

A MITRE Corporation criou o [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), uma base de conhecimento sobre t√°ticas e t√©cnicas usadas por advers√°rios em ataques reais a sistemas de IA.

> H√° um n√∫mero crescente de vulnerabilidades em sistemas habilitados para IA, j√° que a incorpora√ß√£o da IA amplia a superf√≠cie de ataque dos sistemas existentes al√©m dos ataques cibern√©ticos tradicionais. Desenvolvemos o ATLAS para aumentar a conscientiza√ß√£o sobre essas vulnerabilidades √∫nicas e em evolu√ß√£o, √† medida que a comunidade global incorpora cada vez mais IA em diversos sistemas. O ATLAS √© modelado a partir do framework MITRE ATT&CK¬Æ e suas t√°ticas, t√©cnicas e procedimentos (TTPs) complementam os do ATT&CK.

Assim como o framework MITRE ATT&CK¬Æ, amplamente usado em ciberseguran√ßa tradicional para planejar cen√°rios avan√ßados de emula√ß√£o de amea√ßas, o ATLAS oferece um conjunto pesquis√°vel de TTPs que ajudam a entender melhor e se preparar para defender contra ataques emergentes.

Al√©m disso, o Open Web Application Security Project (OWASP) criou uma "[lista Top 10](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" das vulnerabilidades mais cr√≠ticas encontradas em aplica√ß√µes que utilizam LLMs. A lista destaca riscos de amea√ßas como o envenenamento de dados mencionado, al√©m de outras como:

- **Inje√ß√£o de Prompt**: t√©cnica onde atacantes manipulam um Modelo de Linguagem Grande (LLM) por meio de entradas cuidadosamente elaboradas, fazendo-o agir fora do comportamento esperado.
- **Vulnerabilidades na Cadeia de Suprimentos**: os componentes e softwares que comp√µem as aplica√ß√µes usadas por um LLM, como m√≥dulos Python ou conjuntos de dados externos, podem ser comprometidos, levando a resultados inesperados, vieses introduzidos e at√© vulnerabilidades na infraestrutura subjacente.
- **Excesso de Confian√ßa**: LLMs s√£o fal√≠veis e t√™m tend√™ncia a ‚Äúalucinar‚Äù, fornecendo resultados imprecisos ou inseguros. Em v√°rios casos documentados, pessoas aceitaram os resultados como verdadeiros, levando a consequ√™ncias negativas no mundo real.

Rod Trent, Microsoft Cloud Advocate, escreveu um ebook gratuito, [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst), que aprofunda essas e outras amea√ßas emergentes de IA, oferecendo orienta√ß√µes extensas sobre como lidar com esses cen√°rios.

## Testes de Seguran√ßa para Sistemas de IA e LLMs

A intelig√™ncia artificial (IA) est√° transformando diversos dom√≠nios e ind√∫strias, oferecendo novas possibilidades e benef√≠cios para a sociedade. No entanto, a IA tamb√©m apresenta desafios e riscos significativos, como privacidade de dados, vi√©s, falta de explicabilidade e potencial uso indevido. Portanto, √© crucial garantir que os sistemas de IA sejam seguros e respons√°veis, ou seja, que sigam padr√µes √©ticos e legais e possam ser confi√°veis para usu√°rios e partes interessadas.

Testes de seguran√ßa s√£o o processo de avaliar a seguran√ßa de um sistema de IA ou LLM, identificando e explorando suas vulnerabilidades. Isso pode ser realizado por desenvolvedores, usu√°rios ou auditores terceiros, dependendo do prop√≥sito e escopo dos testes. Alguns dos m√©todos mais comuns de testes de seguran√ßa para sistemas de IA e LLMs s√£o:

- **Sanitiza√ß√£o de dados**: processo de remover ou anonimizar informa√ß√µes sens√≠veis ou privadas dos dados de treinamento ou da entrada de um sistema de IA ou LLM. A sanitiza√ß√£o ajuda a prevenir vazamento de dados e manipula√ß√£o maliciosa, reduzindo a exposi√ß√£o de dados confidenciais ou pessoais.
- **Testes adversariais**: processo de gerar e aplicar exemplos adversariais na entrada ou sa√≠da de um sistema de IA ou LLM para avaliar sua robustez e resili√™ncia contra ataques adversariais. Esses testes ajudam a identificar e mitigar vulnerabilidades e fraquezas que podem ser exploradas por atacantes.
- **Verifica√ß√£o do modelo**: processo de verificar a corre√ß√£o e completude dos par√¢metros ou arquitetura do modelo de um sistema de IA ou LLM. A verifica√ß√£o ajuda a detectar e prevenir roubo de modelo, garantindo que ele esteja protegido e autenticado.
- **Valida√ß√£o da sa√≠da**: processo de validar a qualidade e confiabilidade da sa√≠da de um sistema de IA ou LLM. A valida√ß√£o ajuda a detectar e corrigir manipula√ß√µes maliciosas, assegurando que a sa√≠da seja consistente e precisa.

A OpenAI, l√≠der em sistemas de IA, estabeleceu uma s√©rie de _avalia√ß√µes de seguran√ßa_ como parte de sua iniciativa de red teaming, com o objetivo de testar as sa√≠das dos sistemas de IA e contribuir para a seguran√ßa da IA.

> As avalia√ß√µes podem variar desde testes simples de perguntas e respostas at√© simula√ß√µes mais complexas. Como exemplos concretos, aqui est√£o avalia√ß√µes desenvolvidas pela OpenAI para avaliar comportamentos de IA sob diferentes perspectivas:

#### Persuas√£o

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem um sistema de IA consegue enganar outro sistema de IA para dizer uma palavra secreta?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem um sistema de IA consegue convencer outro sistema de IA a doar dinheiro?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem um sistema de IA consegue influenciar o apoio de outro sistema de IA a uma proposta pol√≠tica?

#### Esteganografia (mensagens ocultas)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem um sistema de IA consegue passar mensagens secretas sem ser detectado por outro sistema de IA?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem um sistema de IA consegue comprimir e descomprimir mensagens para permitir o envio de mensagens secretas?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Qu√£o bem um sistema de IA consegue se coordenar com outro sistema de IA sem comunica√ß√£o direta?

### Seguran√ßa em IA

√â fundamental que busquemos proteger os sistemas de IA contra ataques maliciosos, uso indevido ou consequ√™ncias n√£o intencionais. Isso inclui tomar medidas para garantir a seguran√ßa, confiabilidade e confian√ßa nos sistemas de IA, tais como:

- Proteger os dados e algoritmos usados para treinar e executar modelos de IA
- Prevenir acesso n√£o autorizado, manipula√ß√£o ou sabotagem dos sistemas de IA
- Detectar e mitigar vieses, discrimina√ß√£o ou quest√µes √©ticas nos sistemas de IA
- Garantir responsabilidade, transpar√™ncia e explicabilidade das decis√µes e a√ß√µes da IA
- Alinhar os objetivos e valores dos sistemas de IA com os dos humanos e da sociedade

A seguran√ßa em IA √© importante para garantir a integridade, disponibilidade e confidencialidade dos sistemas e dados de IA. Alguns desafios e oportunidades da seguran√ßa em IA s√£o:

- Oportunidade: Incorporar IA em estrat√©gias de ciberseguran√ßa, pois ela pode desempenhar um papel crucial na identifica√ß√£o de amea√ßas e na melhoria dos tempos de resposta. A IA pode ajudar a automatizar e ampliar a detec√ß√£o e mitiga√ß√£o de ataques cibern√©ticos, como phishing, malware ou ransomware.
- Desafio: A IA tamb√©m pode ser usada por advers√°rios para lan√ßar ataques sofisticados, como gerar conte√∫do falso ou enganoso, se passar por usu√°rios ou explorar vulnerabilidades em sistemas de IA. Portanto, desenvolvedores de IA t√™m uma responsabilidade √∫nica de projetar sistemas robustos e resilientes contra usos indevidos.

### Prote√ß√£o de Dados

LLMs podem representar riscos √† privacidade e seguran√ßa dos dados que utilizam. Por exemplo, LLMs podem memorizar e vazar informa√ß√µes sens√≠veis de seus dados de treinamento, como nomes pessoais, endere√ßos, senhas ou n√∫meros de cart√£o de cr√©dito. Eles tamb√©m podem ser manipulados ou atacados por agentes maliciosos que desejam explorar suas vulnerabilidades ou vieses. Por isso, √© importante estar ciente desses riscos e tomar medidas adequadas para proteger os dados usados com LLMs. Algumas a√ß√µes que voc√™ pode tomar para proteger os dados usados com LLMs incluem:

- **Limitar a quantidade e o tipo de dados compartilhados com LLMs**: Compartilhe apenas os dados necess√°rios e relevantes para os fins pretendidos, evitando qualquer dado sens√≠vel, confidencial ou pessoal. Os usu√°rios tamb√©m devem anonimizar ou criptografar os dados compartilhados com LLMs, removendo ou mascarando informa√ß√µes identific√°veis ou usando canais de comunica√ß√£o seguros.
- **Verificar os dados gerados pelos LLMs**: Sempre confira a precis√£o e qualidade das sa√≠das geradas pelos LLMs para garantir que n√£o contenham informa√ß√µes indesejadas ou inadequadas.
- **Reportar e alertar sobre qualquer viola√ß√£o ou incidente de dados**: Fique atento a atividades ou comportamentos suspeitos ou anormais dos LLMs, como gerar textos irrelevantes, imprecisos, ofensivos ou prejudiciais. Isso pode indicar uma viola√ß√£o de dados ou incidente de seguran√ßa.

Seguran√ßa, governan√ßa e conformidade de dados s√£o cr√≠ticas para qualquer organiza√ß√£o que queira aproveitar o poder dos dados e da IA em um ambiente multi-cloud. Proteger e governar todos os seus dados √© uma tarefa complexa e multifacetada. √â necess√°rio proteger e governar diferentes tipos de dados (estruturados, n√£o estruturados e dados gerados por IA) em diferentes locais e m√∫ltiplas nuvens, al√©m de considerar as regulamenta√ß√µes atuais e futuras de seguran√ßa, governan√ßa e IA. Para proteger seus dados, √© importante adotar boas pr√°ticas e precau√ß√µes, tais como:

- Usar servi√ßos ou plataformas de nuvem que ofere√ßam recursos de prote√ß√£o e privacidade de dados.
- Utilizar ferramentas de qualidade e valida√ß√£o de dados para verificar erros, inconsist√™ncias ou anomalias.
- Aplicar frameworks de governan√ßa e √©tica de dados para garantir o uso respons√°vel e transparente dos dados.

### Emulando amea√ßas do mundo real - red teaming em IA

Emular amea√ßas do mundo real √© hoje uma pr√°tica padr√£o na constru√ß√£o de sistemas de IA resilientes, empregando ferramentas, t√°ticas e procedimentos semelhantes para identificar riscos aos sistemas e testar a resposta dos defensores.
> A pr√°tica de red teaming em IA evoluiu para um significado mais amplo: n√£o se limita apenas a identificar vulnerabilidades de seguran√ßa, mas tamb√©m inclui a investiga√ß√£o de outras falhas do sistema, como a gera√ß√£o de conte√∫do potencialmente prejudicial. Sistemas de IA trazem novos riscos, e o red teaming √© fundamental para compreender esses riscos in√©ditos, como inje√ß√£o de prompt e produ√ß√£o de conte√∫do sem fundamento. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)
[![Guidance and resources for red teaming](../../../translated_images/13-AI-red-team.642ed54689d7e8a4d83bdf0635768c4fd8aa41ea539d8e3ffe17514aec4b4824.br.png)]()

A seguir, est√£o os principais insights que moldaram o programa de AI Red Team da Microsoft.

1. **Escopo Amplo do AI Red Teaming:**  
   O AI red teaming agora abrange tanto resultados de seguran√ßa quanto de Responsible AI (RAI). Tradicionalmente, o red teaming focava nos aspectos de seguran√ßa, tratando o modelo como um vetor (por exemplo, roubo do modelo subjacente). No entanto, sistemas de IA introduzem novas vulnerabilidades de seguran√ßa (como inje√ß√£o de prompt, envenenamento), exigindo aten√ß√£o especial. Al√©m da seguran√ßa, o AI red teaming tamb√©m investiga quest√µes de justi√ßa (por exemplo, estere√≥tipos) e conte√∫do prejudicial (como a glorifica√ß√£o da viol√™ncia). A identifica√ß√£o precoce desses problemas permite priorizar os investimentos em defesa.  
2. **Falhas Maliciosas e Benignas:**  
   O AI red teaming considera falhas tanto do ponto de vista malicioso quanto benigno. Por exemplo, ao testar o novo Bing, exploramos n√£o s√≥ como advers√°rios mal-intencionados podem subverter o sistema, mas tamb√©m como usu√°rios comuns podem encontrar conte√∫do problem√°tico ou prejudicial. Diferente do red teaming tradicional de seguran√ßa, que foca principalmente em agentes maliciosos, o AI red teaming leva em conta uma gama mais ampla de perfis e poss√≠veis falhas.  
3. **Natureza Din√¢mica dos Sistemas de IA:**  
   Aplica√ß√µes de IA est√£o em constante evolu√ß√£o. Em aplica√ß√µes com grandes modelos de linguagem, os desenvolvedores se adaptam a requisitos em mudan√ßa. O red teaming cont√≠nuo garante vigil√¢ncia constante e adapta√ß√£o aos riscos que evoluem.

O AI red teaming n√£o √© uma solu√ß√£o completa e deve ser visto como um complemento a controles adicionais, como [controle de acesso baseado em fun√ß√£o (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) e solu√ß√µes abrangentes de gerenciamento de dados. Seu objetivo √© suplementar uma estrat√©gia de seguran√ßa que priorize o uso de solu√ß√µes de IA seguras e respons√°veis, que considerem privacidade e seguran√ßa, ao mesmo tempo que busquem minimizar vieses, conte√∫do prejudicial e desinforma√ß√£o que possam abalar a confian√ßa dos usu√°rios.

Aqui est√° uma lista de leituras adicionais que podem ajudar voc√™ a entender melhor como o red teaming pode ajudar a identificar e mitigar riscos em seus sistemas de IA:

- [Planejando o red teaming para grandes modelos de linguagem (LLMs) e suas aplica√ß√µes](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)  
- [O que √© a OpenAI Red Teaming Network?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)  
- [AI Red Teaming - Uma pr√°tica fundamental para construir solu√ß√µes de IA mais seguras e respons√°veis](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)  
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), uma base de conhecimento sobre t√°ticas e t√©cnicas usadas por advers√°rios em ataques reais a sistemas de IA.

## Verifica√ß√£o de conhecimento

Qual poderia ser uma boa abordagem para manter a integridade dos dados e prevenir seu uso indevido?

1. Ter controles fortes baseados em fun√ß√£o para acesso e gerenciamento de dados  
1. Implementar e auditar a rotulagem de dados para evitar m√° representa√ß√£o ou uso indevido  
1. Garantir que sua infraestrutura de IA suporte filtragem de conte√∫do

A:1, Embora as tr√™s sejam √≥timas recomenda√ß√µes, garantir que voc√™ est√° atribuindo os privil√©gios corretos de acesso aos dados para os usu√°rios √© fundamental para evitar manipula√ß√£o e m√° representa√ß√£o dos dados usados pelos LLMs.

## üöÄ Desafio

Leia mais sobre como voc√™ pode [governar e proteger informa√ß√µes sens√≠veis](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) na era da IA.

## √ìtimo trabalho, continue aprendendo

Ap√≥s concluir esta li√ß√£o, confira nossa [cole√ß√£o de Aprendizado em IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para continuar aprimorando seu conhecimento em IA Generativa!

Siga para a Li√ß√£o 14, onde veremos [o Ciclo de Vida da Aplica√ß√£o de IA Generativa](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informa√ß√µes cr√≠ticas, recomenda-se tradu√ß√£o profissional humana. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.