{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construindo com os Modelos da Família Meta\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Esta lição vai abordar:\n",
    "\n",
    "- Explorar os dois principais modelos da família Meta - Llama 3.1 e Llama 3.2\n",
    "- Entender os casos de uso e cenários para cada modelo\n",
    "- Exemplo de código para mostrar as características únicas de cada modelo\n",
    "\n",
    "## A Família de Modelos Meta\n",
    "\n",
    "Nesta lição, vamos explorar 2 modelos da família Meta ou \"Rebanho Llama\" - Llama 3.1 e Llama 3.2\n",
    "\n",
    "Esses modelos possuem diferentes variantes e estão disponíveis no marketplace de Modelos do Github. Veja mais detalhes sobre como usar os Github Models para [prototipar com modelos de IA](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).\n",
    "\n",
    "Variantes dos Modelos:\n",
    "- Llama 3.1 - 70B Instruct\n",
    "- Llama 3.1 - 405B Instruct\n",
    "- Llama 3.2 - 11B Vision Instruct\n",
    "- Llama 3.2 - 90B Vision Instruct\n",
    "\n",
    "*Nota: O Llama 3 também está disponível nos Github Models, mas não será abordado nesta lição*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1\n",
    "\n",
    "Com 405 bilhões de parâmetros, o Llama 3.1 se enquadra na categoria de LLMs de código aberto.\n",
    "\n",
    "O modelo é uma atualização em relação ao lançamento anterior, Llama 3, oferecendo:\n",
    "\n",
    "- Janela de contexto maior - 128k tokens contra 8k tokens\n",
    "- Maior quantidade máxima de tokens de saída - 4096 contra 2048\n",
    "- Melhor suporte multilíngue - devido ao aumento no número de tokens de treinamento\n",
    "\n",
    "Essas melhorias permitem que o Llama 3.1 lide com casos de uso mais complexos ao construir aplicações de GenAI, incluindo:\n",
    "- Chamada nativa de funções - capacidade de chamar ferramentas e funções externas fora do fluxo de trabalho do LLM\n",
    "- Melhor desempenho em RAG - devido à janela de contexto maior\n",
    "- Geração de dados sintéticos - capacidade de criar dados eficazes para tarefas como fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamada Nativa de Funções\n",
    "\n",
    "O Llama 3.1 foi ajustado para ser mais eficiente ao fazer chamadas de funções ou ferramentas. Ele também possui duas ferramentas integradas que o modelo pode identificar como necessárias de acordo com o pedido do usuário. Essas ferramentas são:\n",
    "\n",
    "- **Brave Search** - Pode ser usada para obter informações atualizadas, como a previsão do tempo, realizando uma busca na web\n",
    "- **Wolfram Alpha** - Pode ser usada para cálculos matemáticos mais complexos, assim você não precisa criar suas próprias funções.\n",
    "\n",
    "Você também pode criar suas próprias ferramentas personalizadas que o LLM pode chamar.\n",
    "\n",
    "No exemplo de código abaixo:\n",
    "\n",
    "- Definimos as ferramentas disponíveis (brave_search, wolfram_alpha) no prompt do sistema.\n",
    "- Enviamos um prompt do usuário perguntando sobre o clima em uma determinada cidade.\n",
    "- O LLM irá responder com uma chamada de ferramenta para o Brave Search, que ficará assim: `<|python_tag|>brave_search.call(query=\"Stockholm weather\")`\n",
    "\n",
    "*Observação: Este exemplo apenas faz a chamada da ferramenta. Se você quiser obter os resultados, será necessário criar uma conta gratuita na página da API do Brave e definir a função em si*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"meta-llama-3.1-405b-instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "\n",
    "tool_prompt=f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=tool_prompt),\n",
    "    UserMessage(content=\"What is the weather in Stockholm?\"),\n",
    "\n",
    "]\n",
    "\n",
    "response = client.complete(messages=messages, model=model_name)\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.2\n",
    "\n",
    "Apesar de ser um LLM, uma limitação do Llama 3.1 é a multimodalidade. Ou seja, a capacidade de usar diferentes tipos de entrada, como imagens, como prompts e fornecer respostas. Essa habilidade é uma das principais novidades do Llama 3.2. Essas funcionalidades também incluem:\n",
    "\n",
    "- Multimodalidade - tem a capacidade de avaliar prompts de texto e imagem\n",
    "- Variações de tamanho pequeno a médio (11B e 90B) - isso oferece opções flexíveis de implantação,\n",
    "- Variações apenas de texto (1B e 3B) - isso permite que o modelo seja implantado em dispositivos de borda / móveis e oferece baixa latência\n",
    "\n",
    "O suporte multimodal representa um grande avanço no mundo dos modelos open source. O exemplo de código abaixo utiliza tanto uma imagem quanto um prompt de texto para obter uma análise da imagem pelo Llama 3.2 90B.\n",
    "\n",
    "### Suporte Multimodal com Llama 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-core\n",
    "%pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"Llama-3.2-90B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that describes images in details.\"\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=[\n",
    "                TextContentItem(text=\"What's in this image?\"),\n",
    "                ImageContentItem(\n",
    "                    image_url=ImageUrl.load(\n",
    "                        image_file=\"sample.jpg\",\n",
    "                        image_format=\"jpg\",\n",
    "                        detail=ImageDetailLevel.LOW)\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O aprendizado não para por aqui, continue sua jornada\n",
    "\n",
    "Depois de concluir esta lição, confira nossa [coleção de aprendizado sobre IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para continuar aprimorando seus conhecimentos em IA Generativa!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Aviso Legal**:  \nEste documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se a tradução profissional feita por humanos. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes do uso desta tradução.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "da4ecf6a45fc7f57cd1bdc8fe6847832",
   "translation_date": "2025-08-25T22:41:56+00:00",
   "source_file": "21-meta/python/githubmodels-assignment.ipynb",
   "language_code": "br"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}