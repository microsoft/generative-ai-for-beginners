{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Esta lição vai abordar:\n",
    "- O que é chamada de função e seus casos de uso\n",
    "- Como criar uma chamada de função usando OpenAI\n",
    "- Como integrar uma chamada de função em uma aplicação\n",
    "\n",
    "## Objetivos de Aprendizagem\n",
    "\n",
    "Após concluir esta lição, você saberá como e entenderá:\n",
    "\n",
    "- O propósito de usar chamada de função\n",
    "- Configurar chamada de função usando o Serviço OpenAI\n",
    "- Projetar chamadas de função eficazes para o caso de uso da sua aplicação\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entendendo Chamadas de Função\n",
    "\n",
    "Nesta lição, queremos criar um recurso para nossa startup de educação que permite aos usuários usar um chatbot para encontrar cursos técnicos. Vamos recomendar cursos que se encaixem no nível de habilidade, função atual e tecnologia de interesse do usuário.\n",
    "\n",
    "Para completar isso, vamos usar uma combinação de:\n",
    " - `OpenAI` para criar uma experiência de chat para o usuário\n",
    " - `Microsoft Learn Catalog API` para ajudar os usuários a encontrar cursos com base no pedido do usuário\n",
    " - `Function Calling` para pegar a consulta do usuário e enviá-la para uma função que fará a requisição à API.\n",
    "\n",
    "Para começar, vamos entender por que queremos usar chamadas de função:\n",
    "\n",
    "print(\"Mensagens na próxima requisição:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # obter uma nova resposta do GPT onde ele pode ver a resposta da função\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que usar Function Calling\n",
    "\n",
    "Se você já completou alguma outra lição deste curso, provavelmente já entende o poder de usar Modelos de Linguagem de Grande Escala (LLMs). Esperamos que também consiga perceber algumas de suas limitações.\n",
    "\n",
    "Function Calling é um recurso do OpenAI Service criado para resolver os seguintes desafios:\n",
    "\n",
    "Formatação Inconsistente de Respostas:\n",
    "- Antes do function calling, as respostas de um modelo de linguagem eram desestruturadas e inconsistentes. Os desenvolvedores precisavam escrever códigos de validação complexos para lidar com cada variação no resultado.\n",
    "\n",
    "Integração Limitada com Dados Externos:\n",
    "- Antes desse recurso, era difícil incorporar dados de outras partes de uma aplicação em um contexto de chat.\n",
    "\n",
    "Ao padronizar os formatos de resposta e permitir uma integração fluida com dados externos, o function calling simplifica o desenvolvimento e reduz a necessidade de lógica adicional de validação.\n",
    "\n",
    "Os usuários não conseguiam obter respostas como \"Qual é o clima atual em Estocolmo?\". Isso porque os modelos eram limitados ao período em que os dados foram treinados.\n",
    "\n",
    "Vamos analisar o exemplo abaixo que ilustra esse problema:\n",
    "\n",
    "Suponha que queremos criar um banco de dados com informações de estudantes para sugerir o curso mais adequado para cada um. Abaixo temos duas descrições de estudantes que são muito parecidas nos dados que apresentam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finshing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos enviar isso para um LLM para analisar os dados. Isso pode ser usado posteriormente em nosso aplicativo para enviar para uma API ou armazenar em um banco de dados.\n",
    "\n",
    "Vamos criar dois prompts idênticos nos quais instruímos o LLM sobre quais informações estamos interessados:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos enviar isto para um LLM para analisar as partes que são importantes para o nosso produto. Assim, podemos criar dois prompts idênticos para instruir o LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de criar esses dois prompts, vamos enviá-los para o LLM usando `openai.ChatCompletion`. Armazenamos o prompt na variável `messages` e atribuímos o papel de `user`. Isso serve para simular uma mensagem de um usuário sendo escrita para um chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mesmo que os prompts sejam iguais e as descrições parecidas, podemos obter formatos diferentes para a propriedade `Grades`.\n",
    "\n",
    "Se você executar a célula acima várias vezes, o formato pode ser `3.7` ou `3.7 GPA`.\n",
    "\n",
    "Isso acontece porque o LLM recebe dados não estruturados na forma do prompt escrito e também retorna dados não estruturados. Precisamos ter um formato estruturado para sabermos o que esperar ao armazenar ou usar esses dados.\n",
    "\n",
    "Usando chamadas funcionais, podemos garantir que receberemos dados estruturados de volta. Ao usar chamadas de função, o LLM na verdade não executa ou roda nenhuma função. Em vez disso, criamos uma estrutura para o LLM seguir em suas respostas. Depois, usamos essas respostas estruturadas para saber qual função executar em nossas aplicações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama de Fluxo de Chamada de Função](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.br.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casos de uso para chamadas de função\n",
    "\n",
    "**Chamar Ferramentas Externas**\n",
    "Chatbots são ótimos para responder perguntas dos usuários. Usando chamadas de função, os chatbots podem usar as mensagens dos usuários para realizar certas tarefas. Por exemplo, um estudante pode pedir ao chatbot: \"Envie um e-mail para meu professor dizendo que preciso de mais ajuda com essa matéria\". Isso pode acionar uma chamada para a função `send_email(to: string, body: string)`\n",
    "\n",
    "**Criar Consultas de API ou Banco de Dados**\n",
    "Os usuários podem buscar informações usando linguagem natural, que é convertida em uma consulta formatada ou requisição de API. Um exemplo seria um professor perguntando: \"Quais alunos concluíram a última tarefa?\", o que poderia acionar uma função chamada `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Criar Dados Estruturados**\n",
    "Os usuários podem pegar um bloco de texto ou um arquivo CSV e usar o LLM para extrair informações importantes. Por exemplo, um estudante pode transformar um artigo da Wikipédia sobre acordos de paz para criar flash cards de IA. Isso pode ser feito usando uma função chamada `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criando Sua Primeira Chamada de Função\n",
    "\n",
    "O processo de criar uma chamada de função inclui 3 passos principais:\n",
    "1. Chamar a API de Chat Completions com uma lista das suas funções e uma mensagem do usuário\n",
    "2. Ler a resposta do modelo para realizar uma ação, ou seja, executar uma função ou chamada de API\n",
    "3. Fazer outra chamada para a API de Chat Completions com a resposta da sua função para usar essa informação e criar uma resposta para o usuário.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fluxo de uma Chamada de Função](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.br.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementos de uma chamada de função\n",
    "\n",
    "#### Entrada do usuário\n",
    "\n",
    "O primeiro passo é criar uma mensagem do usuário. Isso pode ser feito de forma dinâmica pegando o valor de um campo de texto ou você pode definir um valor aqui mesmo. Se esta for sua primeira vez trabalhando com a API de Chat Completions, precisamos definir o `role` e o `content` da mensagem.\n",
    "\n",
    "O `role` pode ser `system` (criando regras), `assistant` (o modelo) ou `user` (o usuário final). Para chamadas de função, vamos definir como `user` e usar uma pergunta de exemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando funções.\n",
    "\n",
    "Agora vamos definir uma função e os parâmetros dessa função. Vamos usar apenas uma função aqui chamada `search_courses`, mas você pode criar várias funções.\n",
    "\n",
    "**Importante**: As funções são incluídas na mensagem do sistema para o LLM e vão contar no limite de tokens disponíveis para você.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definições**\n",
    "\n",
    "A estrutura de definição de função possui vários níveis, cada um com suas próprias propriedades. Veja a seguir um resumo da estrutura aninhada:\n",
    "\n",
    "**Propriedades da Função no Nível Superior:**\n",
    "\n",
    "`name` - O nome da função que queremos que seja chamada.\n",
    "\n",
    "`description` - Esta é a descrição de como a função funciona. Aqui é importante ser específico e claro.\n",
    "\n",
    "`parameters` - Uma lista de valores e o formato que você deseja que o modelo produza em sua resposta.\n",
    "\n",
    "**Propriedades do Objeto Parameters:**\n",
    "\n",
    "`type` - O tipo de dado do objeto de parâmetros (geralmente \"object\")\n",
    "\n",
    "`properties` - Lista dos valores específicos que o modelo usará em sua resposta\n",
    "\n",
    "**Propriedades Individuais de Parâmetro:**\n",
    "\n",
    "`name` - Definido implicitamente pela chave da propriedade (por exemplo, \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - O tipo de dado deste parâmetro específico (por exemplo, \"string\", \"number\", \"boolean\")\n",
    "\n",
    "`description` - Descrição do parâmetro específico\n",
    "\n",
    "**Propriedades Opcionais:**\n",
    "\n",
    "`required` - Um array que lista quais parâmetros são obrigatórios para que a chamada da função seja concluída\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo a chamada da função\n",
    "Depois de definir uma função, agora precisamos incluí-la na chamada para a API de Chat Completion. Fazemos isso adicionando `functions` à requisição. Neste caso, `functions=functions`.\n",
    "\n",
    "Também existe a opção de definir `function_call` como `auto`. Isso significa que vamos deixar o LLM decidir qual função deve ser chamada com base na mensagem do usuário, em vez de escolhermos manualmente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos analisar a resposta e ver como ela está formatada:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Você pode ver que o nome da função foi chamado e, a partir da mensagem do usuário, o LLM conseguiu encontrar os dados para preencher os argumentos da função.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integrando Chamadas de Função em uma Aplicação.\n",
    "\n",
    "Depois de testarmos a resposta formatada do LLM, agora podemos integrar isso em uma aplicação.\n",
    "\n",
    "### Gerenciando o fluxo\n",
    "\n",
    "Para integrar isso na nossa aplicação, vamos seguir os seguintes passos:\n",
    "\n",
    "Primeiro, vamos fazer a chamada aos serviços da OpenAI e armazenar a mensagem em uma variável chamada `response_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos definir a função que irá chamar a API do Microsoft Learn para obter uma lista de cursos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como uma boa prática, vamos então verificar se o modelo deseja chamar uma função. Depois disso, vamos criar uma das funções disponíveis e associá-la à função que está sendo chamada.\n",
    "Em seguida, vamos pegar os argumentos da função e mapeá-los para os argumentos vindos do LLM.\n",
    "\n",
    "Por fim, vamos adicionar a mensagem de chamada da função e os valores que foram retornados pela mensagem `search_courses`. Isso fornece ao LLM todas as informações necessárias para\n",
    "responder ao usuário usando linguagem natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafio de Código\n",
    "\n",
    "Ótimo trabalho! Para continuar aprendendo sobre OpenAI Function Calling, você pode construir: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst\n",
    " - Mais parâmetros da função que possam ajudar os alunos a encontrar mais cursos. Você pode encontrar os parâmetros disponíveis da API aqui:\n",
    " - Criar outra chamada de função que receba mais informações do aluno, como o idioma nativo dele\n",
    " - Criar um tratamento de erros para quando a chamada da função e/ou da API não retornar cursos adequados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Aviso Legal**:  \nEste documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se a tradução profissional feita por humanos. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes do uso desta tradução.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "09e1959b012099c552c48fe94d66a64b",
   "translation_date": "2025-08-25T20:59:43+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "br"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}