{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução \n",
    "\n",
    "Esta lição abordará: \n",
    "- O que é chamada de função e seus casos de uso \n",
    "- Como criar uma chamada de função usando OpenAI \n",
    "- Como integrar uma chamada de função em uma aplicação \n",
    "\n",
    "## Objetivos de Aprendizagem \n",
    "\n",
    "Após concluir esta lição, você saberá como e entenderá: \n",
    "\n",
    "- O propósito de usar chamada de função \n",
    "- Configurar Chamada de Função usando o Serviço OpenAI \n",
    "- Projetar chamadas de função eficazes para o caso de uso da sua aplicação \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entendendo Chamadas de Função\n",
    "\n",
    "Para esta lição, queremos construir um recurso para nossa startup de educação que permita aos usuários usar um chatbot para encontrar cursos técnicos. Recomendaremos cursos que se encaixem no nível de habilidade, cargo atual e tecnologia de interesse deles.\n",
    "\n",
    "Para completar isso, usaremos uma combinação de:\n",
    " - `OpenAI` para criar uma experiência de chat para o usuário\n",
    " - `Microsoft Learn Catalog API` para ajudar os usuários a encontrar cursos com base na solicitação do usuário\n",
    " - `Function Calling` para pegar a consulta do usuário e enviá-la para uma função para fazer a solicitação da API.\n",
    "\n",
    "Para começar, vamos ver por que gostaríamos de usar chamadas de função em primeiro lugar:\n",
    "\n",
    "print(\"Mensagens na próxima solicitação:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # obter uma nova resposta do GPT onde ele pode ver a resposta da função\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que Chamar Funções\n",
    "\n",
    "Se você completou qualquer outra lição deste curso, provavelmente entende o poder de usar Modelos de Linguagem Grande (LLMs). Esperançosamente, você também consegue ver algumas de suas limitações.\n",
    "\n",
    "Chamar Funções é um recurso do Serviço OpenAI projetado para resolver os seguintes desafios:\n",
    "\n",
    "Formatação Inconsistente de Respostas:\n",
    "- Antes de chamar funções, as respostas de um modelo de linguagem grande eram não estruturadas e inconsistentes. Os desenvolvedores precisavam escrever códigos complexos de validação para lidar com cada variação na saída.\n",
    "\n",
    "Integração Limitada com Dados Externos:\n",
    "- Antes desse recurso, era difícil incorporar dados de outras partes de uma aplicação em um contexto de chat.\n",
    "\n",
    "Ao padronizar os formatos de resposta e permitir integração perfeita com dados externos, chamar funções simplifica o desenvolvimento e reduz a necessidade de lógica adicional de validação.\n",
    "\n",
    "Os usuários não conseguiam obter respostas como \"Qual é o clima atual em Estocolmo?\". Isso porque os modelos estavam limitados ao tempo em que os dados foram treinados.\n",
    "\n",
    "Vamos ver o exemplo abaixo que ilustra esse problema:\n",
    "\n",
    "Digamos que queremos criar um banco de dados de dados de estudantes para que possamos sugerir o curso certo para eles. Abaixo temos duas descrições de estudantes que são muito semelhantes nos dados que contêm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos enviar isso para um LLM para analisar os dados. Isso pode ser usado posteriormente em nossa aplicação para enviar para uma API ou armazenar em um banco de dados.\n",
    "\n",
    "Vamos criar dois prompts idênticos nos quais instruímos o LLM sobre quais informações estamos interessados:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos enviar isso para um LLM para analisar as partes que são importantes para o nosso produto. Assim, podemos criar dois prompts idênticos para instruir o LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após criar esses dois prompts, enviaremos eles para o LLM usando `openai.ChatCompletion`. Armazenamos o prompt na variável `messages` e atribuímos o papel de `user`. Isso é para imitar uma mensagem de um usuário sendo escrita para um chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos enviar ambas as solicitações para o LLM e examinar a resposta que recebemos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mesmo que os prompts sejam os mesmos e as descrições sejam semelhantes, podemos obter formatos diferentes da propriedade `Grades`.\n",
    "\n",
    "Se você executar a célula acima várias vezes, o formato pode ser `3.7` ou `3.7 GPA`.\n",
    "\n",
    "Isso ocorre porque o LLM recebe dados não estruturados na forma do prompt escrito e também retorna dados não estruturados. Precisamos ter um formato estruturado para saber o que esperar ao armazenar ou usar esses dados.\n",
    "\n",
    "Ao usar chamadas funcionais, podemos garantir que recebemos dados estruturados de volta. Ao usar chamadas funcionais, o LLM na verdade não chama ou executa nenhuma função. Em vez disso, criamos uma estrutura para o LLM seguir em suas respostas. Em seguida, usamos essas respostas estruturadas para saber qual função executar em nossas aplicações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama de Fluxo de Chamada de Função](../../../../translated_images/br/Function-Flow.083875364af4f4bb.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos então pegar o que é retornado pela função e enviar isso de volta para o LLM. O LLM então responderá usando linguagem natural para responder à consulta do usuário.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casos de Uso para chamadas de função\n",
    "\n",
    "**Chamar Ferramentas Externas**  \n",
    "Chatbots são ótimos para fornecer respostas a perguntas dos usuários. Usando chamadas de função, os chatbots podem usar mensagens dos usuários para completar certas tarefas. Por exemplo, um estudante pode pedir ao chatbot para \"Enviar e-mail para meu instrutor dizendo que preciso de mais ajuda com este assunto\". Isso pode fazer uma chamada de função para `send_email(to: string, body: string)`\n",
    "\n",
    "**Criar Consultas de API ou Banco de Dados**  \n",
    "Os usuários podem encontrar informações usando linguagem natural que é convertida em uma consulta formatada ou requisição de API. Um exemplo disso pode ser um professor que solicita \"Quem são os alunos que completaram a última tarefa\" que poderia chamar uma função chamada `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Criar Dados Estruturados**  \n",
    "Os usuários podem pegar um bloco de texto ou CSV e usar o LLM para extrair informações importantes dele. Por exemplo, um estudante pode converter um artigo da Wikipedia sobre acordos de paz para criar flash cards de IA. Isso pode ser feito usando uma função chamada `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criando Sua Primeira Chamada de Função\n",
    "\n",
    "O processo de criar uma chamada de função inclui 3 etapas principais:  \n",
    "1. Chamar a API de Completações de Chat com uma lista de suas funções e uma mensagem do usuário  \n",
    "2. Ler a resposta do modelo para executar uma ação, ou seja, executar uma função ou chamada de API  \n",
    "3. Fazer outra chamada para a API de Completações de Chat com a resposta da sua função para usar essa informação para criar uma resposta para o usuário.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fluxo de uma Chamada de Função](../../../../translated_images/br/LLM-Flow.3285ed8caf4796d7.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementos de uma chamada de função \n",
    "\n",
    "#### Entrada dos Usuários \n",
    "\n",
    "O primeiro passo é criar uma mensagem do usuário. Isso pode ser atribuído dinamicamente pegando o valor de uma entrada de texto ou você pode atribuir um valor aqui. Se esta for sua primeira vez trabalhando com a API de Conclusões de Chat, precisamos definir o `role` e o `content` da mensagem. \n",
    "\n",
    "O `role` pode ser `system` (criando regras), `assistant` (o modelo) ou `user` (o usuário final). Para chamadas de função, atribuiremos isso como `user` e uma pergunta de exemplo. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando funções.\n",
    "\n",
    "A seguir, definiremos uma função e os parâmetros dessa função. Usaremos apenas uma função aqui chamada `search_courses`, mas você pode criar várias funções.\n",
    "\n",
    "**Importante**: As funções são incluídas na mensagem do sistema para o LLM e serão contabilizadas na quantidade de tokens disponíveis que você tem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definições** \n",
    "\n",
    "A estrutura de definição da função possui múltiplos níveis, cada um com suas próprias propriedades. Aqui está uma divisão da estrutura aninhada:\n",
    "\n",
    "**Propriedades da Função no Nível Superior:**\n",
    "\n",
    "`name` - O nome da função que queremos que seja chamada.\n",
    "\n",
    "`description` - Esta é a descrição de como a função funciona. Aqui é importante ser específico e claro.\n",
    "\n",
    "`parameters` - Uma lista de valores e formato que você deseja que o modelo produza em sua resposta.\n",
    "\n",
    "**Propriedades do Objeto Parameters:**\n",
    "\n",
    "`type` - O tipo de dado do objeto parameters (geralmente \"object\").\n",
    "\n",
    "`properties` - Lista dos valores específicos que o modelo usará para sua resposta.\n",
    "\n",
    "**Propriedades Individuais dos Parâmetros:**\n",
    "\n",
    "`name` - Implicitamente definido pela chave da propriedade (ex.: \"role\", \"product\", \"level\").\n",
    "\n",
    "`type` - O tipo de dado deste parâmetro específico (ex.: \"string\", \"number\", \"boolean\").\n",
    "\n",
    "`description` - Descrição do parâmetro específico.\n",
    "\n",
    "**Propriedades Opcionais:**\n",
    "\n",
    "`required` - Um array listando quais parâmetros são necessários para que a chamada da função seja concluída.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo a chamada da função  \n",
    "Após definir uma função, agora precisamos incluí-la na chamada para a API de Conclusão de Chat. Fazemos isso adicionando `functions` à requisição. Neste caso, `functions=functions`.  \n",
    "\n",
    "Também há uma opção para definir `function_call` como `auto`. Isso significa que deixaremos o LLM decidir qual função deve ser chamada com base na mensagem do usuário, em vez de atribuí-la nós mesmos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos analisar a resposta e ver como ela está formatada:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Você pode ver que o nome da função foi chamado e, a partir da mensagem do usuário, o LLM conseguiu encontrar os dados para preencher os argumentos da função.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Integrando Chamadas de Função em uma Aplicação. \n",
    "\n",
    "\n",
    "Depois de testarmos a resposta formatada do LLM, agora podemos integrá-la em uma aplicação. \n",
    "\n",
    "### Gerenciando o fluxo \n",
    "\n",
    "Para integrar isso em nossa aplicação, vamos seguir os seguintes passos: \n",
    "\n",
    "Primeiro, vamos fazer a chamada aos serviços da OpenAI e armazenar a mensagem em uma variável chamada `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos definir a função que chamará a API do Microsoft Learn para obter uma lista de cursos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como uma boa prática, veremos então se o modelo deseja chamar uma função. Depois disso, criaremos uma das funções disponíveis e a associaremos à função que está sendo chamada.  \n",
    "Em seguida, pegaremos os argumentos da função e os mapearemos para os argumentos do LLM.\n",
    "\n",
    "Por fim, adicionaremos a mensagem de chamada da função e os valores que foram retornados pela mensagem `search_courses`. Isso fornece ao LLM todas as informações necessárias para  \n",
    "responder ao usuário usando linguagem natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora enviaremos a mensagem atualizada para o LLM para que possamos receber uma resposta em linguagem natural em vez de uma resposta formatada em JSON da API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafio de Código \n",
    "\n",
    "Ótimo trabalho! Para continuar seu aprendizado sobre OpenAI Function Calling, você pode construir: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - Mais parâmetros da função que podem ajudar os aprendizes a encontrar mais cursos. Você pode encontrar os parâmetros de API disponíveis aqui: \n",
    " - Criar outra chamada de função que receba mais informações do aprendiz, como sua língua nativa \n",
    " - Criar tratamento de erros quando a chamada da função e/ou chamada da API não retornar cursos adequados \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Aviso Legal**:  \nEste documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes do uso desta tradução.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T10:03:44+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "br"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}