{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Capítulo 7: Construindo Aplicações de Chat\n",
    "## Introdução Rápida à API OpenAI\n",
    "\n",
    "Este notebook foi adaptado do [Repositório de Exemplos do Azure OpenAI](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst), que inclui notebooks que acessam os serviços do [Azure OpenAI](notebook-azure-openai.ipynb).\n",
    "\n",
    "A API Python do OpenAI também funciona com os modelos do Azure OpenAI, com algumas pequenas alterações. Saiba mais sobre as diferenças aqui: [Como alternar entre os endpoints do OpenAI e do Azure OpenAI com Python](https://learn.microsoft.com/azure/ai-services/openai/how-to/switching-endpoints?WT.mc_id=academic-109527-jasmineg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Visão Geral  \n",
    "\"Modelos de linguagem de grande porte são funções que mapeiam texto para texto. Dado um texto de entrada, um modelo de linguagem de grande porte tenta prever o texto que virá em seguida\"(1). Este notebook de \"início rápido\" vai apresentar aos usuários conceitos gerais de LLM, os principais requisitos de pacotes para começar a usar o AML, uma introdução leve ao design de prompts e alguns exemplos curtos de diferentes casos de uso.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Índice\n",
    "\n",
    "[Visão Geral](../../../../07-building-chat-applications/python)  \n",
    "[Como usar o Serviço OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[1. Criando seu Serviço OpenAI](../../../../07-building-chat-applications/python)  \n",
    "[2. Instalação](../../../../07-building-chat-applications/python)  \n",
    "[3. Credenciais](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Casos de Uso](../../../../07-building-chat-applications/python)  \n",
    "[1. Resumir Texto](../../../../07-building-chat-applications/python)  \n",
    "[2. Classificar Texto](../../../../07-building-chat-applications/python)  \n",
    "[3. Gerar Novos Nomes de Produtos](../../../../07-building-chat-applications/python)  \n",
    "[4. Ajustar um Classificador](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Referências](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Crie seu primeiro prompt  \n",
    "Este exercício rápido vai te mostrar o básico de como enviar prompts para um modelo da OpenAI para uma tarefa simples de \"resumo\".\n",
    "\n",
    "\n",
    "**Passos**:  \n",
    "1. Instale a biblioteca OpenAI no seu ambiente Python  \n",
    "2. Carregue as bibliotecas auxiliares padrão e configure suas credenciais de segurança típicas para o Serviço OpenAI que você criou  \n",
    "3. Escolha um modelo para sua tarefa  \n",
    "4. Crie um prompt simples para o modelo  \n",
    "5. Envie sua solicitação para a API do modelo!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\")\n",
    "assert API_KEY, \"ERROR: OpenAI Key is missing\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Encontrando o modelo certo  \n",
    "Os modelos GPT-3.5-turbo ou GPT-4 conseguem entender e gerar linguagem natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Design de Prompt  \n",
    "\n",
    "\"A mágica dos grandes modelos de linguagem é que, ao serem treinados para minimizar esse erro de previsão em enormes quantidades de texto, eles acabam aprendendo conceitos úteis para essas previsões. Por exemplo, eles aprendem conceitos como\"(1):\n",
    "\n",
    "* como soletrar\n",
    "* como funciona a gramática\n",
    "* como parafrasear\n",
    "* como responder perguntas\n",
    "* como manter uma conversa\n",
    "* como escrever em vários idiomas\n",
    "* como programar\n",
    "* etc.\n",
    "\n",
    "#### Como controlar um grande modelo de linguagem  \n",
    "\"De todos os insumos para um grande modelo de linguagem, de longe o mais influente é o prompt de texto(1).\n",
    "\n",
    "Grandes modelos de linguagem podem ser estimulados a produzir respostas de algumas formas:\n",
    "\n",
    "Instrução: Diga ao modelo o que você quer\n",
    "Completação: Induza o modelo a completar o início do que você deseja\n",
    "Demonstração: Mostre ao modelo o que você quer, com:\n",
    "Alguns exemplos no prompt\n",
    "Centenas ou milhares de exemplos em um conjunto de dados de treinamento para ajuste fino\"\n",
    "\n",
    "\n",
    "\n",
    "#### Existem três diretrizes básicas para criar prompts:\n",
    "\n",
    "**Mostre e explique**. Deixe claro o que você quer, seja por instruções, exemplos ou uma combinação dos dois. Se você quer que o modelo ordene uma lista de itens em ordem alfabética ou classifique um parágrafo pelo sentimento, mostre que é isso que você deseja.\n",
    "\n",
    "**Forneça dados de qualidade**. Se você está tentando construir um classificador ou fazer o modelo seguir um padrão, certifique-se de que há exemplos suficientes. Não esqueça de revisar seus exemplos — o modelo geralmente é esperto o bastante para ignorar erros básicos de ortografia e te dar uma resposta, mas também pode assumir que isso foi intencional e isso pode afetar o resultado.\n",
    "\n",
    "**Confira suas configurações.** As configurações de temperature e top_p controlam o quão determinístico o modelo é ao gerar uma resposta. Se você está pedindo uma resposta onde só existe uma resposta correta, é melhor deixar esses valores mais baixos. Se você quer respostas mais variadas, pode aumentar esses valores. O erro mais comum que as pessoas cometem com essas configurações é achar que elas controlam a \"inteligência\" ou \"criatividade\" do modelo.\n",
    "\n",
    "\n",
    "Fonte: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Resumir Texto  \n",
    "#### Desafio  \n",
    "Resuma um texto adicionando um 'tl;dr:' ao final de um trecho. Observe como o modelo entende como realizar várias tarefas sem instruções adicionais. Você pode experimentar prompts mais descritivos do que tl;dr para modificar o comportamento do modelo e personalizar o resumo que recebe(3).  \n",
    "\n",
    "Trabalhos recentes demonstraram avanços significativos em várias tarefas e benchmarks de PLN ao treinar previamente em um grande corpus de texto e, em seguida, ajustar para uma tarefa específica. Embora normalmente a arquitetura seja independente da tarefa, esse método ainda exige conjuntos de dados específicos para ajuste fino, com milhares ou dezenas de milhares de exemplos. Em contraste, humanos geralmente conseguem realizar uma nova tarefa de linguagem com apenas alguns exemplos ou instruções simples – algo que os sistemas atuais de PLN ainda têm dificuldade em fazer. Aqui mostramos que aumentar a escala dos modelos de linguagem melhora bastante o desempenho independente de tarefa com poucos exemplos, às vezes até alcançando competitividade com abordagens anteriores de ajuste fino de última geração. \n",
    "\n",
    "\n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Exercícios para vários casos de uso  \n",
    "1. Resumir Texto  \n",
    "2. Classificar Texto  \n",
    "3. Gerar Novos Nomes de Produtos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Classificar Texto  \n",
    "#### Desafio  \n",
    "Classifique itens em categorias fornecidas no momento da inferência. No exemplo a seguir, fornecemos tanto as categorias quanto o texto a ser classificado no prompt (*playground_reference).\n",
    "\n",
    "Consulta do cliente: Olá, uma das teclas do teclado do meu notebook quebrou recentemente e vou precisar de uma substituição:\n",
    "\n",
    "Categoria classificada:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Gerar Novos Nomes de Produtos\n",
    "#### Desafio\n",
    "Crie nomes de produtos a partir de palavras de exemplo. Aqui, incluímos no prompt informações sobre o produto para o qual vamos gerar nomes. Também fornecemos um exemplo semelhante para mostrar o padrão que desejamos receber. Definimos o valor de temperatura alto para aumentar a aleatoriedade e obter respostas mais inovadoras.\n",
    "\n",
    "Descrição do produto: Um aparelho para fazer milkshake em casa\n",
    "Palavras-chave: rápido, saudável, compacto.\n",
    "Nomes de produtos: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Descrição do produto: Um par de sapatos que serve em qualquer tamanho de pé.\n",
    "Palavras-chave: adaptável, ajuste, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Referências  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Exemplos do OpenAI Studio](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Melhores práticas para ajuste fino do GPT-3 para classificar texto](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Para Mais Ajuda  \n",
    "[Equipe de Comercialização OpenAI](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Colaboradores\n",
    "* Louis Li\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Aviso Legal**:  \nEste documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora busquemos precisão, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se a tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes do uso desta tradução.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "1854bdde1dd56b366f1f6c9122f7d304",
   "translation_date": "2025-08-25T18:13:47+00:00",
   "source_file": "07-building-chat-applications/python/oai-assignment.ipynb",
   "language_code": "br"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}