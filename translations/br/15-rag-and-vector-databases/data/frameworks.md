<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b5466bcedc3c75aa35476270362f626a",
  "translation_date": "2025-07-09T16:30:42+00:00",
  "source_file": "15-rag-and-vector-databases/data/frameworks.md",
  "language_code": "br"
}
-->
# Frameworks de Redes Neurais

Como j√° aprendemos, para treinar redes neurais de forma eficiente precisamos fazer duas coisas:

* Operar com tensores, por exemplo, multiplicar, somar e calcular algumas fun√ß√µes como sigmoid ou softmax
* Calcular gradientes de todas as express√µes, para realizar a otimiza√ß√£o por descida do gradiente

Embora a biblioteca `numpy` possa fazer a primeira parte, precisamos de algum mecanismo para calcular gradientes. No nosso framework que desenvolvemos na se√ß√£o anterior, tivemos que programar manualmente todas as fun√ß√µes derivadas dentro do m√©todo `backward`, que realiza a retropropaga√ß√£o. Idealmente, um framework deveria nos permitir calcular gradientes de *qualquer express√£o* que definirmos.

Outra coisa importante √© poder realizar c√°lculos na GPU, ou em outras unidades especializadas de processamento, como TPU. O treinamento de redes neurais profundas exige *muitos* c√°lculos, e poder paralelizar esses c√°lculos em GPUs √© fundamental.

> ‚úÖ O termo 'paralelizar' significa distribuir os c√°lculos entre m√∫ltiplos dispositivos.

Atualmente, os dois frameworks de redes neurais mais populares s√£o: TensorFlow e PyTorch. Ambos fornecem uma API de baixo n√≠vel para operar com tensores tanto na CPU quanto na GPU. Sobre essa API de baixo n√≠vel, existem tamb√©m APIs de n√≠vel mais alto, chamadas Keras e PyTorch Lightning, respectivamente.

API de Baixo N√≠vel | TensorFlow | PyTorch
------------------|------------|---------
API de Alto N√≠vel | Keras      | PyTorch

As **APIs de baixo n√≠vel** em ambos os frameworks permitem construir os chamados **grafos computacionais**. Esse grafo define como calcular a sa√≠da (geralmente a fun√ß√£o de perda) a partir dos par√¢metros de entrada, e pode ser enviado para execu√ß√£o na GPU, se dispon√≠vel. Existem fun√ß√µes para diferenciar esse grafo computacional e calcular gradientes, que podem ser usados para otimizar os par√¢metros do modelo.

As **APIs de alto n√≠vel** consideram redes neurais basicamente como uma **sequ√™ncia de camadas**, facilitando muito a constru√ß√£o da maioria das redes neurais. O treinamento do modelo geralmente requer preparar os dados e ent√£o chamar uma fun√ß√£o `fit` para realizar o processo.

A API de alto n√≠vel permite construir redes neurais t√≠picas muito rapidamente, sem se preocupar com muitos detalhes. Ao mesmo tempo, a API de baixo n√≠vel oferece muito mais controle sobre o processo de treinamento, sendo muito usada em pesquisas, quando se trabalha com arquiteturas novas de redes neurais.

Tamb√©m √© importante entender que voc√™ pode usar ambas as APIs juntas, por exemplo, pode desenvolver sua pr√≥pria arquitetura de camada usando a API de baixo n√≠vel e depois us√°-la dentro de uma rede maior constru√≠da e treinada com a API de alto n√≠vel. Ou pode definir uma rede usando a API de alto n√≠vel como uma sequ√™ncia de camadas e depois usar seu pr√≥prio loop de treinamento de baixo n√≠vel para realizar a otimiza√ß√£o. Ambas as APIs usam os mesmos conceitos b√°sicos subjacentes e foram projetadas para funcionar bem juntas.

## Aprendizado

Neste curso, oferecemos a maior parte do conte√∫do tanto para PyTorch quanto para TensorFlow. Voc√™ pode escolher seu framework preferido e seguir apenas os notebooks correspondentes. Se n√£o souber qual escolher, leia algumas discuss√µes na internet sobre **PyTorch vs. TensorFlow**. Tamb√©m pode explorar ambos para entender melhor.

Sempre que poss√≠vel, usaremos APIs de alto n√≠vel para simplificar. No entanto, acreditamos que √© importante entender como redes neurais funcionam desde o b√°sico, por isso come√ßamos trabalhando com a API de baixo n√≠vel e tensores. Mas, se quiser come√ßar r√°pido e n√£o gastar muito tempo com esses detalhes, pode pular para os notebooks da API de alto n√≠vel.

## ‚úçÔ∏è Exerc√≠cios: Frameworks

Continue seu aprendizado nos seguintes notebooks:

API de Baixo N√≠vel | Notebook TensorFlow+Keras | PyTorch
------------------|----------------------------|---------
API de Alto N√≠vel | Keras                      | *PyTorch Lightning*

Depois de dominar os frameworks, vamos recapitular o conceito de overfitting.

# Overfitting

Overfitting √© um conceito extremamente importante em aprendizado de m√°quina, e √© fundamental entend√™-lo bem!

Considere o problema de aproximar 5 pontos (representados por `x` nos gr√°ficos abaixo):

!linear | overfit
-------------------------|--------------------------
**Modelo linear, 2 par√¢metros** | **Modelo n√£o linear, 7 par√¢metros**
Erro de treinamento = 5.3 | Erro de treinamento = 0
Erro de valida√ß√£o = 5.1 | Erro de valida√ß√£o = 20

* √Ä esquerda, vemos uma boa aproxima√ß√£o por uma linha reta. Como o n√∫mero de par√¢metros √© adequado, o modelo captura corretamente a distribui√ß√£o dos pontos.
* √Ä direita, o modelo √© muito poderoso. Como temos apenas 5 pontos e o modelo tem 7 par√¢metros, ele pode se ajustar para passar por todos os pontos, fazendo o erro de treinamento ser zero. Por√©m, isso impede o modelo de entender o padr√£o correto dos dados, por isso o erro de valida√ß√£o √© muito alto.

√â muito importante encontrar um equil√≠brio correto entre a complexidade do modelo (n√∫mero de par√¢metros) e a quantidade de amostras de treinamento.

## Por que o overfitting ocorre

  * Dados de treinamento insuficientes
  * Modelo muito complexo
  * Muito ru√≠do nos dados de entrada

## Como detectar overfitting

Como voc√™ pode ver no gr√°fico acima, o overfitting pode ser detectado por um erro de treinamento muito baixo e um erro de valida√ß√£o alto. Normalmente, durante o treinamento, tanto o erro de treinamento quanto o de valida√ß√£o come√ßam a diminuir, mas em algum momento o erro de valida√ß√£o pode parar de diminuir e come√ßar a aumentar. Isso √© um sinal de overfitting e indica que provavelmente devemos parar o treinamento nesse ponto (ou pelo menos salvar uma c√≥pia do modelo).

overfitting

## Como prevenir overfitting

Se voc√™ perceber que est√° ocorrendo overfitting, pode fazer uma das seguintes coisas:

 * Aumentar a quantidade de dados de treinamento
 * Diminuir a complexidade do modelo
 * Usar alguma t√©cnica de regulariza√ß√£o, como Dropout, que veremos mais adiante.

## Overfitting e o Tradeoff Vi√©s-Vari√¢ncia

Overfitting √©, na verdade, um caso de um problema mais geral em estat√≠stica chamado Tradeoff Vi√©s-Vari√¢ncia. Se considerarmos as poss√≠veis fontes de erro no nosso modelo, podemos identificar dois tipos:

* **Erros de vi√©s** s√£o causados pelo algoritmo n√£o conseguir capturar corretamente a rela√ß√£o entre os dados de treinamento. Isso pode acontecer porque o modelo n√£o √© poderoso o suficiente (**underfitting**).
* **Erros de vari√¢ncia** s√£o causados pelo modelo aproximar o ru√≠do dos dados de entrada em vez da rela√ß√£o significativa (**overfitting**).

Durante o treinamento, o erro de vi√©s diminui (√† medida que o modelo aprende a aproximar os dados) e o erro de vari√¢ncia aumenta. √â importante parar o treinamento ‚Äî seja manualmente (quando detectamos overfitting) ou automaticamente (introduzindo regulariza√ß√£o) ‚Äî para evitar o overfitting.

## Conclus√£o

Nesta aula, voc√™ aprendeu sobre as diferen√ßas entre as v√°rias APIs dos dois frameworks de IA mais populares, TensorFlow e PyTorch. Al√©m disso, aprendeu sobre um tema muito importante: overfitting.

## üöÄ Desafio

Nos notebooks que acompanham esta aula, voc√™ encontrar√° 'tarefas' no final; trabalhe nos notebooks e complete as tarefas.

## Revis√£o & Estudo Aut√¥nomo

Pesquise sobre os seguintes t√≥picos:

- TensorFlow
- PyTorch
- Overfitting

Pergunte a si mesmo:

- Qual a diferen√ßa entre TensorFlow e PyTorch?
- Qual a diferen√ßa entre overfitting e underfitting?

## Exerc√≠cio

Neste laborat√≥rio, voc√™ dever√° resolver dois problemas de classifica√ß√£o usando redes totalmente conectadas de camada √∫nica e m√∫ltiplas camadas, utilizando PyTorch ou TensorFlow.

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informa√ß√µes cr√≠ticas, recomenda-se tradu√ß√£o profissional humana. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.