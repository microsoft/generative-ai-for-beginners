<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e2861bbca91c0567ef32bc77fe054f9e",
  "translation_date": "2025-05-20T01:15:09+00:00",
  "source_file": "15-rag-and-vector-databases/README.md",
  "language_code": "br"
}
-->
# Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) e Bancos de Dados Vetoriais

Na li√ß√£o de aplicativos de busca, aprendemos brevemente como integrar seus pr√≥prios dados em Modelos de Linguagem Grande (LLMs). Nesta li√ß√£o, vamos nos aprofundar nos conceitos de fundamentar seus dados em sua aplica√ß√£o LLM, na mec√¢nica do processo e nos m√©todos para armazenar dados, incluindo embeddings e texto.

> **V√≠deo Em Breve**

## Introdu√ß√£o

Nesta li√ß√£o, abordaremos o seguinte:

- Uma introdu√ß√£o ao RAG, o que √© e por que √© usado em IA (intelig√™ncia artificial).

- Entender o que s√£o bancos de dados vetoriais e criar um para nossa aplica√ß√£o.

- Um exemplo pr√°tico de como integrar RAG em uma aplica√ß√£o.

## Objetivos de Aprendizagem

Ap√≥s completar esta li√ß√£o, voc√™ ser√° capaz de:

- Explicar a import√¢ncia do RAG na recupera√ß√£o e processamento de dados.

- Configurar a aplica√ß√£o RAG e fundamentar seus dados em um LLM

- Integra√ß√£o eficaz de RAG e Bancos de Dados Vetoriais em Aplica√ß√µes LLM.

## Nosso Cen√°rio: aprimorando nossos LLMs com nossos pr√≥prios dados

Para esta li√ß√£o, queremos adicionar nossas pr√≥prias notas na startup de educa√ß√£o, o que permite que o chatbot obtenha mais informa√ß√µes sobre os diferentes assuntos. Usando as notas que temos, os alunos poder√£o estudar melhor e entender os diferentes t√≥picos, facilitando a revis√£o para seus exames. Para criar nosso cen√°rio, usaremos:

- `Azure OpenAI:` o LLM que usaremos para criar nosso chatbot

- `AI for beginners' lesson on Neural Networks`: isso ser√° o dado que fundamentaremos nosso LLM

- `Azure AI Search` e `Azure Cosmos DB:` banco de dados vetorial para armazenar nossos dados e criar um √≠ndice de busca

Os usu√°rios poder√£o criar quizzes de pr√°tica a partir de suas notas, cart√µes de revis√£o e resumir em vis√µes gerais concisas. Para come√ßar, vejamos o que √© RAG e como funciona:

## Gera√ß√£o Aumentada por Recupera√ß√£o (RAG)

Um chatbot alimentado por LLM processa prompts do usu√°rio para gerar respostas. Ele √© projetado para ser interativo e engaja os usu√°rios em uma ampla gama de t√≥picos. No entanto, suas respostas s√£o limitadas ao contexto fornecido e aos dados de treinamento fundamentais. Por exemplo, o corte de conhecimento do GPT-4 √© setembro de 2021, ou seja, ele n√£o tem conhecimento de eventos que ocorreram ap√≥s esse per√≠odo. Al√©m disso, os dados usados para treinar LLMs excluem informa√ß√µes confidenciais, como notas pessoais ou o manual de produtos de uma empresa.

### Como funcionam os RAGs (Gera√ß√£o Aumentada por Recupera√ß√£o)

Suponha que voc√™ queira implantar um chatbot que cria quizzes a partir de suas notas, voc√™ precisar√° de uma conex√£o com a base de conhecimento. √â a√≠ que o RAG entra em cena. Os RAGs operam da seguinte forma:

- **Base de conhecimento:** Antes da recupera√ß√£o, esses documentos precisam ser ingeridos e pr√©-processados, normalmente dividindo documentos grandes em partes menores, transformando-os em embeddings de texto e armazenando-os em um banco de dados.

- **Consulta do usu√°rio:** o usu√°rio faz uma pergunta

- **Recupera√ß√£o:** Quando um usu√°rio faz uma pergunta, o modelo de embedding recupera informa√ß√µes relevantes de nossa base de conhecimento para fornecer mais contexto que ser√° incorporado ao prompt.

- **Gera√ß√£o Aumentada:** o LLM aprimora sua resposta com base nos dados recuperados. Isso permite que a resposta gerada n√£o seja apenas baseada em dados pr√©-treinados, mas tamb√©m em informa√ß√µes relevantes do contexto adicionado. Os dados recuperados s√£o usados para aumentar as respostas do LLM. O LLM ent√£o retorna uma resposta √† pergunta do usu√°rio.

A arquitetura dos RAGs √© implementada usando transformadores que consistem em duas partes: um codificador e um decodificador. Por exemplo, quando um usu√°rio faz uma pergunta, o texto de entrada √© 'codificado' em vetores capturando o significado das palavras e os vetores s√£o 'decodificados' em nosso √≠ndice de documentos e geram novo texto com base na consulta do usu√°rio. O LLM usa tanto um modelo codificador-decodificador para gerar a sa√≠da.

Duas abordagens ao implementar RAG de acordo com o artigo proposto: [Gera√ß√£o Aumentada por Recupera√ß√£o para Tarefas de NLP (processamento de linguagem natural) intensivas em conhecimento](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) s√£o:

- **_RAG-Sequence_** usando documentos recuperados para prever a melhor resposta poss√≠vel para uma consulta do usu√°rio

- **RAG-Token** usando documentos para gerar o pr√≥ximo token, depois recuper√°-los para responder √† consulta do usu√°rio

### Por que voc√™ usaria RAGs?

- **Riqueza de informa√ß√£o:** garante que as respostas de texto estejam atualizadas e atuais. Portanto, melhora o desempenho em tarefas espec√≠ficas de dom√≠nio ao acessar a base de conhecimento interna.

- Reduz a fabrica√ß√£o utilizando **dados verific√°veis** na base de conhecimento para fornecer contexto √†s consultas do usu√°rio.

- √â **custo-efetivo** pois s√£o mais econ√¥micos em compara√ß√£o com o ajuste fino de um LLM

## Criando uma base de conhecimento

Nossa aplica√ß√£o √© baseada em nossos dados pessoais, ou seja, a li√ß√£o de Redes Neurais no curr√≠culo de IA Para Iniciantes.

### Bancos de Dados Vetoriais

Um banco de dados vetorial, ao contr√°rio dos bancos de dados tradicionais, √© um banco de dados especializado projetado para armazenar, gerenciar e buscar vetores incorporados. Ele armazena representa√ß√µes num√©ricas de documentos. Dividir os dados em embeddings num√©ricos facilita para nosso sistema de IA entender e processar os dados.

Armazenamos nossos embeddings em bancos de dados vetoriais, pois os LLMs t√™m um limite do n√∫mero de tokens que aceitam como entrada. Como voc√™ n√£o pode passar todos os embeddings para um LLM, precisaremos dividi-los em partes e, quando um usu√°rio fizer uma pergunta, os embeddings mais semelhantes √† pergunta ser√£o retornados juntamente com o prompt. A divis√£o tamb√©m reduz os custos no n√∫mero de tokens passados por um LLM.

Alguns bancos de dados vetoriais populares incluem Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant e DeepLake. Voc√™ pode criar um modelo Azure Cosmos DB usando Azure CLI com o seguinte comando:

Antes de armazenarmos nossos dados, precisaremos convert√™-los em embeddings vetoriais antes de serem armazenados no banco de dados. Se voc√™ estiver trabalhando com documentos grandes ou textos longos, pode dividi-los com base nas consultas que espera. A divis√£o pode ser feita no n√≠vel da frase ou no n√≠vel do par√°grafo. Como a divis√£o deriva significados das palavras ao seu redor, voc√™ pode adicionar algum outro contexto a uma parte, por exemplo, adicionando o t√≠tulo do documento ou incluindo algum texto antes ou depois da parte. Voc√™ pode dividir os dados da seguinte forma:

Uma vez divididos, podemos ent√£o incorporar nosso texto usando diferentes modelos de embedding. Alguns modelos que voc√™ pode usar incluem: word2vec, ada-002 da OpenAI, Azure Computer Vision e muitos mais. Selecionar um modelo para usar depender√° das linguagens que voc√™ est√° usando, do tipo de conte√∫do codificado (texto/imagens/√°udio), do tamanho da entrada que pode codificar e do comprimento da sa√≠da de embedding.

Um exemplo de texto incorporado usando o modelo `text-embedding-ada-002` da OpenAI √©:

## Recupera√ß√£o e Busca Vetorial

Quando um usu√°rio faz uma pergunta, o recuperador a transforma em um vetor usando o codificador de consulta, ele ent√£o busca em nosso √≠ndice de busca de documentos por vetores relevantes no documento que est√£o relacionados √† entrada. Uma vez feito, ele converte tanto o vetor de entrada quanto os vetores de documentos em texto e passa pelo LLM.

### Recupera√ß√£o

A recupera√ß√£o acontece quando o sistema tenta encontrar rapidamente os documentos do √≠ndice que satisfazem os crit√©rios de busca. O objetivo do recuperador √© obter documentos que ser√£o usados para fornecer contexto e fundamentar o LLM em seus dados.

Existem v√°rias maneiras de realizar buscas dentro de nosso banco de dados, como:

- **Busca por palavra-chave** - usada para buscas de texto

- **Busca sem√¢ntica** - usa o significado sem√¢ntico das palavras

- **Busca vetorial** - converte documentos de texto para representa√ß√µes vetoriais usando modelos de embedding. A recupera√ß√£o ser√° feita consultando os documentos cujas representa√ß√µes vetoriais est√£o mais pr√≥ximas da pergunta do usu√°rio.

- **H√≠brida** - uma combina√ß√£o de busca por palavra-chave e busca vetorial.

Um desafio com a recupera√ß√£o surge quando n√£o h√° resposta semelhante √† consulta no banco de dados, o sistema ent√£o retornar√° a melhor informa√ß√£o que puder obter, no entanto, voc√™ pode usar t√°ticas como definir a dist√¢ncia m√°xima para relev√¢ncia ou usar busca h√≠brida que combina palavras-chave e busca vetorial. Nesta li√ß√£o, usaremos busca h√≠brida, uma combina√ß√£o de busca vetorial e por palavra-chave. Armazenaremos nossos dados em um dataframe com colunas contendo as partes, bem como embeddings.

### Similaridade Vetorial

O recuperador buscar√° na base de dados de conhecimento por embeddings que est√£o pr√≥ximos, o vizinho mais pr√≥ximo, pois s√£o textos que s√£o semelhantes. No cen√°rio em que um usu√°rio faz uma consulta, ela √© primeiro incorporada e depois combinada com embeddings semelhantes. A medida comum usada para encontrar a semelhan√ßa entre diferentes vetores √© a similaridade cosseno, que se baseia no √¢ngulo entre dois vetores.

Podemos medir a similaridade usando outras alternativas que podemos usar s√£o a dist√¢ncia Euclidiana, que √© a linha reta entre os pontos finais do vetor, e o produto escalar, que mede a soma dos produtos dos elementos correspondentes de dois vetores.

### √çndice de Busca

Ao fazer a recupera√ß√£o, precisaremos construir um √≠ndice de busca para nossa base de conhecimento antes de realizarmos a busca. Um √≠ndice armazenar√° nossos embeddings e poder√° recuperar rapidamente as partes mais semelhantes, mesmo em um banco de dados grande. Podemos criar nosso √≠ndice localmente usando:

### Re-ranqueamento

Uma vez que voc√™ consultou o banco de dados, pode precisar classificar os resultados dos mais relevantes. Um LLM de re-ranqueamento utiliza Machine Learning para melhorar a relev√¢ncia dos resultados de busca, ordenando-os dos mais relevantes. Usando o Azure AI Search, o re-ranqueamento √© feito automaticamente para voc√™ usando um re-ranqueador sem√¢ntico. Um exemplo de como o re-ranqueamento funciona usando vizinhos mais pr√≥ximos:

## Unindo tudo

O √∫ltimo passo √© adicionar nosso LLM √† mistura para poder obter respostas que s√£o fundamentadas em nossos dados. Podemos implement√°-lo da seguinte forma:

## Avaliando nossa aplica√ß√£o

### M√©tricas de Avalia√ß√£o

- Qualidade das respostas fornecidas garantindo que soem naturais, fluentes e semelhantes a humanos

- Fundamenta√ß√£o dos dados: avaliando se a resposta veio dos documentos fornecidos

- Relev√¢ncia: avaliando se a resposta corresponde e est√° relacionada √† pergunta feita

- Flu√™ncia - se a resposta faz sentido gramaticalmente

## Casos de Uso para usar RAG (Gera√ß√£o Aumentada por Recupera√ß√£o) e bancos de dados vetoriais

Existem muitos casos de uso diferentes onde chamadas de fun√ß√£o podem melhorar seu aplicativo, como:

- Perguntas e Respostas: fundamentando os dados da sua empresa para um chat que pode ser usado por funcion√°rios para fazer perguntas.

- Sistemas de Recomenda√ß√£o: onde voc√™ pode criar um sistema que combina os valores mais semelhantes, por exemplo, filmes, restaurantes e muitos mais.

- Servi√ßos de Chatbot: voc√™ pode armazenar o hist√≥rico de chat e personalizar a conversa com base nos dados do usu√°rio.

- Busca de imagens baseada em embeddings vetoriais, √∫til ao fazer reconhecimento de imagens e detec√ß√£o de anomalias.

## Resumo

Cobrimos as √°reas fundamentais do RAG desde adicionar nossos dados √† aplica√ß√£o, a consulta do usu√°rio e a sa√≠da. Para simplificar a cria√ß√£o do RAG, voc√™ pode usar frameworks como Semanti Kernel, Langchain ou Autogen.

## Tarefa

Para continuar seu aprendizado sobre Gera√ß√£o Aumentada por Recupera√ß√£o (RAG), voc√™ pode construir:

- Construir um front-end para a aplica√ß√£o usando o framework de sua escolha

- Utilizar um framework, seja LangChain ou Semantic Kernel, e recriar sua aplica√ß√£o.

Parab√©ns por completar a li√ß√£o üëè.

## O aprendizado n√£o para por aqui, continue a Jornada

Ap√≥s completar esta li√ß√£o, confira nossa [cole√ß√£o de Aprendizado de IA Generativa](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) para continuar aprimorando seu conhecimento em IA Generativa!

**Aviso Legal**:  
Este documento foi traduzido usando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional humana. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.