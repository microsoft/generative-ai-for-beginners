{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Tatalakayin sa araling ito: \n",
    "- Ano ang pagtawag ng function at ang mga gamit nito \n",
    "- Paano gumawa ng pagtawag ng function gamit ang OpenAI \n",
    "- Paano isama ang pagtawag ng function sa isang aplikasyon \n",
    "\n",
    "## Learning Goals \n",
    "\n",
    "Pagkatapos makumpleto ang araling ito, malalaman mo kung paano at mauunawaan ang: \n",
    "\n",
    "- Layunin ng paggamit ng pagtawag ng function \n",
    "- Pagsasaayos ng Function Call gamit ang OpenAI Service \n",
    "- Paggawa ng epektibong pagtawag ng function para sa gamit ng iyong aplikasyon \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pag-unawa sa Pagtawag ng Function\n",
    "\n",
    "Para sa araling ito, nais nating bumuo ng isang tampok para sa aming education startup na nagpapahintulot sa mga gumagamit na gumamit ng chatbot upang makahanap ng mga teknikal na kurso. Magrerekomenda kami ng mga kurso na angkop sa kanilang antas ng kasanayan, kasalukuyang tungkulin, at teknolohiyang interesado sila.\n",
    "\n",
    "Upang makumpleto ito gagamitin natin ang kombinasyon ng:\n",
    " - `OpenAI` upang lumikha ng karanasan sa chat para sa gumagamit\n",
    " - `Microsoft Learn Catalog API` upang tulungan ang mga gumagamit na makahanap ng mga kurso base sa kahilingan ng gumagamit\n",
    " - `Function Calling` upang kunin ang query ng gumagamit at ipadala ito sa isang function upang gawin ang API request.\n",
    "\n",
    "Upang magsimula, tingnan natin kung bakit nais nating gamitin ang function calling sa unang pagkakataon:\n",
    "\n",
    "print(\"Mga mensahe sa susunod na request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # kumuha ng bagong tugon mula sa GPT kung saan makikita nito ang tugon ng function\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bakit Function Calling\n",
    "\n",
    "Kung nakatapos ka na ng anumang ibang aralin sa kursong ito, malamang na nauunawaan mo ang kapangyarihan ng paggamit ng Large Language Models (LLMs). Sana ay nakikita mo rin ang ilan sa kanilang mga limitasyon.\n",
    "\n",
    "Ang Function Calling ay isang tampok ng OpenAI Service na idinisenyo upang tugunan ang mga sumusunod na hamon:\n",
    "\n",
    "Hindi Konsistenteng Pormat ng Tugon:\n",
    "- Bago ang function calling, ang mga tugon mula sa isang malaking language model ay hindi nakaayos at hindi konsistente. Kailangan ng mga developer na magsulat ng kumplikadong validation code upang hawakan ang bawat pagkakaiba sa output.\n",
    "\n",
    "Limitadong Integrasyon sa Panlabas na Data:\n",
    "- Bago ang tampok na ito, mahirap isama ang data mula sa ibang bahagi ng isang aplikasyon sa konteksto ng chat.\n",
    "\n",
    "Sa pamamagitan ng pag-standardize ng mga pormat ng tugon at pagpapahintulot ng tuloy-tuloy na integrasyon sa panlabas na data, pinapasimple ng function calling ang pag-develop at binabawasan ang pangangailangan para sa karagdagang validation logic.\n",
    "\n",
    "Hindi makakuha ang mga user ng mga sagot tulad ng \"Ano ang kasalukuyang panahon sa Stockholm?\". Ito ay dahil limitado ang mga modelo sa oras kung kailan sila sinanay.\n",
    "\n",
    "Tingnan natin ang halimbawa sa ibaba na nagpapakita ng problemang ito:\n",
    "\n",
    "Sabihin nating gusto nating gumawa ng database ng datos ng mga estudyante upang makapag-suggest ng tamang kurso para sa kanila. Sa ibaba ay may dalawang paglalarawan ng mga estudyante na halos magkapareho ang datos na nilalaman.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_description=\"Emily Johnson is a sophomore majoring in computer science at Duke University. She has a 3.7 GPA. Emily is an active member of the university's Chess Club and Debate Team. She hopes to pursue a career in software engineering after graduating.\"\n",
    " \n",
    "student_2_description = \"Michael Lee is a sophomore majoring in computer science at Stanford University. He has a 3.8 GPA. Michael is known for his programming skills and is an active member of the university's Robotics Club. He hopes to pursue a career in artificial intelligence after finishing his studies.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nais naming ipadala ito sa isang LLM upang i-parse ang data. Maaari itong magamit sa aming aplikasyon upang ipadala ito sa isang API o itago sa isang database.\n",
    "\n",
    "Gumawa tayo ng dalawang magkaparehong prompt na magtuturo sa LLM kung anong impormasyon ang nais naming makuha:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gusto naming ipadala ito sa isang LLM upang suriin ang mga bahagi na mahalaga sa aming produkto. Kaya maaari kaming gumawa ng dalawang magkaparehong prompt upang utusan ang LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_1_description}\n",
    "'''\n",
    "\n",
    "\n",
    "prompt2 = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "major\n",
    "school\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_2_description}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pagkatapos likhain ang dalawang prompt na ito, ipapadala natin ang mga ito sa LLM gamit ang `openai.ChatCompletion`. Itinatago natin ang prompt sa variable na `messages` at itinalaga ang role bilang `user`. Ito ay upang gayahin ang isang mensahe mula sa isang user na isinusulat sa isang chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "deployment=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon maaari na nating ipadala ang parehong mga kahilingan sa LLM at suriin ang tugon na natanggap natin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response1 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt1}]\n",
    ")\n",
    "openai_response1.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_response2 = client.chat.completions.create(\n",
    " model=deployment,    \n",
    " messages = [{'role': 'user', 'content': prompt2}]\n",
    ")\n",
    "openai_response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response1 = json.loads(openai_response1.choices[0].message.content)\n",
    "json_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the response as a JSON object\n",
    "json_response2 = json.loads(openai_response2.choices[0].message.content )\n",
    "json_response2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kahit na pareho ang mga prompt at magkatulad ang mga paglalarawan, maaari tayong makakuha ng iba't ibang format ng property na `Grades`.\n",
    "\n",
    "Kung patakbuhin mo ang cell sa itaas nang maraming beses, ang format ay maaaring `3.7` o `3.7 GPA`.\n",
    "\n",
    "Ito ay dahil ang LLM ay kumukuha ng hindi nakaayos na data mula sa anyo ng nakasulat na prompt at nagbabalik din ng hindi nakaayos na data. Kailangan nating magkaroon ng nakaayos na format upang malaman natin kung ano ang aasahan kapag iniimbak o ginagamit ang data na ito.\n",
    "\n",
    "Sa pamamagitan ng paggamit ng functional calling, masisiguro nating makakatanggap tayo ng nakaayos na data pabalik. Kapag gumagamit ng function calling, ang LLM ay hindi talaga tumatawag o nagpapatakbo ng anumang mga function. Sa halip, gumagawa tayo ng isang istruktura para sundan ng LLM sa mga tugon nito. Pagkatapos ay ginagamit natin ang mga nakaayos na tugon na iyon upang malaman kung anong function ang dapat patakbuhin sa ating mga aplikasyon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Daloy ng Tawag ng Function](../../../../translated_images/Function-Flow.083875364af4f4bb69bd6f6ed94096a836453183a71cf22388f50310ad6404de.tl.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maaari nating kunin ang ibinalik mula sa function at ipadala ito pabalik sa LLM. Ang LLM ay sasagot gamit ang natural na wika upang tugunan ang tanong ng gumagamit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mga Gamit ng paggamit ng function calls\n",
    "\n",
    "**Pagtawag ng Panlabas na Mga Kasangkapan**  \n",
    "Magaling ang mga chatbot sa pagbibigay ng mga sagot sa mga tanong mula sa mga gumagamit. Sa pamamagitan ng paggamit ng function calling, maaaring gamitin ng mga chatbot ang mga mensahe mula sa mga gumagamit upang tapusin ang ilang mga gawain. Halimbawa, maaaring hilingin ng isang estudyante sa chatbot na \"Magpadala ng email sa aking instruktor na nagsasabing kailangan ko ng karagdagang tulong sa paksang ito\". Maaari itong gumawa ng function call sa `send_email(to: string, body: string)`\n",
    "\n",
    "**Gumawa ng API o Database Queries**  \n",
    "Maaaring maghanap ang mga gumagamit ng impormasyon gamit ang natural na wika na kino-convert sa isang naka-format na query o API request. Isang halimbawa nito ay ang isang guro na nagtatanong \"Sino ang mga estudyanteng nakatapos ng huling takdang-aralin\" na maaaring tumawag ng function na pinangalanang `get_completed(student_name: string, assignment: int, current_status: string)`\n",
    "\n",
    "**Paglikha ng Istrakturadong Data**  \n",
    "Maaaring kunin ng mga gumagamit ang isang bloke ng teksto o CSV at gamitin ang LLM upang kunin ang mahahalagang impormasyon mula rito. Halimbawa, maaaring i-convert ng isang estudyante ang isang artikulo sa Wikipedia tungkol sa mga kasunduan sa kapayapaan upang gumawa ng mga AI flash cards. Magagawa ito sa pamamagitan ng paggamit ng function na tinatawag na `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Paglikha ng Iyong Unang Tawag sa Function\n",
    "\n",
    "Ang proseso ng paglikha ng tawag sa function ay may 3 pangunahing hakbang:\n",
    "1. Tawagan ang Chat Completions API gamit ang listahan ng iyong mga function at isang mensahe mula sa user\n",
    "2. Basahin ang tugon ng modelo upang magsagawa ng aksyon tulad ng pagpapatupad ng isang function o API Call\n",
    "3. Gumawa ng isa pang tawag sa Chat Completions API gamit ang tugon mula sa iyong function upang gamitin ang impormasyong iyon sa paggawa ng tugon para sa user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Daloy ng Tawag ng Function](../../../../translated_images/LLM-Flow.3285ed8caf4796d7343c02927f52c9d32df59e790f6e440568e2e951f6ffa5fd.tl.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mga Elemento ng isang pagtawag ng function\n",
    "\n",
    "#### Input ng mga Gumagamit\n",
    "\n",
    "Ang unang hakbang ay gumawa ng mensahe ng gumagamit. Maaari itong itakda nang dinamiko sa pamamagitan ng pagkuha ng halaga mula sa isang text input o maaari kang magtalaga ng halaga dito. Kung ito ang iyong unang beses na gumamit ng Chat Completions API, kailangan nating tukuyin ang `role` at ang `content` ng mensahe.\n",
    "\n",
    "Ang `role` ay maaaring `system` (gumagawa ng mga patakaran), `assistant` (ang modelo) o `user` (ang end-user). Para sa pagtawag ng function, itatalaga natin ito bilang `user` at isang halimbawa ng tanong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [ {\"role\": \"user\", \"content\": \"Find me a good course for a beginner student to learn Azure.\"} ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paglikha ng mga function.\n",
    "\n",
    "Susunod ay magde-define tayo ng isang function at ang mga parameter ng function na iyon. Gagamitin lang natin ang isang function dito na tinatawag na `search_courses` ngunit maaari kang gumawa ng maraming function.\n",
    "\n",
    "**Mahalaga** : Ang mga function ay kasama sa system message sa LLM at isasama sa dami ng mga token na mayroon kang magagamit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "   {\n",
    "      \"name\":\"search_courses\",\n",
    "      \"description\":\"Retrieves courses from the search index based on the parameters provided\",\n",
    "      \"parameters\":{\n",
    "         \"type\":\"object\",\n",
    "         \"properties\":{\n",
    "            \"role\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The role of the learner (i.e. developer, data scientist, student, etc.)\"\n",
    "            },\n",
    "            \"product\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The product that the lesson is covering (i.e. Azure, Power BI, etc.)\"\n",
    "            },\n",
    "            \"level\":{\n",
    "               \"type\":\"string\",\n",
    "               \"description\":\"The level of experience the learner has prior to taking the course (i.e. beginner, intermediate, advanced)\"\n",
    "            }\n",
    "         },\n",
    "         \"required\":[\n",
    "            \"role\"\n",
    "         ]\n",
    "      }\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mga Kahulugan** \n",
    "\n",
    "Ang istruktura ng depinisyon ng function ay may maraming antas, bawat isa ay may sariling mga katangian. Narito ang isang paghahati-hati ng naka-nest na istruktura:\n",
    "\n",
    "**Mga Katangian ng Pangunahing Antas ng Function:**\n",
    "\n",
    "`name` -  Ang pangalan ng function na nais nating tawagin. \n",
    "\n",
    "`description` - Ito ang paglalarawan kung paano gumagana ang function. Mahalaga dito na maging tiyak at malinaw. \n",
    "\n",
    "`parameters` - Isang listahan ng mga halaga at format na nais mong iproduce ng modelo sa kanyang tugon. \n",
    "\n",
    "**Mga Katangian ng Parameters Object:**\n",
    "\n",
    "`type` -  Ang uri ng data ng parameters object (karaniwang \"object\")\n",
    "\n",
    "`properties` - Listahan ng mga tiyak na halaga na gagamitin ng modelo para sa kanyang tugon. \n",
    "\n",
    "**Mga Indibidwal na Katangian ng Parameter:**\n",
    "\n",
    "`name` - Implicit na tinukoy ng property key (halimbawa, \"role\", \"product\", \"level\")\n",
    "\n",
    "`type` - Ang uri ng data ng partikular na parameter na ito (halimbawa, \"string\", \"number\", \"boolean\") \n",
    "\n",
    "`description` - Paglalarawan ng partikular na parameter. \n",
    "\n",
    "**Opsyonal na Mga Katangian:**\n",
    "\n",
    "`required` - Isang array na naglilista kung aling mga parameter ang kinakailangan upang makumpleto ang pagtawag sa function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paggawa ng tawag sa function  \n",
    "Pagkatapos idefine ang isang function, kailangan na natin itong isama sa tawag sa Chat Completion API. Ginagawa natin ito sa pamamagitan ng pagdagdag ng `functions` sa request. Sa kasong ito `functions=functions`. \n",
    "\n",
    "Mayroon ding opsyon na itakda ang `function_call` sa `auto`. Ibig sabihin nito ay hahayaan natin ang LLM na magdesisyon kung aling function ang dapat tawagin base sa mensahe ng user sa halip na tayo ang mag-assign nito.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(model=deployment, \n",
    "                                        messages=messages,\n",
    "                                        functions=functions, \n",
    "                                        function_call=\"auto\") \n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon tingnan natin ang tugon at kung paano ito nakaayos:\n",
    "\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"function_call\": {\n",
    "    \"name\": \"search_courses\",\n",
    "    \"arguments\": \"{\\n  \\\"role\\\": \\\"student\\\",\\n  \\\"product\\\": \\\"Azure\\\",\\n  \\\"level\\\": \\\"beginner\\\"\\n}\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Makikita mo na ang pangalan ng function ay tinawag at mula sa mensahe ng user, nagawa ng LLM na hanapin ang datos upang umangkop sa mga argumento ng function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Pagsasama ng Mga Tawag sa Function sa Isang Aplikasyon. \n",
    "\n",
    "\n",
    "Pagkatapos nating subukan ang na-format na tugon mula sa LLM, maaari na nating isama ito sa isang aplikasyon. \n",
    "\n",
    "### Pamamahala ng daloy \n",
    "\n",
    "Upang maisama ito sa ating aplikasyon, gawin natin ang mga sumusunod na hakbang: \n",
    "\n",
    "Una, gawin natin ang tawag sa mga serbisyo ng OpenAI at itago ang mensahe sa isang variable na tinatawag na `response_message`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon ay ide-define natin ang function na tatawag sa Microsoft Learn API upang makakuha ng listahan ng mga kurso:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_courses(role, product, level):\n",
    "    url = \"https://learn.microsoft.com/api/catalog/\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"product\": product,\n",
    "        \"level\": level\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    modules = response.json()[\"modules\"]\n",
    "    results = []\n",
    "    for module in modules[:5]:\n",
    "        title = module[\"title\"]\n",
    "        url = module[\"url\"]\n",
    "        results.append({\"title\": title, \"url\": url})\n",
    "    return str(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilang pinakamahusay na kasanayan, titingnan muna natin kung nais ng modelo na tumawag ng isang function. Pagkatapos nito, gagawa tayo ng isa sa mga available na function at itutugma ito sa function na tinatawag.  \n",
    "Pagkatapos, kukunin natin ang mga argumento ng function at itutugma ang mga ito sa mga argumento mula sa LLM.\n",
    "\n",
    "Sa huli, idaragdag natin ang mensahe ng pagtawag ng function at ang mga halagang ibinalik ng `search_courses` na mensahe. Ito ay nagbibigay sa LLM ng lahat ng impormasyong kailangan nito upang\n",
    "tumugon sa user gamit ang natural na wika.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call a function\n",
    "if response_message.function_call.name:\n",
    "    print(\"Recommended Function call:\")\n",
    "    print(response_message.function_call.name)\n",
    "    print()\n",
    "\n",
    "    # Call the function. \n",
    "    function_name = response_message.function_call.name\n",
    "\n",
    "    available_functions = {\n",
    "            \"search_courses\": search_courses,\n",
    "    }\n",
    "    function_to_call = available_functions[function_name] \n",
    "\n",
    "    function_args = json.loads(response_message.function_call.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    print(\"Output of function call:\")\n",
    "    print(function_response)\n",
    "    print(type(function_response))\n",
    "\n",
    "\n",
    "    # Add the assistant response and function response to the messages\n",
    "    messages.append( # adding assistant response to messages\n",
    "        {\n",
    "            \"role\": response_message.role,\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": response_message.function_call.arguments,\n",
    "            },\n",
    "            \"content\": None\n",
    "        }\n",
    "    )\n",
    "    messages.append( # adding function response to messages\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\":function_response,\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngayon ay ipapadala natin ang na-update na mensahe sa LLM upang makatanggap tayo ng tugon sa natural na wika sa halip na tugon na naka-format sa API JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages in next request:\")\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "second_response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=deployment,\n",
    "    function_call=\"auto\",\n",
    "    functions=functions,\n",
    "    temperature=0\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge \n",
    "\n",
    "Magaling na trabaho! Upang ipagpatuloy ang iyong pag-aaral ng OpenAI Function Calling maaari kang gumawa: https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst \n",
    " - Higit pang mga parameter ng function na maaaring makatulong sa mga nag-aaral na makahanap ng mas maraming kurso. Maaari mong makita ang mga available na API parameter dito: \n",
    " - Gumawa ng isa pang function call na kumukuha ng higit pang impormasyon mula sa nag-aaral tulad ng kanilang katutubong wika \n",
    " - Gumawa ng error handling kapag ang function call at/o API call ay hindi nagbalik ng anumang angkop na kurso \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Paalala**:\nAng dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagamat nagsusumikap kami para sa katumpakan, pakatandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o di-tumpak na impormasyon. Ang orihinal na dokumento sa orihinal nitong wika ang dapat ituring na pangunahing sanggunian. Para sa mahahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na maaaring magmula sa paggamit ng pagsasaling ito.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "coopTranslator": {
   "original_hash": "5c4a2a2529177c571be9a5a371d7242b",
   "translation_date": "2025-12-19T11:09:30+00:00",
   "source_file": "11-integrating-with-function-calling/python/oai-assignment.ipynb",
   "language_code": "tl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}