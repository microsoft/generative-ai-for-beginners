{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Kabanata 7: Pagbuo ng Chat Applications\n",
    "## Mabilisang Pagsisimula sa Github Models API\n",
    "\n",
    "Ang notebook na ito ay inangkop mula sa [Azure OpenAI Samples Repository](https://github.com/Azure/azure-openai-samples?WT.mc_id=academic-105485-koreyst) na naglalaman ng mga notebook na kumokonekta sa mga serbisyo ng [Azure OpenAI](notebook-azure-openai.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Pangkalahatang-ideya  \n",
    "\"Ang malalaking language model ay mga function na nagmamapa ng teksto sa teksto. Kapag binigyan ng input na string ng teksto, sinusubukan ng isang malaking language model na hulaan ang susunod na teksto\"(1). Ang \"quickstart\" notebook na ito ay magpapakilala sa mga user sa mga pangunahing konsepto ng LLM, mga pangunahing kinakailangan ng package para makapagsimula sa AML, isang maikling pagpapakilala sa prompt design, at ilang maiikling halimbawa ng iba't ibang gamit.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Talaan ng Nilalaman  \n",
    "\n",
    "[Pangkalahatang-ideya](../../../../07-building-chat-applications/python)  \n",
    "[Paano gamitin ang OpenAI Service](../../../../07-building-chat-applications/python)  \n",
    "[1. Paglikha ng iyong OpenAI Service](../../../../07-building-chat-applications/python)  \n",
    "[2. Instalasyon](../../../../07-building-chat-applications/python)    \n",
    "[3. Mga Kredensyal](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Mga Gamit](../../../../07-building-chat-applications/python)    \n",
    "[1. Buodin ang Teksto](../../../../07-building-chat-applications/python)  \n",
    "[2. Iklasipika ang Teksto](../../../../07-building-chat-applications/python)  \n",
    "[3. Gumawa ng Bagong Pangalan ng Produkto](../../../../07-building-chat-applications/python)  \n",
    "[4. I-fine tune ang isang Classifier](../../../../07-building-chat-applications/python)  \n",
    "\n",
    "[Mga Sanggunian](../../../../07-building-chat-applications/python)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Gumawa ng iyong unang prompt  \n",
    "Ang maikling ehersisyong ito ay magbibigay ng pangunahing pagpapakilala sa pagsusumite ng mga prompt sa isang modelo sa Github Models para sa simpleng gawain na \"summarization\".\n",
    "\n",
    "**Mga Hakbang**:  \n",
    "1. I-install ang `azure-ai-inference` na library sa iyong python environment, kung hindi mo pa ito nagagawa.  \n",
    "2. I-load ang mga karaniwang helper na library at i-set up ang Github Models credential.  \n",
    "3. Pumili ng modelo para sa iyong gawain  \n",
    "4. Gumawa ng simpleng prompt para sa modelo  \n",
    "5. I-submit ang iyong request sa model API!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 1. I-install ang `azure-ai-inference`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674254990318
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674829434433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 3. Paghanap ng tamang modelo  \n",
    "Ang mga modelong GPT-3.5-turbo o GPT-4 ay kayang umunawa at gumawa ng natural na wika.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674742720788
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the General Purpose curie model for text\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 4. Disenyo ng Prompt  \n",
    "\n",
    "\"Ang kakaibang kakayahan ng malalaking language model ay dahil sa pagsasanay nila na bawasan ang prediction error gamit ang napakaraming teksto, natututo ang mga model ng mga konseptong mahalaga para sa mga prediksyon. Halimbawa, natutunan nila ang mga konsepto tulad ng\"(1):\n",
    "\n",
    "* paano magbaybay\n",
    "* paano gumagana ang gramatika\n",
    "* paano mag-paraphrase\n",
    "* paano sumagot ng mga tanong\n",
    "* paano makipag-usap\n",
    "* paano magsulat sa iba’t ibang wika\n",
    "* paano mag-code\n",
    "* at iba pa.\n",
    "\n",
    "#### Paano kontrolin ang isang malaking language model  \n",
    "\"Sa lahat ng input sa isang malaking language model, ang pinaka-mahalaga ay ang text prompt(1).\n",
    "\n",
    "Maaaring i-prompt ang malalaking language model para maglabas ng output sa ilang paraan:\n",
    "\n",
    "Instruksyon: Sabihin sa model kung ano ang gusto mo\n",
    "Completion: Hikayatin ang model na tapusin ang simula ng gusto mo\n",
    "Demonstrasyon: Ipakita sa model kung ano ang gusto mo, gamit ang alinman sa:\n",
    "Ilang halimbawa sa prompt\n",
    "Maraming daan o libong halimbawa sa fine-tuning training dataset\"\n",
    "\n",
    "\n",
    "\n",
    "#### May tatlong pangunahing gabay sa paggawa ng prompt:\n",
    "\n",
    "**Ipakita at sabihin.** Gawing malinaw kung ano ang gusto mo, sa pamamagitan ng instruksyon, mga halimbawa, o kombinasyon ng dalawa. Kung gusto mong i-rank ng model ang listahan ng mga bagay ayon sa alpabetikong ayos o i-classify ang isang talata base sa damdamin, ipakita sa kanya na iyon ang gusto mo.\n",
    "\n",
    "**Magbigay ng de-kalidad na data.** Kung sinusubukan mong gumawa ng classifier o gusto mong sundan ng model ang isang pattern, siguraduhing sapat ang mga halimbawa. Siguraduhing i-proofread ang mga halimbawa mo — kadalasan, matalino ang model para makita ang mga simpleng maling baybay at magbigay ng sagot, pero maaari rin nitong isipin na sinadya ito at makaapekto sa sagot.\n",
    "\n",
    "**Suriin ang iyong settings.** Ang temperature at top_p settings ang nagkokontrol kung gaano ka-deterministic ang model sa paggawa ng sagot. Kung sagot na may iisang tamang sagot ang hinihingi mo, mas mababa ang dapat na settings. Kung gusto mo ng mas iba-ibang sagot, mas mataas ang settings. Ang pinaka-karaniwang pagkakamali ng mga tao sa settings na ito ay ang pag-aakalang ito ay \"cleverness\" o \"creativity\" controls.\n",
    "\n",
    "\n",
    "Source: https://learn.microsoft.com/azure/ai-services/openai/overview\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494935186
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create your first prompt\n",
    "text_prompt = \"Should oxford commas always be used?\"\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674494940872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":text_prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Ibuod ang Teksto  \n",
    "#### Hamon  \n",
    "Ibuod ang teksto sa pamamagitan ng pagdagdag ng 'tl;dr:' sa dulo ng isang bahagi ng teksto. Pansinin kung paano nauunawaan ng modelo kung paano gawin ang iba’t ibang gawain kahit walang karagdagang tagubilin. Maaari kang mag-eksperimento gamit ang mas detalyadong mga prompt kaysa sa tl;dr para baguhin ang kilos ng modelo at iakma ang buod na matatanggap mo(3).  \n",
    "\n",
    "Ipinakita ng mga bagong pag-aaral na may malalaking pag-unlad sa maraming gawain at benchmark sa NLP kapag nag-pre-train gamit ang malaking koleksyon ng teksto at sinundan ng fine-tuning para sa isang partikular na gawain. Bagaman karaniwang hindi nakatuon sa isang gawain ang arkitektura, nangangailangan pa rin ang pamamaraang ito ng mga dataset para sa fine-tuning na may libo-libo o sampu-sampung libong halimbawa. Sa kabilang banda, ang mga tao ay kadalasang kayang gawin ang bagong gawain sa wika gamit lamang ang ilang halimbawa o simpleng tagubilin—isang bagay na hirap pa ring gawin ng mga kasalukuyang NLP system. Dito, ipinapakita namin na ang pagpapalaki ng language models ay malaki ang naitutulong sa task-agnostic at few-shot na performance, at minsan ay nakakasabay pa sa mga naunang pinakamahusay na fine-tuning na pamamaraan.  \n",
    "\n",
    "Tl;dr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Mga Ehersisyo para sa Iba't Ibang Gamit  \n",
    "1. Ibuod ang Teksto  \n",
    "2. I-uri ang Teksto  \n",
    "3. Gumawa ng Bagong Pangalan ng Produkto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495198534
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something that current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\\n\\nTl;dr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674495201868
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Iklasipika ang Teksto  \n",
    "#### Hamon  \n",
    "Iklasipika ang mga item sa mga kategoryang ibinibigay sa oras ng inference. Sa sumusunod na halimbawa, parehong ibinibigay ang mga kategorya at ang tekstong ikaklasipika sa prompt (*playground_reference).\n",
    "\n",
    "Tanong ng Customer: Hello, isa sa mga key ng keyboard ng laptop ko ay nasira kamakailan at kailangan ko ng kapalit:\n",
    "\n",
    "Naklasipikang kategorya:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499424645
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Classify the following inquiry into one of the following: categories: [Pricing, Hardware Support, Software Support]\\n\\ninquiry: Hello, one of the keys on my laptop keyboard broke recently and I'll need a replacement:\\n\\nClassified category:\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674499378518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt},])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Gumawa ng Mga Bagong Pangalan ng Produkto\n",
    "#### Hamon\n",
    "Gumawa ng mga pangalan ng produkto mula sa mga halimbawa ng salita. Dito, inilalagay natin sa prompt ang impormasyon tungkol sa produktong gagawan natin ng pangalan. Nagbibigay din tayo ng katulad na halimbawa para ipakita ang pattern na gusto nating makuha. Mataas din ang itinakdang temperature value para mas random at mas makabago ang mga sagot.\n",
    "\n",
    "Paglalarawan ng produkto: Isang home milkshake maker\n",
    "Mga salitang gabay: mabilis, malusog, compact.\n",
    "Mga pangalan ng produkto: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
    "\n",
    "Paglalarawan ng produkto: Isang pares ng sapatos na kasya sa kahit anong laki ng paa.\n",
    "Mga salitang gabay: nababagay, fit, omni-fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1674257087279
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Product description: A home milkshake maker\\nSeed words: fast, healthy, compact.\\nProduct names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n\\nProduct description: A pair of shoes that can fit any foot size.\\nSeed words: adaptable, fit, omni-fit.\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting a few additional, typical parameters during API Call\n",
    "\n",
    "response = client.complete(\n",
    "  model=model_name,\n",
    "  messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "               {\"role\":\"user\",\"content\":prompt}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Mga Sanggunian  \n",
    "- [Openai Cookbook](https://github.com/openai/openai-cookbook?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Mga Halimbawa sa OpenAI Studio](https://oai.azure.com/portal?WT.mc_id=academic-105485-koreyst)  \n",
    "- [Mga pinakamahusay na gawain para sa fine-tuning ng GPT-3 sa pag-uuri ng teksto](https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#?WT.mc_id=academic-105485-koreyst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Para sa Karagdagang Tulong  \n",
    "[OpenAI Commercialization Team](AzureOpenAITeam@microsoft.com)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Mga Nag-ambag\n",
    "* [Chew-Yean Yam](https://www.linkedin.com/in/cyyam/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Paunawa**:  \nAng dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagaman nagsusumikap kami para sa kawastuhan, pakatandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi eksaktong pagsasalin. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na opisyal na sanggunian. Para sa mahahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na maaaring lumitaw mula sa paggamit ng pagsasaling ito.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "coopTranslator": {
   "original_hash": "3eeb8e5cad61b52a8366f6259a53ed49",
   "translation_date": "2025-08-25T17:45:42+00:00",
   "source_file": "07-building-chat-applications/python/githubmodels-assignment.ipynb",
   "language_code": "tl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}